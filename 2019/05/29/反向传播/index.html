<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Swift">
    
    <title>
        
            反向传播 |
        
        Swift&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
        <link rel="shortcut icon" href="/images/huoyin.png">
    
    
<link rel="stylesheet" href="/font/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/font/css/regular.min.css">

    
<link rel="stylesheet" href="/font/css/solid.min.css">

    
<link rel="stylesheet" href="/font/css/brands.min.css">

    
    <script class="keep-theme-configurations">
    const KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.xml"}
    KEEP.theme_config = {"base_info":{"primary_color":"#0066cc","title":"Swift's Blog","author":"Swift","avatar":"/images/atom.svg","logo":"/images/atom.svg","favicon":"/images/huoyin.png"},"menu":{"Archives":"/archives","Tags":"/tags","Categories":"/categories","Links":"/links","About":"/about"},"first_screen":{"enable":true,"background_img":"/images/bg.svg","background_img_dark":"/images/bg.svg","description":"Keep writing and Keep loving!","hitokoto":false},"social_contact":{"enable":true,"links":{"github":"https://github.com/TransformersWsz","weixin":null,"qq":null,"weibo":null,"zhihu":null,"twitter":null,"facebook":null,"email":"antcoder@outlook.com"}},"scroll":{"progress_bar":true,"percent":false,"hide_header":true},"home":{"category":true,"tag":false,"announcement":null},"post":{"author_badge":{"enable":true,"level_badge":true,"custom_badge":["One","Two","Three"]},"word_count":{"wordcount":false,"min2read":false},"datetime_format":"YYYY-MM-DD HH:mm:ss","copyright_info":false,"share":true,"reward":{"enable":false,"img_link":null,"text":null}},"code_block":{"tools":{"enable":true,"style":"mac"},"highlight_theme":"obsidian"},"toc":{"enable":true,"number":true,"expand_all":true,"init_open":false,"layout":"left"},"website_count":{"busuanzi_count":{"enable":true,"site_uv":true,"site_pv":true,"page_pv":true}},"local_search":{"enable":true,"preload":true},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.21"},"waline":{"server_url":null,"reaction":false,"version":2},"giscus":{"repo":null,"repo_id":null,"category":"Announcements","category_id":null,"reactions_enabled":false},"artalk":{"server":null},"disqus":{"shortname":null}},"rss":{"enable":false},"lazyload":{"enable":false},"cdn":{"enable":false,"provider":"cdnjs"},"pjax":{"enable":false},"footer":{"since":2017,"word_count":false,"icp":{"enable":false,"record_code":null,"url":"https://beian.miit.gov.cn"},"site_deploy":{"enable":false,"provider":"github","url":null},"shields_style":{"enable":false,"custom":[{"link_url":null,"img_url":null}]}},"inject":{"enable":false,"css":["/css/custom.css"],"js":[null]},"root":"","links":[{"name":"heary","link":"https://heary.cn/","avatar":"https://heary.cn/images/avatar.jpg","description":"本科及研究生学长"},{"name":"知识就是力量","link":"https://github.com/tricktreat","avatar":"https://avatars.githubusercontent.com/u/25740077?v=4","description":"学长"},{"name":"myths","link":"https://blog.mythsman.com/","avatar":"https://blog.mythsman.com/content/images/2019/07/----_20190713220203.jpg","description":"丁神"},{"name":"mikito","link":"https://mikito.mythsman.com/","avatar":"https://mikito.mythsman.com/content/images/2019/07/2-1.jpg","description":"丁神女友"},{"name":"老猫轩仔","link":"https://www.agedcat.com/","avatar":"https://www.agedcat.com/wp-content/uploads/2021/03/1616508591-bg-47.jpg","description":"郭尔轩"},{"name":"韦阳","link":"https://godweiyang.com/","avatar":"https://godweiyang.com/medias/banner/4.jpg","description":"LightSeq作者"},{"name":"x-hansong","link":"https://blog.xiaohansong.com/index.html","avatar":"https://avatars.githubusercontent.com/u/5747697?v=4","description":"支付宝工程师"},{"name":"XPoet","link":"https://xpoet.cn/","avatar":"https://cdn.jsdelivr.net/gh/XPoet/image-hosting@master/common-use/avatar.jpg","description":"hexo-theme-keep和PicX的开发者"},{"name":"Molunerfinn","link":"https://molunerfinn.com/","avatar":"https://avatars.githubusercontent.com/u/12621342?v=4","description":"PicGo的开发者，WXG员工"},{"name":"Jerry Qu","link":"https://imququ.com/","avatar":"https://cdn.jsdelivr.net/gh/TransformersWsz/image_hosting@master/jerryqu.3yhp1au3nii0.webp","description":"前端开发大佬，现在转向偏管理"},{"name":"Qian Liu","link":"https://siviltaram.github.io/","avatar":"https://siviltaram.github.io/images/profile.png","description":"北航博士，NLP论文高产，热衷于知识分享"},{"name":"浩然","link":"https://ayanamirei1997.github.io/","avatar":"https://avatars.githubusercontent.com/u/31766871?v=4","description":"本科舍友及同学"},{"name":"海鸥","link":"https://hyrious.me/","avatar":"https://avatars.githubusercontent.com/u/8097890?v=4","description":"本科舍友及同学"},{"name":"李长顺","link":"https://zangailcs.github.io/","avatar":"https://transformerswsz.github.io/2019/09/19/picture%20bed/lcs.jpeg","description":"研究生同学"},{"name":"老石谈芯","link":"https://shilicon.com/","avatar":"https://i2.hdslb.com/bfs/face/b7d1228d4df6bcea8f1b9eb01545bb0b02a2aa65.jpg@240w_240h_1c_1s.jpg","description":"帝国理工博士、芯片工程师"},{"name":"LZY","link":"https://blog.luzy.top/","avatar":"https://avatars.githubusercontent.com/u/43703357?v=4","description":"东大本科生"},{"name":"wulc","link":"https://wulc.me/","avatar":"https://wulc.me/files/profile.jpg","description":"字节广告算法工程师"},{"name":"Lil","link":"https://lilianweng.github.io/","avatar":"https://pbs.twimg.com/profile_images/1052456981838086150/JcK3h5I1_400x400.jpg","description":"OpenAI工程师"},{"name":"Hugging Face","link":"https://huggingface.co/blog","avatar":"https://huggingface.co/front/assets/huggingface_logo-noborder.svg","description":"Hugging Face官方博客"},{"name":"小小将","link":"https://www.zhihu.com/people/xiaohuzc/posts","avatar":"https://pica.zhimg.com/v2-4c580ad38bc656abf65b1a7fb14d4573_xl.jpg?source=32738c0c","description":"知乎上的某位大佬"}],"version":"4.0.7"}
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"}
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"}
    KEEP.language_copy_copyright = {"copy":"Copy copyright info","copied":"Copied","title":"Original post title","author":"Original post author","link":"Original post link"}
  </script>
<meta name="generator" content="Hexo 5.4.1"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container border-box">

    <!-- home first screen  -->
    

    <!-- page content -->
    <div class="page-main-content border-box">
        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="border-box header-content">
        <div class="left border-box">
            
                <a class="logo-image border-box" href="/">
                    <img src="/images/atom.svg">
                </a>
            
            <a class="site-name border-box" href="/">
               Swift&#39;s Blog
            </a>
        </div>

        <div class="right border-box">
            <div class="pc">
                <ul class="menu-list">
                    <li class="menu-item">
                        <a class=""
                           href="/"
                        >HOME</a>
                    </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >ARCHIVES</a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/tags"
                            >TAGS</a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/categories"
                            >CATEGORIES</a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/links"
                            >LINKS</a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >ABOUT</a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas search fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            <li class="drawer-menu-item flex-center">
                <a class=""
                   href="/"
                >HOME</a>
            </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives"
                    >ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/tags"
                    >TAGS</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/categories"
                    >CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/links"
                    >LINKS</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about"
                    >ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle border-box">

            <div class="main-content border-box">
                

                    
<div class="fade-in-down-animation">
    <div class="post-page-container border-box">
        <div class="post-content-container border-box">
            

            <div class="post-content-bottom border-box">
                
                    <div class="post-title">
                        反向传播
                    </div>
                

                
                    <div class="post-header border-box">
                        
                            <div class="avatar-box border-box">
                                <img src="/images/atom.svg">
                            </div>
                        
                        <div class="info-box">
                            <div class="author border-box">
                                <span class="name">Swift</span>
                                
                                    <span class="author-badge">Lv6</span>
                                
                            </div>
                            <div class="meta-info border-box">
                                

<div class="post-meta-info-container border-box post">
    <div class="post-meta-info border-box">
        

        
            <span class="meta-info-item post-create-date">
                <i class="icon fa-solid fa-calendar-plus"></i>&nbsp;
                <span class="datetime">2019-05-29 21:56:21</span>
            </span>

            <span class="meta-info-item post-update-date">
                <i class="icon fa-solid fa-file-pen"></i>&nbsp;
                <span class="datetime" data-updated="Mon Sep 18 2023 17:13:11 GMT+0000">2023-09-18 17:13:11</span>
            </span>
        

        
            <span class="meta-info-item post-category border-box"><i class="icon fas fa-folder"></i>&nbsp;
                <ul class="post-category-ul">
                    
                            <li class="category-item"><a href="/categories/Machine-Learning/">Machine Learning</a></li>
                        
                    
                </ul>
            </span>
        

        
            <span class="post-tag meta-info-item border-box">
                <i class="icon fas fa-tags"></i>&nbsp;
                <ul class="post-tag-ul">
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/Algorithm/">Algorithm</a></li>
                        
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/Neural-Networks/">Neural Networks</a></li>
                        
                    
                </ul>
            </span>
        

        
        
        
        
            <span class="meta-info-item post-pv">
                <i class="icon fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
            </span>
        
    </div>

    
</div>

                            </div>
                        </div>
                    </div>
                

                <div class="post-content keep-markdown-body">
                    

                    <p>误差反向传播算法简称反向传播算法(Back Propagation)。使用反向传播算法的多层感知器又称为BP神经网络。</p>
<span id="more"></span>
<p>BP算法是一个迭代算法，它的基本思想如下：</p>
<ol>
<li>将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。</li>
<li>由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；</li>
<li>在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。</li>
<li>迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。</li>
</ol>
<h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><p>有如下一个神经网络：</p>
<p>误差反向传播算法简称反向传播算法(Back Propagation)。使用反向传播算法的多层感知器又称为BP神经网络。</p>
<!--more-->
<p>BP算法是一个迭代算法，它的基本思想如下：</p>
<ol>
<li>将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。</li>
<li>由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；</li>
<li>在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。</li>
<li>迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。</li>
</ol>
<h1 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h1><p>有如下一个神经网络：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224358906.1bsruizweubk.webp"  alt="1.png"></p>
<p>第一层是输入层，包含两个神经元 $i_1$，$i_2$ 和偏置项 $b_1$；第二层是隐藏层，包含两个神经元 $h_1$，$h_2$ 和偏置项 $b_2$；第三层是输出 $o_1$，$o_2$。每条线上标的 $w_i$ 是层与层之间连接的权重。激活函数是 $sigmod$ 函数。我们用 $z$ 表示某神经元的加权输入和；用 $a$ 表示某神经元的输出。</p>
<p>上述各参数赋值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$i_1$</td>
<td style="text-align:center">0.05</td>
</tr>
<tr>
<td style="text-align:center">$i_2$</td>
<td style="text-align:center">0.10</td>
</tr>
<tr>
<td style="text-align:center">$w_1$</td>
<td style="text-align:center">0.15</td>
</tr>
<tr>
<td style="text-align:center">$w_2$</td>
<td style="text-align:center">0.20</td>
</tr>
<tr>
<td style="text-align:center">$w_3$</td>
<td style="text-align:center">0.25</td>
</tr>
<tr>
<td style="text-align:center">$w_4$</td>
<td style="text-align:center">0.30</td>
</tr>
<tr>
<td style="text-align:center">$w_5$</td>
<td style="text-align:center">0.40</td>
</tr>
<tr>
<td style="text-align:center">$w_6$</td>
<td style="text-align:center">0.45</td>
</tr>
<tr>
<td style="text-align:center">$w_7$</td>
<td style="text-align:center">0.50</td>
</tr>
<tr>
<td style="text-align:center">$w_8$</td>
<td style="text-align:center">0.55</td>
</tr>
<tr>
<td style="text-align:center">$b_1$</td>
<td style="text-align:center">0.35</td>
</tr>
<tr>
<td style="text-align:center">$b_2$</td>
<td style="text-align:center">0.60</td>
</tr>
<tr>
<td style="text-align:center">$o_1$</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">$o_2$</td>
<td style="text-align:center">0.99</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Step-1-前向传播"><a href="#Step-1-前向传播" class="headerlink" title="Step 1 前向传播"></a>Step 1 前向传播</h2><h3 id="输入层-—-gt-隐藏层"><a href="#输入层-—-gt-隐藏层" class="headerlink" title="输入层 —-&gt; 隐藏层"></a>输入层 —-&gt; 隐藏层</h3><p>神经元 $h_1$ 的输入加权和：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{h 1} & =w_1 * i_1+w_2 * i_2+b_1 * 1 \\
   & =0.15 * 0.05+0.2 * 0.1+0.35 * 1 \\
   & =0.3775
\end{aligned}</script><p>神经元 $h<em>1$ 的输出 $a</em>{h1}$ ：</p>
<script type="math/tex; mode=display">
a_{h1} = \frac{1}{1+e^{-z_{h1}}} = \frac{1}{1+e^{-0.3775}} = 0.593269992</script><p>同理可得，神经元 $h<em>2$ 的输出 $a</em>{h2}$ ：</p>
<script type="math/tex; mode=display">
a_{h2} = 0.596884378</script><h3 id="隐藏层-—-gt-输出层"><a href="#隐藏层-—-gt-输出层" class="headerlink" title="隐藏层 —-&gt; 输出层"></a>隐藏层 —-&gt; 输出层</h3><p>计算输出层神经元 $o1$ 和 $o2$ 的值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{o 1} & =w_5 * a_{h 1}+w_6 * a_{h 2}+b_2 * 1 \\
   & =0.4 * 0.593269992+0.45 * 0.596884378+0.6 * 1 \\
   & =1.105905967 \\
   a_{o 1} & =\frac{1}{1+e^{-z_{o 1}}} \\
   & =\frac{1}{1+e^{-1.105905967}} \\
   & =0.751365069 \\
   a_{o 2} & =0.772928465
\end{aligned}</script><p>前向传播的过程就结束了，我们得到的输出值是 $[0.751365069, 0.772928465]$ ，与实际值 $[0.01, 0.99]$ 相差还很远。接下来我们对误差进行反向传播，更新权值，重新计算输出。</p>
<h2 id="Step-2-反向传播"><a href="#Step-2-反向传播" class="headerlink" title="Step 2 反向传播"></a>Step 2 反向传播</h2><ol>
<li>计算损失函数：</li>
</ol>
<script type="math/tex; mode=display">
E_{total} = \sum\frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $o_1$ 和 $o_2$ 的损失值，总误差为两者之和：</p>
<script type="math/tex; mode=display">
E_{o_1} = \frac {1}{2}(0.01 - 0.751365069)^2 = 0.274811083 \\
E_{o_2} = \frac {1}{2}(0.99 - 0.772928465)^2 = 0.023560026 \\
E_{total} = E_{o_1} + E_{o_2} = 0.274811083 + 0.023560026 = 0.298371109</script><ol>
<li>隐藏层 —-&gt; 输出层的权值更新</li>
</ol>
<p>以权重参数 $w_5$ 为例，如果我们想知道 $w_5$ 对整体损失产生了多少影响，可以用整体损失对 $w_5$ 求偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w_5} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} }*{ \frac {\partial z_{o_1}} {\partial w_5} }</script><p>下面的图可以更直观了解误差是如何反向传播的：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224659584.31gxu503n220.webp"  alt="2.png"></p>
<p>我们现在分别来计算每个式子的值：</p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
E_{total} = \frac {1}{2}(target_{o_1} - a_{o_1})^2 + \frac {1}{2}(target_{o_2} - a_{o_1})^2 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = 2 * \frac {1}{2} (target_{o_1} - a_{o_1})*-1 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = -(target_{o_1} - a_{o_1}) = 0.751365069-0.01=0.741365069 \\</script><p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
a_{o_1} = \frac {1}{1+e^{-z_{o_1}}} \\
\frac {\partial a_{o_1}} {\partial z_{o_1}} = a_{o_1}*(1-a_{o_1}) = 0.751365069*(1-0.751365069) = 0.186815602</script><p>计算 $\frac {\partial z_{o_1}} {\partial w_5}$ ：</p>
<script type="math/tex; mode=display">
z_{o_1} = w_5*a_{h1} + w_6*a_{h2} + b_2*1 \\
\frac {\partial z_{o_1}} {\partial w_5} = a_{h_1} = 0.593269992</script><p>最后三者相乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = 0.741365069*0.186815602*0.593269992 = 0.082167041</script><p>这样我们就算出整体损失 $E_{total}$ 对 $w_5$ 的偏导值。</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1}) * a_{h_1}</script><p>针对上述公式，为了表达方便，使用 $\delta_{o_1}$ 来表示输出层的误差：</p>
<script type="math/tex; mode=display">
\delta_{o_1} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} } = \frac {\partial E_{total}} {\partial z_{o_1}} \\
\delta_{o_1} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1})</script><p>因此整体损失 $E_{total}$ 对 $w_5$ 的偏导值可以表示为：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}}{\partial w_5} = \delta_{o_1}*a_{h_1}</script><p>最后我们来更新 $w_5$ 的值：</p>
<script type="math/tex; mode=display">
w_5^+ = w_5 - \eta * \frac {\partial E_{total}} {\partial w_5} = 0.4 - 0.5*0.082167041 = 0.35891648 \qquad \eta: 学习率</script><p>同理可更新 $w_6, w_7, w_8$ ：</p>
<script type="math/tex; mode=display">
w_6^+ = 0.408666186 \\
w_7^+ = 0.511301270 \\
w_8^+ = 0.561370121</script><ol>
<li>隐藏层 —-&gt; 隐藏层的权值更新：</li>
</ol>
<p>计算 $\frac {\partial E_{total}} {\partial w_1}$ 与上述方法类似，但需要注意下图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224736113.1umei2jgzokg.webp"  alt="3.png"></p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = \frac {\partial E_{o_1}} {\partial a_{h_1}} + \frac {\partial E_{o_2}} {\partial a_{h_1}}</script><p>先计算 $\frac {\partial E<em>{o_1}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{o_1}}{\partial a_{h_1}} & =\frac{\partial E_{o_1}}{\partial a_{o_1}} * \frac{\partial a_{o_1}}{\partial z_{o_1}} * \frac{\partial z_{o_1}}{\partial a_{h_1}} \\
   & =0.741365069 * 0.186815602 * 0.4 \\
   & =0.055399425
\end{aligned}</script><p>同理可得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{o_2}} {\partial a_{h_1}} = -0.019049119</script><p>两者相加得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = 0.055399425 - 0.019049119 = 0.036350306</script><p>计算 $\frac {a<em>{h_1}} {z</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {a_{h_1}} {z_{h_1}} = a_{h_1} * (1-a_{h_1}) = 0.593269992*(1-0.593269992) = 0.2413007086</script><p>计算 $\frac {\partial z_{h_1}} {\partial w_1}$</p>
<script type="math/tex; mode=display">
\frac {\partial z_{h_1}} {\partial w_1} = i_1 = 0.05</script><p>最后三者相互乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_1} = 0.036350306 * 0.2413007086 * 0.05 = 0.000438568</script><p>为了简化公式，用 $\delta_{h_1}$ 表示隐藏层单元 $h_1$ 的误差： </p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{\text {total }}}{\partial w_1} & =\left(\sum_i \frac{\partial E_{\text {total }}}{\partial a_i} * \frac{\partial a_i}{\partial z_i} * \frac{\partial z_i}{\partial h_1}\right) * \frac{\partial a_{h_1}}{\partial z_{h_1}} * \frac{\partial z_{h_1}}{\partial w_1} \\
   & =\left(\sum_i \delta_i * w_{h_i}\right) * a_{h_1} *\left(1-a_{h_1}\right) * i_1 \\
   & =\delta_{h_1} * i_1
\end{aligned}</script><p>最后更新 $w_1$ 的权值：</p>
<script type="math/tex; mode=display">
w_1^+ = w_1 - \eta * \frac {\partial E_{total}} {\partial w_1} = 0.15 - 0.5*0.000438568 = 0.149780716</script><p>同理，更新 $w_2, w_3, w_4$ 权值：</p>
<script type="math/tex; mode=display">
w_2^+ = 0.19956143 \\
w_3^+ = 0.24975114 \\
w_4^+ = 0.29950229</script><p>这样，反向传播算法就完成了，最后我们再把更新的权值重新计算，不停地迭代。在这个例子中第一次迭代之后，总误差 $E_{total}$ 由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为$[0.015912196,0.984065734](原输入为[0.01,0.99]$ ，证明效果还是不错的。</p>
<h1 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h1><p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225132914.jrimp1wc9o0.webp"  alt="4.png"></p>
<h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$n_l$</td>
<td style="text-align:center">网络层数</td>
</tr>
<tr>
<td style="text-align:center">$y_j$</td>
<td style="text-align:center">输出层第 $j$ 类标签</td>
</tr>
<tr>
<td style="text-align:center">$S_l$</td>
<td style="text-align:center">第 $l$ 层神经元个数（不包括偏置项）</td>
</tr>
<tr>
<td style="text-align:center">$g(x)$</td>
<td style="text-align:center">激活函数</td>
</tr>
<tr>
<td style="text-align:center">$w_{ij}^{l}$</td>
<td style="text-align:center">第 $l-1$ 层的第 $j$ 个神经元连接到第 $l$ 层第 $i$ 个神经元的权重</td>
</tr>
<tr>
<td style="text-align:center">$b_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的偏置</td>
</tr>
<tr>
<td style="text-align:center">$z_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输入加权和</td>
</tr>
<tr>
<td style="text-align:center">$a_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输出（激活值）</td>
</tr>
<tr>
<td style="text-align:center">$\delta_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元产生的错误</td>
</tr>
</tbody>
</table>
</div>
<h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><h3 id="基本公式"><a href="#基本公式" class="headerlink" title="基本公式"></a>基本公式</h3><script type="math/tex; mode=display">
\begin{aligned}
   z_i^l & =\sum_k w_{i k}^l * a_k^{l-1}+b_j^l \\
   g(x) & =\frac{1}{1+e^{-x}} \\
   a_i^l & =g\left(z_i^l\right) \\
   J(\theta) & =\frac{1}{2} \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2 \\
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l}
\end{aligned}</script><h3 id="梯度方向传播公式推导"><a href="#梯度方向传播公式推导" class="headerlink" title="梯度方向传播公式推导"></a>梯度方向传播公式推导</h3><h4 id="初始条件"><a href="#初始条件" class="headerlink" title="初始条件"></a>初始条件</h4><p>以一个输入样本为例：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^{n l} & =\frac{\partial J(\theta)}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-g\left(z_j^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} \frac{\partial\left(y_i-g\left(z_i^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =-\left(y_i-a_i^{n l}\right) g^{\prime}\left(z_i^{l l}\right)
\end{aligned}</script><h4 id="递推公式"><a href="#递推公式" class="headerlink" title="递推公式"></a>递推公式</h4><script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial z_j^{l+1}}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial\left(\sum_k w_{j k}^{l+1} * a_k^l+b_j^{l+1}\right)}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} * g^{\prime}\left(z_i^l\right) \\
   & =g^{\prime}\left(z_i^l\right) * \sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} \\
   \frac{\partial J(\theta)}{\partial w_{i j}^l} & =\frac{\partial J(\theta)}{\partial z_i^l} * \frac{\partial z_i^l}{\partial w_{i j}^l} \\
   & =\delta_i^l * a_j^{l-1} \\
   \frac{\partial J(\theta)}{\partial b_i^l} & =\delta_i^l
\end{aligned}</script><h2 id="反向传播伪代码"><a href="#反向传播伪代码" class="headerlink" title="反向传播伪代码"></a>反向传播伪代码</h2><ol>
<li>输入训练集。</li>
<li>对于训练集的每个样本 $\vec x$ ，设输入层对应的激活值为 $a^l$ ：<ul>
<li>前向传播：$z^l = w^l*a^{l-1}+b^l, a^l = g(z^l)$</li>
<li>计算输出层产生的误差：$\delta^L = \frac {\partial J(\theta)} {\partial a^L} \odot g’(z^L)$</li>
<li>反向传播错误：$\delta^l = ((w^{l+1})^T*\delta^{l+1}) \odot g’(z^l)$</li>
</ul>
</li>
<li>使用梯度下降训练参数：<ul>
<li>$w^l \dashrightarrow w^l - \frac {\alpha} {m} \sum_x\delta^{x, l}*(a^{x, l-1})^T$</li>
<li>$b^l \dashrightarrow  b^l - \frac {\eta} {m} \sum_x\delta^{x, l}$</li>
</ul>
</li>
</ol>
<h1 id="交叉熵损失函数推导"><a href="#交叉熵损失函数推导" class="headerlink" title="交叉熵损失函数推导"></a>交叉熵损失函数推导</h1><p>对于多分类问题，$softmax$ 函数可以将神经网络的输出变成一个概率分布。它只是一个额外的处理层，下图展示了加上了 $softmax$ 回归的神经网络结构图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225454143.6l2hdid9suw0.webp"  alt="softmax"></p>
<p>递推公式仍然和上述递推公式保持一致。初始条件如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   J(\theta) & =-\sum_{i=1}^{S_{n l}} y^t * \ln y^p \\
   y_j^p & =\frac{e^{a_j^{n l}}}{\sum_{k=1}^{S_{n l}} e^{a_k^{n l}}}
\end{aligned}</script><p>$softmax$ 偏导数计算：</p>
<script type="math/tex; mode=display">
\frac {\partial y_j^p} {\partial a_i^{nl}} =
\left\{
\begin{aligned}
-y_i^p*y_j^p \qquad i \neq j \\
y_i^p*(1-y_i^p) i = j
\end{aligned}
\right.</script><h2 id="推导过程-1"><a href="#推导过程-1" class="headerlink" title="推导过程"></a>推导过程</h2><script type="math/tex; mode=display">
\begin{aligned}
   & \frac{\partial J(\theta)}{\partial z_i^{n l}}=\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial a_i^{n l}}=\sum_{j=1}^{S_{n l}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial y_j^p}=-\frac{y_j^t}{y_j^p}
\end{aligned}</script><p>   由上可知:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial a_i^{n l}} & =\frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}}+\sum_{j \neq i}^{S_{n t}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & =-\frac{y_i^t}{y_i^p} * y_i^p *\left(1-y_i^p\right)+\sum_{j \neq i}^{S_{n l}}-\frac{y_j^t}{y_j^p} *\left(-y_j^p * y_i^p\right) \\
   & =-y_i^t+y_i^t * y_i^p+\sum_{j \neq i}^{S_{n l l}}\left(y_j^t * y_i^p\right) \\
   & =-y_i^t+\sum_j^{S_{n+1}}\left(y_j^t * y_i^p\right) \\
   & =y_i^p-y_i^t
\end{aligned}</script><p>   $\therefore$ 反向传播选代算法的初始值为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial z_i^{n l}} & =\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & =\left(y_i^p-y_i^t\right) * a_i^{n l} *\left(1-a_i^{n l}\right)
\end{aligned}</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/charlotte77/p/5629865.html?tdsourcetag=s_pcqq_aiomsg" >一文弄懂神经网络中的反向传播法——BackPropagation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/u014313009/article/details/51039334" >反向传播算法（过程及公式推导）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/nowgood/p/backprop2.html?tdsourcetag=s_pcqq_aiomsg" >反向传播公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<p>第一层是输入层，包含两个神经元 $i_1$，$i_2$ 和偏置项 $b_1$；第二层是隐藏层，包含两个神经元 $h_1$，$h_2$ 和偏置项 $b_2$；第三层是输出 $o_1$，$o_2$。每条线上标的 $w_i$ 是层与层之间连接的权重。激活函数是 $sigmod$ 函数。我们用 $z$ 表示某神经元的加权输入和；用 $a$ 表示某神经元的输出。</p>
<p>上述各参数赋值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$i_1$</td>
<td style="text-align:center">0.05</td>
</tr>
<tr>
<td style="text-align:center">$i_2$</td>
<td style="text-align:center">0.10</td>
</tr>
<tr>
<td style="text-align:center">$w_1$</td>
<td style="text-align:center">0.15</td>
</tr>
<tr>
<td style="text-align:center">$w_2$</td>
<td style="text-align:center">0.20</td>
</tr>
<tr>
<td style="text-align:center">$w_3$</td>
<td style="text-align:center">0.25</td>
</tr>
<tr>
<td style="text-align:center">$w_4$</td>
<td style="text-align:center">0.30</td>
</tr>
<tr>
<td style="text-align:center">$w_5$</td>
<td style="text-align:center">0.40</td>
</tr>
<tr>
<td style="text-align:center">$w_6$</td>
<td style="text-align:center">0.45</td>
</tr>
<tr>
<td style="text-align:center">$w_7$</td>
<td style="text-align:center">0.50</td>
</tr>
<tr>
<td style="text-align:center">$w_8$</td>
<td style="text-align:center">0.55</td>
</tr>
<tr>
<td style="text-align:center">$b_1$</td>
<td style="text-align:center">0.35</td>
</tr>
<tr>
<td style="text-align:center">$b_2$</td>
<td style="text-align:center">0.60</td>
</tr>
<tr>
<td style="text-align:center">$o_1$</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">$o_2$</td>
<td style="text-align:center">0.99</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Step-1-前向传播-1"><a href="#Step-1-前向传播-1" class="headerlink" title="Step 1 前向传播"></a>Step 1 前向传播</h2><h3 id="输入层-—-gt-隐藏层-1"><a href="#输入层-—-gt-隐藏层-1" class="headerlink" title="输入层 —-&gt; 隐藏层"></a>输入层 —-&gt; 隐藏层</h3><p>神经元 $h_1$ 的输入加权和：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{h 1} & =w_1 * i_1+w_2 * i_2+b_1 * 1 \\
   & =0.15 * 0.05+0.2 * 0.1+0.35 * 1 \\
   & =0.3775
\end{aligned}</script><p>神经元 $h<em>1$ 的输出 $a</em>{h1}$ ：</p>
<script type="math/tex; mode=display">
a_{h1} = \frac{1}{1+e^{-z_{h1}}} = \frac{1}{1+e^{-0.3775}} = 0.593269992</script><p>同理可得，神经元 $h<em>2$ 的输出 $a</em>{h2}$ ：</p>
<script type="math/tex; mode=display">
a_{h2} = 0.596884378</script><h3 id="隐藏层-—-gt-输出层-1"><a href="#隐藏层-—-gt-输出层-1" class="headerlink" title="隐藏层 —-&gt; 输出层"></a>隐藏层 —-&gt; 输出层</h3><p>计算输出层神经元 $o1$ 和 $o2$ 的值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{o 1} & =w_5 * a_{h 1}+w_6 * a_{h 2}+b_2 * 1 \\
   & =0.4 * 0.593269992+0.45 * 0.596884378+0.6 * 1 \\
   & =1.105905967 \\
   a_{o 1} & =\frac{1}{1+e^{-z_{o 1}}} \\
   & =\frac{1}{1+e^{-1.105905967}} \\
   & =0.751365069 \\
   a_{o 2} & =0.772928465
\end{aligned}</script><p>前向传播的过程就结束了，我们得到的输出值是 $[0.751365069, 0.772928465]$ ，与实际值 $[0.01, 0.99]$ 相差还很远。接下来我们对误差进行反向传播，更新权值，重新计算输出。</p>
<h2 id="Step-2-反向传播-1"><a href="#Step-2-反向传播-1" class="headerlink" title="Step 2 反向传播"></a>Step 2 反向传播</h2><ol>
<li>计算损失函数：</li>
</ol>
<script type="math/tex; mode=display">
E_{total} = \sum\frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $o_1$ 和 $o_2$ 的损失值，总误差为两者之和：</p>
<script type="math/tex; mode=display">
E_{o_1} = \frac {1}{2}(0.01 - 0.751365069)^2 = 0.274811083 \\
E_{o_2} = \frac {1}{2}(0.99 - 0.772928465)^2 = 0.023560026 \\
E_{total} = E_{o_1} + E_{o_2} = 0.274811083 + 0.023560026 = 0.298371109</script><ol>
<li>隐藏层 —-&gt; 输出层的权值更新</li>
</ol>
<p>以权重参数 $w_5$ 为例，如果我们想知道 $w_5$ 对整体损失产生了多少影响，可以用整体损失对 $w_5$ 求偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w_5} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} }*{ \frac {\partial z_{o_1}} {\partial w_5} }</script><p>下面的图可以更直观了解误差是如何反向传播的：</p>
<p>误差反向传播算法简称反向传播算法(Back Propagation)。使用反向传播算法的多层感知器又称为BP神经网络。</p>
<!--more-->
<p>BP算法是一个迭代算法，它的基本思想如下：</p>
<ol>
<li>将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。</li>
<li>由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；</li>
<li>在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。</li>
<li>迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。</li>
</ol>
<h1 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h1><p>有如下一个神经网络：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224358906.1bsruizweubk.webp"  alt="1.png"></p>
<p>第一层是输入层，包含两个神经元 $i_1$，$i_2$ 和偏置项 $b_1$；第二层是隐藏层，包含两个神经元 $h_1$，$h_2$ 和偏置项 $b_2$；第三层是输出 $o_1$，$o_2$。每条线上标的 $w_i$ 是层与层之间连接的权重。激活函数是 $sigmod$ 函数。我们用 $z$ 表示某神经元的加权输入和；用 $a$ 表示某神经元的输出。</p>
<p>上述各参数赋值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$i_1$</td>
<td style="text-align:center">0.05</td>
</tr>
<tr>
<td style="text-align:center">$i_2$</td>
<td style="text-align:center">0.10</td>
</tr>
<tr>
<td style="text-align:center">$w_1$</td>
<td style="text-align:center">0.15</td>
</tr>
<tr>
<td style="text-align:center">$w_2$</td>
<td style="text-align:center">0.20</td>
</tr>
<tr>
<td style="text-align:center">$w_3$</td>
<td style="text-align:center">0.25</td>
</tr>
<tr>
<td style="text-align:center">$w_4$</td>
<td style="text-align:center">0.30</td>
</tr>
<tr>
<td style="text-align:center">$w_5$</td>
<td style="text-align:center">0.40</td>
</tr>
<tr>
<td style="text-align:center">$w_6$</td>
<td style="text-align:center">0.45</td>
</tr>
<tr>
<td style="text-align:center">$w_7$</td>
<td style="text-align:center">0.50</td>
</tr>
<tr>
<td style="text-align:center">$w_8$</td>
<td style="text-align:center">0.55</td>
</tr>
<tr>
<td style="text-align:center">$b_1$</td>
<td style="text-align:center">0.35</td>
</tr>
<tr>
<td style="text-align:center">$b_2$</td>
<td style="text-align:center">0.60</td>
</tr>
<tr>
<td style="text-align:center">$o_1$</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">$o_2$</td>
<td style="text-align:center">0.99</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Step-1-前向传播-2"><a href="#Step-1-前向传播-2" class="headerlink" title="Step 1 前向传播"></a>Step 1 前向传播</h2><h3 id="输入层-—-gt-隐藏层-2"><a href="#输入层-—-gt-隐藏层-2" class="headerlink" title="输入层 —-&gt; 隐藏层"></a>输入层 —-&gt; 隐藏层</h3><p>神经元 $h_1$ 的输入加权和：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{h 1} & =w_1 * i_1+w_2 * i_2+b_1 * 1 \\
   & =0.15 * 0.05+0.2 * 0.1+0.35 * 1 \\
   & =0.3775
\end{aligned}</script><p>神经元 $h<em>1$ 的输出 $a</em>{h1}$ ：</p>
<script type="math/tex; mode=display">
a_{h1} = \frac{1}{1+e^{-z_{h1}}} = \frac{1}{1+e^{-0.3775}} = 0.593269992</script><p>同理可得，神经元 $h<em>2$ 的输出 $a</em>{h2}$ ：</p>
<script type="math/tex; mode=display">
a_{h2} = 0.596884378</script><h3 id="隐藏层-—-gt-输出层-2"><a href="#隐藏层-—-gt-输出层-2" class="headerlink" title="隐藏层 —-&gt; 输出层"></a>隐藏层 —-&gt; 输出层</h3><p>计算输出层神经元 $o1$ 和 $o2$ 的值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{o 1} & =w_5 * a_{h 1}+w_6 * a_{h 2}+b_2 * 1 \\
   & =0.4 * 0.593269992+0.45 * 0.596884378+0.6 * 1 \\
   & =1.105905967 \\
   a_{o 1} & =\frac{1}{1+e^{-z_{o 1}}} \\
   & =\frac{1}{1+e^{-1.105905967}} \\
   & =0.751365069 \\
   a_{o 2} & =0.772928465
\end{aligned}</script><p>前向传播的过程就结束了，我们得到的输出值是 $[0.751365069, 0.772928465]$ ，与实际值 $[0.01, 0.99]$ 相差还很远。接下来我们对误差进行反向传播，更新权值，重新计算输出。</p>
<h2 id="Step-2-反向传播-2"><a href="#Step-2-反向传播-2" class="headerlink" title="Step 2 反向传播"></a>Step 2 反向传播</h2><ol>
<li>计算损失函数：</li>
</ol>
<script type="math/tex; mode=display">
E_{total} = \sum\frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $o_1$ 和 $o_2$ 的损失值，总误差为两者之和：</p>
<script type="math/tex; mode=display">
E_{o_1} = \frac {1}{2}(0.01 - 0.751365069)^2 = 0.274811083 \\
E_{o_2} = \frac {1}{2}(0.99 - 0.772928465)^2 = 0.023560026 \\
E_{total} = E_{o_1} + E_{o_2} = 0.274811083 + 0.023560026 = 0.298371109</script><ol>
<li>隐藏层 —-&gt; 输出层的权值更新</li>
</ol>
<p>以权重参数 $w_5$ 为例，如果我们想知道 $w_5$ 对整体损失产生了多少影响，可以用整体损失对 $w_5$ 求偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w_5} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} }*{ \frac {\partial z_{o_1}} {\partial w_5} }</script><p>下面的图可以更直观了解误差是如何反向传播的：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224659584.31gxu503n220.webp"  alt="2.png"></p>
<p>我们现在分别来计算每个式子的值：</p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
E_{total} = \frac {1}{2}(target_{o_1} - a_{o_1})^2 + \frac {1}{2}(target_{o_2} - a_{o_1})^2 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = 2 * \frac {1}{2} (target_{o_1} - a_{o_1})*-1 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = -(target_{o_1} - a_{o_1}) = 0.751365069-0.01=0.741365069 \\</script><p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
a_{o_1} = \frac {1}{1+e^{-z_{o_1}}} \\
\frac {\partial a_{o_1}} {\partial z_{o_1}} = a_{o_1}*(1-a_{o_1}) = 0.751365069*(1-0.751365069) = 0.186815602</script><p>计算 $\frac {\partial z_{o_1}} {\partial w_5}$ ：</p>
<script type="math/tex; mode=display">
z_{o_1} = w_5*a_{h1} + w_6*a_{h2} + b_2*1 \\
\frac {\partial z_{o_1}} {\partial w_5} = a_{h_1} = 0.593269992</script><p>最后三者相乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = 0.741365069*0.186815602*0.593269992 = 0.082167041</script><p>这样我们就算出整体损失 $E_{total}$ 对 $w_5$ 的偏导值。</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1}) * a_{h_1}</script><p>针对上述公式，为了表达方便，使用 $\delta_{o_1}$ 来表示输出层的误差：</p>
<script type="math/tex; mode=display">
\delta_{o_1} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} } = \frac {\partial E_{total}} {\partial z_{o_1}} \\
\delta_{o_1} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1})</script><p>因此整体损失 $E_{total}$ 对 $w_5$ 的偏导值可以表示为：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}}{\partial w_5} = \delta_{o_1}*a_{h_1}</script><p>最后我们来更新 $w_5$ 的值：</p>
<script type="math/tex; mode=display">
w_5^+ = w_5 - \eta * \frac {\partial E_{total}} {\partial w_5} = 0.4 - 0.5*0.082167041 = 0.35891648 \qquad \eta: 学习率</script><p>同理可更新 $w_6, w_7, w_8$ ：</p>
<script type="math/tex; mode=display">
w_6^+ = 0.408666186 \\
w_7^+ = 0.511301270 \\
w_8^+ = 0.561370121</script><ol>
<li>隐藏层 —-&gt; 隐藏层的权值更新：</li>
</ol>
<p>计算 $\frac {\partial E_{total}} {\partial w_1}$ 与上述方法类似，但需要注意下图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224736113.1umei2jgzokg.webp"  alt="3.png"></p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = \frac {\partial E_{o_1}} {\partial a_{h_1}} + \frac {\partial E_{o_2}} {\partial a_{h_1}}</script><p>先计算 $\frac {\partial E<em>{o_1}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{o_1}}{\partial a_{h_1}} & =\frac{\partial E_{o_1}}{\partial a_{o_1}} * \frac{\partial a_{o_1}}{\partial z_{o_1}} * \frac{\partial z_{o_1}}{\partial a_{h_1}} \\
   & =0.741365069 * 0.186815602 * 0.4 \\
   & =0.055399425
\end{aligned}</script><p>同理可得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{o_2}} {\partial a_{h_1}} = -0.019049119</script><p>两者相加得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = 0.055399425 - 0.019049119 = 0.036350306</script><p>计算 $\frac {a<em>{h_1}} {z</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {a_{h_1}} {z_{h_1}} = a_{h_1} * (1-a_{h_1}) = 0.593269992*(1-0.593269992) = 0.2413007086</script><p>计算 $\frac {\partial z_{h_1}} {\partial w_1}$</p>
<script type="math/tex; mode=display">
\frac {\partial z_{h_1}} {\partial w_1} = i_1 = 0.05</script><p>最后三者相互乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_1} = 0.036350306 * 0.2413007086 * 0.05 = 0.000438568</script><p>为了简化公式，用 $\delta_{h_1}$ 表示隐藏层单元 $h_1$ 的误差： </p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{\text {total }}}{\partial w_1} & =\left(\sum_i \frac{\partial E_{\text {total }}}{\partial a_i} * \frac{\partial a_i}{\partial z_i} * \frac{\partial z_i}{\partial h_1}\right) * \frac{\partial a_{h_1}}{\partial z_{h_1}} * \frac{\partial z_{h_1}}{\partial w_1} \\
   & =\left(\sum_i \delta_i * w_{h_i}\right) * a_{h_1} *\left(1-a_{h_1}\right) * i_1 \\
   & =\delta_{h_1} * i_1
\end{aligned}</script><p>最后更新 $w_1$ 的权值：</p>
<script type="math/tex; mode=display">
w_1^+ = w_1 - \eta * \frac {\partial E_{total}} {\partial w_1} = 0.15 - 0.5*0.000438568 = 0.149780716</script><p>同理，更新 $w_2, w_3, w_4$ 权值：</p>
<script type="math/tex; mode=display">
w_2^+ = 0.19956143 \\
w_3^+ = 0.24975114 \\
w_4^+ = 0.29950229</script><p>这样，反向传播算法就完成了，最后我们再把更新的权值重新计算，不停地迭代。在这个例子中第一次迭代之后，总误差 $E_{total}$ 由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为$[0.015912196,0.984065734](原输入为[0.01,0.99]$ ，证明效果还是不错的。</p>
<h1 id="公式推导-1"><a href="#公式推导-1" class="headerlink" title="公式推导"></a>公式推导</h1><p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225132914.jrimp1wc9o0.webp"  alt="4.png"></p>
<h2 id="符号说明-1"><a href="#符号说明-1" class="headerlink" title="符号说明"></a>符号说明</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$n_l$</td>
<td style="text-align:center">网络层数</td>
</tr>
<tr>
<td style="text-align:center">$y_j$</td>
<td style="text-align:center">输出层第 $j$ 类标签</td>
</tr>
<tr>
<td style="text-align:center">$S_l$</td>
<td style="text-align:center">第 $l$ 层神经元个数（不包括偏置项）</td>
</tr>
<tr>
<td style="text-align:center">$g(x)$</td>
<td style="text-align:center">激活函数</td>
</tr>
<tr>
<td style="text-align:center">$w_{ij}^{l}$</td>
<td style="text-align:center">第 $l-1$ 层的第 $j$ 个神经元连接到第 $l$ 层第 $i$ 个神经元的权重</td>
</tr>
<tr>
<td style="text-align:center">$b_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的偏置</td>
</tr>
<tr>
<td style="text-align:center">$z_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输入加权和</td>
</tr>
<tr>
<td style="text-align:center">$a_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输出（激活值）</td>
</tr>
<tr>
<td style="text-align:center">$\delta_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元产生的错误</td>
</tr>
</tbody>
</table>
</div>
<h2 id="推导过程-2"><a href="#推导过程-2" class="headerlink" title="推导过程"></a>推导过程</h2><h3 id="基本公式-1"><a href="#基本公式-1" class="headerlink" title="基本公式"></a>基本公式</h3><script type="math/tex; mode=display">
\begin{aligned}
   z_i^l & =\sum_k w_{i k}^l * a_k^{l-1}+b_j^l \\
   g(x) & =\frac{1}{1+e^{-x}} \\
   a_i^l & =g\left(z_i^l\right) \\
   J(\theta) & =\frac{1}{2} \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2 \\
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l}
\end{aligned}</script><h3 id="梯度方向传播公式推导-1"><a href="#梯度方向传播公式推导-1" class="headerlink" title="梯度方向传播公式推导"></a>梯度方向传播公式推导</h3><h4 id="初始条件-1"><a href="#初始条件-1" class="headerlink" title="初始条件"></a>初始条件</h4><p>以一个输入样本为例：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^{n l} & =\frac{\partial J(\theta)}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-g\left(z_j^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} \frac{\partial\left(y_i-g\left(z_i^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =-\left(y_i-a_i^{n l}\right) g^{\prime}\left(z_i^{l l}\right)
\end{aligned}</script><h4 id="递推公式-1"><a href="#递推公式-1" class="headerlink" title="递推公式"></a>递推公式</h4><script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial z_j^{l+1}}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial\left(\sum_k w_{j k}^{l+1} * a_k^l+b_j^{l+1}\right)}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} * g^{\prime}\left(z_i^l\right) \\
   & =g^{\prime}\left(z_i^l\right) * \sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} \\
   \frac{\partial J(\theta)}{\partial w_{i j}^l} & =\frac{\partial J(\theta)}{\partial z_i^l} * \frac{\partial z_i^l}{\partial w_{i j}^l} \\
   & =\delta_i^l * a_j^{l-1} \\
   \frac{\partial J(\theta)}{\partial b_i^l} & =\delta_i^l
\end{aligned}</script><h2 id="反向传播伪代码-1"><a href="#反向传播伪代码-1" class="headerlink" title="反向传播伪代码"></a>反向传播伪代码</h2><ol>
<li>输入训练集。</li>
<li>对于训练集的每个样本 $\vec x$ ，设输入层对应的激活值为 $a^l$ ：<ul>
<li>前向传播：$z^l = w^l*a^{l-1}+b^l, a^l = g(z^l)$</li>
<li>计算输出层产生的误差：$\delta^L = \frac {\partial J(\theta)} {\partial a^L} \odot g’(z^L)$</li>
<li>反向传播错误：$\delta^l = ((w^{l+1})^T*\delta^{l+1}) \odot g’(z^l)$</li>
</ul>
</li>
<li>使用梯度下降训练参数：<ul>
<li>$w^l \dashrightarrow w^l - \frac {\alpha} {m} \sum_x\delta^{x, l}*(a^{x, l-1})^T$</li>
<li>$b^l \dashrightarrow  b^l - \frac {\eta} {m} \sum_x\delta^{x, l}$</li>
</ul>
</li>
</ol>
<h1 id="交叉熵损失函数推导-1"><a href="#交叉熵损失函数推导-1" class="headerlink" title="交叉熵损失函数推导"></a>交叉熵损失函数推导</h1><p>对于多分类问题，$softmax$ 函数可以将神经网络的输出变成一个概率分布。它只是一个额外的处理层，下图展示了加上了 $softmax$ 回归的神经网络结构图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225454143.6l2hdid9suw0.webp"  alt="softmax"></p>
<p>递推公式仍然和上述递推公式保持一致。初始条件如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   J(\theta) & =-\sum_{i=1}^{S_{n l}} y^t * \ln y^p \\
   y_j^p & =\frac{e^{a_j^{n l}}}{\sum_{k=1}^{S_{n l}} e^{a_k^{n l}}}
\end{aligned}</script><p>$softmax$ 偏导数计算：</p>
<script type="math/tex; mode=display">
\frac {\partial y_j^p} {\partial a_i^{nl}} =
\left\{
\begin{aligned}
-y_i^p*y_j^p \qquad i \neq j \\
y_i^p*(1-y_i^p) i = j
\end{aligned}
\right.</script><h2 id="推导过程-3"><a href="#推导过程-3" class="headerlink" title="推导过程"></a>推导过程</h2><script type="math/tex; mode=display">
\begin{aligned}
   & \frac{\partial J(\theta)}{\partial z_i^{n l}}=\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial a_i^{n l}}=\sum_{j=1}^{S_{n l}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial y_j^p}=-\frac{y_j^t}{y_j^p}
\end{aligned}</script><p>   由上可知:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial a_i^{n l}} & =\frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}}+\sum_{j \neq i}^{S_{n t}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & =-\frac{y_i^t}{y_i^p} * y_i^p *\left(1-y_i^p\right)+\sum_{j \neq i}^{S_{n l}}-\frac{y_j^t}{y_j^p} *\left(-y_j^p * y_i^p\right) \\
   & =-y_i^t+y_i^t * y_i^p+\sum_{j \neq i}^{S_{n l l}}\left(y_j^t * y_i^p\right) \\
   & =-y_i^t+\sum_j^{S_{n+1}}\left(y_j^t * y_i^p\right) \\
   & =y_i^p-y_i^t
\end{aligned}</script><p>   $\therefore$ 反向传播选代算法的初始值为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial z_i^{n l}} & =\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & =\left(y_i^p-y_i^t\right) * a_i^{n l} *\left(1-a_i^{n l}\right)
\end{aligned}</script><hr>
<h2 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/charlotte77/p/5629865.html?tdsourcetag=s_pcqq_aiomsg" >一文弄懂神经网络中的反向传播法——BackPropagation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/u014313009/article/details/51039334" >反向传播算法（过程及公式推导）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/nowgood/p/backprop2.html?tdsourcetag=s_pcqq_aiomsg" >反向传播公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<p>我们现在分别来计算每个式子的值：</p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
E_{total} = \frac {1}{2}(target_{o_1} - a_{o_1})^2 + \frac {1}{2}(target_{o_2} - a_{o_1})^2 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = 2 * \frac {1}{2} (target_{o_1} - a_{o_1})*-1 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = -(target_{o_1} - a_{o_1}) = 0.751365069-0.01=0.741365069 \\</script><p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
a_{o_1} = \frac {1}{1+e^{-z_{o_1}}} \\
\frac {\partial a_{o_1}} {\partial z_{o_1}} = a_{o_1}*(1-a_{o_1}) = 0.751365069*(1-0.751365069) = 0.186815602</script><p>计算 $\frac {\partial z_{o_1}} {\partial w_5}$ ：</p>
<script type="math/tex; mode=display">
z_{o_1} = w_5*a_{h1} + w_6*a_{h2} + b_2*1 \\
\frac {\partial z_{o_1}} {\partial w_5} = a_{h_1} = 0.593269992</script><p>最后三者相乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = 0.741365069*0.186815602*0.593269992 = 0.082167041</script><p>这样我们就算出整体损失 $E_{total}$ 对 $w_5$ 的偏导值。</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1}) * a_{h_1}</script><p>针对上述公式，为了表达方便，使用 $\delta_{o_1}$ 来表示输出层的误差：</p>
<script type="math/tex; mode=display">
\delta_{o_1} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} } = \frac {\partial E_{total}} {\partial z_{o_1}} \\
\delta_{o_1} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1})</script><p>因此整体损失 $E_{total}$ 对 $w_5$ 的偏导值可以表示为：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}}{\partial w_5} = \delta_{o_1}*a_{h_1}</script><p>最后我们来更新 $w_5$ 的值：</p>
<script type="math/tex; mode=display">
w_5^+ = w_5 - \eta * \frac {\partial E_{total}} {\partial w_5} = 0.4 - 0.5*0.082167041 = 0.35891648 \qquad \eta: 学习率</script><p>同理可更新 $w_6, w_7, w_8$ ：</p>
<script type="math/tex; mode=display">
w_6^+ = 0.408666186 \\
w_7^+ = 0.511301270 \\
w_8^+ = 0.561370121</script><ol>
<li>隐藏层 —-&gt; 隐藏层的权值更新：</li>
</ol>
<p>计算 $\frac {\partial E_{total}} {\partial w_1}$ 与上述方法类似，但需要注意下图：</p>
<p>误差反向传播算法简称反向传播算法(Back Propagation)。使用反向传播算法的多层感知器又称为BP神经网络。</p>
<!--more-->
<p>BP算法是一个迭代算法，它的基本思想如下：</p>
<ol>
<li>将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。</li>
<li>由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；</li>
<li>在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。</li>
<li>迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。</li>
</ol>
<h1 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h1><p>有如下一个神经网络：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224358906.1bsruizweubk.webp"  alt="1.png"></p>
<p>第一层是输入层，包含两个神经元 $i_1$，$i_2$ 和偏置项 $b_1$；第二层是隐藏层，包含两个神经元 $h_1$，$h_2$ 和偏置项 $b_2$；第三层是输出 $o_1$，$o_2$。每条线上标的 $w_i$ 是层与层之间连接的权重。激活函数是 $sigmod$ 函数。我们用 $z$ 表示某神经元的加权输入和；用 $a$ 表示某神经元的输出。</p>
<p>上述各参数赋值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$i_1$</td>
<td style="text-align:center">0.05</td>
</tr>
<tr>
<td style="text-align:center">$i_2$</td>
<td style="text-align:center">0.10</td>
</tr>
<tr>
<td style="text-align:center">$w_1$</td>
<td style="text-align:center">0.15</td>
</tr>
<tr>
<td style="text-align:center">$w_2$</td>
<td style="text-align:center">0.20</td>
</tr>
<tr>
<td style="text-align:center">$w_3$</td>
<td style="text-align:center">0.25</td>
</tr>
<tr>
<td style="text-align:center">$w_4$</td>
<td style="text-align:center">0.30</td>
</tr>
<tr>
<td style="text-align:center">$w_5$</td>
<td style="text-align:center">0.40</td>
</tr>
<tr>
<td style="text-align:center">$w_6$</td>
<td style="text-align:center">0.45</td>
</tr>
<tr>
<td style="text-align:center">$w_7$</td>
<td style="text-align:center">0.50</td>
</tr>
<tr>
<td style="text-align:center">$w_8$</td>
<td style="text-align:center">0.55</td>
</tr>
<tr>
<td style="text-align:center">$b_1$</td>
<td style="text-align:center">0.35</td>
</tr>
<tr>
<td style="text-align:center">$b_2$</td>
<td style="text-align:center">0.60</td>
</tr>
<tr>
<td style="text-align:center">$o_1$</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">$o_2$</td>
<td style="text-align:center">0.99</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Step-1-前向传播-3"><a href="#Step-1-前向传播-3" class="headerlink" title="Step 1 前向传播"></a>Step 1 前向传播</h2><h3 id="输入层-—-gt-隐藏层-3"><a href="#输入层-—-gt-隐藏层-3" class="headerlink" title="输入层 —-&gt; 隐藏层"></a>输入层 —-&gt; 隐藏层</h3><p>神经元 $h_1$ 的输入加权和：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{h 1} & =w_1 * i_1+w_2 * i_2+b_1 * 1 \\
   & =0.15 * 0.05+0.2 * 0.1+0.35 * 1 \\
   & =0.3775
\end{aligned}</script><p>神经元 $h<em>1$ 的输出 $a</em>{h1}$ ：</p>
<script type="math/tex; mode=display">
a_{h1} = \frac{1}{1+e^{-z_{h1}}} = \frac{1}{1+e^{-0.3775}} = 0.593269992</script><p>同理可得，神经元 $h<em>2$ 的输出 $a</em>{h2}$ ：</p>
<script type="math/tex; mode=display">
a_{h2} = 0.596884378</script><h3 id="隐藏层-—-gt-输出层-3"><a href="#隐藏层-—-gt-输出层-3" class="headerlink" title="隐藏层 —-&gt; 输出层"></a>隐藏层 —-&gt; 输出层</h3><p>计算输出层神经元 $o1$ 和 $o2$ 的值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{o 1} & =w_5 * a_{h 1}+w_6 * a_{h 2}+b_2 * 1 \\
   & =0.4 * 0.593269992+0.45 * 0.596884378+0.6 * 1 \\
   & =1.105905967 \\
   a_{o 1} & =\frac{1}{1+e^{-z_{o 1}}} \\
   & =\frac{1}{1+e^{-1.105905967}} \\
   & =0.751365069 \\
   a_{o 2} & =0.772928465
\end{aligned}</script><p>前向传播的过程就结束了，我们得到的输出值是 $[0.751365069, 0.772928465]$ ，与实际值 $[0.01, 0.99]$ 相差还很远。接下来我们对误差进行反向传播，更新权值，重新计算输出。</p>
<h2 id="Step-2-反向传播-3"><a href="#Step-2-反向传播-3" class="headerlink" title="Step 2 反向传播"></a>Step 2 反向传播</h2><ol>
<li>计算损失函数：</li>
</ol>
<script type="math/tex; mode=display">
E_{total} = \sum\frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $o_1$ 和 $o_2$ 的损失值，总误差为两者之和：</p>
<script type="math/tex; mode=display">
E_{o_1} = \frac {1}{2}(0.01 - 0.751365069)^2 = 0.274811083 \\
E_{o_2} = \frac {1}{2}(0.99 - 0.772928465)^2 = 0.023560026 \\
E_{total} = E_{o_1} + E_{o_2} = 0.274811083 + 0.023560026 = 0.298371109</script><ol>
<li>隐藏层 —-&gt; 输出层的权值更新</li>
</ol>
<p>以权重参数 $w_5$ 为例，如果我们想知道 $w_5$ 对整体损失产生了多少影响，可以用整体损失对 $w_5$ 求偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w_5} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} }*{ \frac {\partial z_{o_1}} {\partial w_5} }</script><p>下面的图可以更直观了解误差是如何反向传播的：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224659584.31gxu503n220.webp"  alt="2.png"></p>
<p>我们现在分别来计算每个式子的值：</p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
E_{total} = \frac {1}{2}(target_{o_1} - a_{o_1})^2 + \frac {1}{2}(target_{o_2} - a_{o_1})^2 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = 2 * \frac {1}{2} (target_{o_1} - a_{o_1})*-1 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = -(target_{o_1} - a_{o_1}) = 0.751365069-0.01=0.741365069 \\</script><p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
a_{o_1} = \frac {1}{1+e^{-z_{o_1}}} \\
\frac {\partial a_{o_1}} {\partial z_{o_1}} = a_{o_1}*(1-a_{o_1}) = 0.751365069*(1-0.751365069) = 0.186815602</script><p>计算 $\frac {\partial z_{o_1}} {\partial w_5}$ ：</p>
<script type="math/tex; mode=display">
z_{o_1} = w_5*a_{h1} + w_6*a_{h2} + b_2*1 \\
\frac {\partial z_{o_1}} {\partial w_5} = a_{h_1} = 0.593269992</script><p>最后三者相乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = 0.741365069*0.186815602*0.593269992 = 0.082167041</script><p>这样我们就算出整体损失 $E_{total}$ 对 $w_5$ 的偏导值。</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1}) * a_{h_1}</script><p>针对上述公式，为了表达方便，使用 $\delta_{o_1}$ 来表示输出层的误差：</p>
<script type="math/tex; mode=display">
\delta_{o_1} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} } = \frac {\partial E_{total}} {\partial z_{o_1}} \\
\delta_{o_1} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1})</script><p>因此整体损失 $E_{total}$ 对 $w_5$ 的偏导值可以表示为：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}}{\partial w_5} = \delta_{o_1}*a_{h_1}</script><p>最后我们来更新 $w_5$ 的值：</p>
<script type="math/tex; mode=display">
w_5^+ = w_5 - \eta * \frac {\partial E_{total}} {\partial w_5} = 0.4 - 0.5*0.082167041 = 0.35891648 \qquad \eta: 学习率</script><p>同理可更新 $w_6, w_7, w_8$ ：</p>
<script type="math/tex; mode=display">
w_6^+ = 0.408666186 \\
w_7^+ = 0.511301270 \\
w_8^+ = 0.561370121</script><ol>
<li>隐藏层 —-&gt; 隐藏层的权值更新：</li>
</ol>
<p>计算 $\frac {\partial E_{total}} {\partial w_1}$ 与上述方法类似，但需要注意下图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224736113.1umei2jgzokg.webp"  alt="3.png"></p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = \frac {\partial E_{o_1}} {\partial a_{h_1}} + \frac {\partial E_{o_2}} {\partial a_{h_1}}</script><p>先计算 $\frac {\partial E<em>{o_1}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{o_1}}{\partial a_{h_1}} & =\frac{\partial E_{o_1}}{\partial a_{o_1}} * \frac{\partial a_{o_1}}{\partial z_{o_1}} * \frac{\partial z_{o_1}}{\partial a_{h_1}} \\
   & =0.741365069 * 0.186815602 * 0.4 \\
   & =0.055399425
\end{aligned}</script><p>同理可得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{o_2}} {\partial a_{h_1}} = -0.019049119</script><p>两者相加得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = 0.055399425 - 0.019049119 = 0.036350306</script><p>计算 $\frac {a<em>{h_1}} {z</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {a_{h_1}} {z_{h_1}} = a_{h_1} * (1-a_{h_1}) = 0.593269992*(1-0.593269992) = 0.2413007086</script><p>计算 $\frac {\partial z_{h_1}} {\partial w_1}$</p>
<script type="math/tex; mode=display">
\frac {\partial z_{h_1}} {\partial w_1} = i_1 = 0.05</script><p>最后三者相互乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_1} = 0.036350306 * 0.2413007086 * 0.05 = 0.000438568</script><p>为了简化公式，用 $\delta_{h_1}$ 表示隐藏层单元 $h_1$ 的误差： </p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{\text {total }}}{\partial w_1} & =\left(\sum_i \frac{\partial E_{\text {total }}}{\partial a_i} * \frac{\partial a_i}{\partial z_i} * \frac{\partial z_i}{\partial h_1}\right) * \frac{\partial a_{h_1}}{\partial z_{h_1}} * \frac{\partial z_{h_1}}{\partial w_1} \\
   & =\left(\sum_i \delta_i * w_{h_i}\right) * a_{h_1} *\left(1-a_{h_1}\right) * i_1 \\
   & =\delta_{h_1} * i_1
\end{aligned}</script><p>最后更新 $w_1$ 的权值：</p>
<script type="math/tex; mode=display">
w_1^+ = w_1 - \eta * \frac {\partial E_{total}} {\partial w_1} = 0.15 - 0.5*0.000438568 = 0.149780716</script><p>同理，更新 $w_2, w_3, w_4$ 权值：</p>
<script type="math/tex; mode=display">
w_2^+ = 0.19956143 \\
w_3^+ = 0.24975114 \\
w_4^+ = 0.29950229</script><p>这样，反向传播算法就完成了，最后我们再把更新的权值重新计算，不停地迭代。在这个例子中第一次迭代之后，总误差 $E_{total}$ 由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为$[0.015912196,0.984065734](原输入为[0.01,0.99]$ ，证明效果还是不错的。</p>
<h1 id="公式推导-2"><a href="#公式推导-2" class="headerlink" title="公式推导"></a>公式推导</h1><p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225132914.jrimp1wc9o0.webp"  alt="4.png"></p>
<h2 id="符号说明-2"><a href="#符号说明-2" class="headerlink" title="符号说明"></a>符号说明</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$n_l$</td>
<td style="text-align:center">网络层数</td>
</tr>
<tr>
<td style="text-align:center">$y_j$</td>
<td style="text-align:center">输出层第 $j$ 类标签</td>
</tr>
<tr>
<td style="text-align:center">$S_l$</td>
<td style="text-align:center">第 $l$ 层神经元个数（不包括偏置项）</td>
</tr>
<tr>
<td style="text-align:center">$g(x)$</td>
<td style="text-align:center">激活函数</td>
</tr>
<tr>
<td style="text-align:center">$w_{ij}^{l}$</td>
<td style="text-align:center">第 $l-1$ 层的第 $j$ 个神经元连接到第 $l$ 层第 $i$ 个神经元的权重</td>
</tr>
<tr>
<td style="text-align:center">$b_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的偏置</td>
</tr>
<tr>
<td style="text-align:center">$z_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输入加权和</td>
</tr>
<tr>
<td style="text-align:center">$a_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输出（激活值）</td>
</tr>
<tr>
<td style="text-align:center">$\delta_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元产生的错误</td>
</tr>
</tbody>
</table>
</div>
<h2 id="推导过程-4"><a href="#推导过程-4" class="headerlink" title="推导过程"></a>推导过程</h2><h3 id="基本公式-2"><a href="#基本公式-2" class="headerlink" title="基本公式"></a>基本公式</h3><script type="math/tex; mode=display">
\begin{aligned}
   z_i^l & =\sum_k w_{i k}^l * a_k^{l-1}+b_j^l \\
   g(x) & =\frac{1}{1+e^{-x}} \\
   a_i^l & =g\left(z_i^l\right) \\
   J(\theta) & =\frac{1}{2} \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2 \\
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l}
\end{aligned}</script><h3 id="梯度方向传播公式推导-2"><a href="#梯度方向传播公式推导-2" class="headerlink" title="梯度方向传播公式推导"></a>梯度方向传播公式推导</h3><h4 id="初始条件-2"><a href="#初始条件-2" class="headerlink" title="初始条件"></a>初始条件</h4><p>以一个输入样本为例：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^{n l} & =\frac{\partial J(\theta)}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-g\left(z_j^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} \frac{\partial\left(y_i-g\left(z_i^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =-\left(y_i-a_i^{n l}\right) g^{\prime}\left(z_i^{l l}\right)
\end{aligned}</script><h4 id="递推公式-2"><a href="#递推公式-2" class="headerlink" title="递推公式"></a>递推公式</h4><script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial z_j^{l+1}}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial\left(\sum_k w_{j k}^{l+1} * a_k^l+b_j^{l+1}\right)}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} * g^{\prime}\left(z_i^l\right) \\
   & =g^{\prime}\left(z_i^l\right) * \sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} \\
   \frac{\partial J(\theta)}{\partial w_{i j}^l} & =\frac{\partial J(\theta)}{\partial z_i^l} * \frac{\partial z_i^l}{\partial w_{i j}^l} \\
   & =\delta_i^l * a_j^{l-1} \\
   \frac{\partial J(\theta)}{\partial b_i^l} & =\delta_i^l
\end{aligned}</script><h2 id="反向传播伪代码-2"><a href="#反向传播伪代码-2" class="headerlink" title="反向传播伪代码"></a>反向传播伪代码</h2><ol>
<li>输入训练集。</li>
<li>对于训练集的每个样本 $\vec x$ ，设输入层对应的激活值为 $a^l$ ：<ul>
<li>前向传播：$z^l = w^l*a^{l-1}+b^l, a^l = g(z^l)$</li>
<li>计算输出层产生的误差：$\delta^L = \frac {\partial J(\theta)} {\partial a^L} \odot g’(z^L)$</li>
<li>反向传播错误：$\delta^l = ((w^{l+1})^T*\delta^{l+1}) \odot g’(z^l)$</li>
</ul>
</li>
<li>使用梯度下降训练参数：<ul>
<li>$w^l \dashrightarrow w^l - \frac {\alpha} {m} \sum_x\delta^{x, l}*(a^{x, l-1})^T$</li>
<li>$b^l \dashrightarrow  b^l - \frac {\eta} {m} \sum_x\delta^{x, l}$</li>
</ul>
</li>
</ol>
<h1 id="交叉熵损失函数推导-2"><a href="#交叉熵损失函数推导-2" class="headerlink" title="交叉熵损失函数推导"></a>交叉熵损失函数推导</h1><p>对于多分类问题，$softmax$ 函数可以将神经网络的输出变成一个概率分布。它只是一个额外的处理层，下图展示了加上了 $softmax$ 回归的神经网络结构图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225454143.6l2hdid9suw0.webp"  alt="softmax"></p>
<p>递推公式仍然和上述递推公式保持一致。初始条件如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   J(\theta) & =-\sum_{i=1}^{S_{n l}} y^t * \ln y^p \\
   y_j^p & =\frac{e^{a_j^{n l}}}{\sum_{k=1}^{S_{n l}} e^{a_k^{n l}}}
\end{aligned}</script><p>$softmax$ 偏导数计算：</p>
<script type="math/tex; mode=display">
\frac {\partial y_j^p} {\partial a_i^{nl}} =
\left\{
\begin{aligned}
-y_i^p*y_j^p \qquad i \neq j \\
y_i^p*(1-y_i^p) i = j
\end{aligned}
\right.</script><h2 id="推导过程-5"><a href="#推导过程-5" class="headerlink" title="推导过程"></a>推导过程</h2><script type="math/tex; mode=display">
\begin{aligned}
   & \frac{\partial J(\theta)}{\partial z_i^{n l}}=\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial a_i^{n l}}=\sum_{j=1}^{S_{n l}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial y_j^p}=-\frac{y_j^t}{y_j^p}
\end{aligned}</script><p>   由上可知:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial a_i^{n l}} & =\frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}}+\sum_{j \neq i}^{S_{n t}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & =-\frac{y_i^t}{y_i^p} * y_i^p *\left(1-y_i^p\right)+\sum_{j \neq i}^{S_{n l}}-\frac{y_j^t}{y_j^p} *\left(-y_j^p * y_i^p\right) \\
   & =-y_i^t+y_i^t * y_i^p+\sum_{j \neq i}^{S_{n l l}}\left(y_j^t * y_i^p\right) \\
   & =-y_i^t+\sum_j^{S_{n+1}}\left(y_j^t * y_i^p\right) \\
   & =y_i^p-y_i^t
\end{aligned}</script><p>   $\therefore$ 反向传播选代算法的初始值为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial z_i^{n l}} & =\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & =\left(y_i^p-y_i^t\right) * a_i^{n l} *\left(1-a_i^{n l}\right)
\end{aligned}</script><hr>
<h2 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/charlotte77/p/5629865.html?tdsourcetag=s_pcqq_aiomsg" >一文弄懂神经网络中的反向传播法——BackPropagation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/u014313009/article/details/51039334" >反向传播算法（过程及公式推导）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/nowgood/p/backprop2.html?tdsourcetag=s_pcqq_aiomsg" >反向传播公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = \frac {\partial E_{o_1}} {\partial a_{h_1}} + \frac {\partial E_{o_2}} {\partial a_{h_1}}</script><p>先计算 $\frac {\partial E<em>{o_1}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{o_1}}{\partial a_{h_1}} & =\frac{\partial E_{o_1}}{\partial a_{o_1}} * \frac{\partial a_{o_1}}{\partial z_{o_1}} * \frac{\partial z_{o_1}}{\partial a_{h_1}} \\
   & =0.741365069 * 0.186815602 * 0.4 \\
   & =0.055399425
\end{aligned}</script><p>同理可得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{o_2}} {\partial a_{h_1}} = -0.019049119</script><p>两者相加得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = 0.055399425 - 0.019049119 = 0.036350306</script><p>计算 $\frac {a<em>{h_1}} {z</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {a_{h_1}} {z_{h_1}} = a_{h_1} * (1-a_{h_1}) = 0.593269992*(1-0.593269992) = 0.2413007086</script><p>计算 $\frac {\partial z_{h_1}} {\partial w_1}$</p>
<script type="math/tex; mode=display">
\frac {\partial z_{h_1}} {\partial w_1} = i_1 = 0.05</script><p>最后三者相互乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_1} = 0.036350306 * 0.2413007086 * 0.05 = 0.000438568</script><p>为了简化公式，用 $\delta_{h_1}$ 表示隐藏层单元 $h_1$ 的误差： </p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{\text {total }}}{\partial w_1} & =\left(\sum_i \frac{\partial E_{\text {total }}}{\partial a_i} * \frac{\partial a_i}{\partial z_i} * \frac{\partial z_i}{\partial h_1}\right) * \frac{\partial a_{h_1}}{\partial z_{h_1}} * \frac{\partial z_{h_1}}{\partial w_1} \\
   & =\left(\sum_i \delta_i * w_{h_i}\right) * a_{h_1} *\left(1-a_{h_1}\right) * i_1 \\
   & =\delta_{h_1} * i_1
\end{aligned}</script><p>最后更新 $w_1$ 的权值：</p>
<script type="math/tex; mode=display">
w_1^+ = w_1 - \eta * \frac {\partial E_{total}} {\partial w_1} = 0.15 - 0.5*0.000438568 = 0.149780716</script><p>同理，更新 $w_2, w_3, w_4$ 权值：</p>
<script type="math/tex; mode=display">
w_2^+ = 0.19956143 \\
w_3^+ = 0.24975114 \\
w_4^+ = 0.29950229</script><p>这样，反向传播算法就完成了，最后我们再把更新的权值重新计算，不停地迭代。在这个例子中第一次迭代之后，总误差 $E_{total}$ 由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为$[0.015912196,0.984065734](原输入为[0.01,0.99]$ ，证明效果还是不错的。</p>
<h1 id="公式推导-3"><a href="#公式推导-3" class="headerlink" title="公式推导"></a>公式推导</h1><p>误差反向传播算法简称反向传播算法(Back Propagation)。使用反向传播算法的多层感知器又称为BP神经网络。</p>
<!--more-->
<p>BP算法是一个迭代算法，它的基本思想如下：</p>
<ol>
<li>将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。</li>
<li>由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；</li>
<li>在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。</li>
<li>迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。</li>
</ol>
<h1 id="示例-4"><a href="#示例-4" class="headerlink" title="示例"></a>示例</h1><p>有如下一个神经网络：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224358906.1bsruizweubk.webp"  alt="1.png"></p>
<p>第一层是输入层，包含两个神经元 $i_1$，$i_2$ 和偏置项 $b_1$；第二层是隐藏层，包含两个神经元 $h_1$，$h_2$ 和偏置项 $b_2$；第三层是输出 $o_1$，$o_2$。每条线上标的 $w_i$ 是层与层之间连接的权重。激活函数是 $sigmod$ 函数。我们用 $z$ 表示某神经元的加权输入和；用 $a$ 表示某神经元的输出。</p>
<p>上述各参数赋值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$i_1$</td>
<td style="text-align:center">0.05</td>
</tr>
<tr>
<td style="text-align:center">$i_2$</td>
<td style="text-align:center">0.10</td>
</tr>
<tr>
<td style="text-align:center">$w_1$</td>
<td style="text-align:center">0.15</td>
</tr>
<tr>
<td style="text-align:center">$w_2$</td>
<td style="text-align:center">0.20</td>
</tr>
<tr>
<td style="text-align:center">$w_3$</td>
<td style="text-align:center">0.25</td>
</tr>
<tr>
<td style="text-align:center">$w_4$</td>
<td style="text-align:center">0.30</td>
</tr>
<tr>
<td style="text-align:center">$w_5$</td>
<td style="text-align:center">0.40</td>
</tr>
<tr>
<td style="text-align:center">$w_6$</td>
<td style="text-align:center">0.45</td>
</tr>
<tr>
<td style="text-align:center">$w_7$</td>
<td style="text-align:center">0.50</td>
</tr>
<tr>
<td style="text-align:center">$w_8$</td>
<td style="text-align:center">0.55</td>
</tr>
<tr>
<td style="text-align:center">$b_1$</td>
<td style="text-align:center">0.35</td>
</tr>
<tr>
<td style="text-align:center">$b_2$</td>
<td style="text-align:center">0.60</td>
</tr>
<tr>
<td style="text-align:center">$o_1$</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">$o_2$</td>
<td style="text-align:center">0.99</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Step-1-前向传播-4"><a href="#Step-1-前向传播-4" class="headerlink" title="Step 1 前向传播"></a>Step 1 前向传播</h2><h3 id="输入层-—-gt-隐藏层-4"><a href="#输入层-—-gt-隐藏层-4" class="headerlink" title="输入层 —-&gt; 隐藏层"></a>输入层 —-&gt; 隐藏层</h3><p>神经元 $h_1$ 的输入加权和：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{h 1} & =w_1 * i_1+w_2 * i_2+b_1 * 1 \\
   & =0.15 * 0.05+0.2 * 0.1+0.35 * 1 \\
   & =0.3775
\end{aligned}</script><p>神经元 $h<em>1$ 的输出 $a</em>{h1}$ ：</p>
<script type="math/tex; mode=display">
a_{h1} = \frac{1}{1+e^{-z_{h1}}} = \frac{1}{1+e^{-0.3775}} = 0.593269992</script><p>同理可得，神经元 $h<em>2$ 的输出 $a</em>{h2}$ ：</p>
<script type="math/tex; mode=display">
a_{h2} = 0.596884378</script><h3 id="隐藏层-—-gt-输出层-4"><a href="#隐藏层-—-gt-输出层-4" class="headerlink" title="隐藏层 —-&gt; 输出层"></a>隐藏层 —-&gt; 输出层</h3><p>计算输出层神经元 $o1$ 和 $o2$ 的值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{o 1} & =w_5 * a_{h 1}+w_6 * a_{h 2}+b_2 * 1 \\
   & =0.4 * 0.593269992+0.45 * 0.596884378+0.6 * 1 \\
   & =1.105905967 \\
   a_{o 1} & =\frac{1}{1+e^{-z_{o 1}}} \\
   & =\frac{1}{1+e^{-1.105905967}} \\
   & =0.751365069 \\
   a_{o 2} & =0.772928465
\end{aligned}</script><p>前向传播的过程就结束了，我们得到的输出值是 $[0.751365069, 0.772928465]$ ，与实际值 $[0.01, 0.99]$ 相差还很远。接下来我们对误差进行反向传播，更新权值，重新计算输出。</p>
<h2 id="Step-2-反向传播-4"><a href="#Step-2-反向传播-4" class="headerlink" title="Step 2 反向传播"></a>Step 2 反向传播</h2><ol>
<li>计算损失函数：</li>
</ol>
<script type="math/tex; mode=display">
E_{total} = \sum\frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $o_1$ 和 $o_2$ 的损失值，总误差为两者之和：</p>
<script type="math/tex; mode=display">
E_{o_1} = \frac {1}{2}(0.01 - 0.751365069)^2 = 0.274811083 \\
E_{o_2} = \frac {1}{2}(0.99 - 0.772928465)^2 = 0.023560026 \\
E_{total} = E_{o_1} + E_{o_2} = 0.274811083 + 0.023560026 = 0.298371109</script><ol>
<li>隐藏层 —-&gt; 输出层的权值更新</li>
</ol>
<p>以权重参数 $w_5$ 为例，如果我们想知道 $w_5$ 对整体损失产生了多少影响，可以用整体损失对 $w_5$ 求偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w_5} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} }*{ \frac {\partial z_{o_1}} {\partial w_5} }</script><p>下面的图可以更直观了解误差是如何反向传播的：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224659584.31gxu503n220.webp"  alt="2.png"></p>
<p>我们现在分别来计算每个式子的值：</p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
E_{total} = \frac {1}{2}(target_{o_1} - a_{o_1})^2 + \frac {1}{2}(target_{o_2} - a_{o_1})^2 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = 2 * \frac {1}{2} (target_{o_1} - a_{o_1})*-1 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = -(target_{o_1} - a_{o_1}) = 0.751365069-0.01=0.741365069 \\</script><p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
a_{o_1} = \frac {1}{1+e^{-z_{o_1}}} \\
\frac {\partial a_{o_1}} {\partial z_{o_1}} = a_{o_1}*(1-a_{o_1}) = 0.751365069*(1-0.751365069) = 0.186815602</script><p>计算 $\frac {\partial z_{o_1}} {\partial w_5}$ ：</p>
<script type="math/tex; mode=display">
z_{o_1} = w_5*a_{h1} + w_6*a_{h2} + b_2*1 \\
\frac {\partial z_{o_1}} {\partial w_5} = a_{h_1} = 0.593269992</script><p>最后三者相乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = 0.741365069*0.186815602*0.593269992 = 0.082167041</script><p>这样我们就算出整体损失 $E_{total}$ 对 $w_5$ 的偏导值。</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1}) * a_{h_1}</script><p>针对上述公式，为了表达方便，使用 $\delta_{o_1}$ 来表示输出层的误差：</p>
<script type="math/tex; mode=display">
\delta_{o_1} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} } = \frac {\partial E_{total}} {\partial z_{o_1}} \\
\delta_{o_1} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1})</script><p>因此整体损失 $E_{total}$ 对 $w_5$ 的偏导值可以表示为：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}}{\partial w_5} = \delta_{o_1}*a_{h_1}</script><p>最后我们来更新 $w_5$ 的值：</p>
<script type="math/tex; mode=display">
w_5^+ = w_5 - \eta * \frac {\partial E_{total}} {\partial w_5} = 0.4 - 0.5*0.082167041 = 0.35891648 \qquad \eta: 学习率</script><p>同理可更新 $w_6, w_7, w_8$ ：</p>
<script type="math/tex; mode=display">
w_6^+ = 0.408666186 \\
w_7^+ = 0.511301270 \\
w_8^+ = 0.561370121</script><ol>
<li>隐藏层 —-&gt; 隐藏层的权值更新：</li>
</ol>
<p>计算 $\frac {\partial E_{total}} {\partial w_1}$ 与上述方法类似，但需要注意下图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224736113.1umei2jgzokg.webp"  alt="3.png"></p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = \frac {\partial E_{o_1}} {\partial a_{h_1}} + \frac {\partial E_{o_2}} {\partial a_{h_1}}</script><p>先计算 $\frac {\partial E<em>{o_1}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{o_1}}{\partial a_{h_1}} & =\frac{\partial E_{o_1}}{\partial a_{o_1}} * \frac{\partial a_{o_1}}{\partial z_{o_1}} * \frac{\partial z_{o_1}}{\partial a_{h_1}} \\
   & =0.741365069 * 0.186815602 * 0.4 \\
   & =0.055399425
\end{aligned}</script><p>同理可得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{o_2}} {\partial a_{h_1}} = -0.019049119</script><p>两者相加得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = 0.055399425 - 0.019049119 = 0.036350306</script><p>计算 $\frac {a<em>{h_1}} {z</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {a_{h_1}} {z_{h_1}} = a_{h_1} * (1-a_{h_1}) = 0.593269992*(1-0.593269992) = 0.2413007086</script><p>计算 $\frac {\partial z_{h_1}} {\partial w_1}$</p>
<script type="math/tex; mode=display">
\frac {\partial z_{h_1}} {\partial w_1} = i_1 = 0.05</script><p>最后三者相互乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_1} = 0.036350306 * 0.2413007086 * 0.05 = 0.000438568</script><p>为了简化公式，用 $\delta_{h_1}$ 表示隐藏层单元 $h_1$ 的误差： </p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{\text {total }}}{\partial w_1} & =\left(\sum_i \frac{\partial E_{\text {total }}}{\partial a_i} * \frac{\partial a_i}{\partial z_i} * \frac{\partial z_i}{\partial h_1}\right) * \frac{\partial a_{h_1}}{\partial z_{h_1}} * \frac{\partial z_{h_1}}{\partial w_1} \\
   & =\left(\sum_i \delta_i * w_{h_i}\right) * a_{h_1} *\left(1-a_{h_1}\right) * i_1 \\
   & =\delta_{h_1} * i_1
\end{aligned}</script><p>最后更新 $w_1$ 的权值：</p>
<script type="math/tex; mode=display">
w_1^+ = w_1 - \eta * \frac {\partial E_{total}} {\partial w_1} = 0.15 - 0.5*0.000438568 = 0.149780716</script><p>同理，更新 $w_2, w_3, w_4$ 权值：</p>
<script type="math/tex; mode=display">
w_2^+ = 0.19956143 \\
w_3^+ = 0.24975114 \\
w_4^+ = 0.29950229</script><p>这样，反向传播算法就完成了，最后我们再把更新的权值重新计算，不停地迭代。在这个例子中第一次迭代之后，总误差 $E_{total}$ 由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为$[0.015912196,0.984065734](原输入为[0.01,0.99]$ ，证明效果还是不错的。</p>
<h1 id="公式推导-4"><a href="#公式推导-4" class="headerlink" title="公式推导"></a>公式推导</h1><p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225132914.jrimp1wc9o0.webp"  alt="4.png"></p>
<h2 id="符号说明-3"><a href="#符号说明-3" class="headerlink" title="符号说明"></a>符号说明</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$n_l$</td>
<td style="text-align:center">网络层数</td>
</tr>
<tr>
<td style="text-align:center">$y_j$</td>
<td style="text-align:center">输出层第 $j$ 类标签</td>
</tr>
<tr>
<td style="text-align:center">$S_l$</td>
<td style="text-align:center">第 $l$ 层神经元个数（不包括偏置项）</td>
</tr>
<tr>
<td style="text-align:center">$g(x)$</td>
<td style="text-align:center">激活函数</td>
</tr>
<tr>
<td style="text-align:center">$w_{ij}^{l}$</td>
<td style="text-align:center">第 $l-1$ 层的第 $j$ 个神经元连接到第 $l$ 层第 $i$ 个神经元的权重</td>
</tr>
<tr>
<td style="text-align:center">$b_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的偏置</td>
</tr>
<tr>
<td style="text-align:center">$z_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输入加权和</td>
</tr>
<tr>
<td style="text-align:center">$a_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输出（激活值）</td>
</tr>
<tr>
<td style="text-align:center">$\delta_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元产生的错误</td>
</tr>
</tbody>
</table>
</div>
<h2 id="推导过程-6"><a href="#推导过程-6" class="headerlink" title="推导过程"></a>推导过程</h2><h3 id="基本公式-3"><a href="#基本公式-3" class="headerlink" title="基本公式"></a>基本公式</h3><script type="math/tex; mode=display">
\begin{aligned}
   z_i^l & =\sum_k w_{i k}^l * a_k^{l-1}+b_j^l \\
   g(x) & =\frac{1}{1+e^{-x}} \\
   a_i^l & =g\left(z_i^l\right) \\
   J(\theta) & =\frac{1}{2} \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2 \\
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l}
\end{aligned}</script><h3 id="梯度方向传播公式推导-3"><a href="#梯度方向传播公式推导-3" class="headerlink" title="梯度方向传播公式推导"></a>梯度方向传播公式推导</h3><h4 id="初始条件-3"><a href="#初始条件-3" class="headerlink" title="初始条件"></a>初始条件</h4><p>以一个输入样本为例：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^{n l} & =\frac{\partial J(\theta)}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-g\left(z_j^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} \frac{\partial\left(y_i-g\left(z_i^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =-\left(y_i-a_i^{n l}\right) g^{\prime}\left(z_i^{l l}\right)
\end{aligned}</script><h4 id="递推公式-3"><a href="#递推公式-3" class="headerlink" title="递推公式"></a>递推公式</h4><script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial z_j^{l+1}}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial\left(\sum_k w_{j k}^{l+1} * a_k^l+b_j^{l+1}\right)}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} * g^{\prime}\left(z_i^l\right) \\
   & =g^{\prime}\left(z_i^l\right) * \sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} \\
   \frac{\partial J(\theta)}{\partial w_{i j}^l} & =\frac{\partial J(\theta)}{\partial z_i^l} * \frac{\partial z_i^l}{\partial w_{i j}^l} \\
   & =\delta_i^l * a_j^{l-1} \\
   \frac{\partial J(\theta)}{\partial b_i^l} & =\delta_i^l
\end{aligned}</script><h2 id="反向传播伪代码-3"><a href="#反向传播伪代码-3" class="headerlink" title="反向传播伪代码"></a>反向传播伪代码</h2><ol>
<li>输入训练集。</li>
<li>对于训练集的每个样本 $\vec x$ ，设输入层对应的激活值为 $a^l$ ：<ul>
<li>前向传播：$z^l = w^l*a^{l-1}+b^l, a^l = g(z^l)$</li>
<li>计算输出层产生的误差：$\delta^L = \frac {\partial J(\theta)} {\partial a^L} \odot g’(z^L)$</li>
<li>反向传播错误：$\delta^l = ((w^{l+1})^T*\delta^{l+1}) \odot g’(z^l)$</li>
</ul>
</li>
<li>使用梯度下降训练参数：<ul>
<li>$w^l \dashrightarrow w^l - \frac {\alpha} {m} \sum_x\delta^{x, l}*(a^{x, l-1})^T$</li>
<li>$b^l \dashrightarrow  b^l - \frac {\eta} {m} \sum_x\delta^{x, l}$</li>
</ul>
</li>
</ol>
<h1 id="交叉熵损失函数推导-3"><a href="#交叉熵损失函数推导-3" class="headerlink" title="交叉熵损失函数推导"></a>交叉熵损失函数推导</h1><p>对于多分类问题，$softmax$ 函数可以将神经网络的输出变成一个概率分布。它只是一个额外的处理层，下图展示了加上了 $softmax$ 回归的神经网络结构图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225454143.6l2hdid9suw0.webp"  alt="softmax"></p>
<p>递推公式仍然和上述递推公式保持一致。初始条件如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   J(\theta) & =-\sum_{i=1}^{S_{n l}} y^t * \ln y^p \\
   y_j^p & =\frac{e^{a_j^{n l}}}{\sum_{k=1}^{S_{n l}} e^{a_k^{n l}}}
\end{aligned}</script><p>$softmax$ 偏导数计算：</p>
<script type="math/tex; mode=display">
\frac {\partial y_j^p} {\partial a_i^{nl}} =
\left\{
\begin{aligned}
-y_i^p*y_j^p \qquad i \neq j \\
y_i^p*(1-y_i^p) i = j
\end{aligned}
\right.</script><h2 id="推导过程-7"><a href="#推导过程-7" class="headerlink" title="推导过程"></a>推导过程</h2><script type="math/tex; mode=display">
\begin{aligned}
   & \frac{\partial J(\theta)}{\partial z_i^{n l}}=\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial a_i^{n l}}=\sum_{j=1}^{S_{n l}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial y_j^p}=-\frac{y_j^t}{y_j^p}
\end{aligned}</script><p>   由上可知:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial a_i^{n l}} & =\frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}}+\sum_{j \neq i}^{S_{n t}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & =-\frac{y_i^t}{y_i^p} * y_i^p *\left(1-y_i^p\right)+\sum_{j \neq i}^{S_{n l}}-\frac{y_j^t}{y_j^p} *\left(-y_j^p * y_i^p\right) \\
   & =-y_i^t+y_i^t * y_i^p+\sum_{j \neq i}^{S_{n l l}}\left(y_j^t * y_i^p\right) \\
   & =-y_i^t+\sum_j^{S_{n+1}}\left(y_j^t * y_i^p\right) \\
   & =y_i^p-y_i^t
\end{aligned}</script><p>   $\therefore$ 反向传播选代算法的初始值为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial z_i^{n l}} & =\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & =\left(y_i^p-y_i^t\right) * a_i^{n l} *\left(1-a_i^{n l}\right)
\end{aligned}</script><hr>
<h2 id="参考-3"><a href="#参考-3" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/charlotte77/p/5629865.html?tdsourcetag=s_pcqq_aiomsg" >一文弄懂神经网络中的反向传播法——BackPropagation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/u014313009/article/details/51039334" >反向传播算法（过程及公式推导）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/nowgood/p/backprop2.html?tdsourcetag=s_pcqq_aiomsg" >反向传播公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<h2 id="符号说明-4"><a href="#符号说明-4" class="headerlink" title="符号说明"></a>符号说明</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$n_l$</td>
<td style="text-align:center">网络层数</td>
</tr>
<tr>
<td style="text-align:center">$y_j$</td>
<td style="text-align:center">输出层第 $j$ 类标签</td>
</tr>
<tr>
<td style="text-align:center">$S_l$</td>
<td style="text-align:center">第 $l$ 层神经元个数（不包括偏置项）</td>
</tr>
<tr>
<td style="text-align:center">$g(x)$</td>
<td style="text-align:center">激活函数</td>
</tr>
<tr>
<td style="text-align:center">$w_{ij}^{l}$</td>
<td style="text-align:center">第 $l-1$ 层的第 $j$ 个神经元连接到第 $l$ 层第 $i$ 个神经元的权重</td>
</tr>
<tr>
<td style="text-align:center">$b_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的偏置</td>
</tr>
<tr>
<td style="text-align:center">$z_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输入加权和</td>
</tr>
<tr>
<td style="text-align:center">$a_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输出（激活值）</td>
</tr>
<tr>
<td style="text-align:center">$\delta_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元产生的错误</td>
</tr>
</tbody>
</table>
</div>
<h2 id="推导过程-8"><a href="#推导过程-8" class="headerlink" title="推导过程"></a>推导过程</h2><h3 id="基本公式-4"><a href="#基本公式-4" class="headerlink" title="基本公式"></a>基本公式</h3><script type="math/tex; mode=display">
\begin{aligned}
   z_i^l & =\sum_k w_{i k}^l * a_k^{l-1}+b_j^l \\
   g(x) & =\frac{1}{1+e^{-x}} \\
   a_i^l & =g\left(z_i^l\right) \\
   J(\theta) & =\frac{1}{2} \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2 \\
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l}
\end{aligned}</script><h3 id="梯度方向传播公式推导-4"><a href="#梯度方向传播公式推导-4" class="headerlink" title="梯度方向传播公式推导"></a>梯度方向传播公式推导</h3><h4 id="初始条件-4"><a href="#初始条件-4" class="headerlink" title="初始条件"></a>初始条件</h4><p>以一个输入样本为例：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^{n l} & =\frac{\partial J(\theta)}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-g\left(z_j^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} \frac{\partial\left(y_i-g\left(z_i^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =-\left(y_i-a_i^{n l}\right) g^{\prime}\left(z_i^{l l}\right)
\end{aligned}</script><h4 id="递推公式-4"><a href="#递推公式-4" class="headerlink" title="递推公式"></a>递推公式</h4><script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial z_j^{l+1}}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial\left(\sum_k w_{j k}^{l+1} * a_k^l+b_j^{l+1}\right)}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} * g^{\prime}\left(z_i^l\right) \\
   & =g^{\prime}\left(z_i^l\right) * \sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} \\
   \frac{\partial J(\theta)}{\partial w_{i j}^l} & =\frac{\partial J(\theta)}{\partial z_i^l} * \frac{\partial z_i^l}{\partial w_{i j}^l} \\
   & =\delta_i^l * a_j^{l-1} \\
   \frac{\partial J(\theta)}{\partial b_i^l} & =\delta_i^l
\end{aligned}</script><h2 id="反向传播伪代码-4"><a href="#反向传播伪代码-4" class="headerlink" title="反向传播伪代码"></a>反向传播伪代码</h2><ol>
<li>输入训练集。</li>
<li>对于训练集的每个样本 $\vec x$ ，设输入层对应的激活值为 $a^l$ ：<ul>
<li>前向传播：$z^l = w^l*a^{l-1}+b^l, a^l = g(z^l)$</li>
<li>计算输出层产生的误差：$\delta^L = \frac {\partial J(\theta)} {\partial a^L} \odot g’(z^L)$</li>
<li>反向传播错误：$\delta^l = ((w^{l+1})^T*\delta^{l+1}) \odot g’(z^l)$</li>
</ul>
</li>
<li>使用梯度下降训练参数：<ul>
<li>$w^l \dashrightarrow w^l - \frac {\alpha} {m} \sum_x\delta^{x, l}*(a^{x, l-1})^T$</li>
<li>$b^l \dashrightarrow  b^l - \frac {\eta} {m} \sum_x\delta^{x, l}$</li>
</ul>
</li>
</ol>
<h1 id="交叉熵损失函数推导-4"><a href="#交叉熵损失函数推导-4" class="headerlink" title="交叉熵损失函数推导"></a>交叉熵损失函数推导</h1><p>对于多分类问题，$softmax$ 函数可以将神经网络的输出变成一个概率分布。它只是一个额外的处理层，下图展示了加上了 $softmax$ 回归的神经网络结构图：</p>
<p>误差反向传播算法简称反向传播算法(Back Propagation)。使用反向传播算法的多层感知器又称为BP神经网络。</p>
<!--more-->
<p>BP算法是一个迭代算法，它的基本思想如下：</p>
<ol>
<li>将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。</li>
<li>由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；</li>
<li>在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。</li>
<li>迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。</li>
</ol>
<h1 id="示例-5"><a href="#示例-5" class="headerlink" title="示例"></a>示例</h1><p>有如下一个神经网络：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224358906.1bsruizweubk.webp"  alt="1.png"></p>
<p>第一层是输入层，包含两个神经元 $i_1$，$i_2$ 和偏置项 $b_1$；第二层是隐藏层，包含两个神经元 $h_1$，$h_2$ 和偏置项 $b_2$；第三层是输出 $o_1$，$o_2$。每条线上标的 $w_i$ 是层与层之间连接的权重。激活函数是 $sigmod$ 函数。我们用 $z$ 表示某神经元的加权输入和；用 $a$ 表示某神经元的输出。</p>
<p>上述各参数赋值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$i_1$</td>
<td style="text-align:center">0.05</td>
</tr>
<tr>
<td style="text-align:center">$i_2$</td>
<td style="text-align:center">0.10</td>
</tr>
<tr>
<td style="text-align:center">$w_1$</td>
<td style="text-align:center">0.15</td>
</tr>
<tr>
<td style="text-align:center">$w_2$</td>
<td style="text-align:center">0.20</td>
</tr>
<tr>
<td style="text-align:center">$w_3$</td>
<td style="text-align:center">0.25</td>
</tr>
<tr>
<td style="text-align:center">$w_4$</td>
<td style="text-align:center">0.30</td>
</tr>
<tr>
<td style="text-align:center">$w_5$</td>
<td style="text-align:center">0.40</td>
</tr>
<tr>
<td style="text-align:center">$w_6$</td>
<td style="text-align:center">0.45</td>
</tr>
<tr>
<td style="text-align:center">$w_7$</td>
<td style="text-align:center">0.50</td>
</tr>
<tr>
<td style="text-align:center">$w_8$</td>
<td style="text-align:center">0.55</td>
</tr>
<tr>
<td style="text-align:center">$b_1$</td>
<td style="text-align:center">0.35</td>
</tr>
<tr>
<td style="text-align:center">$b_2$</td>
<td style="text-align:center">0.60</td>
</tr>
<tr>
<td style="text-align:center">$o_1$</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">$o_2$</td>
<td style="text-align:center">0.99</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Step-1-前向传播-5"><a href="#Step-1-前向传播-5" class="headerlink" title="Step 1 前向传播"></a>Step 1 前向传播</h2><h3 id="输入层-—-gt-隐藏层-5"><a href="#输入层-—-gt-隐藏层-5" class="headerlink" title="输入层 —-&gt; 隐藏层"></a>输入层 —-&gt; 隐藏层</h3><p>神经元 $h_1$ 的输入加权和：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{h 1} & =w_1 * i_1+w_2 * i_2+b_1 * 1 \\
   & =0.15 * 0.05+0.2 * 0.1+0.35 * 1 \\
   & =0.3775
\end{aligned}</script><p>神经元 $h<em>1$ 的输出 $a</em>{h1}$ ：</p>
<script type="math/tex; mode=display">
a_{h1} = \frac{1}{1+e^{-z_{h1}}} = \frac{1}{1+e^{-0.3775}} = 0.593269992</script><p>同理可得，神经元 $h<em>2$ 的输出 $a</em>{h2}$ ：</p>
<script type="math/tex; mode=display">
a_{h2} = 0.596884378</script><h3 id="隐藏层-—-gt-输出层-5"><a href="#隐藏层-—-gt-输出层-5" class="headerlink" title="隐藏层 —-&gt; 输出层"></a>隐藏层 —-&gt; 输出层</h3><p>计算输出层神经元 $o1$ 和 $o2$ 的值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{o 1} & =w_5 * a_{h 1}+w_6 * a_{h 2}+b_2 * 1 \\
   & =0.4 * 0.593269992+0.45 * 0.596884378+0.6 * 1 \\
   & =1.105905967 \\
   a_{o 1} & =\frac{1}{1+e^{-z_{o 1}}} \\
   & =\frac{1}{1+e^{-1.105905967}} \\
   & =0.751365069 \\
   a_{o 2} & =0.772928465
\end{aligned}</script><p>前向传播的过程就结束了，我们得到的输出值是 $[0.751365069, 0.772928465]$ ，与实际值 $[0.01, 0.99]$ 相差还很远。接下来我们对误差进行反向传播，更新权值，重新计算输出。</p>
<h2 id="Step-2-反向传播-5"><a href="#Step-2-反向传播-5" class="headerlink" title="Step 2 反向传播"></a>Step 2 反向传播</h2><ol>
<li>计算损失函数：</li>
</ol>
<script type="math/tex; mode=display">
E_{total} = \sum\frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $o_1$ 和 $o_2$ 的损失值，总误差为两者之和：</p>
<script type="math/tex; mode=display">
E_{o_1} = \frac {1}{2}(0.01 - 0.751365069)^2 = 0.274811083 \\
E_{o_2} = \frac {1}{2}(0.99 - 0.772928465)^2 = 0.023560026 \\
E_{total} = E_{o_1} + E_{o_2} = 0.274811083 + 0.023560026 = 0.298371109</script><ol>
<li>隐藏层 —-&gt; 输出层的权值更新</li>
</ol>
<p>以权重参数 $w_5$ 为例，如果我们想知道 $w_5$ 对整体损失产生了多少影响，可以用整体损失对 $w_5$ 求偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w_5} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} }*{ \frac {\partial z_{o_1}} {\partial w_5} }</script><p>下面的图可以更直观了解误差是如何反向传播的：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224659584.31gxu503n220.webp"  alt="2.png"></p>
<p>我们现在分别来计算每个式子的值：</p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
E_{total} = \frac {1}{2}(target_{o_1} - a_{o_1})^2 + \frac {1}{2}(target_{o_2} - a_{o_1})^2 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = 2 * \frac {1}{2} (target_{o_1} - a_{o_1})*-1 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = -(target_{o_1} - a_{o_1}) = 0.751365069-0.01=0.741365069 \\</script><p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{o_1}}$ ：</p>
<script type="math/tex; mode=display">
a_{o_1} = \frac {1}{1+e^{-z_{o_1}}} \\
\frac {\partial a_{o_1}} {\partial z_{o_1}} = a_{o_1}*(1-a_{o_1}) = 0.751365069*(1-0.751365069) = 0.186815602</script><p>计算 $\frac {\partial z_{o_1}} {\partial w_5}$ ：</p>
<script type="math/tex; mode=display">
z_{o_1} = w_5*a_{h1} + w_6*a_{h2} + b_2*1 \\
\frac {\partial z_{o_1}} {\partial w_5} = a_{h_1} = 0.593269992</script><p>最后三者相乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = 0.741365069*0.186815602*0.593269992 = 0.082167041</script><p>这样我们就算出整体损失 $E_{total}$ 对 $w_5$ 的偏导值。</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1}) * a_{h_1}</script><p>针对上述公式，为了表达方便，使用 $\delta_{o_1}$ 来表示输出层的误差：</p>
<script type="math/tex; mode=display">
\delta_{o_1} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} } = \frac {\partial E_{total}} {\partial z_{o_1}} \\
\delta_{o_1} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1})</script><p>因此整体损失 $E_{total}$ 对 $w_5$ 的偏导值可以表示为：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}}{\partial w_5} = \delta_{o_1}*a_{h_1}</script><p>最后我们来更新 $w_5$ 的值：</p>
<script type="math/tex; mode=display">
w_5^+ = w_5 - \eta * \frac {\partial E_{total}} {\partial w_5} = 0.4 - 0.5*0.082167041 = 0.35891648 \qquad \eta: 学习率</script><p>同理可更新 $w_6, w_7, w_8$ ：</p>
<script type="math/tex; mode=display">
w_6^+ = 0.408666186 \\
w_7^+ = 0.511301270 \\
w_8^+ = 0.561370121</script><ol>
<li>隐藏层 —-&gt; 隐藏层的权值更新：</li>
</ol>
<p>计算 $\frac {\partial E_{total}} {\partial w_1}$ 与上述方法类似，但需要注意下图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224736113.1umei2jgzokg.webp"  alt="3.png"></p>
<p>计算 $\frac {\partial E<em>{total}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = \frac {\partial E_{o_1}} {\partial a_{h_1}} + \frac {\partial E_{o_2}} {\partial a_{h_1}}</script><p>先计算 $\frac {\partial E<em>{o_1}} {\partial a</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{o_1}}{\partial a_{h_1}} & =\frac{\partial E_{o_1}}{\partial a_{o_1}} * \frac{\partial a_{o_1}}{\partial z_{o_1}} * \frac{\partial z_{o_1}}{\partial a_{h_1}} \\
   & =0.741365069 * 0.186815602 * 0.4 \\
   & =0.055399425
\end{aligned}</script><p>同理可得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{o_2}} {\partial a_{h_1}} = -0.019049119</script><p>两者相加得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = 0.055399425 - 0.019049119 = 0.036350306</script><p>计算 $\frac {a<em>{h_1}} {z</em>{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {a_{h_1}} {z_{h_1}} = a_{h_1} * (1-a_{h_1}) = 0.593269992*(1-0.593269992) = 0.2413007086</script><p>计算 $\frac {\partial z_{h_1}} {\partial w_1}$</p>
<script type="math/tex; mode=display">
\frac {\partial z_{h_1}} {\partial w_1} = i_1 = 0.05</script><p>最后三者相互乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_1} = 0.036350306 * 0.2413007086 * 0.05 = 0.000438568</script><p>为了简化公式，用 $\delta_{h_1}$ 表示隐藏层单元 $h_1$ 的误差： </p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{\text {total }}}{\partial w_1} & =\left(\sum_i \frac{\partial E_{\text {total }}}{\partial a_i} * \frac{\partial a_i}{\partial z_i} * \frac{\partial z_i}{\partial h_1}\right) * \frac{\partial a_{h_1}}{\partial z_{h_1}} * \frac{\partial z_{h_1}}{\partial w_1} \\
   & =\left(\sum_i \delta_i * w_{h_i}\right) * a_{h_1} *\left(1-a_{h_1}\right) * i_1 \\
   & =\delta_{h_1} * i_1
\end{aligned}</script><p>最后更新 $w_1$ 的权值：</p>
<script type="math/tex; mode=display">
w_1^+ = w_1 - \eta * \frac {\partial E_{total}} {\partial w_1} = 0.15 - 0.5*0.000438568 = 0.149780716</script><p>同理，更新 $w_2, w_3, w_4$ 权值：</p>
<script type="math/tex; mode=display">
w_2^+ = 0.19956143 \\
w_3^+ = 0.24975114 \\
w_4^+ = 0.29950229</script><p>这样，反向传播算法就完成了，最后我们再把更新的权值重新计算，不停地迭代。在这个例子中第一次迭代之后，总误差 $E_{total}$ 由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为$[0.015912196,0.984065734](原输入为[0.01,0.99]$ ，证明效果还是不错的。</p>
<h1 id="公式推导-5"><a href="#公式推导-5" class="headerlink" title="公式推导"></a>公式推导</h1><p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225132914.jrimp1wc9o0.webp"  alt="4.png"></p>
<h2 id="符号说明-5"><a href="#符号说明-5" class="headerlink" title="符号说明"></a>符号说明</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$n_l$</td>
<td style="text-align:center">网络层数</td>
</tr>
<tr>
<td style="text-align:center">$y_j$</td>
<td style="text-align:center">输出层第 $j$ 类标签</td>
</tr>
<tr>
<td style="text-align:center">$S_l$</td>
<td style="text-align:center">第 $l$ 层神经元个数（不包括偏置项）</td>
</tr>
<tr>
<td style="text-align:center">$g(x)$</td>
<td style="text-align:center">激活函数</td>
</tr>
<tr>
<td style="text-align:center">$w_{ij}^{l}$</td>
<td style="text-align:center">第 $l-1$ 层的第 $j$ 个神经元连接到第 $l$ 层第 $i$ 个神经元的权重</td>
</tr>
<tr>
<td style="text-align:center">$b_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的偏置</td>
</tr>
<tr>
<td style="text-align:center">$z_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输入加权和</td>
</tr>
<tr>
<td style="text-align:center">$a_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输出（激活值）</td>
</tr>
<tr>
<td style="text-align:center">$\delta_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元产生的错误</td>
</tr>
</tbody>
</table>
</div>
<h2 id="推导过程-9"><a href="#推导过程-9" class="headerlink" title="推导过程"></a>推导过程</h2><h3 id="基本公式-5"><a href="#基本公式-5" class="headerlink" title="基本公式"></a>基本公式</h3><script type="math/tex; mode=display">
\begin{aligned}
   z_i^l & =\sum_k w_{i k}^l * a_k^{l-1}+b_j^l \\
   g(x) & =\frac{1}{1+e^{-x}} \\
   a_i^l & =g\left(z_i^l\right) \\
   J(\theta) & =\frac{1}{2} \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2 \\
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l}
\end{aligned}</script><h3 id="梯度方向传播公式推导-5"><a href="#梯度方向传播公式推导-5" class="headerlink" title="梯度方向传播公式推导"></a>梯度方向传播公式推导</h3><h4 id="初始条件-5"><a href="#初始条件-5" class="headerlink" title="初始条件"></a>初始条件</h4><p>以一个输入样本为例：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^{n l} & =\frac{\partial J(\theta)}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-g\left(z_j^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} \frac{\partial\left(y_i-g\left(z_i^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =-\left(y_i-a_i^{n l}\right) g^{\prime}\left(z_i^{l l}\right)
\end{aligned}</script><h4 id="递推公式-5"><a href="#递推公式-5" class="headerlink" title="递推公式"></a>递推公式</h4><script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial z_j^{l+1}}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial\left(\sum_k w_{j k}^{l+1} * a_k^l+b_j^{l+1}\right)}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} * g^{\prime}\left(z_i^l\right) \\
   & =g^{\prime}\left(z_i^l\right) * \sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} \\
   \frac{\partial J(\theta)}{\partial w_{i j}^l} & =\frac{\partial J(\theta)}{\partial z_i^l} * \frac{\partial z_i^l}{\partial w_{i j}^l} \\
   & =\delta_i^l * a_j^{l-1} \\
   \frac{\partial J(\theta)}{\partial b_i^l} & =\delta_i^l
\end{aligned}</script><h2 id="反向传播伪代码-5"><a href="#反向传播伪代码-5" class="headerlink" title="反向传播伪代码"></a>反向传播伪代码</h2><ol>
<li>输入训练集。</li>
<li>对于训练集的每个样本 $\vec x$ ，设输入层对应的激活值为 $a^l$ ：<ul>
<li>前向传播：$z^l = w^l*a^{l-1}+b^l, a^l = g(z^l)$</li>
<li>计算输出层产生的误差：$\delta^L = \frac {\partial J(\theta)} {\partial a^L} \odot g’(z^L)$</li>
<li>反向传播错误：$\delta^l = ((w^{l+1})^T*\delta^{l+1}) \odot g’(z^l)$</li>
</ul>
</li>
<li>使用梯度下降训练参数：<ul>
<li>$w^l \dashrightarrow w^l - \frac {\alpha} {m} \sum_x\delta^{x, l}*(a^{x, l-1})^T$</li>
<li>$b^l \dashrightarrow  b^l - \frac {\eta} {m} \sum_x\delta^{x, l}$</li>
</ul>
</li>
</ol>
<h1 id="交叉熵损失函数推导-5"><a href="#交叉熵损失函数推导-5" class="headerlink" title="交叉熵损失函数推导"></a>交叉熵损失函数推导</h1><p>对于多分类问题，$softmax$ 函数可以将神经网络的输出变成一个概率分布。它只是一个额外的处理层，下图展示了加上了 $softmax$ 回归的神经网络结构图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225454143.6l2hdid9suw0.webp"  alt="softmax"></p>
<p>递推公式仍然和上述递推公式保持一致。初始条件如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   J(\theta) & =-\sum_{i=1}^{S_{n l}} y^t * \ln y^p \\
   y_j^p & =\frac{e^{a_j^{n l}}}{\sum_{k=1}^{S_{n l}} e^{a_k^{n l}}}
\end{aligned}</script><p>$softmax$ 偏导数计算：</p>
<script type="math/tex; mode=display">
\frac {\partial y_j^p} {\partial a_i^{nl}} =
\left\{
\begin{aligned}
-y_i^p*y_j^p \qquad i \neq j \\
y_i^p*(1-y_i^p) i = j
\end{aligned}
\right.</script><h2 id="推导过程-10"><a href="#推导过程-10" class="headerlink" title="推导过程"></a>推导过程</h2><script type="math/tex; mode=display">
\begin{aligned}
   & \frac{\partial J(\theta)}{\partial z_i^{n l}}=\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial a_i^{n l}}=\sum_{j=1}^{S_{n l}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial y_j^p}=-\frac{y_j^t}{y_j^p}
\end{aligned}</script><p>   由上可知:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial a_i^{n l}} & =\frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}}+\sum_{j \neq i}^{S_{n t}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & =-\frac{y_i^t}{y_i^p} * y_i^p *\left(1-y_i^p\right)+\sum_{j \neq i}^{S_{n l}}-\frac{y_j^t}{y_j^p} *\left(-y_j^p * y_i^p\right) \\
   & =-y_i^t+y_i^t * y_i^p+\sum_{j \neq i}^{S_{n l l}}\left(y_j^t * y_i^p\right) \\
   & =-y_i^t+\sum_j^{S_{n+1}}\left(y_j^t * y_i^p\right) \\
   & =y_i^p-y_i^t
\end{aligned}</script><p>   $\therefore$ 反向传播选代算法的初始值为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial z_i^{n l}} & =\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & =\left(y_i^p-y_i^t\right) * a_i^{n l} *\left(1-a_i^{n l}\right)
\end{aligned}</script><hr>
<h2 id="参考-4"><a href="#参考-4" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/charlotte77/p/5629865.html?tdsourcetag=s_pcqq_aiomsg" >一文弄懂神经网络中的反向传播法——BackPropagation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/u014313009/article/details/51039334" >反向传播算法（过程及公式推导）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/nowgood/p/backprop2.html?tdsourcetag=s_pcqq_aiomsg" >反向传播公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<p>递推公式仍然和上述递推公式保持一致。初始条件如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   J(\theta) & =-\sum_{i=1}^{S_{n l}} y^t * \ln y^p \\
   y_j^p & =\frac{e^{a_j^{n l}}}{\sum_{k=1}^{S_{n l}} e^{a_k^{n l}}}
\end{aligned}</script><p>$softmax$ 偏导数计算：</p>
<script type="math/tex; mode=display">
\frac {\partial y_j^p} {\partial a_i^{nl}} =
\left\{
\begin{aligned}
-y_i^p*y_j^p \qquad i \neq j \\
y_i^p*(1-y_i^p) i = j
\end{aligned}
\right.</script><h2 id="推导过程-11"><a href="#推导过程-11" class="headerlink" title="推导过程"></a>推导过程</h2><script type="math/tex; mode=display">
\begin{aligned}
   & \frac{\partial J(\theta)}{\partial z_i^{n l}}=\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial a_i^{n l}}=\sum_{j=1}^{S_{n l}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial y_j^p}=-\frac{y_j^t}{y_j^p}
\end{aligned}</script><p>   由上可知:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial a_i^{n l}} & =\frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}}+\sum_{j \neq i}^{S_{n t}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & =-\frac{y_i^t}{y_i^p} * y_i^p *\left(1-y_i^p\right)+\sum_{j \neq i}^{S_{n l}}-\frac{y_j^t}{y_j^p} *\left(-y_j^p * y_i^p\right) \\
   & =-y_i^t+y_i^t * y_i^p+\sum_{j \neq i}^{S_{n l l}}\left(y_j^t * y_i^p\right) \\
   & =-y_i^t+\sum_j^{S_{n+1}}\left(y_j^t * y_i^p\right) \\
   & =y_i^p-y_i^t
\end{aligned}</script><p>   $\therefore$ 反向传播选代算法的初始值为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial z_i^{n l}} & =\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & =\left(y_i^p-y_i^t\right) * a_i^{n l} *\left(1-a_i^{n l}\right)
\end{aligned}</script><hr>
<h2 id="参考-5"><a href="#参考-5" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/charlotte77/p/5629865.html?tdsourcetag=s_pcqq_aiomsg" >一文弄懂神经网络中的反向传播法——BackPropagation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/u014313009/article/details/51039334" >反向传播算法（过程及公式推导）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/nowgood/p/backprop2.html?tdsourcetag=s_pcqq_aiomsg" >反向传播公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>

                </div>

                

                <div class="post-bottom-tags-and-share border-box">
                    <div>
                        
                            <ul class="post-tags-box border-box">
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/Algorithm/">Algorithm</a>
                                    </li>
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/Neural-Networks/">Neural Networks</a>
                                    </li>
                                
                            </ul>
                        
                    </div>
                    <div>
                        
                            <div class="post-share-container border-box">
    <ul class="share-list-wrap border-box">
        <li class="qq share-item border-box flex-center tooltip"
            data-tooltip-content="Share to QQ"
        >
            <i class="fa-brands fa-qq"></i>
        </li>
        <li class="wechat share-item border-box flex-center tooltip tooltip-img"
            data-tooltip-content="Share to WeChat"
            data-tooltip-img-tip="Scan by WeChat"
            data-tooltip-img-style="background-color: #fff; top: -10px; padding: 0.6rem 0.6rem 0.1rem 0.6rem;"
        >
            <i class="fa-brands fa-weixin"></i>
        </li>
        <li class="weibo share-item border-box flex-center tooltip"
            data-tooltip-content="Share to WeiBo"
        >
            <i class="fa-brands fa-weibo"></i>
        </li>
    </ul>
</div>

                        
                    </div>
                </div>

                

                
                    <div class="post-nav border-box">
                        
                            <div class="prev-post">
                                <a class="prev"
                                   rel="prev"
                                   href="/2019/06/02/CNN/"
                                   title="CNN"
                                >
                                    <span class="left arrow-icon flex-center">
                                        <i class="fas fa-chevron-left"></i>
                                    </span>
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">CNN</span>
                                        <span class="post-nav-item">Prev posts</span>
                                    </span>
                                </a>
                            </div>
                        
                        
                            <div class="next-post">
                                <a class="next"
                                   rel="next"
                                   href="/2019/05/29/GLM/"
                                   title="GLM"
                                >
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">GLM</span>
                                        <span class="post-nav-item">Next posts</span>
                                    </span>
                                    <span class="right arrow-icon flex-center">
                                        <i class="fas fa-chevron-right"></i>
                                    </span>
                                </a>
                            </div>
                        
                    </div>
                

                
                    






                
            </div>
        </div>

        
            <div class="pc-post-toc left-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-1"><span class="nav-number">2.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.1.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82"><span class="nav-number">2.1.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82"><span class="nav-number">2.1.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.2.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="nav-number">3.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E"><span class="nav-number">3.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B"><span class="nav-number">3.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F"><span class="nav-number">3.2.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="nav-number">3.2.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-number">3.3.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC"><span class="nav-number">4.</span> <span class="nav-text">交叉熵损失函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-1"><span class="nav-number">4.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">4.2.</span> <span class="nav-text">参考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-1"><span class="nav-number">4.3.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82-1"><span class="nav-number">4.3.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82-1"><span class="nav-number">4.3.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-1"><span class="nav-number">4.4.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-2"><span class="nav-number">5.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-2"><span class="nav-number">5.1.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82-2"><span class="nav-number">5.1.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82-2"><span class="nav-number">5.1.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-2"><span class="nav-number">5.2.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-1"><span class="nav-number">6.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E-1"><span class="nav-number">6.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-2"><span class="nav-number">6.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F-1"><span class="nav-number">6.2.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-1"><span class="nav-number">6.2.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6-1"><span class="nav-number">6.2.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-1"><span class="nav-number">6.2.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81-1"><span class="nav-number">6.3.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC-1"><span class="nav-number">7.</span> <span class="nav-text">交叉熵损失函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-3"><span class="nav-number">7.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-1"><span class="nav-number">7.2.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-3"><span class="nav-number">8.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-3"><span class="nav-number">8.1.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82-3"><span class="nav-number">8.1.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82-3"><span class="nav-number">8.1.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-3"><span class="nav-number">8.2.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-2"><span class="nav-number">9.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E-2"><span class="nav-number">9.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-4"><span class="nav-number">9.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F-2"><span class="nav-number">9.2.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-2"><span class="nav-number">9.2.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6-2"><span class="nav-number">9.2.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-2"><span class="nav-number">9.2.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81-2"><span class="nav-number">9.3.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC-2"><span class="nav-number">10.</span> <span class="nav-text">交叉熵损失函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-5"><span class="nav-number">10.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-2"><span class="nav-number">10.2.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-3"><span class="nav-number">11.</span> <span class="nav-text">公式推导</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-4"><span class="nav-number">12.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-4"><span class="nav-number">12.1.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82-4"><span class="nav-number">12.1.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82-4"><span class="nav-number">12.1.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-4"><span class="nav-number">12.2.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-4"><span class="nav-number">13.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E-3"><span class="nav-number">13.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-6"><span class="nav-number">13.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F-3"><span class="nav-number">13.2.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-3"><span class="nav-number">13.2.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6-3"><span class="nav-number">13.2.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-3"><span class="nav-number">13.2.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81-3"><span class="nav-number">13.3.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC-3"><span class="nav-number">14.</span> <span class="nav-text">交叉熵损失函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-7"><span class="nav-number">14.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-3"><span class="nav-number">14.2.</span> <span class="nav-text">参考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E-4"><span class="nav-number">14.3.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-8"><span class="nav-number">14.4.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F-4"><span class="nav-number">14.4.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-4"><span class="nav-number">14.4.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6-4"><span class="nav-number">14.4.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-4"><span class="nav-number">14.4.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81-4"><span class="nav-number">14.5.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC-4"><span class="nav-number">15.</span> <span class="nav-text">交叉熵损失函数推导</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-5"><span class="nav-number">16.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-5"><span class="nav-number">16.1.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82-5"><span class="nav-number">16.1.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82-5"><span class="nav-number">16.1.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-5"><span class="nav-number">16.2.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-5"><span class="nav-number">17.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E-5"><span class="nav-number">17.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-9"><span class="nav-number">17.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F-5"><span class="nav-number">17.2.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-5"><span class="nav-number">17.2.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6-5"><span class="nav-number">17.2.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-5"><span class="nav-number">17.2.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81-5"><span class="nav-number">17.3.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC-5"><span class="nav-number">18.</span> <span class="nav-text">交叉熵损失函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-10"><span class="nav-number">18.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-4"><span class="nav-number">18.2.</span> <span class="nav-text">参考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-11"><span class="nav-number">18.3.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-5"><span class="nav-number">18.4.</span> <span class="nav-text">参考</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom border-box">
            
<footer class="footer border-box">
    <div class="border-box website-info-box default">
        
            <div class="copyright-info info-item default">
                &copy;&nbsp;<span>2017</span>&nbsp;-&nbsp;2024
                
                    &nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;&nbsp;<a href="/">Swift</a>
                
            </div>

            <div class="theme-info info-item default">
                Powered by&nbsp;<a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;&&nbsp;Theme&nbsp;<a class="keep-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep</a>
            </div>

            

            
        

        <div class="count-item info-item default">
            

            
                <span class="count-box border-box uv">
                    <span class="item-type border-box">Unique Visitor</span>
                    <span class="item-value border-box uv" id="busuanzi_value_site_uv"></span>
                </span>
            

            
                <span class="count-box border-box pv">
                    <span class="item-type border-box">Page View</span>
                    <span class="item-value border-box pv" id="busuanzi_value_site_pv"></span>
                </span>
            
        </div>
    </div>
</footer>

        </div>
    </div>

    <!-- post tools -->
    
        <div class="post-tools left-toc">
            <div class="post-tools-container border-box">
    <ul class="tools-list border-box">
        <!-- PC TOC show toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- PC go comment -->
        
    </ul>
</div>

        </div>
    

    <!-- side tools -->
    <div class="side-tools">
        <div class="side-tools-container border-box ">
    <ul class="side-tools-list side-tools-show-handle border-box">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-toggle-theme-mode flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list border-box">
        
            <li class="tools-item toggle-show-toc-tablet flex-center">
                <i class="fas fa-list"></i>
            </li>
        

        

        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>

        <li class="tools-item tool-scroll-to-top flex-center show-arrow">
            <i class="arrow fas fa-arrow-up"></i>
            <span class="percent"></span>
        </li>
    </ul>
</div>

    </div>

    <!-- image mask -->
    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    <!-- local search -->
    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

    <!-- tablet toc -->
    
        <div class="tablet-post-toc-mask">
            <div class="tablet-post-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-1"><span class="nav-number">2.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.1.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82"><span class="nav-number">2.1.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82"><span class="nav-number">2.1.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.2.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="nav-number">3.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E"><span class="nav-number">3.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B"><span class="nav-number">3.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F"><span class="nav-number">3.2.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="nav-number">3.2.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-number">3.3.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC"><span class="nav-number">4.</span> <span class="nav-text">交叉熵损失函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-1"><span class="nav-number">4.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">4.2.</span> <span class="nav-text">参考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-1"><span class="nav-number">4.3.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82-1"><span class="nav-number">4.3.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82-1"><span class="nav-number">4.3.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-1"><span class="nav-number">4.4.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-2"><span class="nav-number">5.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-2"><span class="nav-number">5.1.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82-2"><span class="nav-number">5.1.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82-2"><span class="nav-number">5.1.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-2"><span class="nav-number">5.2.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-1"><span class="nav-number">6.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E-1"><span class="nav-number">6.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-2"><span class="nav-number">6.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F-1"><span class="nav-number">6.2.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-1"><span class="nav-number">6.2.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6-1"><span class="nav-number">6.2.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-1"><span class="nav-number">6.2.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81-1"><span class="nav-number">6.3.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC-1"><span class="nav-number">7.</span> <span class="nav-text">交叉熵损失函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-3"><span class="nav-number">7.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-1"><span class="nav-number">7.2.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-3"><span class="nav-number">8.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-3"><span class="nav-number">8.1.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82-3"><span class="nav-number">8.1.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82-3"><span class="nav-number">8.1.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-3"><span class="nav-number">8.2.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-2"><span class="nav-number">9.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E-2"><span class="nav-number">9.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-4"><span class="nav-number">9.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F-2"><span class="nav-number">9.2.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-2"><span class="nav-number">9.2.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6-2"><span class="nav-number">9.2.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-2"><span class="nav-number">9.2.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81-2"><span class="nav-number">9.3.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC-2"><span class="nav-number">10.</span> <span class="nav-text">交叉熵损失函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-5"><span class="nav-number">10.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-2"><span class="nav-number">10.2.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-3"><span class="nav-number">11.</span> <span class="nav-text">公式推导</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-4"><span class="nav-number">12.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-4"><span class="nav-number">12.1.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82-4"><span class="nav-number">12.1.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82-4"><span class="nav-number">12.1.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-4"><span class="nav-number">12.2.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-4"><span class="nav-number">13.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E-3"><span class="nav-number">13.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-6"><span class="nav-number">13.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F-3"><span class="nav-number">13.2.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-3"><span class="nav-number">13.2.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6-3"><span class="nav-number">13.2.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-3"><span class="nav-number">13.2.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81-3"><span class="nav-number">13.3.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC-3"><span class="nav-number">14.</span> <span class="nav-text">交叉熵损失函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-7"><span class="nav-number">14.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-3"><span class="nav-number">14.2.</span> <span class="nav-text">参考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E-4"><span class="nav-number">14.3.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-8"><span class="nav-number">14.4.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F-4"><span class="nav-number">14.4.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-4"><span class="nav-number">14.4.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6-4"><span class="nav-number">14.4.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-4"><span class="nav-number">14.4.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81-4"><span class="nav-number">14.5.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC-4"><span class="nav-number">15.</span> <span class="nav-text">交叉熵损失函数推导</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B-5"><span class="nav-number">16.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-5"><span class="nav-number">16.1.</span> <span class="nav-text">Step 1 前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82-%E2%80%94-gt-%E9%9A%90%E8%97%8F%E5%B1%82-5"><span class="nav-number">16.1.1.</span> <span class="nav-text">输入层 —-&gt; 隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82-%E2%80%94-gt-%E8%BE%93%E5%87%BA%E5%B1%82-5"><span class="nav-number">16.1.2.</span> <span class="nav-text">隐藏层 —-&gt; 输出层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-5"><span class="nav-number">16.2.</span> <span class="nav-text">Step 2 反向传播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-5"><span class="nav-number">17.</span> <span class="nav-text">公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E-5"><span class="nav-number">17.1.</span> <span class="nav-text">符号说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-9"><span class="nav-number">17.2.</span> <span class="nav-text">推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%AC%E5%BC%8F-5"><span class="nav-number">17.2.1.</span> <span class="nav-text">基本公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-5"><span class="nav-number">17.2.2.</span> <span class="nav-text">梯度方向传播公式推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E6%9D%A1%E4%BB%B6-5"><span class="nav-number">17.2.2.1.</span> <span class="nav-text">初始条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F-5"><span class="nav-number">17.2.2.2.</span> <span class="nav-text">递推公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BC%AA%E4%BB%A3%E7%A0%81-5"><span class="nav-number">17.3.</span> <span class="nav-text">反向传播伪代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC-5"><span class="nav-number">18.</span> <span class="nav-text">交叉熵损失函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-10"><span class="nav-number">18.1.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-4"><span class="nav-number">18.2.</span> <span class="nav-text">参考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B-11"><span class="nav-number">18.3.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-5"><span class="nav-number">18.4.</span> <span class="nav-text">参考</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        </div>
    
</main>



<!-- common -->

<script src="/js/utils.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/toggle-theme.js"></script>

<script src="/js/code-block.js"></script>

<script src="/js/main.js"></script>

<script src="/js/libs/anime.min.js"></script>


<!-- local-search -->

    
<script src="/js/local-search.js"></script>



<!-- lazyload -->


<div class="">
    
        <!-- post-helper -->
        
<script src="/js/post/post-helper.js"></script>


        <!-- toc -->
        
            
<script src="/js/post/toc.js"></script>

        

        <!-- copyright-info -->
        

        <!-- share -->
        
            
<script src="/js/post/share.js"></script>

        
    

    <!-- category-page -->
    

    <!-- links-page -->
    

    <!-- photos-page -->
    
</div>

<!-- mermaid -->


<!-- pjax -->



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>

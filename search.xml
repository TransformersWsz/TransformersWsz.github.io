<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AdaBoost</title>
    <url>/2021/07/08/AdaBoost/</url>
    <content><![CDATA[<p><code>AdaBoost</code> 是属于 <code>boosting</code> 的一种经典算法。</p>
<span id="more"></span>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><img src="/2021/07/08/AdaBoost/1.png" class="">
<p>AdaBoost算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目 $T$ ，最终将这 $T$ 个弱学习器通过集合策略进行整合，得到最终的强学习器。</p>
<h2 id="AdaBoost分类算法流程"><a href="#AdaBoost分类算法流程" class="headerlink" title="AdaBoost分类算法流程"></a>AdaBoost分类算法流程</h2><p>输入样本集 $\boldsymbol{T}=\left\{\left(x, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots\left(x_{m}, y_{m}\right)\right\}$ ，类别为$\{ -1, +1 \}$，弱分类器迭代次数 $K$。输出为最终的强分类器 $f(x)$ 。</p>
<ol>
<li><p>初始化样本集权重为：</p>
<script type="math/tex; mode=display">
D(1)=\left(w_{11}, w_{12}, \ldots w_{1 m}\right) ; \quad w_{1 i}=\frac{1}{m} ; \quad i=1,2 \ldots m</script></li>
<li><p>对于 $k=1, 2, \dots, K$ ：</p>
<ol>
<li><p>使用具有权重 $D_k$ 的样本集来训练数据，得到弱分类器 $G_k(x)$</p>
</li>
<li><p>计算 $G_k(x)$ 的分类误差率：</p>
<script type="math/tex; mode=display">
e_{k}=P\left(G_{k}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{m} w_{k i} I\left(G_{k}\left(x_{i}\right) \neq y_{i}\right)</script></li>
<li><p>计算弱分类器的权重系数：</p>
<script type="math/tex; mode=display">
\alpha_{k}=\frac{1}{2} \log \frac{1-e_{k}}{e_{k}}</script><p>从上式可以看出，如果分类误差率 $e_k$ 越大，则对应的弱分类器权重系数 $\alpha_{k}$ 越小。即误差率小的弱分类器权重系数越大。</p>
</li>
<li><p>更新样本集的权重分布：</p>
<script type="math/tex; mode=display">
w_{k+1, i}=\frac{w_{k i}}{Z_{K}} \exp \left(-\alpha_{k} y_{i} G_{k}\left(x_{i}\right)\right) \quad i=1,2, \ldots m</script><p>其中 $Z_K$ 是归一化因子：</p>
<script type="math/tex; mode=display">
Z_{k}=\sum_{i=1}^{m} w_{k i} \exp \left(-\alpha_{k} y_{i} G_{k}\left(x_{i}\right)\right)</script><p>从上式可以看出，如果第 $i$ 个样本分类错误，则 $y_{i} G_{k} &lt; 0$ ，导致样本的权重在第 $k+1$ 个弱分类器中增大；如果分类正确，则权重在第 $k+1$ 个弱分类器中减少。</p>
</li>
</ol>
</li>
<li><p>构建最终分类器：</p>
<script type="math/tex; mode=display">
f(x)=\operatorname{sign}\left(\sum_{k=1}^{K} \alpha_{k} G_{k}(x)\right)</script></li>
</ol>
<p>对于Adaboost多元分类算法，其原理和二元分类类似。最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数：</p>
<script type="math/tex; mode=display">
\alpha_{k}=\frac{1}{2} \log \frac{1-e_{k}}{e_{k}}+\log (R-1)</script><p>其中 $R$ 为类别数。如果 $R=2$ ，那么上式即是二分类的弱分类器系数。</p>
<h2 id="AdaBoost回归算法流程"><a href="#AdaBoost回归算法流程" class="headerlink" title="AdaBoost回归算法流程"></a>AdaBoost回归算法流程</h2><p>输入样本集 $\boldsymbol{T}=\left\{\left(x, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots\left(x_{m}, y_{m}\right)\right\}$ ，弱分类器迭代次数 $K$。输出为最终的强分类器 $f(x)$ 。</p>
<ol>
<li><p>初始化样本集权重为：</p>
<script type="math/tex; mode=display">
D(1)=\left(w_{11}, w_{12}, \ldots w_{1 m}\right) ; \quad w_{1 i}=\frac{1}{m} ; \quad i=1,2 \ldots m</script></li>
<li><p>对于 $k=1, 2, \dots, K$ ：</p>
<ol>
<li><p>使用具有权重 $D_k$ 的样本集来训练数据，得到弱分类器 $G_k(x)$</p>
</li>
<li><p>计算训练集上的最大误差：</p>
<script type="math/tex; mode=display">
E_{k}=\max \left|y_{i}-G_{k}\left(x_{i}\right)\right| i=1,2 \ldots m</script></li>
<li><p>计算每个样本的相对误差：</p>
<ul>
<li>线性误差：$e_{k i}=\frac{\left|y_{i}-G_{k}\left(x_{i}\right)\right|}{E_{k}}$</li>
<li>平方误差：$e_{k i}=\frac{(y_{i}-G_{k}(x_{i}))^2}{E_{k}^2}$</li>
<li>指数误差：$e_{k i}=1-\exp \left(\frac{-\left|y_{i}-G_{k}\left(x_{i}\right)\right|}{E_{k}}\right)$</li>
</ul>
</li>
<li><p>计算回归误差率：</p>
<script type="math/tex; mode=display">
e_k = \sum_{i=1}^m w_{ki} e_{ki}</script></li>
<li><p>计算弱学习器的权重系数：</p>
<script type="math/tex; mode=display">
a_k = \frac{1-e_k}{e_k}</script></li>
<li><p>更新样本集的权重分布：</p>
<script type="math/tex; mode=display">
w_{k+1, i}=\frac{w_{k i}}{Z_{k}} \alpha_{k}^{1-e_{k i}}</script><p>其中 $Z_K$ 是归一化因子：</p>
<script type="math/tex; mode=display">
Z_{k}=\sum_{i=1}^{m} w_{k i} \alpha_{k}^{1-e_{k i}}</script></li>
</ol>
</li>
<li><p>构建最终强学习器：</p>
<script type="math/tex; mode=display">
f(x)=G_{k^{*}}(x)</script></li>
</ol>
<p>即取所有 $ ln \frac{1}{\alpha_{k}}, k=1,2, \ldots . K$ 的中位数值对于序号 $k^{*}$ 对应的弱学习器。</p>
<h2 id="AdaBoost正则化"><a href="#AdaBoost正则化" class="headerlink" title="AdaBoost正则化"></a>AdaBoost正则化</h2><p>为了防止AdaBoost过拟合，我们加入正则化项：</p>
<script type="math/tex; mode=display">
f_{k}(x)=f_{k-1}(x)+v \alpha_{k} G_{k}(x), \quad 0 < v <= 1</script><p>与GBDT类似，该正则化项称作学习率。对于同样的训练集学习效果，较小的 $v$ 意味着我们需要更多的弱学习器的迭代次数。</p>
<h2 id="AdaBoost总结"><a href="#AdaBoost总结" class="headerlink" title="AdaBoost总结"></a>AdaBoost总结</h2><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>Adaboost作为分类器时，分类精度很高</li>
<li>在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活</li>
<li>不容易发生过拟合</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/px_528/article/details/72963977" >Adaboost入门教程——最通俗易懂的原理介绍（图文实例）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/6133937.html" >集成学习之Adaboost算法原理小结<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention Model</title>
    <url>/2019/07/25/Attention%20Model/</url>
    <content><![CDATA[<p>人脑的注意力模型，说到底是一种资源分配模型，在某个特定时刻，你的注意力总是集中在画面中的某个焦点部分，而对其它部分视而不见。Attention Model 被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中。</p>
<span id="more"></span>
<h2 id="RNN的局限"><a href="#RNN的局限" class="headerlink" title="RNN的局限"></a>RNN的局限</h2><p>机器翻译解决的是输入是一串在某种语言中的一句话，输出是目标语言相对应的话的问题，如将德语中的一段话翻译成合适的英语。之前的Neural Machine Translation(NMT)模型中，通常的配置是encoder-decoder结构，即encoder读取输入的句子将其转换为定长的一个向量，然后decoder再将这个向量翻译成对应的目标语言的文字。通常encoder及decoder均采用RNN结构如LSTM或GRU等。如下图所示，我们利用encoder RNN将输入语句信息总结到最后一个hidden vector中，并将其作为decoder初始的hidden vector，利用decoder解码成对应的其他语言中的文字。</p>
<img src="/2019/07/25/Attention%20Model/1.jpg" class="">
<p>但是这个结构有些问题，尤其是RNN机制实际中存在长程梯度消失或梯度爆炸的问题。对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息。所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。</p>
<p>为了解决这一由长序列到定长向量转化而造成的信息损失的瓶颈，Attention注意力机制被引入了。AM跟人类翻译文章时候的思路有些类似，即将注意力关注于我们翻译部分对应的上下文。在Attention模型中，当我们翻译当前词语时，我们会寻找源语句中相对应的几个词语，并结合之前的已经翻译的部分作出相应的 翻译。如下图所示，当我们翻译“knowledge”时，只需将注意力放在源句中“知识”的部分，当翻译“power”时，只需将注意力集中在”力量“。这样，当我们decoder预测目标翻译的时候就可以看到encoder的所有信息，而不仅局限于原来模型中定长的隐藏向量，并且不会丧失长程的信息。</p>
<img src="/2019/07/25/Attention%20Model/2.gif" class="">
<h2 id="Encoder-Decoder框架"><a href="#Encoder-Decoder框架" class="headerlink" title="Encoder-Decoder框架"></a>Encoder-Decoder框架</h2><p>目前绝大部分文献中出现的AM是附着在Encoder-Decoder框架下的，但AM可以看作一种通用的思想，本身并不依赖于Encoder-Decoder。Encoder-Decoder框架可以看作是一种文本处理领域的研究模式，下图是其抽象表示：</p>
<img src="/2019/07/25/Attention%20Model/3.png" class="">
<p>Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对 $ &lt; X, Y &gt; $（例如 $X$ 是一个问句， $Y$ 是答案； $X$ 是一个句子， $Y$ 是抽取的关系三元组； $X$ 是汉语句子， $Y$ 是汉语句子的英文翻译等等），我们的目标是给定输入句子 $X$ ，期待通过Encoder-Decoder框架来生成目标句子 $Y$ 。 $X$ 和 $Y$ 分别由各自的单词序列构成：</p>
<script type="math/tex; mode=display">
X = <x_1, x_2, \dots, x_m> \\
Y = <y_1, y_2, \dots, y_n></script><p>编码器Encoder对输入句子 $X$ 进行编码，将输入句子通过非线性变换转化为中间语义表示 $C$：</p>
<script type="math/tex; mode=display">
C = \mathcal{F}(x_1, x_2, \dots, x_m)</script><p>解码器Decoder的任务是根据句子 $X$ 的中间语义表示 $C$ 和之前已经生成的历史信息 $y_1, y_2, \dots. y_{i-1}$ 来生成i时刻要生成的单词 $y_i$ ：</p>
<script type="math/tex; mode=display">
y_i = \mathcal{G}(C, y_1, y_2, \dots, y_{i-1})</script><p>每个 $y_i$ 都依次产生，那么看起来就是整个系统根据输入句子 $X$ 生成了目标句子 $Y$。</p>
<h2 id="Attention-Model"><a href="#Attention-Model" class="headerlink" title="Attention Model"></a>Attention Model</h2><p>以上介绍的Encoder-Decoder模型可以看作是注意力不集中的分心模型。目标句子 $Y$ 中每个单词的生成过程如下：</p>
<script type="math/tex; mode=display">
\begin{align}
y_1 &= \mathcal{G}(C) \\
y_2 &= \mathcal{G}(C, y_1) \\
y_3 &= \mathcal{G}(C, y_1, y_2)
\end{align}</script><p>从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的语义编码 $C$ 都是一样的，没有任何区别。而语义编码 $C$ 是由句子 $X$ 的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词（$y_1, y_2, y_3$），其实句子$X$ 中任意单词对生成某个目标单词 $y_i$ 来说影响力都是相同的，没有任何区别（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。这就是为何说这个模型没有体现出注意力的缘由。</p>
<p>引入AM模型，以翻译一个英语句子举例：输入 $X$：Tom chase Jerry。 理想输出 $Y$：汤姆追逐杰瑞。</p>
<p>应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p>
<script type="math/tex; mode=display">
(Tom, 0.3) \quad (chase, 0.2) \quad (Jerry, 0.5)</script><p>每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词 $y_i$ 的时候，原先都是相同的中间语义表示 $C$ 会替换成根据当前生成单词而不断变化的 $C_i$ 。AM模型的关键就是这里，即由固定的中间语义表示 $C$ 换成了根据当前输出单词来调整成加入注意力模型的变化的 $C_i$ 。</p>
<img src="/2019/07/25/Attention%20Model/4.png" class="">
<p>即生成目标句子单词的过程成了下面的形式：</p>
<script type="math/tex; mode=display">
\begin{align}
y_1 &= \mathcal{G}(C_1) \\
y_2 &= \mathcal{G}(C_2, y_1) \\
y_3 &= \mathcal{G}(C_3, y_1, y_2)
\end{align}</script><p>而每个 $C_i$ 可能对应着不同的源语句子单词的注意力分配概率分布。比如对于上面的英汉翻译来说，其对应的信息可能如下：</p>
<script type="math/tex; mode=display">
\begin{align}
C_{汤姆} &= g(0.6*f2("Tom"), 0.2*f2(chase), 0.2*f2("Jerry")) \\
C_{追逐} &= g(0.2*f2("Tom"), 0.7*f2(chase), 0.1*f2("Jerry")) \\
C_{杰瑞} &= g(0.3*f2("Tom"), 0.2*f2(chase), 0.5*f2("Jerry")) \\
\end{align}</script><p>其中，$f2$ 函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个 $f2$ 函数的结果往往是某个时刻输入 $x_i$ 后隐层节点的状态值；$g$ 代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，$g$ 函数就是对构成元素加权求和，也就是常常在论文里看到的下列公式：</p>
<script type="math/tex; mode=display">
C_i = \sum_{j=1}^{T_x} \alpha_{ij}h_j</script><p>假设 $i$ 就是上面的“汤姆”，$T_x$ 为3，代表输入句子的长度，$h1=f2(“Tom”), h2=f2(“Chase”), h3=f2(“Jerry”)$，对应的注意力模型权值分别是$0.6,0.2,0.2$，所以 $g$ 函数就是个加权求和函数。$C_i$ 的形成过程如下图所示：</p>
<img src="/2019/07/25/Attention%20Model/5.png" class="">
<p>注意力分配概率分布 $(0.6, 0.2, 0.2)$ 怎么求出来的呢？为了便于说明，我们假设对图2的非AM模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，则图2的转换为下：</p>
<img src="/2019/07/25/Attention%20Model/6.png" class="">
<p>注意力分配概率分布值的通用计算过程：</p>
<img src="/2019/07/25/Attention%20Model/7.png" class="">
<p>对于采用RNN的Decoder来说，如果要生成 $Y_i$ 单词，在时刻 $i$ ，我们是可以知道在生成 $Y_i$ 之前的隐层节点i时刻的输出值 $H_i$ 的。而我们的目的是要计算生成 $Y_i$ 时的输入句子单词“Tom”、“Chase”、“Jerry”对 $Y_i$ 来说的注意力分配概率分布，那么可以用 $i$ 时刻的隐层节点状态 $H_i$ 去和输入句子中每个单词对应的RNN隐层节点状态 $h_j$ 进行对比，即通过函数 $F(h_j, H_i)$ 来获得目标单词 $Y_i$ 和每个输入单词对应的对齐可能性。这个 $F$ 函数在不同论文里可能会采取不同的方法，然后函数 $F$ 的输出经过Softmax进行归一化就得到了注意力分配概率分布。上图显示的是当输出单词为“汤姆”时刻对应的输入句子单词的对齐概率。绝大多数AM模型都是采取上述的计算框架来计算注意力分配概率分布信息<strong>，</strong>区别只是在 $F$ 的定义上可能有所不同。</p>
<h3 id="数学表达"><a href="#数学表达" class="headerlink" title="数学表达"></a>数学表达</h3><img src="/2019/07/25/Attention%20Model/8.jpg" class="">
<ol>
<li>我们首先利用RNN结构得到encoder的hidden state $(h_1, h_2, \dots, h_T)$</li>
<li>假设当前decoder的hidden state 是 $s_{t-1}$，我们可以计算每一个输入位置 $j$ 与当前输出位置的关联性，$e_{tj} = a(s_{t-1}, h_j)$ ，写成相应的向量形式即为 $\vec {e_t} = (a(s_{t-1}, h_1), a(s_{t-1}, h_2), \dots. a(s_{t-1}, h_T))$</li>
<li>对 $\vec {e_t}$ 进行softmax操作得到attention的分布。$\vec {\alpha_t} = softmax(\vec {e_t})$，展开形式为 $\alpha_{tj} = \frac {e^{e_{tj}}} {\sum_{k=1}^T e^{e_{tk}}}$</li>
<li>利用 $\vec {\alpha_t}$ 进行加权求和得到相应的context vector $\vec {c_t} = \sum_{j=1}^T \alpha_{tj}hj$</li>
<li>由此，我们可以计算decoder的下一个hidden state $s_t = f(s_{t-1}, y_{t-1}, c_t)$ 以及该位置的输出 $p(y_t | y_1, \dots, y_{t-1}, \vec x) = g(y_{i-1}, s_t, c_t)$</li>
</ol>
<h3 id="物理含义"><a href="#物理含义" class="headerlink" title="物理含义"></a>物理含义</h3><p>上述内容就是论文里面常常提到的Soft Attention Model（任何单词都会给一个权值，没有筛选条件）的基本思想。那么怎么理解AM模型的物理含义呢？一般文献里会把AM模型看作是单词对齐模型，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。在其他应用里面把AM模型理解成输入句子和目标句子单词之间的对齐概率也是很顺畅的想法。</p>
<h3 id="本质思想"><a href="#本质思想" class="headerlink" title="本质思想"></a>本质思想</h3><p>如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p>
<img src="/2019/07/25/Attention%20Model/9.png" class="">
<p>将Source中的构成元素想象成是由一系列的<Key($h_i$),Value($f2$)>数据对构成，此时给定Target中的某个元素Query($H_i$)，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p>
<script type="math/tex; mode=display">
Attention(Query, Source) = \sum_{i=1}^{L_x} Similarity(Query, Key_i) * Value_i</script><p>$Lx$ 表示 Source 的长度，如一句话中单词的个数。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西($h_i = f2$)，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p>
<p>从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p>
<p>从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p>
<p>AM的具体计算过程如下：</p>
<img src="/2019/07/25/Attention%20Model/10.png" class="">
<p>Query和Key的相似性计算有如下常用几种方法：</p>
<ul>
<li>点积：$Query \ast Key_i$</li>
<li>cosine：$\frac { Query \ast key_i } { | Query | \ast | Key_i | }$</li>
<li>多层感知器：$MLP(Query, Key_i)$</li>
<li>欧式距离：$\sum_{j=1}^n (Query_j - Key_{ij})^2$</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/47063917" >Attention机制详解（一）——Seq2Seq中的Attention<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/malefactor/article/details/78767781" >深度学习中的注意力机制(2017版)<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/guoyaohua/p/9429924.html?tdsourcetag=s_pcqq_aiomsg" >【NLP】Attention Model（注意力模型）学习总结<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Encoder-Decoder</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT</title>
    <url>/2019/07/28/BERT/</url>
    <content><![CDATA[<p><code>BERT</code> 的两阶段如下所示：</p>
<span id="more"></span>
<img src="/2019/07/28/BERT/1.png" class="">
<h2 id="Comparision-Of-Models"><a href="#Comparision-Of-Models" class="headerlink" title="Comparision Of Models"></a>Comparision Of Models</h2><img src="/2019/07/28/BERT/2.png" class="">
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" >A Neural Probabilistic Language Model<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://arxiv.org/pdf/1810.04805.pdf" >BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Paper Reading</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT、RoBerta、XLNet、ALBERT对比</title>
    <url>/2021/03/25/BERT%E3%80%81RoBerta%E3%80%81XLNet%E3%80%81ALBERT%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>记录一下BERT变体的比较。</p>
<span id="more"></span>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>BERT堆叠了多层Transformer的Encoder模块，设计了两个任务来完成预训练：</p>
<ul>
<li>Masked LM：随机mask掉15%的token，其中80%替换为[MASK]，10%替换为其它token，10%保留原单词。</li>
<li>Next Sentence Prediction(NSP)：从训练集中抽取A和B句，50%为A的下一句，50%为其它句子。</li>
</ul>
<h2 id="RoBerta"><a href="#RoBerta" class="headerlink" title="RoBerta"></a>RoBerta</h2><h4 id="静态Mask-VS-动态Mask"><a href="#静态Mask-VS-动态Mask" class="headerlink" title="静态Mask VS 动态Mask"></a>静态Mask VS 动态Mask</h4><ul>
<li>静态Mask：BERT对每一个序列随机选择15%的tokens替换成[MASK]，而一旦被选中，之后的N个epoch就不能再改变。</li>
<li>动态Mask：RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Mask，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。</li>
</ul>
<h4 id="NSP-VS-w-o-NSP"><a href="#NSP-VS-w-o-NSP" class="headerlink" title="NSP VS w/o NSP"></a>NSP VS w/o NSP</h4><p>RoBerta去除了NSP任务，每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL - SENTENCES），而原来的Bert每次只输入两个句子。</p>
<h4 id="hyper-parameter"><a href="#hyper-parameter" class="headerlink" title="hyper-parameter"></a>hyper-parameter</h4><ul>
<li>更大的batch_size</li>
<li>更多的数据</li>
<li>更高的学习率</li>
<li>更长时间的训练</li>
</ul>
<h2 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h2><p><strong>AR LM</strong>：利用上下文单词预测下一个单词的一种模型。但是在这里，上下文单词被限制在两个方向，要么向前，要么向后。</p>
<p><strong>AE LM</strong>：从损坏的输入中重建原始数据的一种模型。它可以同时在向前向后两个方向看到上下文。</p>
<p>BERT存在的问题：</p>
<ul>
<li>掩码导致的微调差异：预训练阶段因为采取引入[Mask]标记来Mask掉部分单词的训练模式，而Fine-tuning阶段是看不到这种被强行加入的Mask标记的，所以两个阶段存在使用模式不一致的情形，这可能会带来一定的性能损失。</li>
<li>预测的标记彼此独立：Bert在第一个预训练阶段，假设句子中多个单词被Mask掉，这些被Mask掉的单词之间没有任何关系，是条件独立的，而有时候这些单词之间是有关系的，XLNet则考虑了这种关系。</li>
</ul>
<p>XLNet在输入侧维持表面的X句子单词顺序，在Transformer内部，看到的已经是被重新排列组合后的顺序，是通过Attention Mask来实现的。从X的输入单词里面，也就是Ti的上文和下文单词中，随机选择i-1个，放到Ti的上文位置中，把其它单词的输入通过Attention Mask隐藏掉，于是就能够达成我们期望的目标。</p>
<h3 id="双流自注意力机制"><a href="#双流自注意力机制" class="headerlink" title="双流自注意力机制"></a>双流自注意力机制</h3><img src="/2021/03/25/BERT%E3%80%81RoBerta%E3%80%81XLNet%E3%80%81ALBERT%E5%AF%B9%E6%AF%94/1.png" class="">
<ul>
<li>content stream self-attention $h_{\theta}\left(\mathbf{x}_{\mathbf{z}_{\leq t}}\right)$：标准的Transformer计算，能同时接触到单词x的特征信息和位置信息。</li>
<li>query stream self-attention $g_{\theta}\left(\mathbf{x}_{\mathbf{z}_{&lt;t}}, z_{t}\right)$：只能接触到单词x的位置信息。</li>
</ul>
<p>计算过程如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&g_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=g_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z}<t}^{(m-1)} ; \theta\right), \quad\left(\text { query stream: use } z_{t} \text { but cannot see } x_{z_{t}}\right)\\
&h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z}<t}^{(m-1)} ; \theta\right), \quad\left(\text { content stream: use both } z_{t} \text { and } x_{z_{t}}\right)
\end{aligned}</script><h4 id="其它改进措施："><a href="#其它改进措施：" class="headerlink" title="其它改进措施："></a>其它改进措施：</h4><ul>
<li>引入Transformer-XL：相对位置编码以及分段RNN机制。解决Transformer对长文档应用不友好的问题。</li>
<li>使用更多更高质量的数据。</li>
</ul>
<h2 id="ALBert"><a href="#ALBert" class="headerlink" title="ALBert"></a>ALBert</h2><h4 id="词嵌入向量参数的因式分解-Factorized-embedding-parameterization"><a href="#词嵌入向量参数的因式分解-Factorized-embedding-parameterization" class="headerlink" title="词嵌入向量参数的因式分解(Factorized embedding parameterization)"></a>词嵌入向量参数的因式分解(<strong>Factorized embedding parameterization</strong>)</h4><script type="math/tex; mode=display">
V \times H > V \times E + E \times H</script><p>在BERT、XLNet中，词表的embedding size(E)和transformer层的hidden size(H)是等同的，所以E=H。但实际上词库的大小一般都很大，这就导致模型参数个数就会变得很大。为了解决这些问题他们提出了一个基于factorization的方法。</p>
<h4 id="跨层参数共享-Cross-layer-parameter-sharing"><a href="#跨层参数共享-Cross-layer-parameter-sharing" class="headerlink" title="跨层参数共享(Cross-layer parameter sharing)"></a>跨层参数共享(Cross-layer parameter sharing)</h4><p>每一层的Transformer可以共享参数，这样一来参数的个数不会以层数的增加而增加。</p>
<h4 id="段落连续性任务-Inter-sentence-coherence-loss"><a href="#段落连续性任务-Inter-sentence-coherence-loss" class="headerlink" title="段落连续性任务(Inter-sentence coherence loss)"></a>段落连续性任务(Inter-sentence coherence loss)</h4><p>后续的研究表示NSP过于简单，性能不可靠。使用段落连续性任务。正例，使用从一个文档中连续的两个文本段落；负例，使用从一个文档中连续的两个文本段落，但位置调换了。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/70257427" >XLNet:运行机制及和Bert的异同比较<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://hackerxiaobai.github.io/2019/10/10/Bert-XLNet-RoBerta-ALBert/" >Bert XLNet RoBerta ALBert<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/84559048" >从BERT, XLNet, RoBERTa到ALBERT<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT面试要点</title>
    <url>/2021/03/21/BERT%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/</url>
    <content><![CDATA[<p>BERT的模型结构如下图所示：</p>
<span id="more"></span>
<img src="/2021/03/21/BERT%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/1.png" class="">
<h2 id="1-两个预训练任务"><a href="#1-两个预训练任务" class="headerlink" title="1. 两个预训练任务"></a>1. 两个预训练任务</h2><h4 id="Task1：Masked-Language-Model"><a href="#Task1：Masked-Language-Model" class="headerlink" title="Task1：Masked Language Model"></a>Task1：Masked Language Model</h4><p>MLM是指在训练的时候随机从输入语料上mask掉一些单词，然后通过上下文来预测该单词。在BERT的实验中，15%的Token会被随机Mask掉。这其中的80%会直接替换为[Mask]，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token。</p>
<p>这么做的原因：如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词。加入随机Token的原因是因为Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ‘hairy’。至于单词带来的负面影响，因为一个单词被随机替换掉的概率只有15%*10% =1.5%，这个负面影响其实是可以忽略不计的。</p>
<h4 id="Task2：Next-Sentence-Prediction"><a href="#Task2：Next-Sentence-Prediction" class="headerlink" title="Task2：Next Sentence Prediction"></a>Task2：Next Sentence Prediction</h4><p>NSP的任务是判断句子B是否是句子A的下文。如果是的话输出’IsNext’，否则输出’NotNext’。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从语料中提取的，它们的关系是NotNext的。这个关系保存在[CLS]符号中。</p>
<h2 id="2-BERT的输入与输出"><a href="#2-BERT的输入与输出" class="headerlink" title="2. BERT的输入与输出"></a>2. BERT的输入与输出</h2><h4 id="输入："><a href="#输入：" class="headerlink" title="输入："></a>输入：</h4><ul>
<li>Token Embeddings: 通过查询词表将文本中的每个字转换为一维向量；</li>
<li>Segmentation Embeddings: 对应BERT里面的下一句的预测任务，所以会有两句拼接起来，上句与下句，上句有上句段向量，下句则有下句段向量，也就是图中A与B；</li>
<li>Postion Embeddings: 由于self-attention不能记住文本的时序信息，所以需要加入位置编码。BERT通过初始化参数矩阵来学习位置信息。</li>
</ul>
<h4 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h4><ul>
<li>输入各字对应的融合全文语义信息后的向量表示</li>
<li>CLS:  编码了整个句子语义的特征向量</li>
</ul>
<h2 id="3-BERT的局限性"><a href="#3-BERT的局限性" class="headerlink" title="3. BERT的局限性"></a>3. BERT的局限性</h2><ul>
<li>BERT 在第一个预训练阶段，假设句子中多个单词被 Mask 掉，这些被 Mask 掉的单词之间没有任何关系，是条件独立的，然而有时候这些单词之间是有关系的，比如”New York is a city”，假设我们 Mask 住”New”和”York”两个词，那么给定”is a city”的条件下”New”和”York”并不独立，因为”New York”是一个实体，看到”New”则后面出现”York”的概率要比看到”Old”后面出现”York”概率要大得多。</li>
<li>BERT 的在预训练时会出现特殊的[MASK]，但是它在下游的 fine-tune 中不会出现，这就出现了预训练阶段和 fine-tune 阶段不一致的问题。其实这个问题对最后结果产生多大的影响也是不够明确的，因为后续有许多 BERT 相关的预训练模型仍然保持了[MASK]标记，也取得了很大的结果，而且很多数据集上的结果也比 BERT 要好。但是确确实实引入[MASK]标记，也是为了构造自编码语言模型而采用的一种折中方式。</li>
</ul>
<h2 id="4-ELMo、OpenAI-GPT、BERT区别"><a href="#4-ELMo、OpenAI-GPT、BERT区别" class="headerlink" title="4. ELMo、OpenAI GPT、BERT区别"></a>4. ELMo、OpenAI GPT、BERT区别</h2><ul>
<li>特征提取器：ELMo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，并且Transformer并行能力强。elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层。</li>
<li>单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。</li>
<li>GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，使用了完整句子。</li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>Attention</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>BOSCH实习总结</title>
    <url>/2020/01/05/BOSCH%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>之前满怀赤诚之心写的总结，还在公司组会的时候读了出来（羞耻），在这儿记录一下吧。等我工作了，再回首看时，一定会显得非常幼稚。</p>
<span id="more"></span>
<p>来这儿实习四个多月了，一年的三分之一，时间确实过得快。我觉得我非常幸运，投对了部门。首先大家都很优秀，都很年轻，有活力，有干劲。同事间氛围很和谐。其次，对于我个人而言，也经历了很大的锻炼，增长了见识。跟其他部门的实习生比，我很庆幸很少去做搬东西、拿快递这样无聊的杂活。如果我这四个月都是做这种事，我真的要疯了，太浪费时间了。</p>
<p>在这段实习期间，我主要做了两个项目。一个是RFID的iOS app，一个是工时可视化的网站。一开始进部门，正好就碰到了RFID的项目。简历上虽说做过iOS的app，但那基本上只是demo，不够系统，也没给别人用过。当大光头突然说我们这个系统12月3号要给客户使用的时候，大家都在吐槽时间不够，一个月之内不可能完成等等。其实说实话，那个时候我还是暗自窃喜的，因为时间太仓促，我app做不成的话也情有可原，自然也不用背锅了。可大家还是争分夺秒地在赶这个项目。肖老板、子轩、沈宋衍每天都会检查我做app的进度以及下一步的任务。讨论的时候，我当时真的好惊讶：哇，沈宋衍思维逻辑怎么这么清晰，每次都能把讨论的情况梳理得井井有条。外企员工能力确实不一样。由于这个app是用新的技术写的，摸着石头过河，每天也会踩坑，也不知道能否做出来，就是硬着头皮做。写代码写到一两点很常见（可能是我太菜了）。当然实习这么累说没有抱怨肯定是不可能的。在12月4号的时候，子轩带着ivy还有我去工厂拿标签，这一天我像打了鸡血一样，在工厂狂剪了两千多个标签😂。工厂的米饭很好吃，也算体验了把出差的感觉。最后到deadline的时候，我们居然把项目做出来了。自豪感还是有的。整个过程中，感受最深的就是老板对我的信任（把这么紧急的项目给实习生做，这个问题之前也问过老板）、子轩对我的关照、沈宋衍超强的能力。</p>
<p>另一个项目就是工时可视化的网站。这是我师傅王政提出的一个idea。如果说上一个app是被动接受任务，那么这一个就是我主动愿意做的。因为它可以切实解决工作中的一些问题。之前部门会议也分享了这个网站。现在已经上线了，大家可以去用。如果大家喜欢用、觉得还不错的话，可以给我提一些feedback，我会继续维护下去。这个是无偿的，毕竟现在还在上学，没有那么强的劳资观念，能从项目实践中成长才是最重要的。师傅，你的每一次good都是对我莫大的鼓励。要是三年之后大家还在用的话，那我就可以很自豪地在简历上写开发了一个工时系统，部门员工至今已使用三年。如果大家不喜欢用的话，那也没关系。这个系统的有些功能肯定还是实用的，比如那四个炫酷的图表可以直接用在工作总结中。定时邮件提醒，这个也很方便，这种机械化的工作就不用嘉静去做了。实习要是能留下点实用的东西，那就是我感到最幸福的事了。</p>
<p>非项目方面，最要感谢嘉静和秋晶了。其实很多问题我都可以自己去解决的，像一些申请流程之类的。但我总是很急躁，懒得花时间在上面。感谢她们不厌其烦地指导我。</p>
<p>博世真的是一家培养你的公司。在这里你可以尽情地学到自己想学的东西。workshop、部门的分享会议等等都是很好的机会。之前emm1参加了那个拾败的活动，每个人都介绍了自己的失败经历（包括我跟一个妹子要微信失败的经历），然后大家从失败经历中总结经验、继续前行。哇，这种活动真的是太充满人文关怀了。还有一些其他的，例如心理咨询热线、员工反馈箱等等。不敢想象国内公司会有这样的机制。当然，在博世，你做什么事情都是要走流程的，这个流程可能会很长。而我性子就比较急，特别反感和害怕这种流程，昨天还吐槽了离个职怎么还这么麻烦。</p>
<p>在博世实习，也相当于接触了社会嘛，肯定也会考虑自己的未来规划和发展。本科生涯即将结束，研究生生涯即将在另一个地方、另一所学校开始。苏州北南京南，不同的路上领略不同的风景。研究生的三年更是坐冷板凳的三年，熬夜熬到两点多该是家常便饭，这一点在本科阶段已经适应了，那种深更半夜搜索枯肠而不可得的痛苦、大冬天晚上一个人在自习教室写着代码看着书的孤独又要重新尝一遍。三年之后出来，或许我就熬不动夜了。找工作、买房买车、关注柴米油盐的社会生活开始了。我爸妈是笃定让我去上海工作的，我暑假夏令营的时候去过一次上海，地铁挤了两趟才挤上去，当时印象就不好了。在上海，这么拥挤的城市，这么高的房价，这么快的生活节奏，我是有点恐惧的。IoT时代的到来，互联网寒冬是否继续都是未知数。我也不知道能否找到合适的工作。今年我看了一些秋招和春招的就业情况，就拿苏大来说，大部分人找的工作并不理想，包括研究生，商院的出来进四大也就那么点工资。我问有些研究生这么低的工资，怎么活的下去。他们调侃说穷人有穷人的活法，人家买的大一点的房子，你就只能买小一点的。人家买的贵一点的车，你就只能买便宜一点地。确实很有道理。总之，对于未来我是偏悲观的。而我现在能做的就是继续去学习，比我优秀的人太多太多了，去向他们学习。</p>
<p>回到这四个月的实习，我其实并没有做太多事情，大家都是做硬件，我也帮不上什么忙，还拿着工资，心里是有点惭愧的。我在最后只能送上自己衷心的祝愿。希望肖老板继续保持年轻、帅气，你是我心目中的男神，很有外企员工的气质；希望子轩能早点掌握编程、精通C语言；希望沈宋衍继续保持极强的编程能力、头发也越来越多；希望师傅王政继续保持那种干劲和活力；希望朱莉继续保持开朗的性格，你是我心目中的女硬件工程师（在我心中不是所有人都可以称为工程师的）；希望欢姐继续保持善良友爱的性格，从你的电话中能感觉到你是个好母亲；希望祝寅在博世能够实现自己的价值，平时跟你聊的话题很多，也很投机；希望peter头发越来越多；希望博士能继续研究出新的算法；希望kevin继续带领着aeemm前进；希望三位女生继续保持年轻漂亮；希望嘉静能够推进我那个网站；希望ivy保养好头发；希望秋晶学业有成；希望新来的实习生无论在编程能力上还是性格上都比我好的多的多。希望大家，在博世，拥抱更好的自己。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>程序人生</tag>
      </tags>
  </entry>
  <entry>
    <title>BST &amp; AVL &amp; RBT</title>
    <url>/2021/08/23/BST%20&amp;%20AVL%20&amp;%20RBT/</url>
    <content><![CDATA[<p>记录各种变体树：</p>
<span id="more"></span>
<h2 id="BST"><a href="#BST" class="headerlink" title="BST"></a>BST</h2><img src="/2021/08/23/BST%20&%20AVL%20&%20RBT/BST.png" class="">
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ul>
<li>根节点的值大于左子树包含的节点的值</li>
<li>根节点的值小于右子树包含的节点的值</li>
<li>左右子树都是BST</li>
</ul>
<h4 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h4><p>假设当前节点为 <code>cur</code> ，待插入节点为 <code>node</code> ，根节点为 <code>root</code> ，分如下四种情况：</p>
<ol>
<li><code>root == None</code>: <code>root=node</code></li>
<li><code>cur.val == node.val</code>: 不做任何处理</li>
<li><code>cur.val &gt; node.val</code>:<ul>
<li><code>if cur.left == None</code>: <code>cur.left = node</code></li>
<li><code>if cur.left != None</code>: 递归左子树</li>
</ul>
</li>
<li><code>cur.val &lt; node.val</code>:<ul>
<li><code>if cur.right == None</code>: <code>cur.right = node</code></li>
<li><code>if cur.right != None</code>: 递归右子树</li>
</ul>
</li>
</ol>
<h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><p>分如下三种情况：</p>
<ol>
<li>删除节点为叶子节点：直接删除</li>
<li>删除节点只有一个子节点：删除节点的父节点指向其唯一的那个子节点</li>
<li>删除节点有两个子节点：选择后继节点（右子树的最小节点）来顶替其位置，然后删除后继节点</li>
</ol>
<h2 id="AVL"><a href="#AVL" class="headerlink" title="AVL"></a>AVL</h2><img src="/2021/08/23/BST%20&%20AVL%20&%20RBT/AVL.png" class="">
<blockquote>
<p>平衡因子：树中某结点其左子树的高度和右子树的高度之差</p>
</blockquote>
<h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><ul>
<li>特殊的BST，树中任意一个节点的平衡因子绝对值小于等于1</li>
</ul>
<p>AVL的插入和删除时间复杂度均为 $O(log_2 n)$ ，$n$ 为树中节点个数。</p>
<p>AVL树大部分操作都和BST树相同, 只有在插入删除结点时, 有可能造成AVL树失去平衡, <strong>而且只有那些在被插入/删除结点到根节点的路径上的结点有可能出现失衡, 因为只有那些结点的子树结构发生了变化</strong>。</p>
<p>这时我们需要一些操作来把树恢复平衡，这些操作叫做AVL树的旋转：</p>
<ul>
<li>LL</li>
<li>RR</li>
<li>LR</li>
<li>RL</li>
</ul>
<p>具体操作可见 <a class="link"   href="https://blog.csdn.net/weixin_36888577/article/details/87211314?utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.base" >平衡二叉树(AVL树)的平衡原理以及插入,删除操作<i class="fas fa-external-link-alt"></i></a> 和 <a class="link"   href="https://www.cnblogs.com/magic-sea/p/11992833.html" >AVL树的插入和删除<i class="fas fa-external-link-alt"></i></a></p>
<h4 id="插入-1"><a href="#插入-1" class="headerlink" title="插入"></a>插入</h4><ul>
<li>当插入新结点导致不平衡时, 我们需要找到距离新节点最近的不平衡结点为轴来转动AVL树来达到平衡</li>
</ul>
<h4 id="删除-1"><a href="#删除-1" class="headerlink" title="删除"></a>删除</h4><ul>
<li>AVL删除节点的操作与和BST一样, 不同的是删除一个结点有可能引起父结点失衡。与插入不同，除了在父节点处旋转外，可能必须在父节点的祖先处再进行旋转。因此，我们必须继续追踪路径，直到到达根为止。</li>
</ul>
<h2 id="RBT"><a href="#RBT" class="headerlink" title="RBT"></a>RBT</h2><img src="/2021/08/23/BST%20&%20AVL%20&%20RBT/RBT.jpeg" class="">
<ul>
<li>一棵含有 $n$ 个节点的红黑树的高度至多为 $2log(n+1)$</li>
<li>RBT的插入和删除时间复杂度均为 $O(log_2 n)$ ，$n$ 为树中节点个数</li>
</ul>
<h4 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h4><p>RBT也是一种特殊的BST，此外它还有如下五个特性：</p>
<ol>
<li>每个节点要么黑色，要么红色</li>
<li>根节点为黑色</li>
<li>每个叶子节点为黑色（这里叶子节点专指值为None的节点）</li>
<li>如果一个节点为红色，那么它的子节点必为黑色</li>
<li>任意一节点到每个叶子节点的路径上都包含相同数量的黑色节点</li>
</ol>
<h4 id="插入、删除"><a href="#插入、删除" class="headerlink" title="插入、删除"></a>插入、删除</h4><p>RBT的插入和删除情况较为复杂，具体案例可见 <a class="link"   href="http://www.360doc.com/content/19/0311/07/25472797_820646156.shtml" >什么是红黑树？面试必问！<i class="fas fa-external-link-alt"></i></a> 和 <a class="link"   href="https://www.cnblogs.com/skywang12345/p/3245399.html" >红黑树(一)之 原理和算法详细介绍<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="RBT相比于BST、AVL有什么优缺点"><a href="#RBT相比于BST、AVL有什么优缺点" class="headerlink" title="RBT相比于BST、AVL有什么优缺点"></a>RBT相比于BST、AVL有什么优缺点</h2><ul>
<li><p>RBT是牺牲了严格的高度平衡的优越条件为代价，它只要求部分地达到平衡要求，降低了对旋转的要求，从而提高了性能。红黑树能够以 $O(log_2 n)$ 的时间复杂度进行搜索、插入、删除操作。此外，由于它的设计，任何不平衡都会在三次旋转之内解决。当然，还有一些更好的，但实现起来更复杂的数据结构能够做到一步旋转之内达到平衡，但红黑树能够给我们一个比较“便宜”的解决方案。</p>
</li>
<li><p>相比于BST，因为红黑树可以能确保树的最长路径不大于两倍的最短路径的长度，所以可以看出它的查找效果是有最低保证的。在最坏的情况下也可以保证 $O(logn)$ 的，这是要好于二叉查找树的。因为二叉查找树最坏情况可以让查找达到 $O(n)$ 。</p>
</li>
<li><p>RBT的算法时间复杂度和AVL相同，但统计性能比AVL更高。AVL在插入和删除中所做的后期维护操作会比RBT要耗时好多，但是他们的查找效率都是 $O(logn)$ ，所以RBT应用还是高于AVL的。实际上插入AVL和RBT的速度取决于你所插入的数据。如果你的数据分布较好,则比较宜于采用AVL(例如随机产生系列数)，但是如果你想处理比较杂乱的情况，则RBT是比较快的。</p>
</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/c_living/article/details/81021510" >BST（二叉搜索树）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/weixin_36888577/article/details/87211314?utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.base" >平衡二叉树(AVL树)的平衡原理以及插入,删除操作<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_25343557/article/details/89110319" >详细图文——AVL树<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/magic-sea/p/11992833.html" >AVL树的插入和删除<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://www.360doc.com/content/19/0311/07/25472797_820646156.shtml" >什么是红黑树？面试必问！<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/skywang12345/p/3245399.html" >红黑树(一)之 原理和算法详细介绍<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/wuchanming/p/4444961.html" >面试题——轻松搞定面试中的红黑树问题<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title>Bagging &amp; RF</title>
    <url>/2021/07/06/Bagging%20&amp;%20RF/</url>
    <content><![CDATA[<p>相对于 <code>boosting</code> ，<code>bagging</code> 更好理解一点。</p>
<span id="more"></span>
<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>算法流程如下：</p>
<ol>
<li>从原始样本中使用Bootstraping方法有放回地随机抽取 $n$ 个训练样本，共进行 $k$ 轮抽取，得到 $k$ 个训练集；</li>
<li>对于 $k$ 个训练集，分别训练出 $k$ 个模型；</li>
<li>在对预测输出进行结合时：<ul>
<li>分类：简单投票法</li>
<li>回归：简单平均法</li>
</ul>
</li>
</ol>
<h2 id="RF"><a href="#RF" class="headerlink" title="RF"></a>RF</h2><p>RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。</p>
<p>传统决策树在选择划分属性时是在当前结点的属性集合（假设有 $d$ 个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 $k$ 个属性的子集，然后再从这个子集中选择一个最优属性用于划分。$k$ 控制了随机性的引入程度：</p>
<ul>
<li>$k=d$ ：基决策树的构建与传统决策树相同；</li>
<li>$k=1$ ：随机选择一个属性进行划分；</li>
</ul>
<p>RF简单、容易实现、计算开销小，但是性能却非常强大。</p>
<h2 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h2><h4 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a>平均法</h4><p>对于输出值连续型，最常用的结合策略是使用平均法：</p>
<ul>
<li>简单平均法：</li>
</ul>
<script type="math/tex; mode=display">
H(x) = \frac{1}{T} \sum_{i=1}^T h_i(x)</script><ul>
<li>加权平均法：</li>
</ul>
<script type="math/tex; mode=display">
H(x) = \sum_{i=1}^T w_i h_i(x), \quad \sum_{i=1}^T w_i = 1</script><p>$w_i$ 一般是从训练数据中学习而得。</p>
<p>在个体学习器性能相差较大时使用加权平均法，反之使用简单平均法。</p>
<h4 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a>投票法</h4><p>对于分类任务来说，假设有 $\{ c_1, c_2, \dots, c_N \}$ 个类别。最常用的结合策略是使用投票法：</p>
<ul>
<li>绝对多数投票法：</li>
</ul>
<script type="math/tex; mode=display">
H(x) = 
\begin{cases}
c_j,& if \  \sum_{i=1}^T h_i^j(x) > 0.5\sum_{k=1}^N \sum_{i=1}^T h_i^k(x)；\\
reject, & otherwise
\end{cases}</script><p>即某类别得票过半数，则预测为该类别；否则拒绝预测。</p>
<ul>
<li>相对多数投票法：</li>
</ul>
<script type="math/tex; mode=display">
H(x) = c_{argmax_j} \sum_{i=1}^T h_i^j(x)</script><p>即预测为得票最多的类别，若同时有多个类别获得最高票，则从中随机选择一个类别。</p>
<ul>
<li>加权投票法：</li>
</ul>
<script type="math/tex; mode=display">
H(x) = c_{argmax_j} \sum_{i=1}^T w_i h_i^j(x), \quad \sum_{i=1}^T w_i = 1</script><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p>当训练数据很多时，一种更为强大的结合策略是使用”学习法“，即通过另一个学习来结合，典型代表就是Stacking。这里把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器。</p>
<p>Stacking先从初始数据集训练出初级学习器，然后”生成“一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。算法流程如下：</p>
<img src="/2021/07/06/Bagging%20&%20RF/1.jpg" class="">
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>C++学习笔记之多态</title>
    <url>/2021/06/01/C++%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%A4%9A%E6%80%81/</url>
    <content><![CDATA[<blockquote>
<p>多态性：相同对象收到不同消息或不同对象收到相同消息时产生不用的实现动作。</p>
</blockquote>
<span id="more"></span>
<p>多态有两种类型：</p>
<ul>
<li>编译时多态性（静态多态）：通过重载函数实现。</li>
<li>运行时多态性（动态多态）：通过虚函数实现。</li>
</ul>
<h2 id="多态与非多态"><a href="#多态与非多态" class="headerlink" title="多态与非多态"></a>多态与非多态</h2><p>实质区别就是函数地址是早绑定还是晚绑定。</p>
<ul>
<li>如果函数的调用，在编译期间就可以确定函数的调用地址，并生产代码，是静态的，就是说地址是早绑定的。</li>
<li>如果函数调用的地址不能在编译期间确定，需要在运行时才确定，这就属于晚绑定。</li>
</ul>
<h2 id="多态的目的"><a href="#多态的目的" class="headerlink" title="多态的目的"></a>多态的目的</h2><ul>
<li>封装：代码模块化。继承：可以扩展已存在的代码。两者目的都是为了代码重用。</li>
<li>多态：接口重用。不论传递过来的究竟是类的哪个对象，函数都能够通过同一个接口调用到适应各自对象的实现方法。</li>
</ul>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>声明基类类型的指针，利用该指针指向任意一个子类对象，调用相应的虚函数，可以根据指向的子类的不同而实现不同的方法。如果没有使用虚函数的话，即没有利用C++多态性，则利用基类指针调用相应的函数的时候，将总被限制在基类函数本身，而无法调用到子类中被重写过的函数。因为没有多态性，函数调用的地址将是固定的，因此将始终调用到同一个函数，这就无法实现“一个接口，多种方法”的目的了。</p>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span> </span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Shape</span> &#123;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="type">int</span> width;</span><br><span class="line">    <span class="type">int</span> height;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Shape</span>(<span class="type">int</span> a = <span class="number">0</span>, <span class="type">int</span> b = <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        width = a;</span><br><span class="line">        height = b;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">area</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Parent class area : &quot;</span> &lt;&lt; <span class="number">-1</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Rectangle</span> : <span class="keyword">public</span> Shape &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Rectangle</span>(<span class="type">int</span> a = <span class="number">0</span>, <span class="type">int</span> b = <span class="number">0</span>) : <span class="built_in">Shape</span>(a,b) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">area</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Rectangle class area: &quot;</span> &lt;&lt; width * height &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Triangle</span> : <span class="keyword">public</span> Shape &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Triangle</span>(<span class="type">int</span> a = <span class="number">0</span>, <span class="type">int</span> b = <span class="number">0</span>) : <span class="built_in">Shape</span>(a, b) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">area</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Triangle class area: &quot;</span> &lt;&lt; width * height / <span class="number">2</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 程序的主函数</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Shape* shape;</span><br><span class="line">    <span class="function">Rectangle <span class="title">rec</span><span class="params">(<span class="number">10</span>, <span class="number">7</span>)</span></span>;</span><br><span class="line">    <span class="function">Triangle  <span class="title">tri</span><span class="params">(<span class="number">10</span>, <span class="number">5</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 存储矩形的地址</span></span><br><span class="line">    shape = &amp;rec;</span><br><span class="line">    <span class="comment">// 调用矩形的求面积函数 area</span></span><br><span class="line">    shape-&gt;<span class="built_in">area</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 存储三角形的地址</span></span><br><span class="line">    shape = &amp;tri;</span><br><span class="line">    <span class="comment">// 调用三角形的求面积函数 area</span></span><br><span class="line">    shape-&gt;<span class="built_in">area</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/37340242" >C++ 多态 - 知乎 (zhihu.com)<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.runoob.com/cplusplus/cpp-polymorphism.html" >C++ 多态 | 菜鸟教程 (runoob.com)<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>C++常见面试题</title>
    <url>/2021/06/02/C++%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<p>记录一下C++常见的面试八股文。</p>
<span id="more"></span>
<h2 id="C-内存分配有哪几种方式？"><a href="#C-内存分配有哪几种方式？" class="headerlink" title="C++内存分配有哪几种方式？"></a>C++内存分配有哪几种方式？</h2><ul>
<li>从静态存储区分配：内存在程序编译的时候已经分配好，这块内存在整个程序的运行期间都存在，例如全局变量、静态变量。</li>
<li>在栈上创建：在执行函数时，函数内局部变量的存储单元都可以在栈上创建，函数执行结束时这些存储单元自动释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存有限。</li>
<li>在堆上分配：程序在运行的时候用malloc或new申请任意多的内存，由程序员手动free或delete来释放内存。动态内存的生存期有开发者自己决定，使用非常灵活，但也会导致内存泄漏的问题。</li>
</ul>
<h2 id="struct和class的异同？"><a href="#struct和class的异同？" class="headerlink" title="struct和class的异同？"></a>struct和class的异同？</h2><ul>
<li>同：struct和class定义类，都可以继承。</li>
<li>异：struct的默认继承权限和默认访问权限是public，而class的默认继承权限和默认访问权限是private。</li>
</ul>
<h2 id="重载和重写的区别？"><a href="#重载和重写的区别？" class="headerlink" title="重载和重写的区别？"></a>重载和重写的区别？</h2><ul>
<li>重载：是指同一可访问区内被声明的几个具有不同参数列（参数的类型，个数，顺序不同）的同名函数，根据参数列表确定调用哪个函数，重载不关心函数返回类型。</li>
<li>重写：指派生类中存在重新定义的函数。其函数名，参数列表，返回值类型，所有都必须同基类中被重写的函数一致。只有函数体不同（花括号内），派生类调用时会调用派生类的重写函数，不会调用被重写函数。重写的基类中被重写的函数必须有 <code>virtual</code> 修饰。</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN</title>
    <url>/2019/06/02/CNN/</url>
    <content><![CDATA[<blockquote>
<p><a class="link"   href="https://en.wikipedia.org/wiki/Convolutional_neural_network?spm=a2c4e.11153940.blogcont637953.7.471b45b8P0daTl" >卷积神经网络（Convolutional Neural Networks）<i class="fas fa-external-link-alt"></i></a>是一种深度学习模型或类似于人工神经网络的多层感知器，常用来分析视觉图像。CNN在图像分类数据集上有非常突出的表现。</p>
</blockquote>
<span id="more"></span>
<h1 id="DNN与CNN"><a href="#DNN与CNN" class="headerlink" title="DNN与CNN"></a>DNN与CNN</h1><p>下图为DNN：</p>
<img src="/2019/06/02/CNN/DNN.jpg" class="">
<p>下图为CNN：</p>
<img src="/2019/06/02/CNN/CNN.jpg" class="">
<p>虽然两张图的结构直观上差异较大，但实际上它们的整体架构是非常相似的。</p>
<ul>
<li>CNN通过一层一层的节点组织起来。</li>
<li>和DNN一样，CNN的每一个节点都是一个神经元。</li>
<li>CNN的输入输出与DNN基本一致。以图像分类为例，CNN的输入层就是图像的原始像素，而输出层中的每一个节点代表了不同类别的可信度。DNN中的损失函数以及参数的优化过程也适用于CNN。</li>
</ul>
<h1 id="CNN结构"><a href="#CNN结构" class="headerlink" title="CNN结构"></a>CNN结构</h1><p>使用DNN处理图像的最大问题在于全连接层的参数太多。对于MNIST数据，每一张图片的大小为28*28*1，1表示图像是黑白的，只有一个彩色通道。假设第一层隐藏层的节点数为500个，那么一个全连接层的神经网络会有28*28*500+500=392500个参数。如果图片采取更大的规格，比如有RGB三个彩色通道，那么参数数量更是巨大。过多的参数会导致计算速度减慢以及过拟合。而CNN可以有效的减少参数的个数。下图是具体的CNN结构图：</p>
<img src="/2019/06/02/CNN/CNN_detail.jpg" class="">
<p>在CNN的前几层中，每一层的节点都被组织成一个三维矩阵。比如将输入的图片组织成一个32*32*3的三维矩阵。从上图中可以看出CNN的前几层中每个节点只和上一层中部分的节点相连。CNN主要由以下5中结构组成：</p>
<h2 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h2><p>输入层是整个神经网络的输入，在处理图像的CNN中，它一般代表了一张图片的像素矩阵。在上图的最左侧的三维矩阵就可以代表一张图片。三维矩阵的长和宽代表了图像的大小，而三维矩阵的深度代表了图像的色彩通道。比如黑白图片的深度为1，而在RGB色彩模式下，图像的深度为3。</p>
<h2 id="卷积层-Convolution-Layer"><a href="#卷积层-Convolution-Layer" class="headerlink" title="卷积层(Convolution Layer)"></a>卷积层(Convolution Layer)</h2><p>卷积层是CNN最重要的部分。和传统全连接层不同，卷积层中每一个节点的输入只是上一层神经网络的一小块。下图为卷积层的过滤器(filter)或者内核(kernel)：</p>
<img src="/2019/06/02/CNN/filter.jpg" class="">
<p>filter可以将当前层神经网络上的一个子节点矩阵转化为下一层神经网络上的一个单位节点矩阵（长宽均为1，但深度不限）。filter的尺寸指的是filter输入节点矩阵的大小，通常有3*3或5*5。filter处理的矩阵深度和当前层神经网络节点矩阵（输入节点矩阵）的深度是一致的，而filter的深度指的是输出单位节点的深度。</p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>下图展示了如何通过filter将一个2*2*3的节点矩阵转化为一个1*1*5的单位节点矩阵。</p>
<img src="/2019/06/02/CNN/g0.png" class="">
<p>一个filter的前向传播过程和全连接层相似，它总共需要2*2*3*5+5个参数，+5表示偏置项参数的个数。假设使用$w_{x,y,z}^i$ 表示对于输出单位节点矩阵中的第i个节点，filter输入节点 $(x, y, z)$ 的权重，使用 $b^i$ 表示第i个输出节点对应的偏置项参数，那么单位矩阵中的第i个节点的取值 $g(i)$ 为：</p>
<script type="math/tex; mode=display">
g(i) = f(\sum_{x=1}^2\sum_{y=1}^2\sum_{z=1}^3 a_{x, y, z}*w_{x, y, z}^i + b^i)</script><p>其中 $a_{x, y, z}$ 为filter节点 $(x, y, z)$ 的取值，$f$ 采用ReLU作为激活函数。上图展示了 $g(0)$ 的计算过程。每一个二维矩阵表示三维矩阵在某一个深度上的取值。</p>
<p>卷积层结构的前向传播就是通过将一个filter从神经网络当前层的左上角移动到右下角（即滑过整个图像），并在移动过程中重复上述运算：</p>
<img src="/2019/06/02/CNN/slide.gif" class="">
<h3 id="调整输出矩阵大小"><a href="#调整输出矩阵大小" class="headerlink" title="调整输出矩阵大小"></a>调整输出矩阵大小</h3><h4 id="全0填充"><a href="#全0填充" class="headerlink" title="全0填充"></a>全0填充</h4><p>为了避免卷积层前向传播过程中节点矩阵的尺寸的变化，可以在当前矩阵的边界上加入全0填充。这样可以使得卷积层前向传播结果矩阵的大小和当前层矩阵保持一致：</p>
<img src="/2019/06/02/CNN/padding.jpg" class="">
<h4 id="步长"><a href="#步长" class="headerlink" title="步长"></a>步长</h4><p>下图显示了filter步长为2且使用全0填充时，卷积层前向传播的过程：</p>
<img src="/2019/06/02/CNN/stride.jpg" class="">
<h4 id="输出矩阵的大小"><a href="#输出矩阵的大小" class="headerlink" title="输出矩阵的大小"></a>输出矩阵的大小</h4><p>宽度：</p>
<script type="math/tex; mode=display">
out_w = \frac {W - F_w + P} {S} + 1</script><p>高度：</p>
<script type="math/tex; mode=display">
out_h = \frac {H - F_h + P} {S} + 1</script><p>深度有人工指定。</p>
<ul>
<li>$W$：输入图像的宽度</li>
<li>$H$：输入图像的高度</li>
<li>$F_w$：filter的宽度</li>
<li>$F_h$：filter的高度</li>
<li>$P$：全0填充的宽度</li>
<li>$S$：移动步幅</li>
</ul>
<h3 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h3><p>当前卷积层中所有过滤器的参数是共享的，这样就可以巨幅减少神经网络上的参数。假设输入层矩阵的的维度为32*32*3，第一层卷积层filter尺寸为5*5，深度为16，那么这个卷积层的参数个数为5*5*3*16+16=1216个。如果使用全连接层，那么全连接层的参数个数为32*32*3*500=1536000个。相比之下，卷积层的参数个数要远远小于全连接层。卷积层的参数个数与图片的大小无关，它只和filter的尺寸、深度以及当前输入层的深度有关。这使得CNN可以很好地扩展到更大的图像数据上。</p>
<h2 id="池化层-Pooling-Layer"><a href="#池化层-Pooling-Layer" class="headerlink" title="池化层(Pooling Layer)"></a>池化层(Pooling Layer)</h2><p><font color="red">池化层不会改变三维矩阵的深度，但是它可以缩小矩阵的大小</font>。通过池化层，可以进一步缩小最后全连接层中节点的个数，从而达到减少整个神经网络参数的目的。使用池化层既可以加快计算速度也可以防止过拟合。</p>
<p>池化层filter的计算不是节点的加权和，而是采用最大值或者平均值计算。使用最大值操作的池化层被称之为最大池化层（max pooling），这是被使用得最多的池化层结构。使用平均值操作的池化层被称之为平均池化层（average pooling）。</p>
<p>池化层前向传播的过程也是通过移动一个类似filter的结构完成的。与卷积层的filter类似，池化层的filter也需要人工设定filter的尺寸、全0填充以及filter的步长。卷积层和池化层的filter移动方式也是相似的，唯一的区别在卷积层使用的filter是横跨整个深度的，而池化层使用的filter只影响一个深度上的节点。所以池化层的filter除了在长和宽两个维度移动外，它还需要在深度这个维度移动。如下图所示：</p>
<img src="/2019/06/02/CNN/pooling.jpg" class="">
<h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>在经过多轮卷积层和池化层的处理之后，在CNN的最后一般会由1到2个全连接层来给出最后的分类结果。经过几轮卷积层和池化层的处理之后，可以认为图像中的信息已经被抽象成了信息含量更高的特征。我们可以将卷积层和池化层看成自动图像特征提取的过程。在提取完成之后，仍然需要使用全连接层来完成分类任务。</p>
<h2 id="Softmax层"><a href="#Softmax层" class="headerlink" title="Softmax层"></a>Softmax层</h2><p>通过Softmax层，可以得到当前样例属于不同种类的概率分布问题。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN反向传播</title>
    <url>/2019/06/03/CNN%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    <content><![CDATA[<p>深度神经网络(DNN)反向传播的公式推导可以参考之前的博客：<a class="link"   href="https://transformerswsz.github.io/2019/05/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/" >反向传播<i class="fas fa-external-link-alt"></i></a></p>
<span id="more"></span>
<p>要套用DNN的反向传播算法到CNN，有几个问题需要解决：</p>
<ul>
<li>池化层没有激活函数，我们可以令池化层的激活函数为 $g(z) = z$，即激活后输出本身，激活函数的导数为1。</li>
<li>池化层在前向传播的时候，对输入矩阵进行了压缩，我们需要反向推导出 $\delta^{l-1}$，这个方法与DNN完全不同。</li>
<li>卷积层通过张量卷积，或者说若干个矩阵卷积求和得到当前层的输出，而DNN的全连接层是直接进行矩阵乘法而得到当前层的输出。我们需要反向推导出 $\delta^{l-1}$，计算方法与DNN也不同。</li>
<li>对于卷积层，由于 $W$ 使用的是卷积运算，那么从 $\delta^l$ 推导出该层的filter的 $W, b$ 方式也不同。</li>
</ul>
<p>在研究过程中，需要注意的是，由于卷积层可以有多个卷积核，各个卷积核的处理方法是完全相同且独立的，为了简化算法公式的复杂度，我们下面提到卷积核都是卷积层中若干卷积核中的一个。</p>
<p>下面将对上述问题进行逐一分析：</p>
<h1 id="已知池化层的-delta-l-，推导上一隐藏层的-delta-l-1"><a href="#已知池化层的-delta-l-，推导上一隐藏层的-delta-l-1" class="headerlink" title="已知池化层的 $\delta^l$，推导上一隐藏层的 $\delta^{l-1 }$"></a>已知池化层的 $\delta^l$，推导上一隐藏层的 $\delta^{l-1 }$</h1><p>在前向传播算法时，池化层一般我们会用MAX或者Average对输入进行池化，池化的区域大小已知。现在我们反过来，要从缩小后的误差 $ \delta^l $，还原前一次较大区域对应的误差。</p>
<p>在反向传播时，我们首先会把 $ \delta^l $ 的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是MAX，则把 $ \delta^l $ 的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是Average，则把 $ \delta^l $ 的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做 $upsample$。</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>假设池化区域为2*2，步长为2，$\delta^l$ 的第k个子矩阵为：</p>
<script type="math/tex; mode=display">
\delta_k^l = \left(
                \begin{array}{ccc}
                    2 & 8 \\
                    4 & 6
                \end{array}
             \right)</script><p>我们先将 $ \delta_k^l $ 还原，即变成：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    0 & 0 & 0 & 0 \\
    0 & 2 & 8 & 0 \\
    0 & 4 & 6 & 0 \\
    0 & 0 & 0 & 0 \\
    \end{array}
\right)</script><p>如果是MAX，假设我们之前在前向传播时记录的最大值位置分别是左上、右下、右上、左下，则转换后的矩阵为：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    2 & 0 & 0 & 0 \\
    0 & 0 & 0 & 8 \\
    0 & 4 & 0 & 0 \\
    0 & 0 & 6 & 0 \\
    \end{array}
\right)</script><p>如果是Average，转换后的矩阵为：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    0.5 & 0.5 & 2 & 2 \\
    0.5 & 0.5 & 2 & 2 \\
    1 & 1 & 1.5 & 1.5 \\
    1 & 1 & 1.5 & 1.5 \\
    \end{array}
\right)</script><p>这样我们就得到了上一层 $ \frac {\partial J(W, b)} {\partial a_k^{l-1}} $ ，要得到 $\delta_k^{l-1}$ ：</p>
<script type="math/tex; mode=display">
\delta_k^{l-1} = (\frac {\partial a_k^{l-1}} {\partial z_k^{l-1}})^T \frac {\partial J(W, b)} {\partial a_k^{l-1}} = upsample(\delta_k^l) \odot \sigma'(z_k^{l-1})</script><p>其中，$upsample$ 函数完成了池化误差矩阵放大与误差重新分配的逻辑。</p>
<p>对于张量 $\delta^l$ ，我们有：</p>
<script type="math/tex; mode=display">
\delta^{l-1} = upsample(\delta^l) \odot \sigma'(z^{l-1})</script><h2 id="已知卷积层的-delta-l-，推导上一层隐藏层的-delta-l-1"><a href="#已知卷积层的-delta-l-，推导上一层隐藏层的-delta-l-1" class="headerlink" title="已知卷积层的 $\delta^l$，推导上一层隐藏层的 $\delta^{l-1}$"></a>已知卷积层的 $\delta^l$，推导上一层隐藏层的 $\delta^{l-1}$</h2><p>在DNN中，我们知道 $\delta^{l-1}$ 和 $\delta^l$ 的递推关系为：</p>
<script type="math/tex; mode=display">
\delta^{l-1} = \frac {\partial J(W, b)} {\partial z^{l-1}} = (\frac {\partial z^l} {\partial z^{l-1}})^T \frac {\partial J(W, b)} {\partial z^l} = (\frac {\partial z^l} {\partial z^{l-1}})^T \delta^l</script><p>注意到 $z^l$ 和 $z^{l-1}$ 的关系为：</p>
<script type="math/tex; mode=display">
z^l = a^{l-1}*W^l + b^l = \sigma(z^{l-1})*W^l + b^l</script><p>因此我们有：</p>
<script type="math/tex; mode=display">
\delta^{l-1} = (\frac {\partial z^l} {\partial z^{l-1}})^T \delta^l = \delta^l * rot180(W^l) \odot \sigma'(z^{l-1})</script><p>这里的式子其实和DNN的类似，区别在于对于含有卷积的式子求导时，卷积核被旋转了180度。即式子中的 $rot180()$，翻转180度的意思是上下翻转一次，接着左右翻转一次。在DNN中这里只是矩阵的转置。那么为什么呢？由于这里都是张量，直接推演参数太多了。我们以一个简单的例子说明为啥这里求导后卷积核要翻转。</p>
<p>假设我们 $l-1$ 层的输出 $a^{l-1}$ 是一个3*3的矩阵，第 $l$ 层的卷积核 $W^l$ 是一个2*2矩阵，步幅为1，则输出 $z^l$ 是一个 2*2的矩阵，这里 $b^l$ 简化为0，则有：</p>
<script type="math/tex; mode=display">
a^{l-1} * W^l = z^l</script><p>我们列出 $a, W, z$ 的矩阵表达式如下：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{ccc}
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} \\
    \end{array}
\right) * 
\left(
    \begin{array}{cc}
    w_{11} & w_{12} \\
    w_{21} & w_{22} \\
    \end{array}
\right)
= 
\left(
    \begin{array}{cc}
    z_{11} & z_{12} \\
    z_{21} & z_{22} \\
    \end{array}
\right)</script><p>根据卷积得出：</p>
<script type="math/tex; mode=display">
z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22} \\
z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} + a_{23}w_{22} \\
z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} + a_{32}w_{22} \\
z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} + a_{33}w_{22} \\</script><p>接着我们模拟反向求导：</p>
<script type="math/tex; mode=display">
\bigtriangledown a^{l-1} = \frac {\partial J(W, b)} {\partial a^{l-1}} = (\frac {\partial z^l} {\partial a^{l-1}})^T \frac {\partial J(W, b)} {\partial z^l} = (\frac {\partial z^l} {\partial a^{l-1}})^T \delta^l</script><p>从上式可以看出，对于 $ a^{l-1} $ 的梯度误差 $\bigtriangledown a^{l-1} $ ，等于第 $l$ 层的梯度误差乘以 $\frac {\partial z^l} {\partial a^{l-1}}$ ，而 $\frac {\partial z^l} {\partial a^{l-1}}$ 对应上面的例子中相关联的 $w$ 的值。假设 $z$ 矩阵对应的反向传播误差是 $\delta_{11}, \delta_{12}, \delta_{21}, \delta_{22} $ 组成的2*2矩阵，则利用上面梯度的式子和4个等式，我们可以分别写出 $\bigtriangledown a^{l-1}$ 的9个标量的梯度。</p>
<p>比如对于 $a_{11}$ 的梯度，由于在4个等式中 $a_{11}$ 只和 $z_{11}$ 有乘积关系，从而我们有：</p>
<script type="math/tex; mode=display">
\bigtriangledown a_{11} = w_{11}\delta_{11}</script><p>对于 $a_{12}$ 的梯度，由于在4个等式中 $a_{12}$ 和 $z_{11}, z_{12}$ 有乘积关系，从而我们有：</p>
<script type="math/tex; mode=display">
\bigtriangledown a_{12} = w_{11}\delta_{12} + w_{12}\delta_{11}</script><p>同理可得：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\bigtriangledown a_{13} &= w_{12}\delta_{12} \\
\bigtriangledown a_{21} &= w_{11}\delta_{21} + w_{21}\delta_{11} \\
\bigtriangledown a_{22} &= w_{11}\delta_{22} + w_{12}\delta_{21} + w_{21}\delta_{12} + w_{22}\delta_{11} \\
\bigtriangledown a_{23} &= w_{12}\delta_{22} + w_{22}\delta_{12} \\
\bigtriangledown a_{31} &= w_{21}\delta_{21} \\
\bigtriangledown a_{32} &= w_{21}\delta_{22} + w_{22}\delta_{21}\\
\bigtriangledown a_{33} &= w_{22}\delta_{22} \\
\end{aligned}
\end{equation}</script><p>这上面9个式子其实可以用一个矩阵卷积的形式表示，即：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    0 & 0 & 0 & 0 \\
    0 & \delta_{11} & \delta_{12} & 0 \\
    0 & \delta_{21} & \delta_{22} & 0 \\
    0 & 0 & 0 & 0
    \end{array}
\right) * 
\left(
    \begin{array}{cc}
    w_{22} & w_{21} \\
    w_{12} & w_{11} \\
    \end{array}
\right)
= 
\left(
    \begin{array}{cc}
    \bigtriangledown a_{11} & \bigtriangledown a_{12} & \bigtriangledown a_{13} \\
    \bigtriangledown a_{21} & \bigtriangledown a_{22} & \bigtriangledown a_{23} \\
    \bigtriangledown a_{31} & \bigtriangledown a_{32} & \bigtriangledown a_{33} 
    \end{array}
\right)</script><p>为了符合梯度计算，我们在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差。这个例子直观的介绍了为什么对含有卷积的式子反向传播时，卷积核要翻转180度的原因。</p>
<p>以上就是卷积层的误差反向传播过程。</p>
<h2 id="已知卷积层的-delta-l-，推导该层的-W-b-的梯度"><a href="#已知卷积层的-delta-l-，推导该层的-W-b-的梯度" class="headerlink" title="已知卷积层的 $\delta^l$，推导该层的 $W, b$ 的梯度"></a>已知卷积层的 $\delta^l$，推导该层的 $W, b$ 的梯度</h2><p>对于全连接层，可以按DNN的反向传播算法求该层 $W, b$ 的梯度，而池化层并没有 $W, b$ ,也不用求 $W, b$ 的梯度。只有卷积层的 $W, b$ 需要求出。</p>
<p>注意到卷积层 $z$ 和 $W, b$ 的关系为：</p>
<script type="math/tex; mode=display">
z^l = a^{l-1} * W^l + b</script><p>因此我们有：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W^l} = a^{l-1} * \delta^l</script><p>注意到此时卷积核并没有反转，主要是此时是层内的求导，而不是反向传播到上一层的求导。具体过程我们可以分析一下。</p>
<p>这里举一个简化的例子，这里输入是矩阵，不是张量，那么对于第 $l$ 层，某个卷积核矩阵 $W$ 的导数可以表示如下：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W_{pq}^l} = \sum_i \sum_j(\delta_{ij}^l a_{i+p-1, j+q-1}^{l-1})</script><p>　那么根据上面的式子，我们有：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W_{11}^l} = a_{11}\delta_{11} + a_{12}\delta_{12} + a_{21}\delta_{21} + a_{22}\delta_{22} \\

\frac {\partial J(W, b)} {\partial W_{12}^l} = a_{12}\delta_{11} + a_{13}\delta_{12} + a_{22}\delta_{21} + a_{23}\delta_{22} \\

\frac {\partial J(W, b)} {\partial W_{13}^l} = a_{13}\delta_{11} + a_{14}\delta_{12} + a_{23}\delta_{21} + a_{24}\delta_{22} \\
...... \\
\frac {\partial J(W, b)} {\partial W_{33}^l} = a_{33}\delta_{11} + a_{34}\delta_{12} + a_{43}\delta_{21} + a_{44}\delta_{22} \\</script><p>最终我们可以一共得到9个式子。整理成矩阵形式后可得：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W^l} = 
\left(
    \begin{array}{cccc}
    a_{11} & a_{12} & a_{13} & a_{14} \\
    a_{21} & a_{22} & a_{23} & a_{24} \\
    a_{31} & a_{32} & a_{33} & a_{34} \\
    a_{41} & a_{42} & a_{43} & a_{44}
    \end{array}
\right)
*
\left(
    \begin{array}{cccc}
    \delta_{11} & \delta_{12} \\
    \delta_{21} & \delta_{22}
    \end{array}
\right)</script><p>从而可以清楚的看到这次我们为什么没有反转的原因。</p>
<p>而对于 $b$，则稍微有些特殊，因为 $ \delta^l $ 是高维张量，而 $b$ 只是一个向量，不能像DNN那样直接和 $ \delta^l $ 相等。通常的做法是将 $ \delta^l $ 的各个子矩阵的项分别求和，得到一个误差向量，即为 $b$ 的梯度：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial b^l} = \sum_{u, v}(\delta^l)_{u, v}</script><h2 id="CNN反向传播算法总结"><a href="#CNN反向传播算法总结" class="headerlink" title="CNN反向传播算法总结"></a>CNN反向传播算法总结</h2><p>现在我们总结下CNN的反向传播算法，以最基本的批量梯度下降法为例来描述反向传播算法。</p>
<p>输入：m个图片样本，CNN模型的层数L和所有隐藏层的类型，对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。对于池化层，要定义池化区域大小k和池化标准（MAX或Average），对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。梯度学习率 $\alpha$,最大迭代次数MAX与停止迭代阈值 $\epsilon$</p>
<p>输出：CNN模型各隐藏层与输出层的 $W, b$</p>
<ol>
<li>初始化各隐藏层与输出层的各 $W, b$ 的值为一个随机值。</li>
<li>for iter to 1 to MAX:<ol>
<li>for i =1 to m：<ol>
<li>将CNN输入 $a^1$ 设置为 $x_i$ 对应的张量</li>
<li>for l = 2 to L-1，根据下面3种情况进行前向传播算法计算：<ul>
<li>如果当前是全连接层：则有 $a^{i, l} = \sigma(z^{i, l}) = \sigma(W^l a^{i, l-1} + b^l)  $</li>
<li>如果当前是卷积层：则有 $a^{i, l} = \sigma(z^{i, l}) = \sigma(W^l * a^{i, l-1} + b^l)  $</li>
<li>如果当前是池化层：则有 $a^{i, l} = pool(a^{i, l-1})$</li>
</ul>
</li>
<li>对于输出层第L层：$a^{i, L} = softmax(z^{i, L}) = softmax(W^L a^{i, L-1} + b^L)$</li>
<li>通过损失函数计算输出层的 $\delta^{i, L}$</li>
<li>for l = L-1 to 2, 根据下面3种情况进行进行反向传播算法计算：<ul>
<li>如果当前是全连接层：$ \delta^{i, l} = (W^{l+1})^T \delta^{i, l+1} \odot \sigma’(z^{i, l}) $</li>
<li>如果当前是卷积层：$ \delta^{i, l} = \delta^{i, l+1} * rot180(W^{l+1}) \odot \sigma’(z^{i, l}) $</li>
<li>如果当前是池化层：$ \delta^{i, l} = upsample(\delta^{i, l+1}) \odot \sigma’(z^{i, l}) $</li>
</ul>
</li>
</ol>
</li>
<li>for l = 2 to L，根据下面2种情况更新第 $l$ 层的 $W^l, b^l$：<ul>
<li>如果当前是全连接层：$W^l = W^l - \alpha \sum_{i=1}^m \delta^{i, l}(a^{i, l-1})^T, b^l = b^l - \alpha \sum_{i=1}^m \delta^{i, l} $</li>
<li>如果当前是卷积层，对于每一个卷积核有：$ W^l = W^l - \alpha \sum_{i=1}^m \delta^{i, l} * a^{i, l-1}, b^l = b^l - \alpha \sum_{i=1}^m \sum_{u, v}(\delta^{i, l})_{u, v} $</li>
</ul>
</li>
<li>如果所有的 $W, b$ 的变化值都小于停止迭代阈值 $\epsilon$，则跳出迭代循环到步骤3。</li>
</ol>
</li>
<li>输出各隐藏层与输出层的线性关系系数矩阵 $W$ 和偏置量 $b$ 。</li>
</ol>
<hr>
<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/6494810.html?tdsourcetag=s_pcqq_aiomsg" >卷积神经网络(CNN)反向传播算法<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF</title>
    <url>/2019/07/07/CRF/</url>
    <content><![CDATA[<p>条件随机场(Conditional Random Fields)是给定一组输入序列条件下另一组输出序列的概率分布模型，在NLP中应用很广泛。</p>
<span id="more"></span>
<h1 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h1><p>假设我们有Bob一天从早到晚的一系列照片，Bob想考考我们，要我们猜这一系列的每张照片对应的活动，比如: 工作的照片，吃饭的照片，唱歌的照片等等。一个比较直观的办法就是，我们找到Bob之前的日常生活的一系列照片，然后找Bob问清楚这些照片代表的活动标记，这样我们就可以用监督学习的方法来训练一个分类模型，比如逻辑回归，接着用模型去预测这一天的每张照片最可能的活动标记。</p>
<p>这种办法虽然是可行的，但是却忽略了一个重要的问题，就是这些照片之间的顺序其实是有很大的时间顺序关系的，而用上面的方法则会忽略这种关系。比如我们现在看到了一张Bob闭着嘴的照片，那么这张照片我们怎么标记Bob的活动呢？比较难去打标记。但是如果我们有Bob在这一张照片前一点点时间的照片的话，那么这张照片就好标记了。如果在时间序列上前一张的照片里Bob在吃饭，那么这张闭嘴的照片很有可能是在吃饭咀嚼。而如果在时间序列上前一张的照片里Bob在唱歌，那么这张闭嘴的照片很有可能是在唱歌。</p>
<p>为了让我们的分类器表现的更好，在标记数据的时候，可以考虑相邻数据的标记信息。这一点，是普通的分类器难以做到的。而这一块，也是CRF比较擅长的地方。</p>
<p>在实际应用中，自然语言处理中的词性标注(POS Tagging)就是非常适合CRF使用的地方。词性标注的目标是给出一个句子中每个词的词性（名词，动词，形容词等）。而这些词的词性往往和上下文的词的词性有关，因此，使用CRF来处理是很适合的。</p>
<h1 id="MRF"><a href="#MRF" class="headerlink" title="MRF"></a>MRF</h1><p>随机场是由若干个位置组成的整体，当给每一个位置中按照某种分布随机赋予一个值之后，其全体就叫做随机场。举个词性标注的例子：假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词…)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。</p>
<p>马尔可夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅与它相邻的位置的赋值有关。就拿上面的例子来说，我们假设所有词的词性只和它相邻的词的词性有关，这个随机场就特化成MRF。比如第三个词的词性除了与自己本身的位置有关外，只与第二个词和第四个词的词性相关。</p>
<h1 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h1><p>CRF是MRF的特例，它假设MRF中只有 $X$ 和 $Y$ 两种变量。$X$ 一般是给定的，而 $Y$ 是在给定 $X$ 的条件下的输出。这样MRF就特化成了CRF。在十个词的句子词性标注中，$X$ 是词，$Y$ 是词性。因此，如果我们假设它是一个MRF，那么它也是CRF。</p>
<p>我们用准确的数学语言来描述：</p>
<p>设 $X$ 和 $Y$ 是随机变量，$P(Y|X)$ 是给定 $X$ 时 $Y$ 的条件概率分布。若 $Y$ 构成MRF，则称条件概率分布 $P(Y|X)$ 是CRF。</p>
<h1 id="Linear-CRF"><a href="#Linear-CRF" class="headerlink" title="Linear CRF"></a>Linear CRF</h1><p>设 $X = (X_1, X_2, \dots , X_n), Y = (Y_1, Y_2, \dots, Y_n)$ 均为线性链的随机变量序列。在给定随机变量序列 $X$ 的情况下，随机变量 $Y$ 的条件概率分布 $P(Y|X)$ 满足马尔可夫性：</p>
<script type="math/tex; mode=display">
P(Y_i | X, Y_1, Y_2, \dots , Y_n) = P(Y_i | X, Y_{i-1}, Y_{i+1})</script><p>则称 $P(Y|X)$ 为线性链条件随机场。</p>
<h2 id="参数化形式"><a href="#参数化形式" class="headerlink" title="参数化形式"></a>参数化形式</h2><p>我们通过特征函数及其权重系数来将Linear CRF转化为机器学习模型。</p>
<p>特征函数分为两类，一类是定义在 $Y$ 节点上的状态特征函数，这类特征函数只与当前节点有关，记为：</p>
<script type="math/tex; mode=display">
s_l(y_i, x, i), \quad l=1, 2, \dots, L</script><p>$i$ 是当前节点在序列的位置，$L$ 表示当前节点的状态特征函数的个数。</p>
<p>另一类是定义在 $Y$ 上下文的转移特征函数，这类特征函数只和当前节点和上一个节点有关，记为：</p>
<script type="math/tex; mode=display">
t_k(y_{i-1}, y_i, x, i), \quad k = 1, 2, \dots, K</script><p> $i$ 是当前节点在序列的位置，$K$ 表示当前节点的转移特征函数的个数。</p>
<p>无论是状态特征函数还是转移特征函数，它们的取值只能是0或1。即满足特征条件或不满足特征条件。同时，我们可以为每个特征函数赋予一个权值，用以表达我们对这个特征函数的信任度。假设 $t_k$ 的权重系数是 $\lambda_k$ ，$s_l$ 权重系数是 $\mu_l$ , 由此得到Linear CRF的参数化形式：</p>
<script type="math/tex; mode=display">
P(y|x) = \frac {1} {Z(x)} e^{\sum_{i, k} \lambda_k t_k(y_{i-1}, y_i, x, i) + \sum_{i, l} \mu_l s_l (y_i, x, i)}</script><p>其中，$Z(x)$ 为规范化因子：</p>
<script type="math/tex; mode=display">
Z(x) = \sum_y e^{\sum_{i, k} \lambda_k t_k(y_{i-1}, y_i, x, i) + \sum_{i, l} \mu_l s_l (y_i, x, i)}</script><p>每个特征函数定义了一个Linear CRF的规则，其系数定义了这个规则的可信度。两者一起构成了Linear CRF的条件概率分布。</p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>这里举一个词性标注的例子。假设输入的都是三个词的句子，即 $X = (X_1, X_2, X_3)$，输出的词性标记为 $Y = (Y_1, Y_2, Y_3)$，其中 $Y \in {\lbrace   1(名词), 2(动词) \rbrace}$。这里只标记出取值为1的特征函数：</p>
<script type="math/tex; mode=display">
t_1 = t_1(y_{i-1}=1, y_i=2, x, i), \quad i = 2, 3, \quad \lambda_1=1 \\
t_2 = t_2(y_1=1, y_2=1, x, 2), \quad \lambda_2 = 0.5 \\
t_3 = t_3(y_2=2, y_3=1, x, 3), \quad \lambda_3 = 1 \\
t_4 = t_4(y_1=2, y_2=1, x, 2), \quad \lambda_4 = 1 \\
t_5 = t_5(y_2=2, y_3=2, x, 3), \quad \lambda_5 = 0.2 \\
s_1 = s_1(y_1=1, x, 1), \quad \mu_1=1 \\
s_2 = s_2(y_i=2, x, i), \quad i=1, 2, \quad \mu_2 = 0.5 \\
s_3 = s_3(y_i=1, x, i), \quad i=2, 3 \quad \mu_3 = 0.8 \\
s_4 = s_4(y_3=2, x, 3) \quad \mu_4 = 0.5</script><p>求标记(1, 2, 2)的概率。</p>
<p>根据上述参数化公式我们有：</p>
<script type="math/tex; mode=display">
P(y|x) \propto e^{\sum_{i, k} \lambda_k t_k(y_{i-1}, y_i, x, i) + \sum_{i, l} \mu_l s_l (y_i, x, i)}</script><p>代入(1, 2, 2)得到：</p>
<script type="math/tex; mode=display">
P(y_1=1, y_2=2, y_3=2|x) \propto e^{3.2}</script><h2 id="简化形式"><a href="#简化形式" class="headerlink" title="简化形式"></a>简化形式</h2><p>我们用 $s_l$ 表示状态特征函数，用 $t_k$ 表示转移特征函数，同时也使用了不同的符号表示权重系数，导致表示起来非常麻烦。这里我们简化一下表示形式。</p>
<p>假设在某节点有 $K_1$ 个状态特征函数和 $K_2$ 个转移特征函数。我们用一个特征函数 $f_k(y_{i-1}, y_i, x, i)$ 来统一表示：</p>
<script type="math/tex; mode=display">
f_k(y_{i-1}, y_i, x, i) = \begin{cases}
t_k(y_{i-1}, y_i, x, i) \quad k=1, 2, \dots, K_1 \\
s_l(y_i, x, i) \quad k = K_1+l, \quad l=1, 2, \dots, K2
\end{cases}</script><p>同时我们也统一 $f_k(y_{i-1}, y_i, x, i)$ 对应的权重系数 $w_k$ 如下：</p>
<script type="math/tex; mode=display">
w_k = \begin{cases}
\lambda_k \quad k = 1, 2, \dots, K_1 \\
\mu_l \quad k = K_1 + l, \quad l = 1, 2, \dots, K_2
\end{cases}</script><p>最终Linear CRF的参数化形式简化如下：</p>
<script type="math/tex; mode=display">
P_w(y|x) = \frac {e^{\sum_{k=1}^K w_k f_k(y_{i-1}, y_i, x, i)}} {\sum_y e^{\sum_{k=1}^K w_k f_k(y_{i-1}, y_i, x, i)}} \quad y 表示一条输出序列，如上例的(1, 2, 2)</script><hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/7048333.html" >条件随机场CRF(一)从随机场到线性链条件随机场<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.zhihu.com/question/53458773/answer/554436625" >条件随机场（CRF）和隐马尔科夫模型（HMM）最大区别在哪里？CRF的全局最优体现在哪里？<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>MRF</tag>
        <tag>Linear CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF损失函数与Viterbi算法</title>
    <url>/2021/03/24/CRF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8EViterbi%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>CRF考虑到了输出层面的关联性，如下图所示：</p>
<span id="more"></span>
<img src="/2021/03/24/CRF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8EViterbi%E7%AE%97%E6%B3%95/1.png" class="">
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>时间步 $t$ 输出的标签值由两部分组成：</p>
<ul>
<li>发射分数：$ h(y_t;X) $</li>
<li>转移分数：$ g(y_t;y_{t-1}) $</li>
</ul>
<p>一条路径标识为 $y_1, y_2, \dots , y_n$ 的概率为：</p>
<script type="math/tex; mode=display">
P(y_1, y_2, \dots, y_n | X) = \frac{1}{Z(X)} e^{h(y_1;x)+\sum_{i=2}^{n}g(y_i;y_{i-1})+h(y_i;X)}</script><p>其中 $Z(X)$ 为归一化因子。在 CRF 模型中，由于我们只考虑了临近标签的联系（马尔可夫假设），因此我们可以递归地算出归一化因子，这使得原来是指数级的计算量降低为线性级别。</p>
<p>具体来说，我们将计算到时刻 $t$ 的归一化因子记为 $Z_t$，并将它分为 $k$ 个部分：</p>
<script type="math/tex; mode=display">
Z_t = Z_t^1 + Z_t^2 + \cdots + Z_t^k</script><p>上式分别是截止到当前时刻 $t$ 中、以标签 $1,2,\cdots, k$ 为终点的所有路径的得分指数和。那么，我们可以递归地计算：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
Z_{t+1}^{(1)}=\left(Z_{t}^{(1)} G_{11}+Z_{t}^{(2)} G_{21}+\cdots+Z_{t}^{(k)} G_{k 1}\right) H_{t+1}(1 \mid X) \\
Z_{t+1}^{(2)}=\left(Z_{t}^{(1)} G_{12}+Z_{t}^{(2)} G_{22}+\cdots+Z_{t}^{(k)} G_{k 2}\right) H_{t+1}(2 \mid X) \\
\vdots \\
Z_{t+1}^{(k)}=\left(Z_{i}^{(1)} G_{1 k}+Z_{t}^{(2)} G_{2 k}+\cdots+Z_{t}^{(k)} G_{k k}\right) H_{t+1}(k \mid X)
\end{array}</script><p>其中$G_{ij} = e^{g(y_j;y_i)}, H(y_{t+1}|X)=e^{h(y_{t+1}|X)}$，上式简写成矩阵形式为：</p>
<script type="math/tex; mode=display">
Z_{t+1} = Z_tG \otimes H_{t+1}</script><p>为了符合损失函数的含义，将其定义为：</p>
<script type="math/tex; mode=display">
Loss = -logP(y_1, y_2, \dots, y_n | X)</script><h2 id="viterbi-算法"><a href="#viterbi-算法" class="headerlink" title="viterbi 算法"></a>viterbi 算法</h2><p>有了损失函数后，就可以通过反向传播结合梯度下降来求解最优参数。</p>
<p>序列标注的目标是找出一条概率最高的路径。假设整个网络的宽度为 $k$，网络长度为 $N$ ，按照穷举法求最佳路径的时间复杂度为 $O(k^N)$，但CRF采用了马尔可夫假设，因此可以使用动态规划来求解，时间复杂度优化到 $O(N \times k^2)$。</p>
<p>具体示例可见：<a class="link"   href="https://www.zhihu.com/question/20136144" >如何通俗地讲解 viterbi 算法？<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.jiqizhixin.com/articles/2018-05-23-3" >简明条件随机场CRF介绍 | 附带纯Keras实现<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_16949707/article/details/107812643" >CRF条件随机场loss函数与维特比算法理解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>Viterbi</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>DataLoader中sampler参数介绍</title>
    <url>/2021/12/24/DataLoader%E4%B8%ADsampler%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p><code>Sampler</code> 决定了 <code>Dataset</code> 的采样顺序。</p>
<span id="more"></span>
<h2 id="DataLoader-Sampler-DataSet-关系"><a href="#DataLoader-Sampler-DataSet-关系" class="headerlink" title="DataLoader | Sampler | DataSet 关系"></a><code>DataLoader</code> | <code>Sampler</code> | <code>DataSet</code> 关系</h2><img src="/2021/12/24/DataLoader%E4%B8%ADsampler%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D/relation.png" class="">
<ul>
<li><code>Sampler</code> : 提供数据集中元素的索引</li>
<li><code>DataSet</code> : 根据 <code>Sampler</code> 提供的索引来检索数据</li>
<li><code>DataLoader</code> : 批量加载数据用于后续的训练和测试</li>
</ul>
<h2 id="Sampler"><a href="#Sampler" class="headerlink" title="Sampler"></a><code>Sampler</code></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Sampler</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Base class for all Samplers.</span></span><br><span class="line"><span class="string">    Every Sampler subclass has to provide an __iter__ method, providing a way</span></span><br><span class="line"><span class="string">    to iterate over indices of dataset elements, and a __len__ method that</span></span><br><span class="line"><span class="string">    returns the length of the returned iterators.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_source</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<p>PyTorch官网已经实现了多种 <code>Sampler</code> :</p>
<h3 id="SequentialSampler"><a href="#SequentialSampler" class="headerlink" title="SequentialSampler"></a><code>SequentialSampler</code></h3><blockquote>
<p>若 <code>shuffle=False</code> ，且未指定 <code>sampler</code> ，默认使用</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SequentialSampler</span>(<span class="title class_ inherited__">Sampler</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Samples elements sequentially, always in the same order.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_source</span>):</span><br><span class="line">        self.data_source = data_source</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(self.data_source)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data_source)</span><br></pre></td></tr></table></figure>
<h3 id="RandomSampler"><a href="#RandomSampler" class="headerlink" title="RandomSampler"></a><code>RandomSampler</code></h3><blockquote>
<p>若 <code>shuffle=True</code> ，且未指定 <code>sampler</code> ，默认使用</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RandomSampler</span>(<span class="title class_ inherited__">Sampler</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Samples elements randomly. If without replacement, then sample from a shuffled dataset.</span></span><br><span class="line"><span class="string">    If with replacement, then user can specify ``num_samples`` to draw.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">        replacement (bool): samples are drawn with replacement if ``True``, default=``False``</span></span><br><span class="line"><span class="string">        num_samples (int): number of samples to draw, default=`len(dataset)`. This argument</span></span><br><span class="line"><span class="string">            is supposed to be specified only when `replacement` is ``True``.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_source</span>):</span><br><span class="line">        self.data_source = data_source</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        n = <span class="built_in">len</span>(self.data_source)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randperm(n).tolist())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br></pre></td></tr></table></figure>
<h3 id="BatchSampler"><a href="#BatchSampler" class="headerlink" title="BatchSampler"></a><code>BatchSampler</code></h3><blockquote>
<p>like <code>sampler</code>, but returns a batch of indices at a time. Mutually exclusive with <code>batch_size</code>, <code>shuffle</code>, <code>sampler</code>, and <code>drop_last</code></p>
</blockquote>
<ul>
<li>在 <code>DataLoader</code> 中设置 <code>batch_sampler=batch_sampler</code> 的时候，上面四个参数都必须是默认值。也很好理解，每次采样返回一个batch，那么 <code>batch_size</code> 肯定为 <code>1</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchSampler</span>(<span class="title class_ inherited__">Sampler</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Wraps another sampler to yield a mini-batch of indices.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sampler (Sampler): Base sampler.</span></span><br><span class="line"><span class="string">        batch_size (int): Size of mini-batch.</span></span><br><span class="line"><span class="string">        drop_last (bool): If ``True``, the sampler will drop the last batch if</span></span><br><span class="line"><span class="string">            its size would be less than ``batch_size``</span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))</span></span><br><span class="line"><span class="string">        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))</span></span><br><span class="line"><span class="string">        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sampler, batch_size, drop_last</span>):</span><br><span class="line">        self.sampler = sampler</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.drop_last = drop_last</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        batch = []</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> self.sampler:</span><br><span class="line">            batch.append(idx)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(batch) == self.batch_size:</span><br><span class="line">                <span class="keyword">yield</span> batch</span><br><span class="line">                batch = []</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch) &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> self.drop_last:</span><br><span class="line">            <span class="keyword">yield</span> batch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.drop_last:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(self.sampler) // self.batch_size</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (<span class="built_in">len</span>(self.sampler) + self.batch_size - <span class="number">1</span>) // self.batch_size</span><br></pre></td></tr></table></figure>
<ul>
<li>可以看到在构造 <code>BatchSampler</code> 实例的时候，需要传入一个sampler作为实参</li>
</ul>
<hr>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><p>最近看到一篇推文，分享了一个使模型训练速度提升20%的Trick—<a class="link"   href="https://mp.weixin.qq.com/s/xGvaW87UQFjetc5xFmKxWg" >BlockShuffle<i class="fas fa-external-link-alt"></i></a> 。fork了原作者的代码，并自定义了 <code>batch_sampler</code> ，源码见：<a class="link"   href="https://github.com/TransformersWsz/BlockShuffleTest" >TransformersWsz/BlockShuffleTest<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/xGvaW87UQFjetc5xFmKxWg" >一个使模型训练速度提升20%的Trick—BlockShuffle<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.zdaiot.com/MLFrameworks/Pytorch/Pytorch%20DataLoader%E8%AF%A6%E8%A7%A3/" >Pytorch DataLoader详解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader" >torch.utils.data — PyTorch 1.10.1 documentation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.codeleading.com/article/79575865698/" >pytorch中用Mnist数据集dataloader 自定义batchsampler - 代码先锋网 (codeleading.com)<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.tqwba.com/x_d/jishu/415752.html" >pytorch 实现一个自定义的dataloader，每个batch都可以实现类别数量均衡 (tqwba.com)<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/76893455" >一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>DataLoader</tag>
        <tag>sampler</tag>
      </tags>
  </entry>
  <entry>
    <title>Dataset</title>
    <url>/2019/07/28/Dataset/</url>
    <content><![CDATA[<p>Here are some detailed descriptions of datasets for NLP experiments:</p>
<span id="more"></span>
<h2 id="MNLI"><a href="#MNLI" class="headerlink" title="MNLI"></a>MNLI</h2><p>Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classification task. Given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to the first one.</p>
<h2 id="QQP"><a href="#QQP" class="headerlink" title="QQP"></a>QQP</h2><p>Quora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent.</p>
<h2 id="QNLI"><a href="#QNLI" class="headerlink" title="QNLI"></a>QNLI</h2><p>Question Natural Language Inference is a version of the Stanford Question Answering Dataset which has been converted to a binary classification task. The positive examples are (question, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.</p>
<h2 id="SST-2"><a href="#SST-2" class="headerlink" title="SST-2"></a>SST-2</h2><p>The Stanford Sentiment Treebank is a binary single-sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment.</p>
<h2 id="CoLA"><a href="#CoLA" class="headerlink" title="CoLA"></a>CoLA</h2><p>The Corpus of Linguistic Acceptability is a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically “acceptable” or not.</p>
<h2 id="STS-B"><a href="#STS-B" class="headerlink" title="STS-B"></a>STS-B</h2><p>The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources. They were annotated with a score from 1<br>to 5 denoting how similar the two sentences are in terms of semantic meaning.</p>
<h2 id="MRPC"><a href="#MRPC" class="headerlink" title="MRPC"></a>MRPC</h2><p>Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.</p>
<h2 id="RTE"><a href="#RTE" class="headerlink" title="RTE"></a>RTE</h2><p>Recognizing Textual Entailment is a binary entailment task similar to MNLI, but with<br>much less training data.</p>
<h2 id="SQuAD-v1-1"><a href="#SQuAD-v1-1" class="headerlink" title="SQuAD v1.1"></a>SQuAD v1.1</h2><p>The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs. Given a question and a passage from Wikipedia containing the answer, the task is to<br>predict the answer text span in the passage.</p>
<h2 id="SQuAD-v2-0"><a href="#SQuAD-v2-0" class="headerlink" title="SQuAD v2.0"></a>SQuAD v2.0</h2><p>The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic.</p>
<h2 id="SWAG"><a href="#SWAG" class="headerlink" title="SWAG"></a>SWAG</h2><p>The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference. Given a sentence, the task is to choose the most plausible continuation among four choices.</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Data</tag>
      </tags>
  </entry>
  <entry>
    <title>FM &amp; DeepFM</title>
    <url>/2022/01/19/FM%20&amp;%20DeepFM/</url>
    <content><![CDATA[<p><code>FM</code> 是搜广推里最经典的算法，这里记录一下原理与公式推导：</p>
<span id="more"></span>
<h1 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h1><h2 id="参数数量和时间复杂度优化"><a href="#参数数量和时间复杂度优化" class="headerlink" title="参数数量和时间复杂度优化"></a>参数数量和时间复杂度优化</h2><p>当我们使用一阶原始特征和二阶组合特征来刻画样本的时候，会得到如下式子：</p>
<script type="math/tex; mode=display">
\hat{y}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} w_{i j} x_{i} x_{j}</script><p>$x_i$ 和 $x_j$ 分别表示两个不同的特征取值，对于 $n$ 维的特征来说，这样的二阶组合特征一共有 $\frac{n(n-1)}{2}$ 种，也就意味着我们需要同样数量的权重参数。但是由于现实场景中的特征是高维稀疏的，导致 $n$ 非常大，比如上百万，这里<strong>两两特征组合的特征量级 $C_n^2$</strong> ，所带来的参数量就是一个天文数字。对于一个上百亿甚至更多参数空间的模型来说，我们需要海量训练样本才可以保证完全收敛。这是非常困难的。</p>
<p>FM解决这个问题的方法非常简单，它不再是简单地为交叉之后的特征对设置参数，而是设置了一种计算特征参数的方法。</p>
<p>FM模型引入了新的矩阵 $V$ ，它是一个 $n \times k$ 的二维矩阵。这里的 $k$ 是超参，一般不会很大，比如16、32之类。对于特征每一个维度 $x_i$ ，我们都可以找到一个表示向量 $v_i \in R^k$ 。从NLP的角度来说，就是为每个特征学习一个embedding。原先的参数量从 $O(n^2)$ 降低到了 $O(k \times n)$ 。ALBERT论文的因式分解思想跟这个非常相似：$O(V \times H) \ggg O(V \times E + E \times H)$</p>
<p>有了 $V$ 矩阵，上式就可以改写成如下形式：</p>
<script type="math/tex; mode=display">
\hat{y}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=1}^{n} v_{i}^{T} v_{j} x_{i} x_{j}</script><p>当 $k$ 足够大的时候，即 $k = n$ ，那么就有 $W = V$ 。在实际的应用场景当中，我们并不需要设置非常大的K，因为特征矩阵往往非常稀疏，我们可能没有足够多的样本来训练这么大量的参数，并且<strong>限制K也可以一定程度上提升FM模型的泛化能力</strong>。</p>
<p>此外这样做还有一个好处就是<strong>有利于模型训练</strong>，因为对于有些稀疏的特征组合来说，我们所有的样本当中可能都是空的。比如在刚才的例子当中用户A和电影B的组合，可能用户A在电影B上就没有过任何行为，那么这个数据就是空的，我们也不可能训练出任何参数来。但是引入了 $V$ 之后，虽然这两项缺失，但是我们针对用户A和电影B分别训练出了向量参数，我们用这两个向量参数点乘，就得到了这个交叉特征的系数。</p>
<p>虽然我们将模型的参数降低到了 $O(k \times n)$ ，但预测一条样本所需要的时间复杂度仍为 $O(k \times n^2)$ ，这仍然是不可接受的。所以对它进行优化也是必须的，并且这里的优化非常简单，可以<strong>直接通过数学公式的变形推导</strong>得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{i=1}^{n} \sum_{j=i+1}^{n} v_{i}^{T} v_{j} x_{i} x_{j} &=\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} v_{i}^{T} v_{j} x_{i} x_{j}-\frac{1}{2} \sum_{i=1}^{n} v_{i}^{T} v_{j} x_{i} x_{j} \\
&=\frac{1}{2}\left(\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{j, f} x_{i} x_{j}-\sum_{i=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{i, f} x_{i} x_{i}\right) \\
&=\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)\left(\sum_{j=1}^{n} v_{j, f} x_{j}\right)-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \\
&=\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)
\end{aligned}</script><p>FM模型预测的时间复杂度优化到了 $O(k \times n)$ .</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>优化过后的式子如下：</p>
<script type="math/tex; mode=display">
\hat{y}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)</script><p>针对FM模型我们一样可以使用梯度下降算法来进行优化。既然要使用梯度下降，那么我们就需要写出模型当中所有参数的偏导，主要分为三个部分：</p>
<ul>
<li>$w_0$ : $\frac{\partial \theta}{\partial w_{0}}=1$</li>
<li>$\sum_{i=1}^{n} w_{i} x_{i}$ : $\frac{\partial 0}{\partial w_{i}}=x_{i}$</li>
<li>$\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)$ : $\frac{\partial \hat{y}}{\partial v_{i, f}}  = \frac{1}{2} (2x_i (\sum_{j=1}^{n} v_{j, f} x_{j}) - 2v_{i,f} x_i^2) = x_{i} \sum_{j=1}^{n} v_{j, f} x_{j}-v_{i, f} x_{i}^{2}$</li>
</ul>
<p>综合如下：</p>
<script type="math/tex; mode=display">
\frac{\partial \hat{y}}{\partial \theta}= \begin{cases}1, & \text { if } \theta \text { is } w_{0} \\ x_{i}, & \text { if } \theta \text { is } w_{i} \\ x_{i} \sum_{j=1}^{n} v_{j, f} x_{j}-v_{i, f} x_{i}^{2} & \text { if } \theta \text { is } v_{i, f}\end{cases}</script><p>由于 $\sum_{j=1}^n v_{j,f} x_j$ 是可以提前计算好存储起来的，因此我们对所有参数的梯度计算也都能在 $O(1)$ 时间复杂度内完成。</p>
<h2 id="拓展到-d-维"><a href="#拓展到-d-维" class="headerlink" title="拓展到 $d$ 维"></a>拓展到 $d$ 维</h2><p>参照刚才的公式，可以写出FM模型推广到d维的方程：</p>
<script type="math/tex; mode=display">
\hat{y}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{l=2}^{d} \sum_{i_1=1}^{n-l+1} \cdots \sum_{i_{l}=i_{l-1}+1}^{n}\left(\Pi_{j-1}^{l} x_{i_{j}}\right)\left(\sum_{f=1}^{k} \Pi_{j=1}^{l} v_{i_{j}, f}^{l}\right)</script><p>以 $d=3$ 为例，上式为：</p>
<script type="math/tex; mode=display">
\hat{y}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i} + \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} x_{i} x_{j}\left(\sum_{t=1}^{k} v_{i, t} v_{j, t}\right)+\sum_{i=1}^{n-2} \sum_{j=i+1}^{n-1} \sum_{l=j+1}^{n} x_{i} x_{j} x_{l}\left(\sum_{t=1}^{k} v_{i, t} v_{j, t} v_{l, t}\right)</script><p>它的复杂度是 $O(k \times n^d)$ 。当 $d=2$ 的时候，我们通过一系列变形将它的复杂度优化到了 $O(k \times n)$ 。而当 $d &gt; 2$ 的时候，没有很好的优化方法，而且三重特征的交叉往往没有意义，并且会过于稀疏，所以我们一般情况下只会使用 $d=2$ 的情况。</p>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">ndim = <span class="built_in">len</span>(feature_names)  <span class="comment"># 原始特征数量</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, k</span>):</span><br><span class="line">        <span class="built_in">super</span>(FM, self).__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.k = k</span><br><span class="line">        self.w = nn.Linear(self.dim, <span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 初始化V矩阵</span></span><br><span class="line">        self.v = nn.Parameter(torch.rand(self.dim, self.k) / <span class="number">100</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        linear = self.w(x)</span><br><span class="line">        <span class="comment"># 二次项</span></span><br><span class="line">        quadradic = <span class="number">0.5</span> * torch.<span class="built_in">sum</span>(torch.<span class="built_in">pow</span>(torch.mm(x, self.v), <span class="number">2</span>) - torch.mm(torch.<span class="built_in">pow</span>(x, <span class="number">2</span>), torch.<span class="built_in">pow</span>(self.v, <span class="number">2</span>)))</span><br><span class="line">        <span class="comment"># 套一层sigmoid转成分类模型，也可以不加，就是回归模型</span></span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(linear + quadradic)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">fm = FM(ndim, k)</span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line">optimizer = torch.optim.SGD(fm.parameters(), lr=<span class="number">0.005</span>, weight_decay=<span class="number">0.001</span>)</span><br><span class="line">iteration = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    fm.train()</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        output = fm(X)</span><br><span class="line">        l = loss_fn(output.squeeze(dim=<span class="number">1</span>), y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        iteration += <span class="number">1</span>        </span><br><span class="line">        <span class="keyword">if</span> iteration % <span class="number">200</span> == <span class="number">199</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fm.<span class="built_in">eval</span>()</span><br><span class="line">                output = fm(X_eva_tensor)</span><br><span class="line">                l = loss_fn(output.squeeze(dim=<span class="number">1</span>), y_eva_tensor)</span><br><span class="line">                acc = ((torch.<span class="built_in">round</span>(output).long() == y_eva_tensor.view(-<span class="number">1</span>, <span class="number">1</span>).long()).<span class="built_in">sum</span>().<span class="built_in">float</span>().item()) / <span class="number">1024</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125;, iteration: &#123;&#125;, loss: &#123;&#125;, acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, iteration, l.item(), acc))</span><br><span class="line">            fm.train()</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h1><img src="/2022/01/19/FM%20&%20DeepFM/DeepFM.png" class="">
<script type="math/tex; mode=display">
\hat{y}=\operatorname{sigmoid}\left(y_{F M}+y_{D N N}\right)</script><h2 id="FM-1"><a href="#FM-1" class="headerlink" title="FM"></a>FM</h2><img src="/2022/01/19/FM%20&%20DeepFM/FM.png" class="">
<p>该组件就是在计算FM：</p>
<script type="math/tex; mode=display">
y_{F M}=\langle w, x\rangle+\sum_{j_{1}=1}^{d} \sum_{j_{2}=j_{1}+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{j_{1}} \cdot x_{j_{2}}</script><p>注意不是：$w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)$</p>
<ul>
<li>每个 $Field$ 是one-hot形式，黄色的圆表示 $1$ ，蓝色的代表 $0$</li>
<li>连接黄色圆的黑线就是在做：$\langle w, x\rangle$</li>
<li>连接embedding的红色线就是在做：$\sum_{j_{1}=1}^{d} \sum_{j_{2}=j_{1}+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{j_{1}} \cdot x_{j_{2}}$</li>
</ul>
<h2 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h2><img src="/2022/01/19/FM%20&%20DeepFM/Deep.png" class="">
<p>DNN部分比较简单，但它是与FM部分共享Embedding的。</p>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s?__biz=Mzg5NTYyMDgyNg==&amp;mid=2247489278&amp;idx=1&amp;sn=f3652394955d719bf02a91ca3b179ed2&amp;source=41#wechat_redirect" >原创 | 想做推荐算法？先把FM模型搞懂再说<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://fancyerii.github.io/2019/12/19/deepfm/" >DeepFM模型CTR预估理论与实战<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/57873613" >深度推荐模型之DeepFM<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/techflow/p/14260630.html" >吃透论文——推荐算法不可不看的DeepFM模型<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Recommender Systems</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT常见问答</title>
    <url>/2021/04/07/GBDT%E5%B8%B8%E8%A7%81%E9%97%AE%E7%AD%94/</url>
    <content><![CDATA[<p>关于GBDT的算法原理和实例讲解可见：</p>
<span id="more"></span>
<ul>
<li><a class="link"   href="https://blog.csdn.net/zpalyq110/article/details/79527653?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control" >GBDT算法原理以及实例讲解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/105497113" >GBDT总结<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<p>下面是涉及到的GBDT的面试问答：</p>
<ol>
<li><p>基本原理</p>
<p>通过多轮迭代，每轮迭代产生一个弱分类器（利用CART回归树构建），每个分类器在上一轮分类器的残差基础上进行训练。最后将这些弱分类器线性组合成一个强学习器。</p>
</li>
<li><p>GBDT如何做特征选择？</p>
<ol>
<li>遍历样本的特征，对于每个特征，遍历样本的切分点，选择最优的特征的最优切分点；</li>
<li>判断最优时使用平方误差。使用一个特征及其切分点可将样本分为两部分，每部分都计算一个标签的平均值，计算标签平均值与标签的平方误差之和，平方误差最小的特征–切分点组合即是最优的。</li>
</ol>
</li>
<li><p>GBDT如何构建特征？</p>
<p>gbdt 本身是不能产生特征的，但是我们可以利用gbdt去产生特征的组合。</p>
<img src="/2021/04/07/GBDT%E5%B8%B8%E8%A7%81%E9%97%AE%E7%AD%94/combine_features.png" class="">
<p>如上图所示，使用 GBDT 生成了两棵树，两颗树一共有五个叶子节点。我们将样本 X 输入到两颗树当中去，样本X 落在了第一棵树的第二个叶子节点，第二颗树的第一个叶子节点，于是我们便可以依次构建一个五纬的特征向量，每一个纬度代表了一个叶子节点，样本落在这个叶子节点上面的话那么值为1，没有落在该叶子节点的话，那么值为0。</p>
<p>​    于是对于该样本，我们可以得到一个向量 $[0,1,0,1,0]$ 作为该样本的组合特征，和原来的特征一起输入到逻辑回归当中进行训练。实验证明这样会得到比较显著的效果提升。</p>
</li>
<li><p>GBDT如何用于分类？</p>
<p>GBDT无论用于分类还是回归一直都是使用的CART 回归树。因为gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的。这里的残差就是当前模型的负梯度值 。这个要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的。残差相减是有意义的。如果选用的弱分类器是分类树，类别相减是没有意义的。上一轮输出的是样本 $x$ 属于A类，本一轮训练输出的是样本 $x$ 属于B类。 A 和 B 很多时候甚至都没有比较的意义，A类-B类是没有意义的。</p>
<p>​    具体到多分类这个任务上来，假设样本 $X$ 总共有 $K$ 类。来了一个样本  $x$ ，我们需要使用GBDT来判断 $x$ 属于样本的哪一类。</p>
<p>​    第一步 我们在训练的时候，是针对样本 $X$ 每个可能的类都训练一个分类回归树。举例说明，目前样本有三类，也就是 $K = 3$ 。样本 $x$ 属于第二类。那么针对该样本 $x$ 的分类结果，其实我们可以用一个 三维向量 $[0,1,0]$ 来表示。 $0$ 表示样本不属于该类， $1$ 表示样本属于该类。由于样本已经属于第二类了，所以第二类对应的向量维度为 $1$，其他位置为 $0$ 。</p>
<p>​    针对样本有三类的情况，我们实质上是在每轮的训练的时候是同时训练三颗树。第一颗树针对样本x的第一类，输入为 $(x,0)$ 。第二颗树输入针对样本x的第二类，输入为 $(x,1)$ 。第三颗树针对样本x的第三类，输入为 $(x,0)$</p>
<p>​    在这里每颗树的训练过程其实就是就是我们之前已经提到过的CATR TREE 的生成过程。在此处我们参照之前的生成树的程序 即可以就解出三颗树，以及三颗树对 $x$ 类别的预测值 $f_1(x),f_2(x),f_3(x)$ 。那么在此类训练中，我们仿照多分类的逻辑回归 ，使用softmax来产生概率，则属于类别 $1$ 的概率：</p>
<script type="math/tex; mode=display">
p_1 = \frac{e^{f_1(x)}}{\sum_{k=1}^3 e^{f_k(x)}}</script><p>​    并且我们我们可以针对类别1求出残差$y_1 = 0-p_1(x)$；类别2 求出残差$y_2 = 0-p_2(x)$；类别3 求出残差$y_3 = 0-p_3(x)$</p>
<p>​    然后开始第二轮训练 针对第一类 输入为$(x, y_1(x))$， 针对第二类输入为$(x, y_2(x))$，针对 第三类输入为$(x, y_3(x))$。继续训练出三颗树。一直迭代M轮。每轮构建3颗树。</p>
<p>​    所以当 $K=3$ ，我们其实应该有三个式子：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&F_{1 M}(x)=\sum_{m=1}^{M} \hat{C_{1 m}} I\left(x \epsilon R_{1 m}\right) \\
&F_{2 M}(x)=\sum_{m=1}^{M} C_{2 m} I\left(x \epsilon R_{2 m}\right) \\
&F_{3 M}(x)=\sum_{m=1}^{M} \hat{C_{3 m}} I\left(x \epsilon R_{3 m}\right)
\end{aligned}</script><p>​    当训练完毕以后，新来一个样本x，我们需要预测该样本的类别的时候，便可以有这三个式子产生三个值$F_{1M}(x),F_{2M}(x),F_{3M}(x)$。样本属于 某个类别c的概率为：</p>
<script type="math/tex; mode=display">
p_c = \frac{e^{F_{cM}(x)}}{\sum_{k=1}^3 e^{F_{kM}(x)}}</script></li>
</ol>
<ol>
<li><p>GBDT通过什么方式减少误差？</p>
<p>每棵树都是在拟合当前模型的预测值和真实值之间的误差，GBDT是通过不断迭代来使得误差见小的过程。</p>
</li>
<li><p>GBDT的效果相比于传统的LR，SVM效果为什么好一些 ？</p>
<p>GBDT基于树模型，继承了树模型的优点（对异常点鲁棒（使用了Huber损失函数和Quantile损失函数）、不相关的特征干扰性低（LR需要加正则）、可以很好地处理缺失值、受噪音的干扰小）</p>
<p>注：相对于RF，GBDT对异常值比较敏感，原因是当前的错误会延续给下一棵树。</p>
</li>
<li><p>RF和GBDT的异同</p>
<ul>
<li>相同点：都是由多棵树组成，最终结果由多棵树一起决定。</li>
<li>不同点：</li>
<li>集成学习：RF属于bagging思想，而GBDT是boosting思想</li>
<li>偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差</li>
<li>训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本</li>
<li>并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)</li>
<li>最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合</li>
<li>数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感</li>
<li>泛化能力：RF不易过拟合，而GBDT容易过拟合</li>
</ul>
</li>
<li><p>比较LR和GBDT，说说什么情景下GBDT不如LR？</p>
<ul>
<li>LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程。</li>
<li>GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合。</li>
</ul>
<p>当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。具体示例见：<a class="link"   href="https://zhuanlan.zhihu.com/p/156047718" >12. 比较LR和GBDT，说说什么情景下GBDT不如LR<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p>GBDT的参数有哪些，如何调参 ？</p>
<ul>
<li>学习率</li>
<li>最大弱学习器的个数（太小欠拟合，太大过拟合 ）</li>
<li>子采样（防止过拟合，太小欠拟合。GBDT中是不放回采样 ）</li>
<li>最大特征数</li>
<li>最大树深（太大过拟合）</li>
<li>内部节点再划分所需最小样本数（越大越防过拟合 ）</li>
<li>叶子节点最小的样本权重和（如果存在较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重。越大越防过拟合） </li>
<li>最大叶子节点数（太大过拟合）</li>
</ul>
</li>
<li><p>GBDT的优缺点？</p>
<p>优点：</p>
<ul>
<li><p>可以灵活处理各种类型的数据，包括连续值和离散值。 </p>
</li>
<li><p>在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。 </p>
</li>
<li><p>使用一些健壮的损失函数，对异常值的鲁棒性非常强</p>
<p>缺点：由于弱学习器之间存在依赖关系，难以并行训练数据。</p>
</li>
</ul>
</li>
<li><p>GBDT的“梯度提升”体现在哪个阶段？</p>
<p>在构建CART树时使用了损失函数的负梯度，而不是所谓的残差=真值-预测值；实际上是一种更宽广的概念，但是在平方损失的情况下，上面等式是成立的。另外使用损失函数的梯度可以保证损失函数最小值。所以GBDT的梯度提升体现在构建CART树的所需的负梯度阶段，其利用最速下降的近似方法。</p>
</li>
<li><p>怎样设置单棵树的停止生长条件？</p>
<ul>
<li>最大深度</li>
<li>最多叶子结点数</li>
<li>结点分裂时的最小样本数</li>
<li>loss满足约束条件</li>
</ul>
</li>
<li><p>GBDT哪些部分可以并行？</p>
<ul>
<li>计算并更新每个样本的负梯度；</li>
<li>分裂挑选最佳特征及其分割点时，对特征计算相应的误差及均值时；</li>
<li>最后预测过程中，每个样本将之前的所有树的结果累加的时候。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/132726342" >GBDT详解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/xwl198937/article/details/79749048" >GBDT几问<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/YangTinTin/article/details/104930839" >GBDT算法原理及常见面试问题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/25496196" >N问GBDT<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/ModifyRong/p/7744987.html" >机器学习算法GBDT的面试要点总结-上篇<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>GCN</title>
    <url>/2020/05/24/GCN/</url>
    <content><![CDATA[<p>最近两周断断续续学习了GCN有关的知识，在此主要记录一下GCN状态更新的公式推导。</p>
<span id="more"></span>
<h2 id="图卷积起缘"><a href="#图卷积起缘" class="headerlink" title="图卷积起缘"></a>图卷积起缘</h2><p>我们先探讨一个问题：<strong>为什么研究者们要设计图卷积操作，传统的卷积不能直接用在图上吗？</strong> 要理解这个问题，我们首先要理解能够应用传统卷积的<strong>图像(欧式空间)</strong>与<strong>图(非欧空间)</strong>的区别。如果把图像中的每个像素点视作一个结点，如下图左侧所示，一张图片就可以看作一个非常稠密的图；下图右侧则是一个普通的图。阴影部分代表<strong>卷积核</strong>，左侧是一个传统的卷积核，右侧则是一个图卷积核。</p>
<img src="/2020/05/24/GCN/convolution.png" class="">
<p>仔细观察上图，可以发现两点不同：</p>
<ol>
<li><p>在图像为代表的欧式空间中，结点的邻居数量都是固定的。比如说绿色结点的邻居始终是8个(边缘上的点可以做Padding填充)。但在图这种非欧空间中，结点有多少邻居并不固定。目前绿色结点的邻居结点有2个，但其他结点也会有5个邻居的情况。</p>
</li>
<li><p>欧式空间中的卷积操作实际上是用<strong>固定大小可学习的卷积核</strong>来抽取像素的特征，比如这里就是抽取绿色结点对应像素及其相邻像素点的特征。但是因为图里的邻居结点不固定，所以传统的卷积核不能直接用于抽取图上结点的特征。</p>
</li>
</ol>
<p>真正的难点聚焦于<strong>邻居结点数量不固定</strong>上。目前主流的研究从2条路来解决这件事：</p>
<ol>
<li><p>提出一种方式把非欧空间的图转换成欧式空间。</p>
</li>
<li><p>找出一种可处理变长邻居结点的卷积核在图上抽取特征。</p>
</li>
</ol>
<p>这两条实际上也是后续图卷积神经网络的设计原则，<strong>图卷积</strong>的本质是想找到<strong>适用于图的可学习卷积核</strong>。</p>
<h2 id="图卷积框架"><a href="#图卷积框架" class="headerlink" title="图卷积框架"></a>图卷积框架</h2><img src="/2020/05/24/GCN/framework.png" class="">
<p>如上图所示，输入的是整张图，前向传播过程如下：</p>
<ol>
<li><p>在<code>Convolution Layer 1</code>里，对每个结点的邻居都进行一次卷积操作，并用卷积的结果更新该结点；</p>
</li>
<li><p>经过激活函数如<code>ReLU</code>；</p>
</li>
<li><p>再过一层卷积层<code>Convolution Layer 2</code>与一层激活函数；</p>
</li>
<li><p>重复1~3步骤，直到层数达到预期深度。</p>
</li>
</ol>
<p>与GNN类似，图卷积神经网络也有一个局部输出函数，用于将结点的状态(包括隐藏状态与结点特征)转换成任务相关的标签，比如水军账号分类，这种任务称为<code>Node-Level</code>的任务；也有一些任务是对整张图进行分类的，比如化合物分类，这种任务称为<code>Graph-Level</code>的任务。<strong>卷积操作关心每个结点的隐藏状态如何更新</strong>，而对于<code>Graph-Level</code>的任务，它们会在卷积层后加入更多操作。</p>
<h3 id="卷积操作"><a href="#卷积操作" class="headerlink" title="卷积操作"></a>卷积操作</h3><h4 id="空域卷积-Spatial-Convolution"><a href="#空域卷积-Spatial-Convolution" class="headerlink" title="空域卷积(Spatial Convolution)"></a>空域卷积(Spatial Convolution)</h4><p>从设计理念上看，空域卷积与深度学习中的卷积的应用方式类似，其核心在于<strong>聚合邻居结点的信息</strong>。比如说，一种最简单的无参卷积方式可以是：将所有直连邻居结点的隐藏状态加和，来更新当前结点的隐藏状态。</p>
<img src="/2020/05/24/GCN/spatial.png" class="">
<p>常见的空域卷积网络有如下几种：</p>
<ul>
<li><p><a class="link"   href="https://arxiv.org/abs/1704.01212" >消息传递网络(Message Passing Neural Network)<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://arxiv.org/abs/1706.02216" >图采样与聚合(Graph Sample and Aggregate)<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://arxiv.org/pdf/1605.05273" >图结构序列化(PATCHY-SAN)<i class="fas fa-external-link-alt"></i></a></p>
</li>
</ul>
<h4 id="频域卷积-Spectral-Convolution"><a href="#频域卷积-Spectral-Convolution" class="headerlink" title="频域卷积(Spectral Convolution)"></a>频域卷积(Spectral Convolution)</h4><p>空域卷积非常直观地借鉴了图像里的卷积操作，但它缺乏一定的理论基础。而频域卷积则不同，它主要利用的是<strong>图傅里叶变换(Graph Fourier Transform)</strong>实现卷积。简单来讲，它利用图的<strong>拉普拉斯矩阵(Laplacian matrix)</strong>导出其频域上的的拉普拉斯算子，再类比频域上的欧式空间中的卷积，导出图卷积的公式。虽然公式的形式与空域卷积非常相似，但频域卷积的推导过程却有些艰深晦涩。因此本文主要来推导GCN的状态更新公式。</p>
<h5 id="傅里叶变换-Fourier-Transform"><a href="#傅里叶变换-Fourier-Transform" class="headerlink" title="傅里叶变换(Fourier Transform)"></a>傅里叶变换(Fourier Transform)</h5><p><strong>FT</strong>会将一个在空域(或时域)上定义的函数分解成频域上的若干频率成分。换句话说，傅里叶变换可以将一个函数从空域变到频域。用 $F$ 来表示傅里叶变换的话，这里有一个很重要的恒等式：</p>
<script type="math/tex; mode=display">
(f * g)(t) = F^{-1}[F[f(t)] \odot F[g(t)]]</script><p>$f$ 经过傅里叶变换后 $\hat{f}$ 如下所示：</p>
<script type="math/tex; mode=display">
\hat{f}(t)=\int f(x) e^{-2 \pi i x t} d x</script><p>其中$i = \sqrt{-1}$ 是虚数单位，$t$ 是任意实数。$e^{-2 \pi i x t}$ 是类比构造傅里叶变换的关键。它实际上是拉普拉斯算子$\Delta$的广义特征函数。</p>
<p>特征向量需要满足的定义式是：对于矩阵$A$，其特征向量满足的条件应是矩阵与特征向量$x$做乘法的结果，与特征向量乘标量λλ的结果一样，即满足如下等式：</p>
<script type="math/tex; mode=display">
Ax = \lambda x</script><p>$\Delta$ 作用在 $e^{-2 \pi i x t}$ 满足上述特征向量的定义：</p>
<script type="math/tex; mode=display">
\Delta e^{-2 \pi i x t}=\frac{\partial^{2}}{\partial t^{2}} e^{-2 \pi i x t}=-4 \pi^{2} x^{2} \exp ^{-2 \pi i x t}</script><p>$\Delta$ 即为 $-4 \pi^2 x^2$，注意这里 $t$ 是变量，$x$ 是常量。本质上，傅里叶变换是将$f(t)$映射到了以$\left\{e^{-2 \pi i x t}\right\}$为基向量的空间中。</p>
<h5 id="图上的傅里叶变换"><a href="#图上的傅里叶变换" class="headerlink" title="图上的傅里叶变换"></a>图上的傅里叶变换</h5><p>图上的拉普拉斯矩阵 $L$ 可以按照如下公式分解：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
    L= I_N - D^{- \frac {1} {2}}AD^{- \frac {1} {2}} = U \Lambda U^{T} \\
    U=\left(u_{1}, u_{2}, \cdots, u_{n}\right) \\
    \Lambda=\left[\begin{array}{ccc}
    \lambda_{1} & \dots & 0 \\
    \dots & \dots & \dots \\
    0 & \dots & \lambda_{n}
    \end{array}\right]
\end{array}</script><p>$I_N$ 为单位矩阵，$D$ 为度矩阵，$A$ 为领阶矩阵，$u$ 是特征向量，$\lambda$ 是特征值。</p>
<p>图上的卷积与传统的卷积非常相似，这里 $f$ 是特征函数，$g$ 是卷积核：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
(f * g)=F^{-1}[F[f] \odot F[g]] \\
\left(f *_{G} g\right)=U\left(U^{T} f \odot U^{T} g\right)=U\left(U^{T} g \odot U^{T} f\right)
\end{array}</script><p>如果把 $U^{T} g$ 整体看作可学习的卷积核，记作 $g_{\theta}$ ，最终图上的卷积公式即是：</p>
<script type="math/tex; mode=display">
o=\left(f *_{G} g\right)_{\theta}=U g_{\theta} U^{T} f</script><h5 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h5><p>频域卷积即是在 $g_\theta$ 上做文章。类比 $L$ 的分解公式，我们将 $g_\theta$ 看作是 $\Lambda$ 的函数 $g_\theta(\Lambda)$ 。由于特征分解计算量是非常巨大的，使用Chebyshev对 $g_\theta(\Lambda)$ 继续宁近似估计：</p>
<script type="math/tex; mode=display">
g_{\theta^{\prime}}(\Lambda) \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{\Lambda})</script><p>$\tilde{\Lambda}=\frac{2}{\lambda_{\max }} \Lambda-I_{N}$ ，$\theta^{\prime} \in \mathbb{R}^{K}$ 是Chebyshev系数。切比雪夫多项式是递归定义的：$T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x), T_0(x) = 1, T_1(x) = x$ 。</p>
<p>基于上述假设吗，图上卷积公式近似为：</p>
<script type="math/tex; mode=display">
o \approx  \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{\Lambda}) f =  \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{L}) f</script><p>$\tilde{L}=\frac{2}{\lambda_{max}} L-I_{N}$，因为$\left(U \Lambda U^{\top}\right)^{k}=U \Lambda^{k} U^{\top}$ </p>
<p>取$K = 1$，将上述公式展开：</p>
<script type="math/tex; mode=display">
o \approx \theta_{0}^{\prime} f + \theta_{1}^{\prime}\left(L-I_{N}\right) f = \theta_{0}^{\prime} f - \theta_{1}^{\prime} D^{-\frac{1}{2}} A D^{-\frac{1}{2}} f</script><p>为了防止模型过拟合，我们还可以将参数进一步合并：</p>
<script type="math/tex; mode=display">
o \approx \theta\left(I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right) f</script><p>$\theta=\theta_{0}^{\prime}=-\theta_{1}^{\prime}$，$I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ 的特征值取值范围现在是 $[0, 2]$ 。为了防止误差在反向传播的过程中出现梯度弥散，将该式进行归一化：</p>
<script type="math/tex; mode=display">
I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} \\
\tilde{A}=A+I_{N} \\
\tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}</script><p>现在我们可以将 $l$ 层隐藏状态 $H^l \in \mathbb{R}^{N \times C}$ ，$C$ 是某个node的特征数量，那么最终的状态更新公式为：</p>
<script type="math/tex; mode=display">
H^{l+1} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^l \Theta</script><p>$\Theta \in \mathbb{R}^{C \times F}$ 是可训练的卷积核参数。</p>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><p><a class="link"   href="https://arxiv.org/abs/1809.05679" >Graph Convolutional Networks for Text Classification<i class="fas fa-external-link-alt"></i></a> 该篇论文使用GCN对圣经的章节进行分类。具体的实现思路见<a href="https://towardsdatascience.com/text-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f"><strong>Text-based Graph Convolutional Network — Bible Book Classification</strong></a>，代码见 <a class="link"   href="https://github.com/plkmo/Bible_Text_GCN" >https://github.com/plkmo/Bible_Text_GCN<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_2.html" >从图(Graph)到图卷积(Graph Convolution)：漫谈图神经网络模型 (二)
<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/54505069" >图卷积网络(GCN)新手村完全指南<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>GLM</title>
    <url>/2019/05/29/GLM/</url>
    <content><![CDATA[<h2 id="为什么要引入GLM？"><a href="#为什么要引入GLM？" class="headerlink" title="为什么要引入GLM？"></a>为什么要引入GLM？</h2><p>我们知道了”回归“一般是用于预测样本的值，这个值通常是连续的。但是受限于其连续的特性，一般用它来进行分类的效果往往很不理想。为了保留线性回归”简单效果又不错“的特点，又想让它能够进行分类，因此需要对预测值再做一次处理。这个多出来的处理过程，就是GLM所做的最主要的事。而处理过程的这个函数，我们把它叫做连接函数。</p>
<span id="more"></span>
<p>如下图是一个广义模型的流程：</p>
<img src="/2019/05/29/GLM/1.jpg" class="">
<p>图中，当一个处理样本的回归模型是线性模型，且连接函数满足一定特性（特性下面说明）时，我们把模型叫做广义线性模型。因为广义模型的最后输出可以为离散，也可以为连续，因此，用广义模型进行分类、回归都是可以的。</p>
<p>但是为什么线性回归是广义线性模型的子类呢，因为连接函数是 $f(x)=x$ 本身的时候，也就是不做任何处理时，它其实就是一个线性回归。</p>
<p>所以模型的问题就转化成获得合适的连接函数？以及有了连接函数，怎么求其预测函数 $h_\theta(x)$ ？</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>刚才说了，只有连接函数满足一定特性才属于广义线性模型。特性是什么呢？先简单描述下背景。</p>
<p>在广义线性模型中，为了提高可操作性，因此限定了概率分布必须满足指数族分布：</p>
<script type="math/tex; mode=display">
p(y;\eta) = b(y)e^{ {\eta^T}{T(y)-a(\eta)} }</script><blockquote>
<ul>
<li>$\eta$ 称为这个分布的 <strong>自然参数(natural parameter)</strong> 或者 <strong>规范参数(canonical parameter)</strong>。$\eta=\theta^TX$ ，即自然参数=参数与自变量X的线性组合。</li>
<li>$T(y)$ 称为 <strong>充分统计量(sufficient statistic)</strong>。</li>
<li>$a(\eta)$ 称为 <strong>对数分割函数(log partition function)</strong>，$e^{-a(\eta)}$ 是分布 $p(y;\eta)$ 的归一化常数，用来确保该分布对 $y$ 的积分为 1。</li>
<li>当 $T,a,b$ 固定之后，也就确定了这样一个以 $\eta$ 为参数的分布族。</li>
</ul>
</blockquote>
<h3 id="广义线性模型的三个假设"><a href="#广义线性模型的三个假设" class="headerlink" title="广义线性模型的三个假设"></a>广义线性模型的三个假设</h3><ul>
<li>$(y|x;\theta)\sim Exponential Family(\eta)$：给定样本 $x$ 和 参数 $\theta$ ，样本分类 $y$ 服从指数分布。</li>
<li>给定一个 $x$ ，我们需要的目标函数为 $h_\theta(x) = E[T(y) | x]$ 。</li>
<li>$\eta = (\vec \theta)^T \vec X$ ，即自然参数 $\eta$ 和 输入 $\vec X$ 满足线性关系。</li>
</ul>
<h2 id="连接函数的获取"><a href="#连接函数的获取" class="headerlink" title="连接函数的获取"></a>连接函数的获取</h2><p>从上图可以看到 $\eta$ 为函数的输入，而 $h_\theta(x)$ 为函数的输出，所以有公式：</p>
<script type="math/tex; mode=display">
h_\theta(x) = f(\eta)</script><p>但是我们会把 $f$ 的逆 $f^{-1}$ 称为<strong>连接函数</strong> ， 也即以 $h_\theta(x)$ 为自变量，$\eta$ 为因变量的函数为连接函数：</p>
<script type="math/tex; mode=display">
\eta = f^{-1}(h_\theta(x))</script><p>所以求连接函数的步骤也就变成：</p>
<ol>
<li>将 $\vec Y$ 、$\vec X$ 所满足的分布转换成指数分布形式。</li>
<li>在指数分布形式中获得 $T(y)$ 的函数形式和 $\eta$  的值。</li>
<li>算出 $E[T(y)|x]$ 和 $\eta$ 的关系，并把 $(\vec \theta)^T$ 代入到$\eta$ 中，获得连接函数。</li>
</ol>
<h2 id="常见连接函数求解及对应回归"><a href="#常见连接函数求解及对应回归" class="headerlink" title="常见连接函数求解及对应回归"></a>常见连接函数求解及对应回归</h2><h3 id="伯努利分布-—-gt-Logistic-回归"><a href="#伯努利分布-—-gt-Logistic-回归" class="headerlink" title="伯努利分布 —-&gt; Logistic 回归"></a>伯努利分布 —-&gt; Logistic 回归</h3><p>伯努利分布只有0、1两种情况，因此它的概率分布可以写成：</p>
<script type="math/tex; mode=display">
p(y;\phi) = \phi^y(1-\phi)^{1-y} \qquad y=[0,1] \qquad \phi: 实验为1发生的概率</script><p>下面是逻辑回归的推导过程：</p>
<img src="/2019/05/29/GLM/2.png" class="">
<h3 id="多项分布-—-gt-softmax-回归"><a href="#多项分布-—-gt-softmax-回归" class="headerlink" title="多项分布 —-&gt; softmax 回归"></a>多项分布 —-&gt; softmax 回归</h3><p>前面说过的分类问题都是处理那些分两类的问题。比如区分猫或者狗的问题，就是一类是或者否的问题。但是现实生活中还有更加多的多类问题。比如猫分类，有田园猫，布偶猫，暹罗猫各种猫，这里就不能够用两分类来做了。 </p>
<p>这里先设问题需要区分 $k$ 类，即 $y \in \lbrace1, 2, 3, …, k\rbrace$ 。此处无疑使用多项式来建模。多项式分布是二项分布的一个扩展，其取值可以从$\lbrace1, 2, 3, …, k\rbrace$ 中取，可以简单建模如下：</p>
<script type="math/tex; mode=display">
\left[
\begin{matrix}
 1      & 0      & \cdots & 0      \\
 0      & 1      & \cdots & 0      \\
 \vdots & \vdots & \ddots & \vdots \\
 0      & 0      & \cdots & 1      \\
\end{matrix}
\right]</script><p>例如当$y=2$ 时，第二个数为1，其它数为0，因此它的概率分布如下：</p>
<img src="/2019/05/29/GLM/3.png" class="">
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/xierhacker/article/details/53364408" >机器学习笔记五：广义线性模型（GLM）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/BYRans/p/4905420.html" >Softmax回归（Softmax Regression）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://cloud.tencent.com/developer/article/1005793" >机器学习之回归（二）：广义线性模型（GLM）<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Neural Networks</tag>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>GloVe</title>
    <url>/2021/07/21/GloVe/</url>
    <content><![CDATA[<p>GloVe的全称叫Global Vectors for Word Representation，它是一个基于全局词频统计（count-based &amp; overall statistics）的词表征工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性、类比性等。</p>
<span id="more"></span>
<h2 id="构建共现矩阵"><a href="#构建共现矩阵" class="headerlink" title="构建共现矩阵"></a>构建共现矩阵</h2><p>设共现矩阵为 $X$ ，其元素为 $X_{i,j}$ 。</p>
<p>$X_{i,j}$ 的意义为：在整个语料库中，单词 $i$ 和单词 $j$ 共同出现在一个窗口中的次数。</p>
<p>具体示例见：<a class="link"   href="https://blog.csdn.net/coderTC/article/details/73864097" >https://blog.csdn.net/coderTC/article/details/73864097<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="词向量与共现矩阵的近似关系"><a href="#词向量与共现矩阵的近似关系" class="headerlink" title="词向量与共现矩阵的近似关系"></a>词向量与共现矩阵的近似关系</h2><p>构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系：</p>
<script type="math/tex; mode=display">
\log X_{i k}=w_{i}^{T} w_{k}+b_{i}+b_{k}</script><p>具体公式推导见：<a class="link"   href="https://zhuanlan.zhihu.com/p/42073620" >https://zhuanlan.zhihu.com/p/42073620<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="构造损失函数"><a href="#构造损失函数" class="headerlink" title="构造损失函数"></a>构造损失函数</h2><script type="math/tex; mode=display">
J=\sum_{i k} f\left(X_{i k}\right)\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}</script><p>$f(x)$ 为权重函数，满足如下三个特点：</p>
<ul>
<li>$f(0)=0$ ，即两个单词没有在同一个滑动窗口中出现过，那么它们不应该参与到loss的计算中；</li>
<li>$f(x)$ 为非递减函数，即这些单词的权重要大于那些很少在一起出现的单词；</li>
<li>$f(x)$ 不能过大，达到一定程度后不再增加。如果汉语中“这”出现很多次，但重要程度很小；</li>
</ul>
<p>综上 $f(x)$ 定义如下：</p>
<script type="math/tex; mode=display">
f(x)=\left\{\begin{array}{c}
\left(\frac{x}{x_{\max }}\right)^{\alpha}, \text { if } x<x_{\max } \\
1, \text { otherwise }
\end{array}\right.</script><h2 id="GloVe与LSA、Word2Vec的区别"><a href="#GloVe与LSA、Word2Vec的区别" class="headerlink" title="GloVe与LSA、Word2Vec的区别"></a>GloVe与LSA、Word2Vec的区别</h2><ul>
<li>LSA是基于奇异值分解（SVD）的算法，该方法对term-document矩阵（矩阵的每个元素为tf-idf）进行奇异值分解，从而得到term的向量表示和document的向量表示。此处使用的tf-idf主要还是term的全局统计特征。而我们SVD的复杂度是很高的，所以它的计算代价比较大。还有一点是它对所有单词的统计权重都是一致的。</li>
<li>word2vec最大的缺点则是没有充分利用所有的语料，只利用了局部的上下文特征。</li>
<li>GloVe模型就是将这两中特征合并到一起的，即使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/coderTC/article/details/73864097" >理解GloVe模型（Global vectors for word representation）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/42073620" >（十五）通俗易懂理解——Glove算法原理<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/NLP-LOVE/ML-NLP" >ML-NLP<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>Word2Vec</tag>
      </tags>
  </entry>
  <entry>
    <title>IoC &amp; DI</title>
    <url>/2018/05/10/IoC&amp;DI/</url>
    <content><![CDATA[<p>总算搞懂这两个 <code>spring</code> 的核心概念了。</p>
<span id="more"></span>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近在学习Spring框架，它的核心就是IoC容器。要掌握Spring框架，就必须要理解控制反转的思想以及依赖注入的实现方式。下面，我们将围绕下面几个问题来探讨控制反转与依赖注入的关系以及在Spring中如何应用。</p>
<ul>
<li>什么是控制反转？</li>
<li>什么是依赖注入？</li>
<li>它们之间有什么关系？</li>
<li>如何在Spring框架中应用依赖注入？</li>
</ul>
<h1 id="控制反转"><a href="#控制反转" class="headerlink" title="控制反转"></a>控制反转</h1><p>在讨论控制反转之前，我们先来看看软件系统中耦合的对象：</p>
<img src="/2018/05/10/IoC&DI/1.jpg" class="">
<p>从图中可以看到，软件中的对象就像齿轮一样，协同工作，但是互相耦合，一个零件不能正常工作，整个系统就崩溃了。这是一个强耦合的系统。齿轮组中齿轮之间的啮合关系,与软件系统中对象之间的耦合关系非常相似。对象之间的耦合关系是无法避免的，也是必要的，这是协同工作的基础。现在，伴随着工业级应用的规模越来越庞大，对象之间的依赖关系也越来越复杂，经常会出现对象之间的多重依赖性关系，因此，架构师和设计师对于系统的分析和设计，将面临更大的挑战。对象之间耦合度过高的系统，必然会出现牵一发而动全身的情形。</p>
<p>为了解决对象间耦合度过高的问题，软件专家Michael Mattson提出了IOC理论，用来实现对象之间的“解耦”。</p>
<p><strong>控制反转(Inversion of Control)</strong>是一种是面向对象编程中的一种设计原则，用来减低计算机代码之间的耦合度。其基本思想是：借助于“第三方”实现具有依赖关系的对象之间的解耦:</p>

<p>由于引进了中间位置的“第三方”，也就是IOC容器，使得A、B、C、D这4个对象没有了耦合关系，齿轮之间的传动全部依靠“第三方”了，全部对象的控制权全部上缴给“第三方”IOC容器，所以，IOC容器成了整个系统的关键核心，它起到了一种类似“粘合剂”的作用，把系统中的所有对象粘合在一起发挥作用，如果没有这个“粘合剂”，对象与对象之间会彼此失去联系，这就是有人把IOC容器比喻成“粘合剂”的由来。</p>
<p>我们再来看看，控制反转(IOC)到底为什么要起这么个名字？我们来对比一下：</p>
<ol>
<li>软件系统在没有引入IOC容器之前，如图1所示，对象A依赖于对象B，那么对象A在初始化或者运行到某一点的时候，自己必须主动去创建对象B或者使用已经创建的对象B。无论是创建还是使用对象B，控制权都在自己手上。</li>
<li>软件系统在引入IOC容器之后，这种情形就完全改变了，如图2所示，由于IOC容器的加入，对象A与对象B之间失去了直接联系，所以，当对象A运行到需要对象B的时候，IOC容器会主动创建一个对象B注入到对象A需要的地方。</li>
</ol>
<p>通过前后的对比，我们不难看出来：对象A获得依赖对象B的过程,由主动行为变为了被动行为，控制权颠倒过来了，这就是“控制反转”这个名称的由来。</p>
<p>控制反转不只是软件工程的理论，在生活中我们也有用到这种思想。再举一个现实生活的例子：<br>海尔公司作为一个电器制商需要把自己的商品分销到全国各地，但是发现，不同的分销渠道有不同的玩法，于是派出了各种销售代表玩不同的玩法，随着渠道越来越多，发现，每增加一个渠道就要新增一批人和一个新的流程，严重耦合并依赖各渠道商的玩法。实在受不了了，于是制定业务标准，开发分销信息化系统，只有符合这个标准的渠道商才能成为海尔的分销商。让各个渠道商反过来依赖自己标准。反转了控制，倒置了依赖。</p>
<p>我们把海尔和分销商当作软件对象，分销信息化系统当作IOC容器，可以发现，在没有IOC容器之前，分销商就像图1中的齿轮一样，增加一个齿轮就要增加多种依赖在其他齿轮上，势必导致系统越来越复杂。开发分销系统之后，所有分销商只依赖分销系统，就像图2显示那样，可以很方便的增加和删除齿轮上去。</p>
<h1 id="依赖注入"><a href="#依赖注入" class="headerlink" title="依赖注入"></a>依赖注入</h1><p><strong>依赖注入(Dependency Injection)</strong>就是将实例变量传入到一个对象中去(Dependency injection means giving an object its instance variables)。</p>
<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>如果在 <code>Class A</code> 中，有 <code>Class B</code> 的实例，则称 <code>Class A</code> 对 <code>Class B</code> 有一个依赖。例如下面类 <code>Human</code> 中用到一个 <code>Father</code> 对象，我们就说类 <code>Human</code> 对类 <code>Father</code> 有一个依赖：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Human</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    Father father;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Human</span><span class="params">()</span> &#123;</span><br><span class="line">        father = <span class="keyword">new</span> <span class="title class_">Father</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>仔细看这段代码我们会发现存在一些问题：</p>
<ol>
<li>如果现在要改变 <code>father</code> 生成方式，如需要用 <code>new Father(String name)</code> 初始化 <code>father</code>，需要修改 <code>Human</code> 代码。</li>
<li>如果想测试不同 <code>Father</code> 对象对 <code>Human</code>的影响很困难，因为 <code>father</code> 的初始化被写死在了 <code>Human</code> 的构造函数中。</li>
<li>如果 <code>new Father()</code> 过程非常缓慢，单测时我们希望用已经初始化好的 <code>father</code> 对象 <code>Mock</code> 掉这个过程也很困难。</li>
</ol>
<h2 id="注入"><a href="#注入" class="headerlink" title="注入"></a>注入</h2><p>上面将依赖在构造函数中直接初始化是一种 <code>Hard init</code> 方式，弊端在于两个类不够独立，不方便测试。我们还有另外一种 <code>Init</code> 方式，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Human</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    Father father;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Human</span><span class="params">(Father father)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.father = father;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面代码中，我们将 father 对象作为构造函数的一个参数传入。在调用 <code>Human</code> 的构造方法之前外部就已经初始化好了 <code>Father</code> 对象。像这种非自己主动初始化依赖，而通过外部来传入依赖的方式，我们就称为依赖注入。<br>现在我们发现上面 1 中存在的两个问题都很好解决了，简单的说依赖注入主要有两个好处：</p>
<ol>
<li>解耦，将依赖之间解耦。</li>
<li>因为已经解耦，所以方便做单元测试，尤其是 Mock 测试。</li>
</ol>
<h1 id="控制反转和依赖注入的关系"><a href="#控制反转和依赖注入的关系" class="headerlink" title="控制反转和依赖注入的关系"></a>控制反转和依赖注入的关系</h1><p>我们已经分别解释了控制反转和依赖注入的概念。有些人会把控制反转和依赖注入等同，但实际上它们有着本质上的不同：</p>
<ul>
<li><strong>控制反转</strong>是一种思想。</li>
<li><strong>依赖注入</strong>是一种设计模式。</li>
</ul>
<p>IoC框架使用依赖注入作为实现控制反转的方式，但是控制反转还有其他的实现方式，例如说<a class="link"   href="http://martinfowler.com/articles/injection.html#UsingAServiceLocator" >ServiceLocator<i class="fas fa-external-link-alt"></i></a>，所以不能将控制反转和依赖注入等同。</p>
<h2 id="Spring中的依赖注入"><a href="#Spring中的依赖注入" class="headerlink" title="Spring中的依赖注入"></a>Spring中的依赖注入</h2><p>上面我们提到，依赖注入是实现控制反转的一种方式。下面我们结合Spring的IoC容器，简单描述一下这个过程：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MovieLister</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> MovieFinder finder;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setFinder</span><span class="params">(MovieFinder finder)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.finder = finder;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ColonMovieFinder</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setFilename</span><span class="params">(String filename)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.filename = filename;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们先定义两个类，可以看到都使用了依赖注入的方式，通过外部传入依赖，而不是自己创建依赖。那么问题来了，谁把依赖传给他们，也就是说谁负责创建 <code>finder</code>，并且把 <code>finder</code> 传给 <code>MovieLister</code>。答案是Spring的IoC容器。</p>
<p>要使用IoC容器，首先要进行配置。这里我们使用xml的配置，也可以通过代码注解方式配置。下面是<code>spring.xml</code>的内容：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">beans</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;MovieLister&quot;</span> <span class="attr">class</span>=<span class="string">&quot;spring.MovieLister&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;finder&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ref</span> <span class="attr">local</span>=<span class="string">&quot;MovieFinder&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;MovieFinder&quot;</span> <span class="attr">class</span>=<span class="string">&quot;spring.ColonMovieFinder&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;filename&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>movies1.txt<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在Spring中，每个bean代表一个对象的实例，默认是单例模式，即在程序的生命周期内，所有的对象都只有一个实例，进行重复使用。通过配置bean，IoC容器在启动的时候会根据配置生成bean实例。具体的配置语法参考Spring文档。这里只要知道IoC容器会根据配置创建 <code>MovieFinder</code>，在运行的时候把 <code>MovieFinder</code> 赋值给 <code>MovieLister</code> 的 <code>finder</code> 属性，完成依赖注入的过程。</p>
<p>下面给出测试代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testWithSpring</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">ApplicationContext</span> <span class="variable">ctx</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileSystemXmlApplicationContext</span>(<span class="string">&quot;spring.xml&quot;</span>);<span class="comment">//1</span></span><br><span class="line">    <span class="type">MovieLister</span> <span class="variable">lister</span> <span class="operator">=</span> (MovieLister) ctx.getBean(<span class="string">&quot;MovieLister&quot;</span>);<span class="comment">//2</span></span><br><span class="line">    Movie[] movies = lister.moviesDirectedBy(<span class="string">&quot;Sergio Leone&quot;</span>);</span><br><span class="line">    assertEquals(<span class="string">&quot;Once Upon a Time in the West&quot;</span>, movies[<span class="number">0</span>].getTitle());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>根据配置生成 <code>ApplicationContext</code>，即IoC容器。</li>
<li>从容器中获取 <code>MovieLister</code> 的实例。</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>控制反转是一种在软件工程中解耦合的思想，调用类只依赖接口，而不依赖具体的实现类，减少了耦合。控制权交给了容器，在运行的时候才由容器决定将具体的实现动态的“注入”到调用类的对象中。</li>
<li>依赖注入是一种设计模式，可以作为控制反转的一种实现方式。依赖注入就是将实例变量传入到一个对象中去(Dependency injection means giving an object its instance variables)。</li>
<li>通过IoC框架，类A依赖类B的强耦合关系可以在运行时通过容器建立，也就是说把创建B实例的工作移交给容器，类A只管使用就可以。</li>
</ol>
<hr>
<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><ul>
<li><a class="link"   href="http://blog.xiaohansong.com/2015/10/21/IoC-and-DI/" >http://blog.xiaohansong.com/2015/10/21/IoC-and-DI/<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>K-means</title>
    <url>/2021/07/28/K-means/</url>
    <content><![CDATA[<p>无监督学习中的典型代表，它将类别相同的样本汇聚在一起，但是聚类好之后，它并不知道每个样本的类别（看到之前有些博客说做分类的，当时一直困扰我，现在才知道这只是聚类，不是分类）。</p>
<span id="more"></span>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><ol>
<li>随机选择 $k$ 个样本作为初始化聚类中心：$a_1, a_2, \dots, a_k$ ；</li>
<li>针对数据集每个样本 $x_i$ ，计算其到每个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；</li>
<li>针对每个类别 $a_j$ ，重新计算它的聚类中心：$a_j = \frac{1}{|c_j|} \sum_{x \in c_j} x$ （即该类所有样本的质心）；</li>
<li>重复2-3步，直到达到某个终止条件（迭代次数、最小误差变化等）。</li>
</ol>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>容易理解，聚类效果不错，虽然是局部最优， 往往局部最优已经足够</li>
<li>处理大数据集的时候，该算法可以保证较好的伸缩性</li>
<li>算法复杂性低</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>$K$ 需要认为设定，不同的取值对最终聚类效果影响非常大</li>
<li>对初始簇类中心敏感，不同的初始取值对最终聚类效果影响非常大</li>
<li>对异常值敏感</li>
<li>不适合离散的分类、样本类别不平衡的分类、非凸形状的分类</li>
</ul>
<h2 id="如何选择-K-值？"><a href="#如何选择-K-值？" class="headerlink" title="如何选择 $K$ 值？"></a>如何选择 $K$ 值？</h2><p>基本思想还是最小化类内距离，最大化类间距离，使同一簇内样本尽可能相似，不同簇中样本尽可能不相似。</p>
<h4 id="手肘法"><a href="#手肘法" class="headerlink" title="手肘法"></a>手肘法</h4><p>随着 $K$ 值增大，误差值会越来越小（举一个极端的例子：当每一个样本被分为一个类时，类内间距最小，但这显然不是我们想要的）。因此可根据不同 $K$ 值下的误差曲线选择使误差平方和下降最快的 $K$ 值。当大于此 $K$ 值时，$K$ 值增大，但误差减少量很小。即选择曲线上的拐点最佳。在下面这个图中即选择 $k=2$ ，将样本分为两类。</p>
<img src="/2021/07/28/K-means/1.png" class="">
<h4 id="Gap-statistic"><a href="#Gap-statistic" class="headerlink" title="Gap statistic"></a>Gap statistic</h4><p>手肘法的缺点在于需要人工看不够自动化，这里提出Gap statistic：</p>
<script type="math/tex; mode=display">
Gap(K) = E(log D_k) - log(D_k)</script><p>其中 $D_k$ 为损失函数，这里 $E(log D_k)$ 指的是 $log(D_k)$ 的期望。这个数值通常通过蒙特卡洛模拟产生，我们在样本里所在的区域中按照均匀分布随机产生和原始样本数一样多的随机样本，并对这个随机样本做 K-Means，从而得到一个 $D_k$ 。如此往复多次，通常20次，我们可以得到20个 $log D_k$ 。对这20个数值求平均值，就得到了 $E(log D_k)$ ​的近似值。最终可以计算 Gap Statisitc。而 Gap statistic 取得最大值所对应的 $K$ 就是最佳的 $K$ 。</p>
<img src="/2021/07/28/K-means/2.jpeg" class="">
<p>由图可见，当 $K=3$ 时，$Gap(K)$ 取值最大，所以最佳的簇数是 $K=3$ 。</p>
<h2 id="如何初始化聚类中心？"><a href="#如何初始化聚类中心？" class="headerlink" title="如何初始化聚类中心？"></a>如何初始化聚类中心？</h2><p>这里主要介绍K-means++算法，主要思想是选择离已选中心点最远的点。这也比较符合常理，聚类中心当然是互相离得越远越好。</p>
<p>算法流程如下：</p>
<ol>
<li><p>随机选择一个中心点 $a_1$ ；</p>
</li>
<li><p>对于数据集中的每一个点 $x_i$ ，计算它与之前 $n$ 个聚类中心最远的距离 $D(x_i)$ ，并以 $\frac{D(x_i)^2}{\sum_{j=1}^n D(x_j)^2}$ 的概率选择作为新中心点 $a_i$ ；</p>
</li>
<li><p>重复2步骤，直到选出 $K$ 个聚类中心。</p>
</li>
</ol>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/78798251" >【机器学习】K-means（非常详细）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/m0_46568930/article/details/111991654" >【机器学习】——K_means如何选择k值？<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/weixin_42029738/article/details/81978038" >K-means原理、优化及应用<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/dpengwang/article/details/86574999" >K-means++ 中选择初始聚类中心<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM</title>
    <url>/2019/07/07/LSTM/</url>
    <content><![CDATA[<p>记录一下LSTM的模型结构与原理。</p>
<span id="more"></span>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><img src="/2019/07/07/LSTM/1.jpeg" class="">
<p>下面详细介绍LSTM的三个门：</p>
<h3 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h3><p>遗忘门决定了上一时刻的单元状态 $c_{t-1}$ 有多少保留到当前时刻 $c_t$ 。</p>
<img src="/2019/07/07/LSTM/2.jpeg" class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)</script><h3 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h3><p>输入门决定了当前时刻网络的输入 $x_t$ 有多少保存到单元状态 $c_t$ 。</p>
<img src="/2019/07/07/LSTM/3.jpeg" class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)</script><h3 id="当前输入的单元状态-tilde-c-t"><a href="#当前输入的单元状态-tilde-c-t" class="headerlink" title="当前输入的单元状态 $\tilde{c_{t}}$"></a>当前输入的单元状态 $\tilde{c_{t}}$</h3><p>可以把它想象成不包含上一时刻的长期状态 $c_{t-1}$ 时，我们生成的当前此刻的长期状态 $\tilde{c_{t}}$ 。</p>
<img src="/2019/07/07/LSTM/4.jpeg" class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
\tilde{c}_{t}=\tanh \left(W_{c} \cdot\left[h_{t-1}, x_{t}\right]+b_{c}\right)</script><h3 id="计算当前时刻的单元状态-c-t"><a href="#计算当前时刻的单元状态-c-t" class="headerlink" title="计算当前时刻的单元状态 $c_t$"></a>计算当前时刻的单元状态 $c_t$</h3><img src="/2019/07/07/LSTM/5.jpeg" class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
c_{t}=f_{t} \circ c_{t-1}+i_{t} \circ \tilde{c}_{t}</script><p>这样，我们就能够将当前的记忆 $\tilde{c_{t}}$ 和长期的记忆 $c_{t-1}$ 组合在一起，形成新的单元状态 $c_t$ 。由于遗忘门的控制，它可以保存很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。</p>
<h3 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h3><p>用来控制单元状态 $c_t$ 有多少输入到 LSTM 的当前输出值 $h_t$ 。</p>
<img src="/2019/07/07/LSTM/6.jpeg" class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
o_{t}=\sigma\left(W_{o} \cdot\left[h_{t-1}, x_{t}\right]+b_{o}\right)</script><h3 id="计算输出值-h-t"><a href="#计算输出值-h-t" class="headerlink" title="计算输出值 $h_t$"></a>计算输出值 $h_t$</h3><img src="/2019/07/07/LSTM/7.jpeg" class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
h_{t}=o_{t} \circ \tanh \left(c_{t}\right)</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a class="link"   href="https://zhuanlan.zhihu.com/p/44124492" >LSTM：RNN最常用的变体<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Bash及Vim常用命令</title>
    <url>/2018/02/07/Linux%20Bash%E5%8F%8AVim%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>本人算是Linux菜鸟一个，只用到一些很基础的命令，在此记录一下。</p>
<span id="more"></span>
<h2 id="Bash-快捷键"><a href="#Bash-快捷键" class="headerlink" title="Bash 快捷键"></a>Bash 快捷键</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>ctrl + a</code></td>
<td style="text-align:center">移到命令行首</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + e</code></td>
<td style="text-align:center">移到命令行尾</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + f</code></td>
<td style="text-align:center">按字符右移</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + b</code></td>
<td style="text-align:center">按字符左移</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + u</code></td>
<td style="text-align:center">从光标处(不包含)删除至命令行首(包含)</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + k</code></td>
<td style="text-align:center">从光标处(不包含)删除至命令行尾(包含)</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + w</code></td>
<td style="text-align:center">从光标处(不包含)删除至单词字首(包含)</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + d</code></td>
<td style="text-align:center">删除光标处的字符</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + h</code></td>
<td style="text-align:center">删除光标前的字符</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + l</code></td>
<td style="text-align:center">清屏</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + c</code></td>
<td style="text-align:center">终止命令</td>
</tr>
<tr>
<td style="text-align:center"><code>o</code></td>
<td style="text-align:center">在光标所在位置的下一行打开新行插入</td>
</tr>
<tr>
<td style="text-align:center"><code>O</code></td>
<td style="text-align:center">在光标所在位置的上一行打开新行插入</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Vim-快捷键"><a href="#Vim-快捷键" class="headerlink" title="Vim 快捷键"></a>Vim 快捷键</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>x</code></td>
<td style="text-align:center">删除光标所在处字符</td>
</tr>
<tr>
<td style="text-align:center"><code>X</code></td>
<td style="text-align:center">删除光标所在处前一个字符</td>
</tr>
<tr>
<td style="text-align:center"><code>u</code></td>
<td style="text-align:center">撤销</td>
</tr>
<tr>
<td style="text-align:center"><code>i</code></td>
<td style="text-align:center">在当前光标之前插入文本</td>
</tr>
<tr>
<td style="text-align:center"><code>a</code></td>
<td style="text-align:center">在当前光标之后插入文本</td>
</tr>
<tr>
<td style="text-align:center"><code>gg</code></td>
<td style="text-align:center">跳转到文件头</td>
</tr>
<tr>
<td style="text-align:center"><code>shift + g</code></td>
<td style="text-align:center">跳转到文件末尾行首</td>
</tr>
<tr>
<td style="text-align:center"><code>dd</code></td>
<td style="text-align:center">删除一行</td>
</tr>
<tr>
<td style="text-align:center"><code>dw</code></td>
<td style="text-align:center">从光标处(包含)删除到下一个单词开头</td>
</tr>
<tr>
<td style="text-align:center"><code>de</code></td>
<td style="text-align:center">从光标处(包含)删除到本单词末尾</td>
</tr>
<tr>
<td style="text-align:center"><code>db</code></td>
<td style="text-align:center">从光标处(不包含)删除到前一个单词</td>
</tr>
<tr>
<td style="text-align:center"><code>0</code></td>
<td style="text-align:center">移动到行首</td>
</tr>
<tr>
<td style="text-align:center"><code>shift + $</code></td>
<td style="text-align:center">移动到行尾</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Bash</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux定时任务 - crontab</title>
    <url>/2019/02/19/Linux%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%20-%20crontab/</url>
    <content><![CDATA[<p>Linux系统是由 <code>cron</code> 这个系统服务来控制的。Linux系统上面原本就有非常多的计划性任务，因此这个系统服务是默认启动的。但是使用者也可以设置计划任务，Linux系统提供了控制计划任务的命令：<code>crontab</code></p>
<span id="more"></span>
<h1 id="crond-进程"><a href="#crond-进程" class="headerlink" title="crond 进程"></a><code>crond</code> 进程</h1><p><code>crond</code> 是Linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动 <code>crond</code> 进程，<font color="green"><code>crond</code> 进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。</font></p>
<h1 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h1><p>Linux下的任务调度分为两类：</p>
<ul>
<li><p>系统任务调度：系统周期性所要执行的工作。</p>
<ul>
<li><p>常见的系统工作有：</p>
<ul>
<li>写缓存数据到硬盘</li>
<li>日志清理等</li>
</ul>
</li>
<li><p>全局配置文件( <code>/etc</code> 目录 )</p>
<ul>
<li><code>cron.d</code> : 系统自动定期执行的任务。</li>
<li><code>crontab</code> : 设定定时任务执行文件。</li>
<li><code>cron.deny</code> : 用于控制不让哪些用户使用 <code>crontab</code> 的功能。</li>
<li><code>cron.hourly</code> : 每小时执行一次的任务。</li>
<li><code>cron.daily</code> : 每天执行一次的任务。</li>
<li><code>cron.weekly</code> : 每周执行一次的任务。</li>
<li><code>cron.monthly</code> : 每个月执行一次的任务。</li>
</ul>
</li>
<li><p><code>/etc/crontab</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SHELL=/bin/bash</span><br><span class="line">PATH=/sbin:/bin:/usr/sbin:/usr/bin</span><br><span class="line">MAILTO=root</span><br><span class="line"></span><br><span class="line"><span class="comment"># For details see man 4 crontabs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Example of job definition:</span></span><br><span class="line"><span class="comment"># .---------------- minute (0 - 59)</span></span><br><span class="line"><span class="comment"># |  .------------- hour (0 - 23)</span></span><br><span class="line"><span class="comment"># |  |  .---------- day of month (1 - 31)</span></span><br><span class="line"><span class="comment"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</span></span><br><span class="line"><span class="comment"># |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</span></span><br><span class="line"><span class="comment"># |  |  |  |  |</span></span><br><span class="line"><span class="comment"># *  *  *  *  * user-name  command to be executed</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>SHELL</code> 指定来系统要使用哪个shell，这里是bash。</li>
<li><code>PATH</code> 指定系统执行命令的路径。</li>
<li><code>MAILTO</code> 指定crond的任务执行信息将通过电子邮件发送给root用户。</li>
</ul>
</li>
</ul>
</li>
<li><p>用户任务调度：用户定期要执行的工作。</p>
<ul>
<li>常见的用户工作有：<ul>
<li>数据备份</li>
<li>定时邮件提醒等</li>
</ul>
</li>
<li>所有用户定义的 <code>crontab</code> 文件都被保存在 <code>/var/spool/cron</code> 目录中。其文件名与用户名一致。每个用户都有自己的 <code>cron</code> 配置文件,通过 <code>crontab -e</code> 就可以编辑,一般情况下我们编辑好用户的 <code>cron</code> 配置文件保存退出后,系统会自动就存放于 <code>/var/spool/cron/</code> 目录中,文件以用户名命名。Linux的 <code>crond</code> 进程是每隔一分钟去读取一次 <code>/var/spool/cron</code> , <code>/etc/crontab</code> , <code>/etc/cron.d</code> 下面所有的内容。</li>
</ul>
</li>
</ul>
<h1 id="crontab-格式说明"><a href="#crontab-格式说明" class="headerlink" title="crontab 格式说明"></a><code>crontab</code> 格式说明</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># .---------------- minute (0 - 59)</span></span><br><span class="line"><span class="comment"># |  .------------- hour (0 - 23)</span></span><br><span class="line"><span class="comment"># |  |  .---------- day of month (1 - 31)</span></span><br><span class="line"><span class="comment"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</span></span><br><span class="line"><span class="comment"># |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</span></span><br><span class="line"><span class="comment"># |  |  |  |  |</span></span><br><span class="line"><span class="comment"># *  *  *  *  * user-name  command to be executed</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>minute</code> : 表示分钟，可以是从0到59之间的任何整数。</p>
</li>
<li><p><code>hour</code> : 表示小时，可以是从0到23之间的任何整数。</p>
</li>
<li><p><code>day</code> : 表示日期，可以是从1到31之间的任何整数。</p>
</li>
<li><p><code>month</code> : 表示月份，可以是从1到12之间的任何整数。</p>
</li>
<li><p><code>week</code> : 表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。</p>
</li>
<li><p><code>command</code> : 要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。</p>
</li>
</ul>
<p>在以上各个字段中，还可以使用以下特殊字符：</p>
<ul>
<li><p><code>*</code> : 代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。</p>
</li>
<li><p><code>,</code> : 可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”</p>
</li>
<li><p><code>-</code> : 可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”</p>
</li>
<li><p><code>/</code> : 可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。</p>
</li>
</ul>
<h1 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h1><ol>
<li>在 <code>/root</code> 编写一个 <code>hello.py</code> 脚本，内容如下：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello World&quot;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>在 <code>/root</code> 编写一个 <code>test.sh</code> 脚本，内容如下：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">python /root/hello.py &gt;&gt; test.txt</span><br></pre></td></tr></table></figure>
<ol>
<li>输入 <code>crontab -e</code> 命令进入编辑模式，编辑内容如下：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">*/5 * * * * /root/test.sh</span><br></pre></td></tr></table></figure>
<ol>
<li><code>crontab -l</code> 和 <code>crontab -r</code> 分别可以列出当前用户定时任务和删除当前用户定时任务</li>
</ol>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><ol>
<li><code>* * * * * commad</code> -&gt; 每一分钟执行一次command</li>
<li><code>3,15 8-11 */2 * * /etc/init.d/network restart</code> -&gt; 每隔两天的上午8点到11店的第3和第15分钟执行 <code>/etc/init.d/network restart</code></li>
</ol>
<h1 id="踩点"><a href="#踩点" class="headerlink" title="踩点"></a>踩点</h1><ol>
<li>有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。在crontab文件中定义多个调度任务时，需要特别注意的一个问题就是环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这 样，系统执行任务调度时就没有问题了。不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shell脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。例如上文的：<font color="color">#!/bin/bash</font></li>
<li>新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。</li>
</ol>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/intval/p/5763929.html" >Linux定时任务Crontab命令详解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/zoulongbin/p/6187238.html" >Linux 定时任务crontab_014<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>定时任务</tag>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux开机自启动</title>
    <url>/2019/07/01/Linux%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8/</url>
    <content><![CDATA[<p>记录一下Linux开机自启动的原理与实践。</p>
<span id="more"></span>
<blockquote>
<p>实验环境：CentOS7</p>
</blockquote>
<h1 id="etc-init-d"><a href="#etc-init-d" class="headerlink" title="/etc/init.d"></a><code>/etc/init.d</code></h1><p>这是指向 <code>/etc/rc.d/init.d</code> 的软链接。这个目录存放的是一些脚本，一般是linux以rpm包安装时设定的一些服务的启动脚本。系统在安装时装了很多rpm包，这里面就有很多对应的脚本。执行这些脚本可以用来<code>start || stop || reload || status || restart</code> 这些服务。举个例子来说，如果你要重新启动 sendmail 的话，而且你的 sendmail 是以 rpm 来安装的，那么使用 <code>/etc/rc.d/init.d/sendmail restart</code> 就可以直接启动 sendmail 了。</p>
<h2 id="runlevel"><a href="#runlevel" class="headerlink" title="runlevel"></a><code>runlevel</code></h2><p><code>/etc/rc.d/init.d/</code> 这个目录下的脚本就类似与windows中的注册表，在系统启动的时候执行。程序运行到这里(init进程读取了运行级别)， 是该运行init.d里的脚本了，但是并不是直接运行，而是有选择的。因为系统并不需要启动所有的服务。系统是如何选择哪些需要启动哪些不要呢？这时 <code>runlevel</code> 就起作用了。在RH9和FC7的源码中它都是一开始就 <code>check_runlevel()</code> ，知道了运行级别之后，对于每一个运行级别，在 <code>/etc/rc.d/</code> 下都有一个子目录分别是 <code>rc0.d, rc1.d ... rc6.d</code> 。每个目录下都是到 <code>/etc/rc.d/init.d/</code> 目录的一部分脚本一些软链接。每个级别要执行哪些服务就在相对应的目录下，比如级别6要启动的服务就都放在rc6.d下，但是放在这个rc6.d下的都是一些软链接文件，链接到 <code>/etc/rc.d/init.d/</code> 中相对应的文件，真正干活的是 <code>/etc/rc.d/init.d/</code> 里的脚本。<br>(<code>/etc/</code> 下的 <code>rc0.d, rc1.d ... rc6.d</code> 是指向 <code>/etc/rc.d/</code> 下的 <code>rc0.d, rc1.d ... rc6.d</code> 的软链接。)</p>
<h2 id="KS"><a href="#KS" class="headerlink" title="KS"></a><code>KS</code></h2><p>如上图所示，你会发现许多 <code>rc#.d</code> 形式存在的目录，这里 <code>#</code> 代表一个指定的初始化级别，范围是0~6）:</p>
<ul>
<li><strong><em>0</em></strong>：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动</li>
<li><strong><em>1</em></strong>：单用户工作状态，root权限，用于系统维护，禁止远程登陆</li>
<li><strong><em>2</em></strong>：多用户状态(没有NFS)</li>
<li><strong><em>3</em></strong>：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式</li>
<li><strong><em>4</em></strong>：系统未使用，保留</li>
<li><strong><em>5</em></strong>：X11控制台，登陆后进入图形GUI模式</li>
<li><strong><em>6</em></strong>：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动</li>
</ul>
<p>在这些目录之下，包含了许多对进程进行控制的脚本。这些脚本要么以 <code>K##</code> 开头，要么以 <code>S##</code> 开头：</p>
<ul>
<li><strong><em>K</em></strong>：kill，系统将终止对应的服务</li>
<li><strong>S</strong>：start，系统将启动对应的服务</li>
<li><strong>##</strong>：同一运行级别下脚本执行的顺序，数值小的先执行，数值大的后执行。很多时候这些执行顺序是很重要的，比如要启动Apache服务，就必须先配置网络接口。</li>
</ul>
<h1 id="etc-rc-local"><a href="#etc-rc-local" class="headerlink" title="/etc/rc.local"></a><code>/etc/rc.local</code></h1><p>这是指向 <code>/etc/rc.d/rc.local</code> 的软链接。这是使用者自订开机启动程序,把需要开机自动运行的程序写在这个脚本里。也就是说，我有任何想要在开机时就进行的工作时，直接将他写入 <code>/etc/rc.d/rc.local</code> ， 那么该工作就会在开机的时候自动被载入。这一点和windows里面的“启动”菜单有点像。该脚本是在系统初始化级别脚本运行之后再执行的，因此可以安全地在里面添加你想在系统启动之后执行的脚本。常见的情况是开机自启动 <code>mongod</code> 服务：<br></p>
<h1 id="开机自启动"><a href="#开机自启动" class="headerlink" title="开机自启动"></a>开机自启动</h1><p>常见的开机自启动有两种方法：</p>
<ul>
<li>通过将 shell script写入到 <code>/etc/rc.d/rc.local</code></li>
<li>通过 <code>/etc/rc.d/init.d/</code>。假设 <code>/etc/rc.d/init.d/radisd</code> 已存在，文件内容有两行注释如下：</li>
</ul>

<p>表示 <code>redisd</code> 的运行级别是 <code>2 3 4 5</code>，各级别 <strong><em>S(2 3 4 5)</em></strong> 分数为90，<strong><em>K(0 6)</em></strong> 分数为10。<code>chkconfig redisd on</code> 即可将 <code>redisd</code> 添加到系统服务中。这样我们以后就可以通过 <code>systemctl start redisd || systemctl stop redisd</code> 来启动和关闭 <code>redisd</code> 服务。同样的，在 <code>/etc/rc.d/rc(2\3\4\5).d</code> 文件夹下就会有 <code>S90redisd</code> 软链接到 <code>/etc/rc.d/init.d/redisd</code> ，在 <code>/etc/rc.d/rc(0\6).d</code> 文件夹下就会有 <code>K10redisd</code> 软链接到 <code>/etc/rc.d/init.d/redisd</code> 。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/songpeiying/article/details/79933181" >查看Linux服务器的运行级别<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/fatt/p/4790561.html" >/etc/rc.local 与 /etc/init.d Linux 开机自动运行程序<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Oracle中call和exec区别</title>
    <url>/2017/10/09/Oracle%E4%B8%ADcall%E5%92%8Cexec%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>在<font color="red">SQL Plus</font>中这两种方法都可以使用：</p>
<ul>
<li><code>exec pro_name(参数1..)</code></li>
<li><code>call pro_name(参数1..)</code></li>
</ul>
<span id="more"></span>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><ol>
<li>exec是SQL Plus命令，只能在SQL Plus中使用；call为SQL命令，没有限制.</li>
<li><font color="red">存储过程或函数没有参数时,exec可以直接跟过程名（可以省略()），但call则必须带上().</font>

</li>
</ol>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><h3 id="exec"><a href="#exec" class="headerlink" title="exec"></a>exec</h3><ul>
<li>调用存储过程<ul>
<li>有参数：<code>exec mypro(12,&#39;fsdf&#39;);</code></li>
<li>没有参数：<code>exec mypro;</code>，也可以写成<code>exec mypro();</code></li>
</ul>
</li>
<li>调用函数<ul>
<li>有参数：<code>var counts number;exec :counts:=myfunc(&#39;fsde&#39;);</code></li>
<li>没有参数：<code>var counts number;exec :counts:=myfunc;</code>，也可以写成<code>var counts number;exec :counts:=myfunc();</code></li>
</ul>
</li>
</ul>
<h3 id="call"><a href="#call" class="headerlink" title="call"></a>call</h3><ul>
<li>调用存储过程<ul>
<li>有参数：<code>call mypro(23,&#39;fth&#39;);</code></li>
<li>无参数：<code>call mypro();</code></li>
</ul>
</li>
<li>调用函数<ul>
<li>有参数：<code>var counts number;call myfunc(&#39;asd&#39;) into :counts;</code></li>
<li>无参数：<code>var counts number;call myfunc() into :counts;</code></li>
</ul>
</li>
</ul>
<h2 id="其他注意事项"><a href="#其他注意事项" class="headerlink" title="其他注意事项"></a>其他注意事项</h2><ul>
<li>oracle 中字符串应该是单引号而不是双引号</li>
<li>每写完一条sql语句应加上 <font color="red">;</font></li>
<li>为了防止call 和 exec 无参数的存储过程或函数的错误，建议全部加上<font color="red">()</font></li>
</ul>
]]></content>
      <categories>
        <category>DataBase</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title>P &amp; NP &amp; NPC</title>
    <url>/2019/01/24/P%20&amp;%20NP%20&amp;%20NPC/</url>
    <content><![CDATA[<p>这是计算机界的经典问题了。</p>
<span id="more"></span>
<h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><p>常见的时间复杂度分为两种级别：</p>
<ul>
<li>多项式级：$O(1), O(log(n)), O(n^k) $</li>
<li>非多项式级：$O(a^n), O(n!)$</li>
</ul>
<p>当我们在解决一个问题时，我们选择的算法通常都需要是多项式级的复杂度，非多项式级的复杂度需要的时间太多，往往会超时，除非是数据规模非常小。</p>
<p>自然地，人们会想到一个问题：会不会所有的问题都可以找到复杂度为多项式级的算法呢？很遗憾，答案是否定的。有些问题甚至根本不可能找到一个正确的算法来，这称之为“不可解问题”(Undecidable Decision Problem)。<a class="link"   href="http://www.matrix67.com/blog/article.asp?id=62" >The Halting Problem<i class="fas fa-external-link-alt"></i></a>就是一个著名的不可解问题。比如，输出从1到n这n个数的全排列。不管你用什么方法，你的复杂度都是阶乘级，因为你总得用阶乘级的时间打印出结果来。有人说，这样的“问题”不是一个“正规”的问题，正规的问题是让程序解决一个问题，输出一个“YES”或“NO”（这被称为判定性问题），或者一个什么什么的最优值（这被称为最优化问题）。那么，根据这个定义，我也能举出一个不大可能会有多项式级算法的问题来：Hamilton回路。问题是这样的：给你一个图，问你能否找到一条经过每个顶点一次且恰好一次（不遗漏也不重复）最后又走回来的路（满足这个条件的路径叫做Hamilton回路）。这个问题现在还没有找到多项式级的算法。事实上，这个问题就是我们后面要说的NPC问题。</p>
<h1 id="P问题"><a href="#P问题" class="headerlink" title="P问题"></a>P问题</h1><blockquote>
<p>如果一个问题可以找到一个能在多项式的时间里解决它的算法，那么这个问题就属于P问题。</p>
</blockquote>
<p>我们常见到的一些信息奥赛的题目都是P问题。道理很简单，一个用穷举换来的非多项式级时间的超时程序不会涵盖任何有价值的算法。</p>
<h1 id="NP问题"><a href="#NP问题" class="headerlink" title="NP问题"></a>NP问题</h1><blockquote>
<p>可以在多项式的时间里验证一个解的问题。</p>
</blockquote>
<p>举个例子：给你一个边带权值的图，让你找出一条路径使得该路径边权小于100。这个图是非常复杂的，你只能用非多项式的时间去穷举每一条路径，这是非常耗时的。但如果我给你一条路径（即一个解），你只需要在多项式的时间内遍历这条路径，将这条路径的权值相加，如果权值和小于100，那么这个解就是正确的，即：我在多项式时间内验证了一个解。</p>
<font color="green">之所以要定义NP问题，是因为通常只有NP问题才可能找到多项式的算法。我们不会指望一个连多项式地验证一个解都不行的问题存在一个解决它的多项式级的算法。</font>


<h1 id="P问题属于NP问题"><a href="#P问题属于NP问题" class="headerlink" title="P问题属于NP问题"></a>P问题属于NP问题</h1><p>所有的P类问题都是NP问题。也就是说，能多项式地解决一个问题，必然能多项式地验证一个问题的解——既然正解都出来了，验证任意给定的解也只需要比较一下就可以了。关键是，人们想知道，是否所有的NP问题都是P类问题。现在，所有对NP问题的研究都集中在一个问题上，即究竟是否有P=NP？通常所谓的“NP问题”，其实就一句话：证明或推翻P=NP。</p>
<h1 id="约化（Reducibility-归约）"><a href="#约化（Reducibility-归约）" class="headerlink" title="约化（Reducibility , 归约）"></a>约化（Reducibility , 归约）</h1><p>简单地说，一个问题A可以约化为问题B的含义即是，可以用问题B的解法解决问题A，或者说，问题A可以“变成”问题B。《算法导论》上举了这么一个例子。比如说，现在有两个问题：求解一个一元一次方程和求解一个一元二次方程。那么我们说，前者可以约化为后者，意即知道如何解一个一元二次方程那么一定能解出一元一次方程。我们可以写出两个程序分别对应两个问题，那么我们能找到一个“规则”，按照这个规则把解一元一次方程程序的输入数据变一下，用在解一元二次方程的程序上，两个程序总能得到一样的结果。这个规则即是：两个方程的对应项系数不变，一元二次方程的二次项系数为0。按照这个规则把前一个问题转换成后一个问题，两个问题就等价了。同样地，我们可以说，Hamilton回路可以约化为TSP问题(Travelling Salesman Problem，旅行商问题)：在Hamilton回路问题中，两点相连即这两点距离为0，两点不直接相连则令其距离为1，于是问题转化为在TSP问题中，是否存在一条长为0的路径。Hamilton回路存在当且仅当TSP问题中存在长为0的回路。</p>
<p>“问题A可约化为问题B”有一个重要的直观意义：B的时间复杂度高于或者等于A的时间复杂度。也就是说，问题A不比问题B难。这很容易理解。正如解一元二次方程比解一元一次方程难，因为解决前者的方法可以用来解决后者。</p>
<p>很显然，约化具有一项重要的性质：约化具有传递性。如果问题A可约化为问题B，问题B可约化为问题C，则问题A一定可约化为问题C。这个道理非常简单，就不必阐述了。</p>
<p>现在再来说一下约化的标准概念就不难理解了：如果能找到这样一个变化法则，对任意一个程序A的输入，都能按这个法则变换成程序B的输入，使两程序的输出相同，那么我们说，问题A可约化为问题B。</p>
<p>当然，我们所说的“可约化”是指的可“多项式地”约化(Polynomial-time Reducible)，即变换输入的方法是能在多项式的时间里完成的。约化的过程只有用多项式的时间完成才有意义。</p>
<p>从约化的定义中我们看到，一个问题约化为另一个问题，时间复杂度增加了，问题的应用范围也增大了。通过对某些问题的不断约化，我们能够不断寻找复杂度更高，但应用范围更广的算法来代替复杂度虽然低，但只能用于很小的一类问题的算法。再回想前面讲的P和NP问题，联想起约化的传递性，自然地，我们会想问，如果不断地约化上去，不断找到能“通吃”若干小NP问题的一个稍复杂的大NP问题，那么最后是否有可能找到一个时间复杂度最高，并且能“通吃”所有的 NP问题的这样一个超级NP问题？答案居然是肯定的。也就是说，存在这样一个NP问题，所有的NP问题都可以约化成它。换句话说，只要解决了这个问题，那么所有的NP问题都解决了。这种问题的存在难以置信，并且更加不可思议的是，这种问题不只一个，它有很多个，它是一类问题。这一类问题就是传说中的NPC 问题，也就是NP-完全问题。NPC问题的出现使整个NP问题的研究得到了飞跃式的发展。我们有理由相信，NPC问题是最复杂的问题。再次回到全文开头，我们可以看到，人们想表达一个问题不存在多项式的高效算法时应该说它“属于NPC问题”。此时，我的目的终于达到了，我已经把NP问题和NPC问题区别开了。</p>
<h1 id="NPC问题"><a href="#NPC问题" class="headerlink" title="NPC问题"></a>NPC问题</h1><p>NPC问题的定义非常简单。同时满足下面两个条件的问题就是NPC问题：</p>
<ul>
<li><p>它得是一个NP问题；</p>
</li>
<li><p>所有的NP问题都可以约化到它。</p>
<p>证明一个问题是 NPC问题也很简单。先证明它至少是一个NP问题，再证明其中一个已知的NPC问题能约化到它（由约化的传递性，则NPC问题定义的第二条也得以满足；至于第一个NPC问题是怎么来的，下文将介绍），这样就可以说它是NPC问题了。</p>
<font color="green">既然所有的NP问题都能约化成NPC问题，那么只要任意一个NPC问题找到了一个多项式的算法，那么所有的NP问题都能用这个算法解决了，NP也就等于P 了。因此，给NPC找一个多项式算法太不可思议了。因此，前文才说，“正是NPC问题的存在，使人们相信P≠NP”。我们可以就此直观地理解，NPC问题目前没有多项式的有效算法，只能用指数级甚至阶乘级复杂度的搜索。</font>

</li>
</ul>
<h1 id="逻辑电路"><a href="#逻辑电路" class="headerlink" title="逻辑电路"></a>逻辑电路</h1><p>不要以为NPC问题是一纸空谈。NPC问题是存在的。确实有这么一个非常具体的问题属于NPC问题。下文即将介绍它。</p>

<h2 id="电路可满足性"><a href="#电路可满足性" class="headerlink" title="电路可满足性"></a>电路可满足性</h2><blockquote>
<p>具有一个输出的布尔组合电路是可满足的，指的是它有一组可满足的赋值，使得组合电路的输出为1。</p>
<p>电路可满足性(Circuit Satisfiablity，简记Circuit SAT)问题指的是，给定一个由逻辑门AND、<br>OR和NOT构成的组合电路<em>C</em>，它可满足吗?</p>
</blockquote>
<p>上述电路当输入0,1,0,1时，输出为1.</p>
<p>当然肯定也存在输出永远为true的逻辑电路，即无论输入是什么，输出都是true。我们就说这个逻辑电路不存在使输出为false的一组输入。</p>
<p>逻辑电路问题属于NPC问题。这是有严格证明的。它显然属于NP问题，并且可以直接证明所有的NP问题都可以约化到它（不要以为NP问题有无穷多个将给证明造成不可逾越的困难）。证明过程相当复杂，其大概意思是说任意一个NP问题的输入和输出都可以转换成逻辑电路的输入和输出（想想计算机内部也不过是一些 0和1的运算），因此对于一个NP问题来说，问题转化为了求出满足结果为True的一个输入（即一个可行解）。</p>
<p>有了第一个NPC问题后，一大堆NPC问题就出现了，因为再证明一个新的NPC问题只需要将一个已知的NPC问题约化到它就行了。后来，Hamilton 回路成了NPC问题，TSP问题也成了NPC问题。<font color="green">现在被证明是NPC问题的有很多，任何一个找到了多项式算法的话所有的NP问题都可以完美解决了。</font>因此说，正是因为NPC问题的存在，P=NP变得难以置信。P=NP问题还有许多有趣的东西，有待大家自己进一步的挖掘。攀登这个信息学的巅峰是我们这一代的终极目标。现在我们需要做的，至少是不要把概念弄混淆了。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="http://www.matrix67.com/blog/archives/105" >什么是P问题、NP问题和NPC问题<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>时间复杂度</tag>
        <tag>归约</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA</title>
    <url>/2021/07/30/PCA/</url>
    <content><![CDATA[<p><code>PCA</code> 是最经典的降维算法。</p>
<h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><p>在统计学中，<strong>方差</strong>是用来度量单个随机变量的离散程度，而<strong>协方差</strong>则一般用来衡量两个随机变量的联合变化程度。</p>
<span id="more"></span>
<h4 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h4><script type="math/tex; mode=display">
\sigma_{x}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}</script><p>$n$ 表示样本数量，$\bar{x}$ 表示观测样本的均值。</p>
<h4 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h4><script type="math/tex; mode=display">
\sigma(x, y) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})</script><p>$\bar{x}, \bar{y}$ 分别表示两个随机变量所对应的观测样本均值。方差 $\sigma_x^2$ 可看作随机变量 $x$ 关于自身的协方差 $\sigma(x, x)$ 。</p>
<h4 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h4><p>给定 $d$ 个随机变量 $x_k$ ，$k=1, 2, \dots, d$ 。我们用 $x_{ki}$ 表示随机变量 $x_k$ 中的第 $i$ 个观测样本，每个随机变量所对应的观测样本数量均为 $n$ 。</p>
<p>对于这些随机变量，我们可以根据协方差的定义，求出两两之间的协方差，即：</p>
<script type="math/tex; mode=display">
\sigma\left(x_{a}, x_{b}\right)=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{a i}-\bar{x}_{a}\right)\left(x_{b i}-\bar{x}_{b}\right)</script><p>因此协方差矩阵为：</p>
<script type="math/tex; mode=display">
\Sigma=\left[\begin{array}{ccc}
\sigma\left(x_{1}, x_{1}\right) & \cdots & \sigma\left(x_{1}, x_{d}\right) \\
\vdots & \ddots & \vdots \\
\sigma\left(x_{d}, x_{1}\right) & \cdots & \sigma\left(x_{d}, x_{d}\right)
\end{array}\right] \in \mathbb{R}^{d \times d}</script><p>其中，对角线上的元素为各个随机变量的方差，非对角线上的元素为两两随机变量之间的协方差。</p>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>PCA(主成分分析)是比较常见的线性降维方法，通过线性投影将高维数据映射到低维数据中，所期望的是在投影的维度上，新特征自身的方差尽量大，方差越大特征越有效，尽量使产生的新特征间的相关性越小。</p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>假设有 $m$ 条数据，每条数据有 $n$ 个特征。 $x_j^i$ 表示第 $i$ 个样本的第 $j$ 个特征。</p>
<ol>
<li><p>均值归一化：</p>
<script type="math/tex; mode=display">
 \begin{aligned}
 \mu_{j} &=\frac{1}{m} \sum_{i=1}^{m} x_{j}^{i} \\
 x_{j}^{i} &:= \frac{x_{j}^{i}-\mu_{j}}{s_{j}}
 \end{aligned}</script><p> 其中 $s_j = max(x_j) - min(x_j)$</p>
</li>
<li><p>计算协方差矩阵：</p>
<script type="math/tex; mode=display">
 \Sigma = \frac{1}{m} X X^T \in \mathbb{R}^{n \times n}</script></li>
<li><p>计算特征向量：</p>
<script type="math/tex; mode=display">
 [U, S, V] = svd(\Sigma)</script><p> 其中左奇异向量、奇异值矩阵、右奇异向量：$U \in \mathbb{R}^{n \times n}, S \in \mathbb{R}^{n \times m}, V \in \mathbb{R}^{m \times m}$</p>
</li>
<li><p>从 $U$ 中取前 $k$ 列：$U_{reduce} \in \mathbb{R}^{n \times k}$</p>
</li>
<li>计算得到降维后的数据：$Z = U_{reduce}^T * X, Z \in \mathbb{R}^{k \times m}$</li>
</ol>
<h3 id="如何选择-k-？"><a href="#如何选择-k-？" class="headerlink" title="如何选择 $k$ ？"></a>如何选择 $k$ ？</h3><script type="math/tex; mode=display">
\frac{\sum_{i=1}^{k} s_{i i}}{\sum_{i=1}^{n} s_{i i}} \geqslant 0.99</script><p>选择满足上述条件的最小 $k$</p>
<h3 id="降维的应用"><a href="#降维的应用" class="headerlink" title="降维的应用"></a>降维的应用</h3><ul>
<li>数据压缩，减少占用的存储空间</li>
<li>加快算法计算速度</li>
<li>低维平面可以可视化数据</li>
</ul>
<h3 id="PCA为什么要用协方差矩阵的特征向量矩阵来做投影矩阵呢？"><a href="#PCA为什么要用协方差矩阵的特征向量矩阵来做投影矩阵呢？" class="headerlink" title="PCA为什么要用协方差矩阵的特征向量矩阵来做投影矩阵呢？"></a>PCA为什么要用协方差矩阵的特征向量矩阵来做投影矩阵呢？</h3><p>降维的目的就是“降噪”和“去冗余”。</p>
<p>“降噪”的目的就是使保留下来的维度间的相关性尽可能小，而“去冗余”的目的就是使保留下来的维度含有的“能量”即方差尽可能大。</p>
<p>我们要最大化方差来保留更多的信息。去噪。<br>有趣的是，协方差矩阵能同时表现不同维度间的相关性以及各个维度上的方差。</p>
<p>协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。协方差矩阵的主对角线上的元素是各个维度上的方差(即能量)，其他元素是两两维度间的协方差(即相关性)。</p>
<p>先看“降噪”，让保留下的不同维度间的相关性尽可能小，也就是说让协方差矩阵中非对角线元素都基本为零。达到这个目的的方式——矩阵对角化。</p>
<p>再看“去冗余”，对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。我们只取那些含有较大能量(特征值)的维度，其余的就舍掉即可。</p>
<h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><p>LDA(线性判别分析)是一种经典的降维方法。和PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。  </p>
<p>LDA分类思想简单总结如下：  </p>
<ul>
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。  </li>
<li>对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。  </li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。  </li>
</ul>
<p>如果用一句话概括LDA思想：<strong>投影后类内方差最小，类间方差最大。</strong></p>
<h2 id="LDA和PCA异同"><a href="#LDA和PCA异同" class="headerlink" title="LDA和PCA异同"></a>LDA和PCA异同</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">异同点</th>
<th style="text-align:left">LDA</th>
<th style="text-align:left">PCA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">相同点</td>
<td style="text-align:left">1. 两者均可以对数据进行降维；<br />2. 两者在降维时均使用了矩阵特征分解的思想；<br />3. 两者都假设数据符合高斯分布；</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">不同点</td>
<td style="text-align:left">有监督</td>
<td style="text-align:left">无监督</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">降维最多降到 $k-1$ 维</td>
<td style="text-align:left">降维多少没有限制</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">可以用于降维，还可以用于分类</td>
<td style="text-align:left">只用于降维</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">选择分类性能最好的投影方向</td>
<td style="text-align:left">选择样本点投影具有最大方差的方向</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">更明确，更能反映样本间差异</td>
<td style="text-align:left">目的较为模糊</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/37609917" >方差、协方差<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/6251584.html" >奇异值分解(SVD)原理与在降维中的应用<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/zhq9695/article/details/83009196" >吴恩达机器学习（十二）主成分分析（降维、PCA）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_34069667/article/details/107996892" >数据分析面试【机器学习】总结之-PCA主成成分分析 常见面试题整理<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title>PowerDesigner连接MySQL逆向生成物理模型</title>
    <url>/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>根据MySQL现有的表格结构来反向生成含有依赖关系表格模型。</p>
<span id="more"></span>
<p>系统环境：Win10 64位系统</p>
<h2 id="下载安装ODBC"><a href="#下载安装ODBC" class="headerlink" title="下载安装ODBC"></a>下载安装ODBC</h2><p>到<a class="link"   href="https://dev.mysql.com/downloads/connector/odbc/" >MySQL官网<i class="fas fa-external-link-alt"></i></a>上下载ODBC，选择<font color=red>mysql-connector-odbc-5.3.9-win32.msi</font> 这一点非常重要，下面会说明理由。安装就很简单了，一路next下去</p>
<img src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/1.png" class="">
<h2 id="配置ODBC数据源"><a href="#配置ODBC数据源" class="headerlink" title="配置ODBC数据源"></a>配置ODBC数据源</h2><p>1.打开管理工具（不知道在哪儿的话，可以问cortana），双击<font color=red>ODBC数据源(32位)</font>，如下图所示：</p>
<img src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/2.png" class="">
<p>2.点击添加，选择<font color=red>MySQL ODBC 5.3 Unicode Driver</font></p>
<img src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/3.png" class="">
<p>3.点击完成，会弹出配置界面，前面两个随便填写，<font color=red>User和Password就填写你连接数据库的用户名和密码，Database选择你所要连接的数据库</font>，点击Test会弹出连接成功的提示框</p>
<img src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/4.png" class="">，点击OK就配置完成了

## 使用PowerDesigner逆向生成物理模型
1.打开PowerDesigner新建模型，DBMS选择MySQL5.0

<img src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/5.png" class="">
<p>2.菜单栏 Database -&gt; Connect，点击弹出连接界面。从下拉菜单中选择刚刚配置的ODBC数据源，点击Connect即可连接成功。</p>
<img src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/6.png" class="">
<font color=red>注意：现在来解释一下为什么选择32位的安装包，如果选择了64位的，此时点击Connect会弹出报错框：在指定的DSN中，驱动程序和应用程序的体系结构不匹配，SQLSTATE=IM014.具体原因我也不知道。</font>

<p>3.菜单栏 Database -&gt; Update Model From Database…，弹出如下界面：</p>
<img src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/7.png" class="">
<p>点击确定，PowerDesigner默认选中所有数据库的所有表，要想生成我们想要的数据库的物理模型，先反选一下Deselect All，</p>
<img src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/8.png" class="">
<p>再选中partysystem数据库，Select All即选中该数据库中的所有表</p>
<img src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/9.png" class="">
<p>最后点击OK，即生成我们想要的物理模型。</p>
<img src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/10.png" class="">
<p>我这个数据库里面的表结构比较单一，所以生成的物理模型很简单。</p>
]]></content>
      <categories>
        <category>软件工具</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>PowerDesigner</tag>
      </tags>
  </entry>
  <entry>
    <title>Relation Classification with Entity Type Restriction</title>
    <url>/2022/05/10/Relation-Classification-with-Entity-Type-Restriction/</url>
    <content><![CDATA[<p>这是一篇ACL Findings的论文，idea很简单，但却非常奏效。</p>
<span id="more"></span>
<p>关系分类旨在预测句子中两个实体间的关系，这篇论文通过实体类型来限制关系的搜索范围。例如两个实体类型都是<code>person</code>，那么他们的关系就可以排除<code>出生地</code>，这样就能减少候选关系的数量：</p>
<img src="/2022/05/10/Relation-Classification-with-Entity-Type-Restriction/example.jpg" class="">
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><img src="/2022/05/10/Relation-Classification-with-Entity-Type-Restriction/model.jpg" class="">
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><img src="/2022/05/10/Relation-Classification-with-Entity-Type-Restriction/algorithm.jpg" class="">
<script type="math/tex; mode=display">
\begin{aligned}
R_{(t s, t o)} &=\left\{r \in R \mid(s, o) \in D_{r}\right\} \\
&=\{r \in R \mid t s \in S(r) \text { and } \text { to } \in O(r)\}
\end{aligned}</script><ol>
<li>首先根据句子中实体的类型将句子归好组</li>
<li>对于每一组，收集所有关系组成一个集合 $R_{(t s, t o)}$</li>
<li>针对该关系集合学习一个分类器 $f_g$</li>
</ol>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>由于该方法是模型无关的，所以作者在两个代表性模型上做了实验：</p>
<img src="/2022/05/10/Relation-Classification-with-Entity-Type-Restriction/result.png" class="">
<p><code>RECENT</code> 分别比 <code>GCN</code> 和 <code>SpanBERT</code> 高了6.9和4.4，提升还是非常明显的。</p>
<hr>
<h2 id="Cite"><a href="#Cite" class="headerlink" title="Cite"></a>Cite</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;lyu-chen-2021-relation,</span><br><span class="line">    title = &quot;Relation Classification with Entity Type Restriction&quot;,</span><br><span class="line">    author = &quot;Lyu, Shengfei and Chen, Huanhuan&quot;,</span><br><span class="line">    booktitle = &quot;Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021&quot;,</span><br><span class="line">    month = aug,</span><br><span class="line">    year = &quot;2021&quot;,</span><br><span class="line">    address = &quot;Online&quot;,</span><br><span class="line">    publisher = &quot;Association for Computational Linguistics&quot;,</span><br><span class="line">    url = &quot;https://aclanthology.org/2021.findings-acl.34&quot;,</span><br><span class="line">    doi = &quot;10.18653/v1/2021.findings-acl.34&quot;,</span><br><span class="line">    pages = &quot;390--395&quot;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Relation Classification</tag>
      </tags>
  </entry>
  <entry>
    <title>STL学习</title>
    <url>/2021/08/12/STL%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>记录一下常见的STL的概念和用法。</p>
<span id="more"></span>
<h2 id="vector"><a href="#vector" class="headerlink" title="vector"></a><code>vector</code></h2><p><code>vector</code> 是一个自动扩容的容器，支持随机访问，底层通过动态数组实现。</p>
<h4 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h4><p>当 <code>vector</code> 执行 <code>insert</code> 或者 <code>push_back</code> 时，如果当容器的存储数量已经达到容量，就会触发扩容。具体流程如下：</p>
<ol>
<li>申请新的内存空间（原内存空间的1.5~2倍）</li>
<li>把原空间的元素拷贝到新的空间里</li>
<li>释放原空间</li>
<li>数组指针指向新空间</li>
</ol>
<h4 id="成员函数"><a href="#成员函数" class="headerlink" title="成员函数"></a>成员函数</h4><ul>
<li><code>size()</code> : 容器中元素的个数</li>
<li><code>capacity()</code> : 器在分配那块内存上可以容纳的元素的个数</li>
<li><code>resize(n)</code> : 强制将容器改为容纳为 <code>n</code> 个数据，分三种情况讨论：<ol>
<li><code>n &lt; size()</code> : 容器尾部元素被销毁</li>
<li><code>n &gt; size()</code> : 新构造的元素会添加到末尾</li>
<li><code>n &gt; capacity()</code> : 在元素加入前会进行重新分配</li>
</ol>
</li>
<li><code>reserve(n)</code> : 强制容器把它的容量改为不小于 <code>n</code> 。如果 <code>n</code> 小于当前容量，则 <code>vector</code> 会忽略它</li>
</ul>
<p><code>resize</code> 和 <code>reserve</code> 都保证了 <code>vector</code> 空间的大小，至少达到它们参数所指定的大小。</p>
<h4 id="push-back-与-emplace-back-区别"><a href="#push-back-与-emplace-back-区别" class="headerlink" title="push_back 与 emplace_back 区别"></a><code>push_back</code> 与 <code>emplace_back</code> 区别</h4><p><code>emplace_back</code> 和 <code>push_back</code> 的区别，就在于底层实现的机制不同。</p>
<ul>
<li><code>push_back</code> 向容器尾部添加元素时，首先会创建这个元素，然后再将这个元素拷贝或者移动到容器中（如果是拷贝的话，事后会自行销毁先前创建的这个元素）；</li>
<li><code>emplace_back</code> 是C++ 11标准新增加的成员函数。在实现时，则是直接在容器尾部创建这个元素，省去了拷贝或移动元素的过程。</li>
</ul>
<h2 id="map-与-unordered-map"><a href="#map-与-unordered-map" class="headerlink" title="map 与 unordered_map"></a><code>map</code> 与 <code>unordered_map</code></h2><ul>
<li><code>map</code> 内部结构是由<a class="link"   href="https://transformerswsz.github.io/2021/08/23/BST%20&amp;%20AVL%20&amp;%20RBT/" >RBT<i class="fas fa-external-link-alt"></i></a>来实现的。查找、插入、删除都很稳定，时间复杂度为 $O(log_2 n)$</li>
<li><code>unordered_map</code> 内部是一个hash_table，一般是由一个大vector，vector元素节点可挂接链表来解决冲突。<ul>
<li><code>hash_map</code>：由于在C++标准库中没有定义散列表hash_map，标准库的不同实现者将提供一个通常名为hash_map的非标准散列表。因为这些实现不是遵循标准编写的，所以它们在功能和性能保证上都有微妙的差别。从C++11开始，哈希表实现已添加到C++标准库标准。决定对类使用备用名称，以防止与这些非标准实现的冲突，并防止在其代码中有hash_table的开发人员无意中使用新类。所选择的备用名称是unordered_map，它更具描述性，因为它暗示了类的映射接口和其元素的无序性质。可见 <code>hash_map</code> 跟 <code>unordered_map</code> 本质是一样的，只不过 <code>unordered_map</code> 被纳入了C++标准库标准。</li>
</ul>
</li>
</ul>
<img src="/2021/08/12/STL%E5%AD%A6%E4%B9%A0/unordered_map.jpeg" class="">
<h4 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h4><ul>
<li>若考虑有序，查询速度稳定，非频繁查询那么考虑使用 <code>map</code></li>
<li>若非常高频查询(100个元素以上，unordered_map都会比map快)，内部元素可非有序，数据大超过1k甚至几十万上百万时候就要考虑使用 <code>unordered_map</code></li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/Payshent/article/details/73835795?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.base&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.base" >STL中vector的实现及面试问题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/xx18030637774/article/details/82780878" >C/C++之vector的内存管理和效率<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://c.biancheng.net/view/vip_7721.html" >快速入门c++ stl<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_30392565/article/details/51835770" >hash_map/unordered_map原理和使用整理<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>C++</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM</title>
    <url>/2019/06/22/SVM/</url>
    <content><![CDATA[<p>支持向量机(Support Vector Machine)是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM的学习算法是求解凸二次规划的最优化算法。</p>
<span id="more"></span>
<p>假设在一个二维线性可分的数据集中，图一A所示，我们要找到一条把两组数据分开，这条直线可以是图一B中的直线，也可以是图一C中的直线，或者图一D中的直线，但是哪条直线能够达到最好的泛化能力呢？那就是一个能使两类之间的空间大小最大的一个超平面。</p>
<img src="/2019/06/22/SVM/1.png" class="">
<p>这个超平面在二维平面上看到的就是一条直线，在三维空间中就是一个平面。因此，我们把这个划分数据的决策边界统称为超平面。<font color="green">离这个超平面最近的点就叫做支持向量，点到超平面的距离叫间隔。</font>支持向量机就是要使超平面和支持向量之间的间隔尽可能的大，这样超平面才可以将两类样本准确的分开，而保证间隔尽可能的大就是保证我们的分类器误差尽可能的小，尽可能的健壮。</p>
<h1 id="线性可分SVM"><a href="#线性可分SVM" class="headerlink" title="线性可分SVM"></a>线性可分SVM</h1><p>要使得支持向量到超平面的间隔最大化，我们首先定义超平面 $h(x)$ ：</p>
<script type="math/tex; mode=display">
h(x) = w^Tx + b \qquad w为权重向量，b为偏置向量</script><p>样本点 $x$ 到最优超平面的几何间隔为：</p>
<script type="math/tex; mode=display">
r = \frac {h(x)} {||w||} = \frac {w^T + b} {||w||}</script><p>$||w||$ 是向量 $w$ 的内积，即 $||w|| = \sqrt{w_0^2 + w_1^2 + \dots +  w_n^2 }$ ，而 $h(x)$ 表示函数间隔：</p>
<script type="math/tex; mode=display">
\hat{r} = h(x)</script><p>函数间隔 $h(x)$ 不是一个标准的间隔度量，它不适合用来做最大化的间隔值。因为，一旦超平面固定以后，如果我们人为的放大或缩小 $w$ 和 $b$ 值，那这个超平面也会无限的放大或缩小，这将对分类造成严重影响。而几何间隔是函数间隔除以 $w$ ，当 $w$ 的值无限放大或缩小时，$||w||$ 也会等倍地放大或缩小，而整个 $r$ 保持不变，它只随着超平面的移动而变化，不受两个参数的影响。因而，我们用几何间隔来做最大化间隔度量。</p>
<h2 id="最大化间隔"><a href="#最大化间隔" class="headerlink" title="最大化间隔"></a>最大化间隔</h2><p>在SVM中，我们把几何间隔 $r$ 作为最大化间隔，并且采用 $-1$ 和 $+1$ 作为类别标签。</p>
<p>如下图所示，在这个 $\mathbb{R}^2$ 空间中，假设我们已经确定了一个超平面（图中虚线），这个超平面的函数关系式为 $h(x) = w^Tx + b = 0$ 。我们想要所有的样本点都尽可能的原理这个超平面，只需保证支持向量的点 $x^*$ 尽可能地远离它。</p>
<img src="/2019/06/22/SVM/2.png" class="">
<p>我们把其中一个支持向量 $x^*$ 到最优超平面的距离定义为：</p>
<script type="math/tex; mode=display">
r^* = \frac {h(x^*)} {||w||} = 
\begin{cases}
\frac {1} {||w||}&  \quad {if : y* = h(x^*) = +1}\\
\frac {-1} {||w||}& \quad {if : y* = h(x^*) = -1}
\end{cases}</script><p>这是我们通过把函数间隔 $h(x)$ 固定为 $1$ 而得来的。我们可以把这个式子想象成还存在两个平面，这两个平面分别是 $w^Tx_s+b=1$ 和 $w^Tx_s+b=-1$ ，对应上图中的两根实线。这些支持向量 $x_s$ 就在这两个平面上，这两个平面离最优超平面的距离越大，我们的间隔也就越大。对于其他的点 $x_i$ 如果满足 $w^Tx_i+b&gt;1$ ，则被分为 $1$ 类，如果满足满足 $w^Tx_i+b&lt;-1$ ，则被分为 $-1$ 类。即有约束条件：</p>
<script type="math/tex; mode=display">
\begin{cases}
w^Tx_i+b \geqslant 1 &  \quad y_i = +1 \\
w^Tx_i+b \leqslant -1 & \quad y_i = -1
\end{cases}</script><p>支持向量到超平面的距离知道后，那么分割的间隔 $\gamma$ 为：</p>
<script type="math/tex; mode=display">
\gamma = 2r^* = \frac {2} {||w||}</script><p>注意到最大化 $\frac {2} {||w||}$ 和最小化 $\frac {1} {2} ||w||^2$ 是等价的，于是就得到下面的线性可分支持向量机学习的最优化问题：</p>
<script type="math/tex; mode=display">
\begin{cases}
\min_{w, b} \; \frac {1} {2} \|w\|^2 \\
y_i(w^Tx_i+b) \geqslant 1, \quad (i = 1, \dots , n) 
\end{cases}</script><p>这种式子采用拉格朗日乘数法来求解，即：</p>
<script type="math/tex; mode=display">
L(x) = f(x) + \sum \alpha g(x)</script><p>$f(x)$ 是我们需要最小化的目标函数， $g(x)$ 是不等式约束条件， $\alpha$ 是对应的约束系数，也称拉格朗日乘子。为了使得拉格朗日函数得到最优解，我们需要加入能使该函数有最优化解法的KKT条件，或者叫最优化条件。即假设存在一点 $x^*$：</p>
<ul>
<li>$L(x^{\star})$ 对 $x^{\star}$ 求导为 $0$</li>
<li>$\alpha_ig_i(x^*)=0$ 对于所有的 $i = 1,\dots,n$</li>
</ul>
<p>这样构造的拉格朗日函数为：</p>
<script type="math/tex; mode=display">
L(w, b, a) = \frac {1} {2} w^Tw - \sum_{i=1}^n a_i[y_i(w^T x_i + b) - 1]</script><p>以上的KKT条件 $\alpha_i[y_i(w^Tx_i+b)-1] = 0$ 表示，只有距离最优超平面的支持向量 $(x_i, y_i)$ 对应的 $\alpha$ 非零，其他所有点集的 $\alpha$ 等于零。综上所述，引入拉格朗日乘子后，我们的目标变为：</p>
<script type="math/tex; mode=display">
\min_{w,b}\max_{a \geqslant 0} L(w, b, a)</script><p>根据拉格朗日对偶性，目标问题的对偶问题是极大极小问题，即先求 $L(w, b, \alpha)$ 对 $w, b$ 的极小，再求对 $\alpha$ 的极大：</p>
<script type="math/tex; mode=display">
\max_{a \geqslant 0} \min_{w, b} L(w, b, \alpha)</script><p>用 $L(w, b, \alpha)$ 对 $w$ 和 $b$ 分别求导，并令其等于 $0$ ：</p>
<script type="math/tex; mode=display">
\begin{cases}
\frac {\partial L(w, b, \alpha)} {\partial w} = 0\\
\frac {\partial L(w, b, \alpha)} {\partial b} = 0
\end{cases}</script><p>得到：</p>
<script type="math/tex; mode=display">
\begin{cases}
w = \sum_{i=1}^n \alpha_i y_i x_i \\
\sum_{i=1}^n \alpha_i y_i = 0
\end{cases}</script><p>把该式代入原来的拉格朗日式子得（推导见《统计学习方法》P103~P105）：</p>
<script type="math/tex; mode=display">
L(\alpha) = \sum_{i=1}^n \alpha_i - \frac {1} {2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \geqslant 0 (i = 1, \dots, n)</script><p>该 $L(\alpha)$ 函数消去了向量 $w$ 和向量 $b$ ，仅剩 $\alpha$ 这个未知参数，只要我们能够最大化 $L(\alpha)$，就能求出对应的 $\alpha$ ，进而求得  $w$ 和 $b$ 。对于如何求解 $\alpha$，SMO算法给出了完美的解决方案，下一节我们详细讲述。这里我们假设通过SMO算法确定了最优 $\alpha^*$，则：</p>
<script type="math/tex; mode=display">
w^* = \sum_{i=1}^n \alpha_i^* y_i x_i</script><p>最后使用一个正的支持向量 $x^*$ ，就可以计算出 $b$ ：</p>
<script type="math/tex; mode=display">
b^* = 1 - w^{*T} x^*</script><h2 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a><a id="softgap">软间隔</a></h2><p>以上的推导都是在<font color="red">完全线性可分</font>的条件下进行的，但是现实世界的许多问题并不都是线性可分的，尤其存在许多复杂的非线性可分的情形。要解决这些线性不可分问题，有如下两种方法：</p>
<ul>
<li>放宽严格的间隔，构造<a href="#softgap">软间隔</a>。</li>
<li>运用<a href="#kernel">核函数</a>将数据映射到高维空间，然后在高维空间中寻找超平面进行线性划分。</li>
</ul>
<p>我们首先讨论软间隔。假设两个类有几个数据点混在一起，这些点对最优超平面形成了噪声干扰，软间隔就是要扩展一下我们的目标函数和KKT条件，允许少量这样的噪声存在。具体地说，就要引入松驰变量 $\xi$ 来量化分类器的违规行为：</p>
<script type="math/tex; mode=display">
\begin{cases}
\min_{w, b} \; \frac {1} {2} \|w\|^2 + C\sum_{i=1}^n \xi_i, \quad C为惩罚因子 \\
y_i (w^T x_i + b) \geqslant 1 - \xi_i , \quad i = 1,\dots, n \\
\xi_i \geqslant 0 ,  \quad i = 1, \dots , n
\end{cases}</script><h3 id="C-和-xi"><a href="#C-和-xi" class="headerlink" title="$C$ 和 $\xi$"></a>$C$ 和 $\xi$</h3><img src="/2019/06/22/SVM/3.jpg" class="">
<p>如上图所示，$\xi$ 表示噪声样本点到本类样本点边界的偏移距离。$C$ 可被视为一个由用户依据经验或分析选定的“正则化”参数。噪声点在现实世界是天然存在的，如果对于他们不进行容错，那么我们是无论如何也不能把样本分开的。而引入惩罚因子，目的就是，对这类误分的样本进行容错，相当于把点拉到正确一侧：</p>
<ul>
<li>当 $C$ 很大时，$\xi$ 趋近于0，表示惩罚很大，容忍度很低。这样错分较少，对样本的拟合性较好，但容易过拟合。</li>
<li>当 $C$ 很小时，$\xi$ 变大，表示惩罚很小，容忍度高。这样错分较多，对样本的拟合性下降。</li>
</ul>
<p>对上述不等式同样运用拉格朗日乘子法和KKT条件得（推导见《统计学习方法》P109~P111）：</p>
<script type="math/tex; mode=display">
L(\alpha) = \sum_{i=1}^n \alpha_i - \frac {1} {2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
\sum_{i=1}^n \alpha_i y_i = 0, \quad 0 \leqslant \alpha_i \leqslant C (i = 1, \dots , n)</script><p>可以看到，松驰变量 $\xi$ 没有出现在 $L(\alpha)$ 中，线性可分与不可分的差异体现在约束 $\alpha_i \geqslant 0$ 被替换成了约束 $0 \leqslant \alpha_i \leqslant C$。但是，这两种情况下求解 $w$ 和 $b$ 是非常相似的，对于支持向量的定义也都是一致的。在不可分情况下，对应的KKT条件为：</p>
<script type="math/tex; mode=display">
\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i] = 0, \quad (i = 1, \dots, n)</script><h2 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h2><p>1996年， John Platt发布了一个称为SMO的强大算法，用于训练SVM。 SMO表示序列最小优化（Sequential Minimal Optimization）。 Platt的SMO算法是将大优化问题分解为多个小优化问题来求解，这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果是完全一致的。</p>
<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>求出一系列 $\alpha$，一旦求出了这些  $\alpha$，就很容易计算出权重向量 $w$ 和 $b$，并得到分隔超平面。</p>
<h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><p>每次循环中选择两个 $\alpha$ 进行优化处理。一旦找到一对合适的 $\alpha$ ，那么就增大其中一个同时减小另一个。这里所谓的“合适”就是指两个 $\alpha$ 必须要符合一定的条件，条件之一就是这两个 $\alpha$ 必须要在间隔边界之外，而其第二个条件则是这两个  $\alpha$ 还没有进行过区间化处理或者不在边界上。<br>对SMO具体的分析如下，在上一节我们已经得出了：</p>
<script type="math/tex; mode=display">
L(\alpha) = \sum_{i=1}^n \alpha_i - \frac {1} {2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
\sum_{i=1}^n \alpha_i y_i = 0, \quad 0 \leqslant \alpha_i \leqslant C (i = 1, \dots , n)</script><p>其中 $(x_i, y_i)$ 已知，$C$ 可以人为设定。现在就是要最大化 $L(\alpha)$ ，求得参数 $\alpha = [\alpha_1, \alpha_2, \dots, \alpha_n]$。SMO算法是一次选择两个 $\alpha$ 进行优化，那我们就选择 $\alpha_1$ 和 $\alpha_2$ ，然后把其它参数 $[\alpha_3, \alpha_4, \dots, \alpha_n]$ 固定，这样 $\alpha_1, \alpha_2$ 表示为下面的式子，其中 $\zeta$ 为实数值：</p>
<script type="math/tex; mode=display">
\alpha_1 y_1 + \alpha_2 y_2 = - \sum_{i=3}^n \alpha_i y_i = \zeta</script><p>然后用 $\alpha_2$ 来表示 $\alpha_1$ ：</p>
<script type="math/tex; mode=display">
\alpha_1 = \frac {\zeta - \alpha_2 y_2} {y_1}</script><p>把上式代入 $L(\alpha)$ 中：</p>
<script type="math/tex; mode=display">
L(\alpha) = L(\frac {\zeta - \alpha_2 y_2} {y_1}, \alpha_2, \dots, \alpha_n)</script><p>省略一系列化解过程后，最后会化解成我们熟悉的一元二次方程，$a, b, c$ 均是实数值：</p>
<script type="math/tex; mode=display">
L(\alpha_2) = a \alpha_2^2 + b \alpha_2 + c</script><p>最后对 $\alpha_2$ 求导，解得 $\alpha_2$ 的具体值，我们暂时把这个实数值叫 $\alpha_2^{\star}$ ，而这个 $\alpha_2^{\star}$ 需要满足一个条件 $L \leqslant \alpha_2^{\star} \leqslant H$ ，如下图所示：</p>
<img src="/2019/06/22/SVM/4.png" class="">
<p>根据之前的条件 $0 \leqslant \alpha_i \leqslant C$ 和等式 $\alpha_1 y_1 + \alpha_2 y_2 = \zeta$ ，当 $y_1$ 和 $y_2$ 异号时：</p>
<script type="math/tex; mode=display">
\begin{cases}
L &= \max(0, \alpha_2 - \alpha_1) \\
H &= \min(C, C + \alpha_2 - \alpha_1)
\end{cases}</script><p>当 $y_1$ 和 $y_2$ 同号时：</p>
<script type="math/tex; mode=display">
\begin{cases}
L &= \max(0, \alpha_2 + \alpha_1 - C) \\
H &= \min(C, \alpha_2 + \alpha_1)
\end{cases}</script><p>最后，满足条件的 $\alpha_2$ 应该由下面的式子得到， $\alpha_2^{\star\star}$ 才为最终的值：</p>
<script type="math/tex; mode=display">
\alpha_2^{**} = \begin{cases}
H, \quad \alpha_2^* > H \\
\alpha_2^*, \quad L \leqslant \alpha_2^* \leqslant H \\
L, \quad \alpha_2^* < L
\end{cases}</script><p>求得 $\alpha_2^{\star\star}$ 后就能求得 $\alpha_1^{\star\star}$ 了，然后我们重复地按照最优化 $ (\alpha_1, \alpha_2) $ 的方式继续选择 $ (\alpha_3, \alpha_4), (\alpha_5, \alpha_6), \dots, (\alpha_{n-1}, \alpha_n) $ 进行优化求解，这样 $ \alpha = [\alpha_1, \alpha_2, \dots, \alpha_n] $ 求解出来后，整个线性划分问题就迎刃而解。</p>
<h1 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a><a id="kernel">核函数</a></h1><p>对于以上几节讲的SVC算法，我们都在线性可分或存在一些噪声点的情况下进行的二分类，但是如果我们存在两组数据，它们的散点图如下图所示，你可以看出这完全是一个非线性不可分的问题，我们无法使用之前讲的SVC算法在这个二维空间中找到一个超平面把这些数据点准确的分开。</p>
<img src="/2019/06/22/SVM/5.png" class="">
<p>解决这个划分问题我们需要引入一个核函数，核函数能够恰当地计算给定数据的内积，将数据从输入空间的非线性转变到特征空间，特征空间具有更高甚至无限的维度，从而使得数据在该空间中被转换成线性可分的。如下图所示，我们把二维平面的一组数据，通过核函数映射到了一个三维空间中，这样，我们的超平面就面成了一个平面（在二维空间中是一条直线），这个平面就可以准确的把数据划分开了。</p>
<img src="/2019/06/22/SVM/6.png" class="">
<p>核函数有Sigmoid核、线性核、多项式核和高斯核等，其中高斯核和多项式核比较常用，两种核函数均可以把低维数据映射到高维数据。高斯核的公式如下，$\sigma$ 是达到率，即函数值跌落到 $0$ 的速度参数：</p>
<script type="math/tex; mode=display">
K(x_1, x_2) = e^{\frac {- \|x_1 - x_2\|^2} {2 \sigma^2}}</script><p>多项式核函数的公式如下，$R$ 为实数，$d$ 为低维空间的维数：</p>
<script type="math/tex; mode=display">
K(x_1, x_2) = (\langle x_1, x_2 \rangle + R)^d</script><p>应用于我们的上个例子，我们先定义用 $\phi : x \to H$ 表示从输入空间 $x \subset \mathbb{R}^n$ 到特征空间 $H$ 的一个非线性变换。假设在特征空间中的问题是线性可分的，那么对应的最优超平面为：</p>
<script type="math/tex; mode=display">
w^{\phi T} \phi(x) + b = 0</script><p>通过拉格朗日函数我们推导出：</p>
<script type="math/tex; mode=display">
w^{\phi *} = \sum_{i=1}^n \alpha_i^* y_i \phi(x_i)</script><p>代入上式的特征空间的最优超平面为：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n \alpha_i^* y_i \phi^T(x_i) \phi(x) + b = 0</script><p>这里的 $\phi^T(x_i) \phi(x)$ 表示内积，用核函数代替内积则为：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n \alpha_i^* y_i K(x_i, x) + b = 0</script><p>我们的核函数均是内积函数，通过在低维空间对输入向量求内积来映射到高维空间，从而解决在高维空间中数据线性可分的问题。</p>
<p>我们以多项式核来解释一下为什么核函数可以把低维数据映射成高维数据。</p>
<p>假设有两个输入样本，它们均为二维行向量 $a = [x_1, x_2], b = [x_3, x_4]$ ，他们的内积为：</p>
<script type="math/tex; mode=display">
\langle a, b \rangle = a b^T = \begin{bmatrix} x_1 & x_2 \end{bmatrix} \begin{bmatrix} x_3 \\ x_4 \end{bmatrix} = x_1 x_3 + x_2 x_4</script><p>用多项式核函数进行映射，令 $R=0, d=2$：</p>
<script type="math/tex; mode=display">
K(a, b) = (\langle x_1, x_2 \rangle)^2 = (x_1 x_3 + x_2 x_4)^2 = x_1^2 x_3^2 + 2x_1 x_2 x_3 x_4 + x_2^2 x_4^2 = \phi(a) \phi(b)</script><p>按照线性代数中的标准定义， $\phi(a)$ 和 $\phi(b)$ 为映射后的三维行向量和三维列向量，即：</p>
<script type="math/tex; mode=display">
\phi(a) = \begin{bmatrix} x_1^2 & \sqrt 2 x_1 x_2 & x_2^2 \end{bmatrix} \\
\phi(b) = \begin{bmatrix} x_3^2 \\ \sqrt2 x_3 x_4 \\ x_4^2  \end{bmatrix}</script><p>它们的内积用向量的方式表示则更直观：</p>
<script type="math/tex; mode=display">
\phi(a) \phi(b) = \begin{bmatrix} x_1^2 & \sqrt 2 x_1 x_2 & x_2^2 \end{bmatrix} \begin{bmatrix} x_3^2 \\ \sqrt2 x_3 x_4 \\ x_4^2  \end{bmatrix} = x_1^2 x_3^2 + 2x_1 x_2 x_3 x_4 + x_2^2 x_4^2</script><p>这样我们就把二维数据映射成了三维数据。对于高斯核的映射，会用到泰勒展开式，这个后面再学习了。</p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>上面说了那么多，全是从数学角度进行分析推导的。你可能明白了SVM的数学原理，当你进行编程的时候，还是一脸懵逼。因为如果按照上面的求解过程来的话，实在是太复杂了。但是在计算机里，求解SVM却是非常简单的事。我们只需给出SVC的损失函数，然后使用GD算法，就能很好地求出 $\theta$ ，也就是上述的 $w$ ：</p>
<script type="math/tex; mode=display">
J(\theta) = \min_{\theta} C [ \sum_{i=1}^m y^{(i)} cost_1(\theta^T x^{(i)}) + (1 - y^{(i)})cost_0(\theta^T x^{(i)}) ] + \frac {1} {2} \sum_{j=1}^n \theta_j^2 \quad C 为惩罚因子</script><p>上述函数分析具体可见 <a class="link"   href="https://github.com/TransformersWsz/Halfrost-Field/blob/master/contents/Machine_Learning/Support_Vector_Machines.ipynb" >https://github.com/TransformersWsz/Halfrost-Field/blob/master/contents/Machine_Learning/Support_Vector_Machines.ipynb<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/vipyoumay/p/7560061.html?tdsourcetag=s_pcqq_aiomsg" >SVM原理以及Tensorflow 实现SVM分类(附代码) <i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://baijiahao.baidu.com/s?id=1621964725382082396&amp;wfr=spider&amp;for=pc" >深度讲解支持向量机背后的数学思想<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/TransformersWsz/Halfrost-Field/blob/master/contents/Machine_Learning/Support_Vector_Machines.ipynb" >Support_Vector_Machines.ipynb<i class="fas fa-external-link-alt"></i></a></li>
<li>《统计学习方法》</li>
<li><a class="link"   href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1052089362&amp;courseId=1004570029" >吴恩达机器学习<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Classification</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM常见面试问答</title>
    <url>/2021/06/17/SVM%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E7%AD%94/</url>
    <content><![CDATA[<p>记录一下SVM的常见面试问答：</p>
<span id="more"></span>
<h2 id="1-SVM原理"><a href="#1-SVM原理" class="headerlink" title="1. SVM原理"></a>1. SVM原理</h2><p>SVM 是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。</p>
<ul>
<li>当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即硬间隔支持向量机；</li>
<li>当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即软间隔支持向量机；</li>
<li>当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</li>
</ul>
<h2 id="2-SVM为什么采用间隔最大化？"><a href="#2-SVM为什么采用间隔最大化？" class="headerlink" title="2. SVM为什么采用间隔最大化？"></a>2. SVM为什么采用间隔最大化？</h2><ul>
<li>唯一解：当训练数据线性可分时，存在无穷多个超平面可以将两类数据正确分开。线性可分支持向量利用间隔最大化求得最优超平面，此时，解是唯一的。</li>
<li>强泛化性：划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强。</li>
</ul>
<h2 id="3-SVM为什么要从原始问题变为对偶问题来求解？"><a href="#3-SVM为什么要从原始问题变为对偶问题来求解？" class="headerlink" title="3. SVM为什么要从原始问题变为对偶问题来求解？"></a>3. SVM为什么要从原始问题变为对偶问题来求解？</h2><p>原问题：</p>
<script type="math/tex; mode=display">
\min _{w, b} \max _{\lambda} L(w, b, \lambda)=\frac{1}{2}\|w\|^{2}+\sum_{i=1}^{n} \lambda_{i}\left[1-y_{i}\left(w^{T} x_{i}+b\right)\right]</script><p>对偶问题：</p>
<script type="math/tex; mode=display">
\max _{\lambda} \min _{w, b} L(w, b, \lambda)=\frac{1}{2}\|w\|^{2}+\sum_{i=1}^{n} \lambda_{i}\left[1-y_{i}\left(w^{T} x_{i}+b\right)\right]</script><ul>
<li>对偶问题往往更易求解：<ul>
<li>问题复杂度降低：由求特征向量 $w$ 转化为求比例系数 $\lambda$，在原始问题下，求解的复杂度与 $w$ 的维度有关。在对偶问题下，只与样本数量 $n$ 有关；</li>
<li>求解更高效：因为只用求解比例系数 $\lambda$ ，而 $\lambda$ 只有支持向量才为非0，其他全为0；</li>
</ul>
</li>
<li>可以自然引入核函数，进而推广到非线性分类问题。</li>
</ul>
<h2 id="4-KKT条件的作用"><a href="#4-KKT条件的作用" class="headerlink" title="4. KKT条件的作用"></a>4. KKT条件的作用</h2><script type="math/tex; mode=display">
d^{*}=\max _{\alpha, \beta: \alpha_{i} \geq 0} \min _{w} \mathcal{L}(w, \alpha, \beta) \leq \min _{w} \max _{\alpha, \beta: \alpha_{i} \geq 0} \mathcal{L}(w, \alpha, \beta)=p^{*}</script><h2 id="5-SMO基本原理"><a href="#5-SMO基本原理" class="headerlink" title="5. SMO基本原理"></a>5. SMO基本原理</h2><p>SMO每次选择两个变量 $\alpha_i$ 和 $\alpha_j$ ，并固定其它参数。在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p>
<ol>
<li>选取一对需更新的变量 $\alpha_i$ 和 $\alpha_j$ ；</li>
<li>固定 $\alpha_i$ 和 $\alpha_j$ 以外的参数，求解对偶问题 $max_{\alpha} (\sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j})$ 以获得更新后的  $\alpha_i$ 和 $\alpha_j$  .</li>
</ol>
<p>至于如何选取，SMO采用了启发式方法：使选取的两变量所对应的样本之间的间隔最大。直观解释：这样的两变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。</p>
<h2 id="6-什么是软间隔支持向量机？"><a href="#6-什么是软间隔支持向量机？" class="headerlink" title="6. 什么是软间隔支持向量机？"></a>6. 什么是软间隔支持向量机？</h2><blockquote>
<p>当训练数据近似线性可分时，软间隔最大化允许出现分类错误，此时超平面不能将所有训练数据点都分类正确。</p>
</blockquote>
<p>引入松弛变量 $\xi$ 来量化分类器的违规行为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min _{W, b, \xi} \frac{1}{2}\|W\|^{2}+C \sum_{i=1}^{n} \xi_{i} \\
&\text { s.t. } y_{i}\left(X_{i}^{T} W+b\right) \geq 1-\xi_{i} \\
&\xi_{i} \geq 0, i=1,2, \ldots n
\end{aligned}</script><p>$C$ 是惩罚参数，越小对误分类惩罚越小，容易欠拟合；越大对误分类惩罚越大，容易过拟合。</p>
<h2 id="7-核函数作用以及如何选取核函数？"><a href="#7-核函数作用以及如何选取核函数？" class="headerlink" title="7. 核函数作用以及如何选取核函数？"></a>7. 核函数作用以及如何选取核函数？</h2><blockquote>
<p>当样本在原始空间线性不可分时，核函数可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。</p>
</blockquote>
<p>核函数有如下几种：</p>
<ul>
<li>线性核</li>
<li>多项式核</li>
<li>高斯核（RBF核）</li>
<li>Sigmoid核</li>
</ul>
<p>核函数选取技巧：</p>
<ul>
<li>如果特征的数量大到和样本数量差不多，则选用LR或者线性核函数；</li>
<li>如果特征的数量小，样本的数量正常，则选用高斯核函数；</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/43827793" >SVM 高频面试题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/35755150" >推导 | SVM详解（1）SVM基本型<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/33940537" >支持向量机SVM总结之拉格朗日对偶问题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/93715996" >【机器学习面试总结】—— SVM<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/breezezz/p/11303722.html" >拉格朗日对偶性<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>SimCSE论文及源码解读</title>
    <url>/2022/05/01/SimCSE%E8%AE%BA%E6%96%87%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>对比学习的思想是拉近同类样本的距离，增大不同类样本的距离，目标是要从样本中学习到一个好的语义表示空间。SimCSE是一种简单的无监督对比学习框架，它通过对同一句子两次Dropout得到一对正样例，将该句子与同一个batch内的其它句子作为一对负样例。模型结构如下所示：</p>
<span id="more"></span>
<p><img src="https://cdn.jsdelivr.net/gh/TransformersWsz/image_hosting@master/simcse.ldig50thwww.jpg" alt="simcse"></p>
<p>损失函数为：</p>
<script type="math/tex; mode=display">
\ell_{i}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i}}, \mathbf{h}_{i}^{z_{i}^{\prime}}\right) / \tau}}{\sum_{j=1}^{N} e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i}}, \mathbf{h}_{j}^{z_{j}^{\prime}}\right) / \tau}}</script><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>在作者的代码中，并不是将一个句子输入到模型中两次，而是复制一份放到同一个batch里。模型的核心是 <a href="https://github.com/princeton-nlp/SimCSE/blob/e3aa97b6d04c3d84f6bc46abb06c1bd056cab6d7/simcse/models.py#L97"><code>cl_forward</code></a> 函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cl_forward</span>(<span class="params">cls,</span></span><br><span class="line"><span class="params">    encoder,</span></span><br><span class="line"><span class="params">    input_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    attention_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    token_type_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    position_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    head_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    inputs_embeds=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    labels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    output_attentions=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    output_hidden_states=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    return_dict=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    mlm_input_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    mlm_labels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    return_dict = return_dict <span class="keyword">if</span> return_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> cls.config.use_return_dict</span><br><span class="line">    ori_input_ids = input_ids    <span class="comment"># 形状为[bs, num_sent, sent_len], bs=32</span></span><br><span class="line">    batch_size = input_ids.size(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># Number of sentences in one instance</span></span><br><span class="line">    <span class="comment"># 2: pair instance，[自己，自己]; 3: pair instance with a hard negative，[自己，自己，难例]</span></span><br><span class="line">    num_sent = input_ids.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    mlm_outputs = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># Flatten input for encoding</span></span><br><span class="line">    input_ids = input_ids.view((-<span class="number">1</span>, input_ids.size(-<span class="number">1</span>))) <span class="comment"># [bs * num_sent, sent_len]</span></span><br><span class="line">    attention_mask = attention_mask.view((-<span class="number">1</span>, attention_mask.size(-<span class="number">1</span>))) <span class="comment"># [bs * num_sent, sent_len]</span></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        token_type_ids = token_type_ids.view((-<span class="number">1</span>, token_type_ids.size(-<span class="number">1</span>))) <span class="comment"># [bs * num_sent, sent_len]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get raw embeddings, [bs, num_sent, sent_len, hidden_size]</span></span><br><span class="line">    outputs = encoder(</span><br><span class="line">        input_ids,</span><br><span class="line">        attention_mask=attention_mask,</span><br><span class="line">        token_type_ids=token_type_ids,</span><br><span class="line">        position_ids=position_ids,</span><br><span class="line">        head_mask=head_mask,</span><br><span class="line">        inputs_embeds=inputs_embeds,</span><br><span class="line">        output_attentions=output_attentions,</span><br><span class="line">        output_hidden_states=<span class="literal">True</span> <span class="keyword">if</span> cls.model_args.pooler_type <span class="keyword">in</span> [<span class="string">&#x27;avg_top2&#x27;</span>, <span class="string">&#x27;avg_first_last&#x27;</span>] <span class="keyword">else</span> <span class="literal">False</span>,</span><br><span class="line">        return_dict=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MLM auxiliary objective</span></span><br><span class="line">    <span class="keyword">if</span> mlm_input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mlm_input_ids = mlm_input_ids.view((-<span class="number">1</span>, mlm_input_ids.size(-<span class="number">1</span>)))</span><br><span class="line">        mlm_outputs = encoder(</span><br><span class="line">            mlm_input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            position_ids=position_ids,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            inputs_embeds=inputs_embeds,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            output_hidden_states=<span class="literal">True</span> <span class="keyword">if</span> cls.model_args.pooler_type <span class="keyword">in</span> [<span class="string">&#x27;avg_top2&#x27;</span>, <span class="string">&#x27;avg_first_last&#x27;</span>] <span class="keyword">else</span> <span class="literal">False</span>,</span><br><span class="line">            return_dict=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pooling</span></span><br><span class="line">    pooler_output = cls.pooler(attention_mask, outputs)</span><br><span class="line">    pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-<span class="number">1</span>))) <span class="comment"># (bs, num_sent, hidden_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># If using &quot;cls&quot;, we add an extra MLP layer</span></span><br><span class="line">    <span class="comment"># (same as BERT&#x27;s original implementation) over the representation.</span></span><br><span class="line">    <span class="keyword">if</span> cls.pooler_type == <span class="string">&quot;cls&quot;</span>:</span><br><span class="line">        pooler_output = cls.mlp(pooler_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Separate representation, [bs, hidden_size], 同一样本经过“两次Dropout”得到的两个句向量</span></span><br><span class="line">    z1, z2 = pooler_output[:,<span class="number">0</span>], pooler_output[:,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Hard negative</span></span><br><span class="line">    <span class="keyword">if</span> num_sent == <span class="number">3</span>:</span><br><span class="line">        z3 = pooler_output[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gather all embeddings if using distributed training</span></span><br><span class="line">    <span class="keyword">if</span> dist.is_initialized() <span class="keyword">and</span> cls.training:</span><br><span class="line">        <span class="comment"># Gather hard negative</span></span><br><span class="line">        <span class="keyword">if</span> num_sent &gt;= <span class="number">3</span>:</span><br><span class="line">            z3_list = [torch.zeros_like(z3) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dist.get_world_size())]</span><br><span class="line">            dist.all_gather(tensor_list=z3_list, tensor=z3.contiguous())</span><br><span class="line">            z3_list[dist.get_rank()] = z3</span><br><span class="line">            z3 = torch.cat(z3_list, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dummy vectors for allgather</span></span><br><span class="line">        z1_list = [torch.zeros_like(z1) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dist.get_world_size())]</span><br><span class="line">        z2_list = [torch.zeros_like(z2) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dist.get_world_size())]</span><br><span class="line">        <span class="comment"># Allgather</span></span><br><span class="line">        dist.all_gather(tensor_list=z1_list, tensor=z1.contiguous())</span><br><span class="line">        dist.all_gather(tensor_list=z2_list, tensor=z2.contiguous())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Since allgather results do not have gradients, we replace the</span></span><br><span class="line">        <span class="comment"># current process&#x27;s corresponding embeddings with original tensors</span></span><br><span class="line">        z1_list[dist.get_rank()] = z1</span><br><span class="line">        z2_list[dist.get_rank()] = z2</span><br><span class="line">        <span class="comment"># Get full batch embeddings: (bs x N, hidden)</span></span><br><span class="line">        z1 = torch.cat(z1_list, <span class="number">0</span>)</span><br><span class="line">        z2 = torch.cat(z2_list, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [bs, bs]，计算该样本与其它样本的相似度</span></span><br><span class="line">    cos_sim = cls.sim(z1.unsqueeze(<span class="number">1</span>), z2.unsqueeze(<span class="number">0</span>))</span><br><span class="line">    <span class="comment"># Hard negative</span></span><br><span class="line">    <span class="keyword">if</span> num_sent &gt;= <span class="number">3</span>:</span><br><span class="line">        z1_z3_cos = cls.sim(z1.unsqueeze(<span class="number">1</span>), z3.unsqueeze(<span class="number">0</span>))</span><br><span class="line">        cos_sim = torch.cat([cos_sim, z1_z3_cos], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [bs, ], 内容为[0,1,...,bs-1]，表示每个样本最相似的样本下标</span></span><br><span class="line">    labels = torch.arange(cos_sim.size(<span class="number">0</span>)).long().to(cls.device)</span><br><span class="line">    <span class="comment"># 此处显示出对比学习loss和常规交叉熵loss的区别，</span></span><br><span class="line">    <span class="comment"># 对比学习的label数是[bs,bs]，而交叉熵的label数是[bs, label_nums]</span></span><br><span class="line">    loss_fct = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate loss with hard negatives</span></span><br><span class="line">    <span class="keyword">if</span> num_sent == <span class="number">3</span>:</span><br><span class="line">        <span class="comment"># Note that weights are actually logits of weights</span></span><br><span class="line">        z3_weight = cls.model_args.hard_negative_weight</span><br><span class="line">        weights = torch.tensor(</span><br><span class="line">            [[<span class="number">0.0</span>] * (cos_sim.size(-<span class="number">1</span>) - z1_z3_cos.size(-<span class="number">1</span>)) + [<span class="number">0.0</span>] * i + [z3_weight] + [<span class="number">0.0</span>] * (z1_z3_cos.size(-<span class="number">1</span>) - i - <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(z1_z3_cos.size(-<span class="number">1</span>))]</span><br><span class="line">        ).to(cls.device)</span><br><span class="line">        cos_sim = cos_sim + weights</span><br><span class="line"></span><br><span class="line">    loss = loss_fct(cos_sim, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate loss for MLM</span></span><br><span class="line">    <span class="keyword">if</span> mlm_outputs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> mlm_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mlm_labels = mlm_labels.view(-<span class="number">1</span>, mlm_labels.size(-<span class="number">1</span>))</span><br><span class="line">        prediction_scores = cls.lm_head(mlm_outputs.last_hidden_state)</span><br><span class="line">        masked_lm_loss = loss_fct(prediction_scores.view(-<span class="number">1</span>, cls.config.vocab_size), mlm_labels.view(-<span class="number">1</span>))</span><br><span class="line">        loss = loss + cls.model_args.mlm_weight * masked_lm_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">        output = (cos_sim,) + outputs[<span class="number">2</span>:]</span><br><span class="line">        <span class="keyword">return</span> ((loss,) + output) <span class="keyword">if</span> loss <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> output</span><br><span class="line">    <span class="keyword">return</span> SequenceClassifierOutput(</span><br><span class="line">        loss=loss,</span><br><span class="line">        logits=cos_sim,</span><br><span class="line">        hidden_states=outputs.hidden_states,</span><br><span class="line">        attentions=outputs.attentions,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>上述代码考虑诸多场景，比如分布式训练、难例三元组、mlm mask，写的较为复杂。</p>
<p>以下是简化版，更加符合论文的表述：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss_func = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simcse_loss</span>(<span class="params">batch_emb</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于无监督SimCSE训练的loss</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batch_size = batch_emb.size(<span class="number">0</span>)    <span class="comment"># [bs, hidden_size]</span></span><br><span class="line">    <span class="comment"># 构造标签, [bs, 2], bs=64</span></span><br><span class="line">    y_true = torch.cat([torch.arange(<span class="number">1</span>, batch_size, step=<span class="number">2</span>, dtype=torch.long).unsqueeze(<span class="number">1</span>),</span><br><span class="line">                        torch.arange(<span class="number">0</span>, batch_size, step=<span class="number">2</span>, dtype=torch.long).unsqueeze(<span class="number">1</span>)],</span><br><span class="line">                       dim=<span class="number">1</span>).reshape([batch_size,])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算score和loss</span></span><br><span class="line">    norm_emb = F.normalize(batch_emb, dim=<span class="number">1</span>, p=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># [bs, bs]，计算该样本与其它样本的相似度</span></span><br><span class="line">    sim_score = torch.matmul(norm_emb, norm_emb.transpose(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 对角线的位置，也就是自身的余弦相似度，肯定为1，不产生loss，需要mask掉</span></span><br><span class="line">    sim_score = sim_score - torch.eye(batch_size) * <span class="number">1e12</span></span><br><span class="line">    sim_score = sim_score * <span class="number">20</span>    <span class="comment"># 温度系数</span></span><br><span class="line">    loss = loss_func(sim_score, y_true)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></p>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ul>
<li>如果同一个batch里有其它语义相似的正样本，但在这里被当作了负样例处理，不是也拉远了同类样本的距离吗？</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://github.com/princeton-nlp/SimCSE" >princeton-nlp/SimCSE<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/IDWih5h2rLNqr3g0s8Y9zQ" >“被玩坏了”的Dropout<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/12UvfXnaB4NTy54wWIFZdQ" >细节满满！理解对比学习和SimCSE，就看这6个知识点<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/483453992" >SIMCSE算法源码分析<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Dropout</tag>
        <tag>Contrastive Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Swift可选类型总结</title>
    <url>/2017/12/28/Swift%E5%8F%AF%E9%80%89%E7%B1%BB%E5%9E%8B%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>Swift的可选(Optional)类型，用于处理值缺失的情况。可选表示<font color="red">“那儿有一个值，并且它等于x”或者“那儿没有值，为nil”</font>。它的定义通过在类型声明后加一个 <code>?</code> 操作符来完成的:<br><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> str <span class="operator">=</span> <span class="type">String</span>?</span><br></pre></td></tr></table></figure><br><span id="more"></span></p>
<p><code>Optional</code> 其实是个 <code>enum</code> ，里面有 <code>None</code> 和 <code>Some</code> 两种类型。其实所谓的 <code>nil</code> 就是 <code>Optional.None</code> ，当你声明一个可选变量的时候没有提供初始值，它的值默认为 <code>nil</code> 。 <code>非nil</code> 就是 <code>Optional.Some</code> ，然后会通过 <code>Some(T)</code> 包装(<code>wrap</code>)原始值，这也是为什么在使用 <code>Optional</code> 的时候要拆包(<code>unwrap</code> : 从 <code>enum</code> 中取出来原始值)的原因。下面是 <code>enum Optional</code>  的定义:</p>
<figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> <span class="title class_">Optional</span>&lt;<span class="title class_">Wrapped</span>&gt; : <span class="title class_">ExpressibleByNilLiteral</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> none</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="keyword">some</span>(<span class="type">Wrapped</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">init</span>(<span class="keyword">_</span> <span class="params">some</span>: <span class="type">Wrapped</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">func</span> <span class="title function_">map</span>&lt;<span class="type">U</span>&gt;(<span class="keyword">_</span> <span class="params">transform</span>: (<span class="type">Wrapped</span>) <span class="keyword">throws</span> -&gt; <span class="type">U</span>) <span class="keyword">rethrows</span> -&gt; <span class="type">U</span>?</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">func</span> <span class="title function_">flatMap</span>&lt;<span class="type">U</span>&gt;(<span class="keyword">_</span> <span class="params">transform</span>: (<span class="type">Wrapped</span>) <span class="keyword">throws</span> -&gt; <span class="type">U</span>?) <span class="keyword">rethrows</span> -&gt; <span class="type">U</span>?</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">init</span>(<span class="params">nilLiteral</span>: ())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">var</span> unsafelyUnwrapped: <span class="type">Wrapped</span> &#123; <span class="keyword">get</span> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>既然这样， 那我们如何理解上述变量的声明呢？<br><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> str <span class="operator">=</span> <span class="type">String</span>?</span><br><span class="line"><span class="comment">//我声明了一个Optional类型的变量，它可能包含一个String值，也可能什么都不包含，即nil</span></span><br></pre></td></tr></table></figure><br>也就是说我们实际上声明的是一个 <code>Optional</code> 类型，而不是 <code>String</code> 类型。</p>
<h2 id="和-的比较"><a href="#和-的比较" class="headerlink" title="? 和 ! 的比较"></a>? 和 ! 的比较</h2><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Cocoa</span><br><span class="line"><span class="keyword">var</span> str : <span class="type">String</span>?</span><br><span class="line">str <span class="operator">=</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line"><span class="keyword">if</span> str <span class="operator">!=</span> <span class="literal">nil</span>&#123;</span><br><span class="line">    <span class="comment">//print(str)</span></span><br><span class="line">    <span class="built_in">print</span>(str<span class="operator">!</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;字符串为nil&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出结果为: <font color="green">Hello World</font></p>
<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ul>
<li>如果是执行 <code>print(str)</code> 这句话，那么输出为 <code>Optional(&quot;Hello World&quot;)</code>。</li>
<li>使用 <code>!</code> 来获取一个不存在的可选值会导致运行时错误。使用 <code>!</code> 来强制解析值之前，一定要确定可选包含一个 <code>非nil</code> 的值。</li>
</ul>
<p>怎么使用 <code>Optional</code> 值呢？在苹果文档中也有提到说，在使用 <code>Optional</code> 值的时候需要在具体的操作，比如调用方法、属性、下标索引等前面需要加上一个?，如果是 <code>nil</code> 值，也就是 <code>Optional.None</code> ，会跳过后面的操作不执行，如果有值，就是 <code>Optional.Some</code> ，可能就会拆包(<code>unwrap</code>)，然后对拆包后的值执行后面的操作，来保证执行这个操作的安全性。</p>
<p>举例如下：<br><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span> length <span class="operator">=</span> str<span class="operator">?</span>.count</span><br><span class="line"><span class="comment">//如果你确定有值的话，也可以这样写</span></span><br><span class="line"><span class="comment">//let length = str!.count</span></span><br></pre></td></tr></table></figure></p>
<h3 id="拆包-unwrap"><a href="#拆包-unwrap" class="headerlink" title="拆包(unwrap)"></a>拆包(unwrap)</h3><p>上文提到 <code>Optional</code> 值需要拆包才能得到原来值，并判断其值是否为空才能对其操作。下面介绍两种拆包方法：</p>
<ul>
<li>可选绑定(optional binding)</li>
</ul>
<p>使用可选绑定（optional binding）来判断可选类型是否包含值，如果包含就把值赋给一个临时常量或者变量。可选绑定可以用在if和while语句中来对可选类型的值进行判断并把值赋给一个常量或者变量。举例如下：<br><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Cocoa</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> str : <span class="type">String</span>? <span class="operator">=</span> <span class="string">&quot;Hello&quot;</span></span><br><span class="line"><span class="keyword">let</span> world <span class="operator">=</span> <span class="string">&quot;World&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">let</span> const <span class="operator">=</span> str&#123;</span><br><span class="line">    <span class="built_in">print</span>(const <span class="operator">+</span> <span class="string">&quot; &quot;</span> <span class="operator">+</span> world)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;str is nil&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>硬解包<br>即直接在可选类型后面加一个 <code>!</code> 来表示它肯定有值。举例如下：<figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Cocoa</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> str : <span class="type">String</span>? <span class="operator">=</span> <span class="string">&quot;Hello&quot;</span></span><br><span class="line"><span class="keyword">let</span> world <span class="operator">=</span> <span class="string">&quot;World&quot;</span></span><br><span class="line"><span class="keyword">if</span> str <span class="operator">!=</span> <span class="literal">nil</span>&#123;</span><br><span class="line">    <span class="built_in">print</span>(str<span class="operator">!</span> <span class="operator">+</span> <span class="string">&quot; &quot;</span> <span class="operator">+</span> world)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;str is nil&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="错误示范"><a href="#错误示范" class="headerlink" title="错误示范"></a><font color="red">错误示范</font></h4><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Cocoa</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> str:<span class="type">String</span>?</span><br><span class="line"><span class="keyword">let</span> world <span class="operator">=</span> <span class="string">&quot;Hi&quot;</span></span><br><span class="line"><span class="built_in">print</span>(str<span class="operator">!</span> <span class="operator">+</span> world)</span><br></pre></td></tr></table></figure>
<p>以上代码在编译阶段不会报错.因为使用了硬解包, 编译器认为可选类型是有值的, 所以编译是通过的。当代码运行起来时，会报错：</p>
<font color="red">Fatal error: Unexpectedly found nil while unwrapping an Optional value.</font>

<h3 id="隐式拆包-自动解析"><a href="#隐式拆包-自动解析" class="headerlink" title="隐式拆包(自动解析)"></a>隐式拆包(自动解析)</h3><p>你可以在声明可选变量时使用感叹号 <code>!</code> 替换问号<code>?</code>。这样可选变量在使用时就不需要再加一个感叹号 <code>!</code> 来获取值，它会自动解析。<br>举例如下:<br><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Cocoa</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> str:<span class="type">String</span>!</span><br><span class="line">str <span class="operator">=</span> <span class="string">&quot;Hello World!&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> str <span class="operator">!=</span> <span class="literal">nil</span> &#123;</span><br><span class="line">   <span class="built_in">print</span>(str)</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&quot;str is nil&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>等于说你每次对这种类型的值操作时，都会自动在操作前补上一个 <code>!</code> 进行拆包，然后在执行后面的操作，当然如果该值是 <code>nil</code> ，会报错crash掉。</p>
<p>总而言之，<code>?</code> 和 <code>!</code> 坑还是很多的，需要不断在实践中检验和体会。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.jianshu.com/p/89a2afb82488" >Swift中 ！和 ？的区别及使用<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://www.runoob.com/swift/swift-optionals.html" >Swift 可选(Optionals)类型<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Swift</tag>
        <tag>可选类型</tag>
      </tags>
  </entry>
  <entry>
    <title>ThinkPHP多表回滚无效</title>
    <url>/2018/04/15/ThinkPHP%E5%A4%9A%E8%A1%A8%E5%9B%9E%E6%BB%9A%E6%97%A0%E6%95%88/</url>
    <content><![CDATA[<p>今天首次用到了多表回滚，遇到了一个坑，记录一下。</p>
<span id="more"></span>
<p><font color="red">错误代码如下：</font><br><figure class="highlight php"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="variable">$Member</span> = <span class="title function_ invoke__">D</span>(<span class="string">&quot;Member&quot;</span>);</span><br><span class="line">    <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">startTrans</span>();</span><br><span class="line">    <span class="variable">$member_condition</span>[<span class="string">&#x27;id&#x27;</span>] = <span class="number">11641</span>;</span><br><span class="line">    <span class="variable">$member_data</span>[<span class="string">&#x27;id&#x27;</span>] = <span class="number">10000</span>;</span><br><span class="line">    <span class="variable">$member_res</span> = <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">where</span>(<span class="variable">$member_condition</span>)-&gt;<span class="title function_ invoke__">save</span>(<span class="variable">$member_data</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="variable">$member_res</span> === <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            <span class="variable">$User</span> = <span class="title function_ invoke__">D</span>(<span class="string">&quot;User&quot;</span>);</span><br><span class="line">            <span class="variable">$User</span>-&gt;<span class="title function_ invoke__">startTrans</span>();</span><br><span class="line">            <span class="variable">$user_condition</span>[<span class="string">&#x27;account&#x27;</span>] = <span class="string">&#x27;111111&#x27;</span>;</span><br><span class="line">            <span class="variable">$user_data</span>[<span class="string">&#x27;username&#x27;</span>] = <span class="string">&quot;4324&quot;</span>;</span><br><span class="line">            <span class="variable">$user_res</span> = <span class="variable">$User</span>-&gt;<span class="title function_ invoke__">where</span>(<span class="variable">$user_condition</span>)-&gt;<span class="title function_ invoke__">save</span>(<span class="variable">$user_data</span>);</span><br><span class="line">            <span class="keyword">if</span> (<span class="variable">$user_res</span> === <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="variable">$User</span>-&gt;<span class="title function_ invoke__">commit</span>();</span><br><span class="line">                <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">commit</span>();</span><br><span class="line">                <span class="keyword">echo</span> <span class="string">&quot;全部修改成功&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="variable">$User</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">                <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">                <span class="keyword">echo</span> <span class="string">&quot;User表未受影响，全部回滚！&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">catch</span> (<span class="built_in">Exception</span> <span class="variable">$e</span>)&#123;</span><br><span class="line">            <span class="variable">$User</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">            <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">            <span class="keyword">echo</span> <span class="string">&quot;User修改异常，全部回滚！&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">&quot;Member表未受影响，回滚！&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="keyword">catch</span> (<span class="built_in">Exception</span> <span class="variable">$e</span>)&#123;</span><br><span class="line">    <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">&quot;Member修改异常！&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>我的思路是对 <code>Member</code> 和 <code>User</code> 分别开启事务，只要有一个表修改失败，那么就全部回滚。但事实确是开启了两个事务后，这两个事务都无法回滚。如果只开启一个事务，那么该事务是可以回滚的。在tp官方文档里面也没找到什么解释。解决方法如下所示：<br><figure class="highlight php"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="variable">$Model</span> = <span class="title function_ invoke__">M</span>();</span><br><span class="line">	<span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">startTrans</span>();</span><br><span class="line">	<span class="variable">$member_condition</span>[<span class="string">&#x27;id&#x27;</span>] = <span class="number">11641</span>;</span><br><span class="line">    <span class="variable">$member_data</span>[<span class="string">&#x27;id&#x27;</span>] = <span class="number">10000</span>;</span><br><span class="line">    <span class="variable">$member_res</span> = <span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">table</span>(<span class="string">&#x27;party_member&#x27;</span>)-&gt;<span class="title function_ invoke__">where</span>(<span class="variable">$member_condition</span>)-&gt;<span class="title function_ invoke__">save</span>(<span class="variable">$member_data</span>);</span><br><span class="line"></span><br><span class="line">    <span class="variable">$user_condition</span>[<span class="string">&#x27;account&#x27;</span>] = <span class="string">&#x27;111111&#x27;</span>;</span><br><span class="line">    <span class="variable">$user_data</span>[<span class="string">&#x27;username&#x27;</span>] = <span class="string">&quot;4324&quot;</span>;</span><br><span class="line">    <span class="variable">$user_res</span> = <span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">table</span>(<span class="string">&#x27;party_user&#x27;</span>)-&gt;<span class="title function_ invoke__">where</span>(<span class="variable">$user_condition</span>)-&gt;<span class="title function_ invoke__">save</span>(<span class="variable">$user_data</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="variable">$member_res</span> === <span class="number">1</span> &amp;&amp; <span class="variable">$user_res</span> === <span class="number">1</span>) &#123;</span><br><span class="line">	    <span class="keyword">echo</span> <span class="string">&quot;commit&quot;</span>;</span><br><span class="line">        <span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">commit</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">&quot;rollback&quot;</span>;</span><br><span class="line">        <span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="keyword">catch</span> (<span class="built_in">Exception</span> <span class="variable">$e</span>)&#123;</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">&quot;发生异常&quot;</span>;</span><br><span class="line">    <span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>对于多表的事务处理，先用 M 函数实例化一个空对象，使用 table 方法进行多个表的操作，如果操作成功则提交，失败则回滚。</p>
<p>另外一点需要说明的是，在有些集成环境中MySQL默认的引擎是 <code>MyISAM</code>，若想提供事务支持，需将数据库引擎改为 <code>InnoDB</code> 。</p>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>ThinkPHP</tag>
        <tag>多表回滚</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2019/07/25/Transformer/</url>
    <content><![CDATA[<p>要点如下：</p>
<span id="more"></span>
<img src="/2019/07/25/Transformer/1.png" class="">
<img src="/2019/07/25/Transformer/2.jpg" class="">
<h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><ul>
<li><code>Self-Attention</code>：表示自注意。在机器翻译中，attention分配通常是目标单词对源语句各单词的概率分布。而self-attention表示source —&gt; source的attention分配，这样每个单词便能捕获与其他所有单词的关系特征，解决了RNN无法学习长程特征的问题。</li>
<li><code>Multi-Head</code>：表示 $X$ 同时做多次映射得到多个query、key、value。</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://arxiv.org/abs/1706.03762" >Attention Is All You Need<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://jalammar.github.io/illustrated-transformer/" >The Illustrated Transformer<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Encoder-Decoder</tag>
        <tag>Paper Reading</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer面试要点</title>
    <url>/2021/03/18/Transformer%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/</url>
    <content><![CDATA[<p>记录一下常见的Transformer面试要点：</p>
<span id="more"></span>
<p>Transformer的核心在如下两张图上：</p>
<img src="/2021/03/18/Transformer%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/1.png" class="">
<h2 id="1-为什么Transformer-需要进行-Multi-head-Attention？"><a href="#1-为什么Transformer-需要进行-Multi-head-Attention？" class="headerlink" title="1. 为什么Transformer 需要进行 Multi-head Attention？"></a>1. 为什么Transformer 需要进行 Multi-head Attention？</h2><img src="/2021/03/18/Transformer%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/2.png" class="">
<ul>
<li>将模型分为多个头，形成多个子空间，让模型去关注不同方面的信息；</li>
<li>把多头注意力看作一个ensemble，模型内部的集成，类似于CNN中使用的多个卷积核，所以很多时候可以认为多头注意力可以帮助我们捕捉到更为丰富的特征信息。</li>
</ul>
<h2 id="2-Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？"><a href="#2-Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？" class="headerlink" title="2. Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？"></a>2. Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</h2><p>如果Q,K,V都是一个值,那么就变为了Self-Attention的形式：</p>
<img src="/2021/03/18/Transformer%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/3.png" class="">
<p>在实践中，Q和K的乘法是为了得到二者的相似度，一般我们的K和V是相同的，Q和K进行操作是为了得到一个attention score矩阵，这样可以得到Q关于V的表示，但一般我们再计算Q,K,V的时候会先都分别乘上一个不同的矩阵W，这么做可以增加模型的表达能力，实践中经常也会带来一定的帮助。</p>
<h2 id="3-Transformer中的attention为什么要进行scaled？"><a href="#3-Transformer中的attention为什么要进行scaled？" class="headerlink" title="3. Transformer中的attention为什么要进行scaled？"></a>3. Transformer中的attention为什么要进行scaled？</h2><p>softmax的计算公式如下：</p>
<img src="/2021/03/18/Transformer%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/4.png" class="">
<ul>
<li>非常大的$d_k$值会将softmax推向梯度非常小的区域，梯度消失为0，造成参数更新困难</li>
<li>$\frac{1}{\sqrt{d_k}}$ 使得$D(\frac{qk}{\sqrt{d_k}})=1$，有效地控制了梯度消失的问题</li>
</ul>
<h2 id="4-Attention相对于CNN、RNN的优势？"><a href="#4-Attention相对于CNN、RNN的优势？" class="headerlink" title="4. Attention相对于CNN、RNN的优势？"></a>4. Attention相对于CNN、RNN的优势？</h2><ul>
<li>参数少，算力要求低</li>
<li>并行化，速度快</li>
<li>可解释性强，不会遗忘长文本的信息</li>
</ul>
<h2 id="5-Attention的计算方式"><a href="#5-Attention的计算方式" class="headerlink" title="5. Attention的计算方式"></a>5. Attention的计算方式</h2><ul>
<li>多层MLP：$a(q, k)=w_{2}^{T} \tanh \left(W_{1}[q ; k]\right)$</li>
<li>BiLinear: $a(q, k)=q^{T} W k$</li>
<li>Scaled-Dot Product: $a(q, k)=\frac{q^{T} k}{\sqrt{d_{k}}}$</li>
<li>欧式距离</li>
<li>cosine</li>
</ul>
<h2 id="6-残差网络的作用"><a href="#6-残差网络的作用" class="headerlink" title="6. 残差网络的作用"></a>6. 残差网络的作用</h2><p>ResNet的目标是在网络加深的情况下解决网络退化的问题。</p>
<h2 id="7-LayerNorm的作用，为什么不用BN？"><a href="#7-LayerNorm的作用，为什么不用BN？" class="headerlink" title="7. LayerNorm的作用，为什么不用BN？"></a>7. LayerNorm的作用，为什么不用BN？</h2><p>归一化的作用：</p>
<ul>
<li>保持每一层特征分布的稳定性，将梯度从饱和区拉回非饱和区，从而加快模型训练速度，缓解过拟合</li>
</ul>
<p>LN not BN：</p>
<ul>
<li><p>BN对batch_size很敏感，LN不存在这个问题</p>
</li>
<li><p>CV使用BN是认为不同卷积核feature map（channel维）之间的差异性很重要，LN会损失channel的差异性，对于batch内的不同样本，同一卷积核提取特征的目的性是一致的，所以使用BN仅是为了进一步保证同一个卷积核在不同样本上提取特征的稳定性。</p>
<p>而NLP使用LN是认为batch内不同样本同一位置token之间的差异性更重要，而embedding维，网络对于不同token提取的特征目的性是一致的，使用LN是为了进一步保证在不同token上提取的稳定性。NLP每个序列的长度是不一致的，BN不适用。</p>
</li>
</ul>
<h2 id="8-Position-Encoding的设计思路"><a href="#8-Position-Encoding的设计思路" class="headerlink" title="8. Position Encoding的设计思路"></a>8. Position Encoding的设计思路</h2><script type="math/tex; mode=display">
PE(pos, 2i) = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\
PE(pos, 2i+1) = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})</script><h3 id="原因："><a href="#原因：" class="headerlink" title="原因："></a>原因：</h3><ul>
<li>单词在句子中的位置和排列顺序非常重要，它们不仅是一个句子的语法结构的组成部分，更是表达语义的重要概念；</li>
<li>Transformer使用纯attention结构，丢失了词序信息，有必要把词序信号加到词向量上帮助模型学习这些信息。</li>
</ul>
<h3 id="线性分配一个数值给每个时间步的缺点？"><a href="#线性分配一个数值给每个时间步的缺点？" class="headerlink" title="线性分配一个数值给每个时间步的缺点？"></a>线性分配一个数值给每个时间步的缺点？</h3><ul>
<li>数值巨大，且模型可能遇到比训练集所有句子都要长的句子；</li>
<li>数据集中不一定在所有数值上都会包含相对应长度的句子，也就是模型很有可能没有看到过任何一个这样的长度的样本句子，这会严重影响模型的泛化能力；</li>
</ul>
<h4 id="良好的PE方案需满足以下要求："><a href="#良好的PE方案需满足以下要求：" class="headerlink" title="良好的PE方案需满足以下要求："></a>良好的PE方案需满足以下要求：</h4><ul>
<li>它能为每个时间步输出一个独一无二的编码；</li>
<li>不同长度的句子之间，任何两个时间步之间的距离应该保持一致；</li>
<li>模型应该能毫不费力地泛化到更长的句子。它的值应该是有界的；</li>
<li>它必须是确定性的。</li>
</ul>
<h3 id="相对位置的线性关系"><a href="#相对位置的线性关系" class="headerlink" title="相对位置的线性关系"></a>相对位置的线性关系</h3><p>正弦曲线函数的位置编码的另一个特点是，它能让模型毫不费力地关注相对位置信息。具体公式推导见<a class="link"   href="https://zhuanlan.zhihu.com/p/106644634" >More: 相对位置的线性关系<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>Attention</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>UI设计常识</title>
    <url>/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/</url>
    <content><![CDATA[<p>前端的一个基础知识：</p>
<span id="more"></span>
<h1 id="手机屏幕尺寸"><a href="#手机屏幕尺寸" class="headerlink" title="手机屏幕尺寸"></a>手机屏幕尺寸</h1><blockquote>
<p>屏幕对角线长度，单位为英寸。</p>
</blockquote>
<img src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/1.jpg" class="">
<hr>

<h1 id="显示分辨率"><a href="#显示分辨率" class="headerlink" title="显示分辨率"></a>显示分辨率</h1><blockquote>
<p>屏幕拥有的像素总数，单位为像素（pixel，简写px）</p>
</blockquote>
<img src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/2.jpg" class="">
<hr>

<h1 id="屏幕像素密度"><a href="#屏幕像素密度" class="headerlink" title="屏幕像素密度"></a>屏幕像素密度</h1><blockquote>
<p>Pixels Per Inch，简写PPI或ppi，指的是每英寸所拥有的像素数。</p>
</blockquote>
<img src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/3.jpg" class="">
<img src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/4.jpg" class="">
<p>同样是尺寸大小为5英寸的屏幕，显示分辨率为 4 <em> 4px 的屏幕显示质量大于显示分辨率为 3 </em> 3px 的屏幕。</p>
<hr>

<h1 id="逻辑分辨率与虚拟尺寸单位"><a href="#逻辑分辨率与虚拟尺寸单位" class="headerlink" title="逻辑分辨率与虚拟尺寸单位"></a>逻辑分辨率与虚拟尺寸单位</h1><p>由于市面上手机种类繁多，不同的屏幕尺寸与不同的显示分辨率，为开发提供了极大的不便。</p>
<img src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/5.jpg" class="">
<p>为了尽可能减少开发人员的工作成本，开发人员需要一套统一的分辨率和尺寸单位，由此衍生出一个新的分辨率——逻辑分辨率（单位是虚拟尺寸单位）：</p>
<ul>
<li>Android 的虚拟尺寸单位是dp（用于元素）和sp（用于字体）。</li>
<li>iOS 的虚拟尺寸单位是pt。</li>
</ul>
<p>正常来说，设计师设计时采用的是显示分辨率（单位：px），程序员开发时采用的逻辑分辨率（单位：虚拟尺寸单位）。</p>
<hr>

<h1 id="逻辑分辨率与转换率的制定"><a href="#逻辑分辨率与转换率的制定" class="headerlink" title="逻辑分辨率与转换率的制定"></a>逻辑分辨率与转换率的制定</h1><p>设置逻辑分辨率的原因是为了通过将显示分辨率通过一定的倍数（转换率）缩放至一个新的分辨率大小，使得原本不同手机的显示分辨率差异缩小。开发中采用的分辨率时，方便于适配更多的机型。</p>
<img src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/6.jpg" class="">
<p>在制定转换率与逻辑分辨率的时候，我们需要注意如下三点：</p>
<ol>
<li>不同iphone间的逻辑分辨率尽量接近。</li>
<li>转换率最好是整数。</li>
<li>不同iphone转换成逻辑分辨率后的ppi尽量接近。<hr>

</li>
</ol>
<h1 id="切图"><a href="#切图" class="headerlink" title="切图"></a>切图</h1><p>对于@2x的设计来说，1 <em> 1 pt = 2 </em> 2px；对于@3x的设计来说，1 <em> 1 pt = 3 </em> 3 px.</p>
<p>所以设计好的@1x的图，要把它切开，由 1 <em> 1 pt 的大小切成 2 </em> 2 px 和 3 * 3 px 的大小。故谓之切图。 </p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.zhihu.com/question/26195746" >切图常说的@1X@2X@3X是什么意思？<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>UI设计</tag>
        <tag>前端</tag>
        <tag>分辨率</tag>
      </tags>
  </entry>
  <entry>
    <title>VAE</title>
    <url>/2022/01/12/VAE/</url>
    <content><![CDATA[<p>这段时间看了VAE的有关知识，但网上关于VAE的讲解较为理论复杂，我这里就记录一下自己的想法了。</p>
<span id="more"></span>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>VAE从概率的角度描述隐空间与输入样本，它将样本的隐变量建模为<strong>概率分布</strong>, 而非像AE一样把隐变量看做是离散的值。</p>
<h2 id="AE-VS-VAE"><a href="#AE-VS-VAE" class="headerlink" title="AE VS VAE"></a>AE VS VAE</h2><img src="/2022/01/12/VAE/1.png" class="">
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><img src="/2022/01/12/VAE/2.png" class="">
<p>我们假设隐变量的概率分布为标准正态分布$N(0, 1)$（这种分布不是必须的，也可以是其它分布）。而描述正态分布需要有两个参数$\mu_x, \sigma_x$，在encoder端使用神经网络来拟合这两个参数。在decoder端，使用神经网络来还原出原始图像。因此，VAE的损失函数分为两部分：</p>
<ul>
<li><p>正则化项，也就是KL Loss</p>
</li>
<li><p>重构损失</p>
</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
L &= L_{Recon} + L_{KL} \\
&= \|x-\hat{x}\|^{2}+\mathrm{KL}[N(\mu_{x}, \sigma_{x}), N(0, 1)] \\
&= \|x-d(z)\|^{2}+KL[N(\mu_{x}, \sigma_{x}), N(0, 1)]
\end{aligned}</script><p>关于$KL\left[N\left(\mu_{x}, \sigma_{x}\right), N(0,1)\right]$的推导如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& KL\left(N\left(\mu, \sigma^{2}\right) \| N(0,1)\right) \\
&= \int \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{\frac{-(x-\mu)^{2}}{2 \sigma^{2}} }\left(\log \frac{\frac{e^{ \frac{-(x-\mu)^{2}}{2 \sigma^{2}} }}{\sqrt{2 \pi \sigma^{2}}} }{\frac{e^{\frac{-x^{2}}{2}}}{\sqrt{2 \pi}} }\right) d x \\
&= \int \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{\frac{-(x-\mu)^{2}}{2 \sigma^{2}} } \log \left\{\frac{1}{\sqrt{\sigma^{2}}} \exp \left\{\frac{1}{2}\left[x^{2}- \frac{(x-\mu)^{2}}{\sigma^{2}} \right]\right\}\right\} d x \\
&= \frac{1}{2} \int \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{\frac{-(x-\mu)^{2}}{2 \sigma^{2}} }\left[-\log \sigma^{2}+x^{2}- \frac{(x-\mu)^{2}}{\sigma^{2}} \right] d x \\
&= \frac{1}{2}\left(-\log \sigma^{2}+\mu^{2}+\sigma^{2}-1\right)
\end{aligned}</script><h2 id="重参数技巧"><a href="#重参数技巧" class="headerlink" title="重参数技巧"></a>重参数技巧</h2><p>我们从概率分布中采样出 $z$ ，但是该过程是不可导的。VAE通过重参数化使得梯度不因采样而断裂。</p>
<img src="/2022/01/12/VAE/3.png" class="">
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实VAE可以看成一个做降维的model，我们希望把一个高维的特征投影到一个低维的流型上。而在VAE中，这个低维流型就是一个多元标准正态分布。为了使投影准确，于是通过希望每一个样本$X_i$的计算出来的期望与方差都接近与我们希望投影的分布，所以这里就有了相KL Loss。至于重构损失，是可以使采样的时候更加准确，能够采样到我们在encode的时候投影到的点。</p>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><p><a class="link"   href="https://adaning.github.io/posts/9047.html" >Pytorch实现: VAE<i class="fas fa-external-link-alt"></i></a> 这篇博客实现了VAE，整体上代码简单易懂。在generation阶段，我们只需从学习到的概率分布中采样，然后送入decoder中解码，即可获得生成的图片。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><p><a class="link"   href="https://zhuanlan.zhihu.com/p/34998569" >变分自编码器VAE：原来是这么一回事<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" >Understanding Variational Autoencoders (VAEs)<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://adaning.github.io/posts/9047.html" >Pytorch实现: VAE<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://adaning.github.io/posts/53598.html" >变分自编码器入门<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://colab.research.google.com/drive/1ZhmA2XxJ3oZC7A-U2mpUdB2eZZLz5NfW?usp=sharing#scrollTo=E7R4BFye1eAW" >VAE.ipynb - Colaboratory<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://www.bilibili.com/video/BV1Wv411h7kN?p=45" >李宏毅2021春机器学习课程<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/GAN%20(v3" >VAE.pdf(ntu.edu.tw)<i class="fas fa-external-link-alt"></i></a>)</p>
</li>
<li><p><a class="link"   href="https://blog.csdn.net/StreamRock/article/details/81258543" >VAE的推导<i class="fas fa-external-link-alt"></i></a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>AutoEncoder</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows Terminal主题配置</title>
    <url>/2022/03/01/Windows%20Terminal%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>个人在Windows Terminal上配置了一款ubuntu的主题，图片和设置文件见：<a class="link"   href="https://github.com/TransformersWsz/theme_for_windows_terminal" >A theme for windows terminal<i class="fas fa-external-link-alt"></i></a></p>
<span id="more"></span>
<img src="/2022/03/01/Windows%20Terminal%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/example.jpg" class="">]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>terminal</tag>
      </tags>
  </entry>
  <entry>
    <title>XGBoost</title>
    <url>/2021/05/31/XGBoost/</url>
    <content><![CDATA[<p>记录一下XGBoost的的学习过程。</p>
<span id="more"></span>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>我们要预测一家人对电子游戏的喜好程度，有年龄、性别、职业这些特征。根据之前训练出来的多棵树来对这些样本打分，如下图所示：</p>
<img src="/2021/05/31/XGBoost/1.png" class="">
<p>注意，<font color="red">上述分数是由训练所得</font>。与GBDT类似，两棵树的结论累加起来便是最终结论。如果不考虑工程实现、解决问题上的一些差异，XGBoost与GBDT比较大的不同就是目标函数的定义：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Obj^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t)}\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\
&=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text { constant }
\end{aligned}</script><p>前 $t-1$ 棵树的复杂度之和可以用一个常量 $constant$ 表示。上述公式由两部分组成：</p>
<ul>
<li>损失函数：揭示训练误差</li>
<li>正则化项：惩罚复杂模型</li>
</ul>
<h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>前沿知识：泰勒展开式：</p>
<script type="math/tex; mode=display">
f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}</script><p>定义如下符号：</p>
<script type="math/tex; mode=display">
g_{i}=\frac{\partial \; l\left(y_{i}, \hat{y_i}^{(t-1)}\right)}{\partial \;  {\hat{y_i}^{(t-1)}} } \\ 

h_{i}=\frac{\partial^2 \; l\left(y_{i}, \hat{y_i}^{(t-1)}\right)}{\partial^2 \;  {\hat{y_i}^{(t-1)}} }</script><p>因此：</p>
<script type="math/tex; mode=display">
O b j^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)+\text { constant }</script><p>对应关系如下：</p>
<ul>
<li>(1)式 $x$ $\Leftrightarrow$ (2)式 $\hat{y_i}^{(t-1)}$</li>
<li>(1)式 $\Delta x$ $\Leftrightarrow$ (2)式 $f_{t}\left(x_{i}\right)$</li>
</ul>
<p>由于 $\hat{y_i}^{(t-1)}$ 是已知的，因此 $l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)$ 也是个常数项，可以合并到 $constant$ 去。将 $constant$ 去掉，上述公式可以简化为：</p>
<script type="math/tex; mode=display">
O b j^{(t)} \simeq \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)</script><h3 id="正则化项"><a href="#正则化项" class="headerlink" title="正则化项"></a>正则化项</h3><img src="/2021/05/31/XGBoost/2.png" class="">
<ul>
<li>$q(x)$ 表示将样本 $x$ 映射到某个叶子节点的编号上</li>
<li>$w$ 表示叶子节点的得分</li>
</ul>
<p>注意，<font color="red">多个样本可以落到同一个叶子节点上，这时它们的得分是一样的</font>。</p>
<p>XGBoost定义树的复杂度如下：</p>
<img src="/2021/05/31/XGBoost/3.png" class="">
<h3 id="重新组织损失函数"><a href="#重新组织损失函数" class="headerlink" title="重新组织损失函数"></a>重新组织损失函数</h3><p>由于 $w$ 是我们要求的参数，因此将上述公式组织成关于 $w$ 的函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
O b j^{(t)} & \simeq \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\
&=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\lambda \frac{1}{2} \sum_{j=1}^{T} w_{j}^{2} \\
&=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T
\end{aligned}</script><ul>
<li>$I_{j}=\left\{i \mid q\left(x_{i}\right)=j\right\}$ 表示样本下标集合：这些样本可以落到下标为 $j$ 的叶子节点</li>
</ul>
<p>这样上式可以看作关于 $w$ 的一元二次函数。</p>
<p>定义 $G_{j}=\sum_{i \in I_{j}} g_{i} \quad H_{j}=\sum_{i \in I_{j}} h_{i}$ ，上式继续简化为：</p>
<script type="math/tex; mode=display">
Obj^{(t)} = \sum_{j=1}^T \left[ \frac{1}{2} (H_j + \lambda) w_j^2 + G_j w_j \right] + \gamma T</script><p>当 $w_j = - \frac{G_j}{H_j + \lambda}$ 时，$Obj^{(t)}$ 取得最小：$-\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T$</p>
<p>下图给个示例：</p>
<img src="/2021/05/31/XGBoost/4.png" class="">
<hr>
<h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h3 id="1-二阶泰勒展开的优势在哪儿？"><a href="#1-二阶泰勒展开的优势在哪儿？" class="headerlink" title="1. 二阶泰勒展开的优势在哪儿？"></a>1. 二阶泰勒展开的优势在哪儿？</h3><p>PPT上是这样说的：</p>
<img src="/2021/05/31/XGBoost/5.png" class="">
<p>主要有如下两点理由：</p>
<ul>
<li>XGBoost是以mse为基础推导出来的，在mse的情况下，xgboost的目标函数展开就是一阶项+二阶项的形式，而其他类似logloss这样的目标函数不能表示成这种形式。为了后续推导的统一，所以将目标函数进行二阶泰勒展开，就可以直接自定义损失函数了，只要二阶可导即可，增强了模型的扩展性。</li>
<li>二阶信息能够让梯度收敛的更快，类似牛顿法比SGD收敛更快。一阶信息描述梯度变化方向，二阶信息可以描述梯度变化方向是如何变化的。</li>
</ul>
<h3 id="2-XGBoost与GBDT的区别"><a href="#2-XGBoost与GBDT的区别" class="headerlink" title="2. XGBoost与GBDT的区别"></a>2. XGBoost与GBDT的区别</h3><ul>
<li>基分类器：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的逻辑回归或者线性回归。</li>
<li>导数信息：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。</li>
<li>正则项：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。</li>
<li>列抽样：XGBoost支持列采样，与随机森林类似，用于防止过拟合。</li>
<li>缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。</li>
<li>并行化：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。</li>
</ul>
<h3 id="3-XGBoost为什么可以并行训练？"><a href="#3-XGBoost为什么可以并行训练？" class="headerlink" title="3. XGBoost为什么可以并行训练？"></a>3. XGBoost为什么可以并行训练？</h3><ul>
<li>XGBoost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。</li>
<li>XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。</li>
</ul>
<h3 id="4-XGBoost如何防止过拟合？"><a href="#4-XGBoost如何防止过拟合？" class="headerlink" title="4. XGBoost如何防止过拟合？"></a>4. XGBoost如何防止过拟合？</h3><ul>
<li>目标函数添加正则项：叶子节点个数+叶子节点权重的L2正则化</li>
<li>列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可）</li>
<li>子采样：每轮计算可以不使用全部样本，使算法更加保守</li>
<li>shrinkage: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间</li>
</ul>
<h3 id="5-XGBoost如何处理缺失值？"><a href="#5-XGBoost如何处理缺失值？" class="headerlink" title="5. XGBoost如何处理缺失值？"></a>5. XGBoost如何处理缺失值？</h3><ul>
<li>在特征 $k$ 上寻找最佳分割点时，不会对该列特征缺失的样本进行遍历，而只对该列特征值为非缺失的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找分割点的时间开销。</li>
<li>在逻辑实现上，为了保证完备性，会将该特征值缺失的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。</li>
<li>如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。</li>
</ul>
<h3 id="6-XGBoost中的一棵树的停止生长条件"><a href="#6-XGBoost中的一棵树的停止生长条件" class="headerlink" title="6.  XGBoost中的一棵树的停止生长条件"></a>6.  XGBoost中的一棵树的停止生长条件</h3><ul>
<li>当新引入的一次分裂所带来的增益 $Gain &lt; \gamma$ 时，放弃当前的分裂。</li>
<li>当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合。</li>
<li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。</li>
</ul>
<h3 id="7-XGBoost中如何对树进行剪枝？"><a href="#7-XGBoost中如何对树进行剪枝？" class="headerlink" title="7. XGBoost中如何对树进行剪枝？"></a>7. XGBoost中如何对树进行剪枝？</h3><ul>
<li>在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。</li>
<li>在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。</li>
<li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。</li>
<li>XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。</li>
</ul>
<h3 id="8-XGBoost如何分裂节点？"><a href="#8-XGBoost如何分裂节点？" class="headerlink" title="8. XGBoost如何分裂节点？"></a>8. XGBoost如何分裂节点？</h3><p>从树深度0开始，每一节点都遍历所有的特征，比如年龄、性别等等，然后对于某个特征，<strong>先按照该特征里的值进行排序，然后线性扫描该特征进而确定最好的分割点</strong>，最后对所有特征进行分割后，我们选择所谓的增益Gain最高的那个特征。Gain的计算公式如下：</p>
<img src="/2021/05/31/XGBoost/6.png" class="">
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/156047718" >XGBoost原理及常见面试题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/v_JULY_v/article/details/81410574" >通俗理解kaggle比赛大杀器xgboost<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>ajax发送请求无法加载等待模态框？</title>
    <url>/2018/07/15/ajax%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82%E6%97%A0%E6%B3%95%E5%8A%A0%E8%BD%BD%E7%AD%89%E5%BE%85%E6%A8%A1%E6%80%81%E6%A1%86%EF%BC%9F/</url>
    <content><![CDATA[<p>虽说现在谈论jQuery已经很low了，但出于维护旧项目的需要，还是重新学习了一遍。当我们向后台发送请求的时候，为了照顾用户体验，需要使用等待模态框框来过渡。今天遇到的一个坑是，无论怎么发送请求，界面都不会出现模态框，即使有也是一闪而过。代码如下：</p>
<span id="more"></span>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>test<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;./js/jquery-3.1.1.js&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">&quot;./css/bootstrap.min.css&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/css&quot;</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;./js/bootstrap.min.js&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">function</span> <span class="title function_">firmSubmit</span>(<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">            $(<span class="string">&quot;#submit&quot;</span>).<span class="title function_">click</span>(<span class="keyword">function</span> (<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="keyword">var</span> sex = <span class="built_in">parseInt</span>($(<span class="string">&#x27;#sex&#x27;</span>).<span class="title function_">val</span>());</span></span><br><span class="line"><span class="language-javascript">                <span class="keyword">if</span> (sex == <span class="number">0</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                    <span class="title function_">alert</span>(<span class="string">&quot;性别不能为空！&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">                    $(<span class="string">&#x27;#sex&#x27;</span>).<span class="title function_">focus</span>();</span></span><br><span class="line"><span class="language-javascript">                    <span class="keyword">return</span> <span class="literal">false</span>;</span></span><br><span class="line"><span class="language-javascript">                &#125;</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">                $.<span class="title function_">ajax</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">type</span>: <span class="string">&quot;post&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">url</span>: <span class="string">&quot;/api&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">dataType</span>: <span class="string">&quot;json&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">data</span>: &#123; <span class="string">&quot;sex&quot;</span>: sex &#125;,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">beforeSend</span>: <span class="keyword">function</span> (<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                        $(<span class="string">&quot;#loadingModal&quot;</span>).<span class="title function_">modal</span>(<span class="string">&quot;show&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">                    &#125;,</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">success</span>: <span class="keyword">function</span> (<span class="params">json</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                        <span class="keyword">if</span> (json.<span class="property">result</span> == <span class="number">1</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                            <span class="title function_">alert</span>(<span class="string">&quot;提交成功&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">                        &#125;</span></span><br><span class="line"><span class="language-javascript">                        <span class="keyword">else</span> &#123;</span></span><br><span class="line"><span class="language-javascript">                            <span class="title function_">alert</span>(<span class="string">&quot;提交失败，请重新检查！&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">                        &#125;</span></span><br><span class="line"><span class="language-javascript">                    &#125;,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">complete</span>: <span class="keyword">function</span> (<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                        $(<span class="string">&quot;#loadingModal&quot;</span>).<span class="title function_">modal</span>(<span class="string">&quot;hide&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">                        <span class="variable language_">window</span>.<span class="property">location</span>.<span class="property">href</span> = <span class="string">&quot;/in&quot;</span>;</span></span><br><span class="line"><span class="language-javascript">                    &#125;</span></span><br><span class="line"><span class="language-javascript">                &#125;);</span></span><br><span class="line"><span class="language-javascript">            &#125;);</span></span><br><span class="line"><span class="language-javascript">        &#125;</span></span><br><span class="line"><span class="language-javascript">        $(<span class="keyword">function</span> (<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">            <span class="comment">//使用getJSON方法读取json数据,</span></span></span><br><span class="line"><span class="language-javascript">            <span class="keyword">var</span> options = [<span class="string">&quot;sex&quot;</span>];</span></span><br><span class="line"><span class="language-javascript">            $.<span class="title function_">ajaxSetup</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="attr">async</span>: <span class="literal">false</span></span></span><br><span class="line"><span class="language-javascript">            &#125;);</span></span><br><span class="line"><span class="language-javascript">            $.<span class="title function_">getJSON</span>(<span class="string">&quot;/getsex&quot;</span>, <span class="keyword">function</span> (<span class="params">data</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; options.<span class="property">length</span>; i++) &#123;</span></span><br><span class="line"><span class="language-javascript">                    $.<span class="title function_">each</span>(data[options[i]], <span class="keyword">function</span> (<span class="params">key, val</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                        <span class="keyword">var</span> str = <span class="string">&quot;&lt;option value=&quot;</span> + key + <span class="string">&quot;&gt;&quot;</span> + val + <span class="string">&quot;&lt;/option&gt;&quot;</span>;</span></span><br><span class="line"><span class="language-javascript">                        $(<span class="string">&quot;#&quot;</span> + options[i]).<span class="title function_">append</span>(str);</span></span><br><span class="line"><span class="language-javascript">                    &#125;);</span></span><br><span class="line"><span class="language-javascript">                &#125;</span></span><br><span class="line"><span class="language-javascript">            &#125;);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">            <span class="comment">//表单判空并提交</span></span></span><br><span class="line"><span class="language-javascript">            <span class="title function_">firmSubmit</span>();</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">        &#125;);</span></span><br><span class="line"><span class="language-javascript">    </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">        <span class="selector-class">.main</span> &#123;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">width</span>: <span class="number">38%</span>;</span></span><br><span class="line"><span class="language-css">        &#125;</span></span><br><span class="line"><span class="language-css"></span></span><br><span class="line"><span class="language-css">        <span class="selector-class">.commonlabel</span> &#123;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">font-size</span>: <span class="number">16px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">margin-left</span>: <span class="number">5px</span>;</span></span><br><span class="line"><span class="language-css">        &#125;</span></span><br><span class="line"><span class="language-css"></span></span><br><span class="line"><span class="language-css">        <span class="selector-class">.loading</span> &#123;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">height</span>: <span class="number">80px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">width</span>: <span class="number">80px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">background</span>: <span class="built_in">url</span>(<span class="string">&#x27;./img/load.gif&#x27;</span>) no-repeat center;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">opacity</span>: <span class="number">0.7</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">position</span>: fixed;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">left</span>: <span class="number">50%</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">top</span>: <span class="number">50%</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">margin-left</span>: -<span class="number">40px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">margin-top</span>: -<span class="number">40px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">z-index</span>: <span class="number">1001</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">background-color</span>: <span class="number">#dad8d8</span>;</span></span><br><span class="line"><span class="language-css">            -moz-<span class="attribute">border-radius</span>: <span class="number">20px</span>;</span></span><br><span class="line"><span class="language-css">            -webkit-<span class="attribute">border-radius</span>: <span class="number">20px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">border-radius</span>: <span class="number">20px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">filter</span>: progid:DXImageTransform.Microsoft.<span class="built_in">Alpha</span>(opacity=<span class="number">70</span>);</span></span><br><span class="line"><span class="language-css">        &#125;</span></span><br><span class="line"><span class="language-css">    </span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;loadingModal&quot;</span> <span class="attr">class</span>=<span class="string">&quot;modal fade&quot;</span> <span class="attr">data-keyboard</span>=<span class="string">&quot;false&quot;</span> <span class="attr">tabindex</span>=<span class="string">&quot;-1&quot;</span> <span class="attr">data-backdrop</span>=<span class="string">&quot;static&quot;</span> <span class="attr">data-role</span>=<span class="string">&quot;dialog&quot;</span> <span class="attr">aria-labelledby</span>=<span class="string">&quot;myModalLabel&quot;</span></span></span><br><span class="line"><span class="tag">        <span class="attr">aria-hidden</span>=<span class="string">&quot;true&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;loading&quot;</span> <span class="attr">class</span>=<span class="string">&quot;loading&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;main center-block&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">style</span>=<span class="string">&quot;height: 30px;&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;form-group&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">&quot;sex&quot;</span> <span class="attr">class</span>=<span class="string">&quot;commonlabel&quot;</span>&gt;</span>性别<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;sex&quot;</span> <span class="attr">class</span>=<span class="string">&quot;form-control&quot;</span> <span class="attr">name</span>=<span class="string">&quot;sex&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">button</span> <span class="attr">id</span>=<span class="string">&quot;submit&quot;</span> <span class="attr">class</span>=<span class="string">&quot;btn btn-primary&quot;</span> <span class="attr">style</span>=<span class="string">&quot;width: 20%; margin: 0 auto; display: block; float: left;&quot;</span>&gt;</span>提交<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>后来才发现是 <code>$.ajaxSetup(&#123;async: false&#125;);</code> 搞的鬼。查了一下官方文档，它是这样解释的：</p>
<blockquote>
<p>async (default: true)<br>Type: Boolean<br>By default, all requests are sent asynchronously (i.e. this is set to true by default). If you need synchronous requests, set this option to false. Cross-domain requests and dataType: “jsonp” requests do not support synchronous operation. Note that synchronous requests may temporarily lock the browser, disabling any actions while the request is active. As of jQuery 1.8, the use of async: false with jqXHR ($.Deferred) is deprecated; you must use the success/error/complete callback options instead of the corresponding methods of the jqXHR object such as jqXHR.done().</p>
</blockquote>
<p>也就是说设置 <code>async: false</code> 会锁住浏览器，禁止浏览器的任何行为。比如用户点击按钮、下拉滚动条等行为浏览器都不会有响应，直到该同步请求完成。所以发送同步请求期间，模态框不会出现。但在请求完成后，会执行：<br><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">complete</span>: <span class="keyword">function</span> (<span class="params"></span>) &#123;</span><br><span class="line">    $(<span class="string">&quot;#loadingModal&quot;</span>).<span class="title function_">modal</span>(<span class="string">&quot;hide&quot;</span>);</span><br><span class="line">    <span class="variable language_">window</span>.<span class="property">location</span>.<span class="property">href</span> = <span class="string">&quot;/in&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>因此会出现模态框一闪而过的情况。解决方法也很简单，在发送请求前将 <code>$.ajaxSetup(&#123;async: true&#125;);</code> 设置回来即可。</p>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>Ajax</tag>
      </tags>
  </entry>
  <entry>
    <title>args &amp; kwargs</title>
    <url>/2017/12/29/args%20&amp;%20kwargs/</url>
    <content><![CDATA[<p><code>args</code> 和 <code>kwargs</code> 是python函数中最常用的传参形式，用法非常灵活。</p>
<span id="more"></span>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>Python支持可变参数，最简单的方法莫过于使用默认参数。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_defargs</span>(<span class="params">one, two=<span class="number">2</span></span>):    <span class="comment"># 参数one没有默认值，two的默认值为2</span></span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;Required argument: &#x27;</span>, one)</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;Optional argument: &#x27;</span>, two)</span><br><span class="line"></span><br><span class="line">test_defargs(<span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Required argument: 1</span></span><br><span class="line"><span class="string">Optional argument: 2</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">test_defargs(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Required argument: 1</span></span><br><span class="line"><span class="string">Optional argument: 3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>另外一种达到可变参数 (Variable Argument) 的方法：<br>使用 <code>*args</code> 和 <code>**kwargs</code> 语法。<br><code>*args</code> 是可变的位置参数(positional arguments)列表，<br><code>**kwargs</code> 是可变的关键词参数(keyword arguments)列表。<br>并且规定位置参数必须位于关键词参数之前，即 <code>*args</code> 必须位于 <code>**kwargs</code> 之前。</p>
<h2 id="位置参数"><a href="#位置参数" class="headerlink" title="位置参数"></a>位置参数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_hello</span>(<span class="params">name, sex</span>):</span><br><span class="line">    sex_dict = &#123;<span class="number">0</span>: <span class="string">&#x27;先生&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;女士&#x27;</span>&#125;    <span class="comment"># key: value</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;hello %s %s, welcome to Python world!&#x27;</span> %(name, sex_dict.get(sex%<span class="number">2</span>, <span class="string">&#x27;先生&#x27;</span>)))    <span class="comment"># if no such a key, print &#x27;先生&#x27;</span></span><br><span class="line"></span><br><span class="line">print_hello(<span class="string">&#x27;Chen&#x27;</span>, <span class="number">2</span>)    <span class="comment"># 位置参数要求先后顺序，对应name和sex</span></span><br><span class="line">print_hello(<span class="string">&#x27;Chen&#x27;</span>, <span class="number">3</span>)    <span class="comment"># 两个参数的顺序必须一一对应，且少一个参数都不可以</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hello Chen 女士, welcome to Python world!</span></span><br><span class="line"><span class="string">hello Chen 先生, welcome to Python world!</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="关键字参数"><a href="#关键字参数" class="headerlink" title="关键字参数"></a>关键字参数</h2><p>用于函数调用，通过“键-值”形式加以指定。<br>使用关键词参数可以让函数更加清晰、容易使用，同时也清除了参数的顺序需求。</p>
<p>以下是用关键字参数正确调用函数的实例<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print_hello(<span class="string">&#x27;Chen&#x27;</span>, sex=<span class="number">1</span>)    <span class="comment"># 有位置参数时，位置参数必须在关键字参数的前面</span></span><br><span class="line">print_hello(name=<span class="string">&#x27;Chen&#x27;</span>, sex=<span class="number">1</span>)    <span class="comment"># 关键字参数之间不存在先后顺序的</span></span><br><span class="line">print_hello(sex=<span class="number">1</span>, name=<span class="string">&#x27;Chen&#x27;</span>)</span><br></pre></td></tr></table></figure><br>以下是错误的调用方式:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print_hello(name=<span class="string">&#x27;Chen&#x27;</span>, <span class="number">1</span>)    <span class="comment"># 有位置参数时，位置参数必须在关键字参数的前面</span></span><br><span class="line">print_hello(sex=<span class="number">1</span>, <span class="string">&#x27;Chen&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="默认参数"><a href="#默认参数" class="headerlink" title="默认参数"></a>默认参数</h2><p>使用关键词参数时，可为参数提供默认值，调用函数时可传可不传该默认参数的值（注意：所有位置参数必须出现在默认参数前，包括函数定义和调用）</p>
<p>正确的默认参数定义方式 —&gt; 位置参数在前，默认参数在后:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_hello2</span>(<span class="params">name, sex=<span class="number">1</span></span>):    <span class="comment"># 默认sex=1</span></span><br><span class="line">    sex_dict = &#123;<span class="number">1</span>: <span class="string">&#x27;先生&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;女士&#x27;</span>&#125;    <span class="comment"># key: value</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;hello %s %s, welcome to circus!&#x27;</span> %(name, sex_dict.get(sex, <span class="string">&#x27;先生&#x27;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 错误的定义方式</span></span><br><span class="line"><span class="comment"># def print_hello2(sex=1, name):</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用时不传sex的值，则使用默认值1</span></span><br><span class="line">print_hello2(<span class="string">&#x27;Chen&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hello Chen 先生, welcome to circus!</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用时传入sex的值，并指定为2。无视原来的sex=1，仅本次生效，不会改变函数sex的默认值</span></span><br><span class="line">print_hello2(<span class="string">&#x27;Liu&#x27;</span>, sex=<span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hello Liu 女士, welcome to circus!</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="收集参数-Packing"><a href="#收集参数-Packing" class="headerlink" title="收集参数(Packing)"></a>收集参数(Packing)</h2><blockquote>
<p>被收集到一起的位置参数或关键词参数</p>
</blockquote>
<p>有些时候，我们在定义参数时不确定会传递多少个参数（或者不传递），甚至我们想要根据实际情况传入任意个参数，这个时候 <code>packing</code> 就派上了用场。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_args</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line"></span><br><span class="line">print_args(<span class="number">1</span>)</span><br><span class="line">print_args(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(1,)</span></span><br><span class="line"><span class="string">(1, 2, 3)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>可以看到，结果是作为一个元组(tuple)打印出来的，因为仅传入1时，输出的括号里面有一个逗号。实际上，<code>*args</code> 前的 <code>*</code> 可以理解为将我们需要传入的所有值放置在了同一个元组里面，这就是收集 (packing) 的过程。</p>
<h3 id="位置参数的收集传递-————-packing"><a href="#位置参数的收集传递-————-packing" class="headerlink" title="位置参数的收集传递 ———— *packing"></a>位置参数的收集传递 ———— *packing</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_args2</span>(<span class="params">num, *args</span>):</span><br><span class="line">    <span class="built_in">print</span>(num)</span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line"></span><br><span class="line">print_args2(<span class="number">1</span>)</span><br><span class="line">print_args2(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1    print(num)</span></span><br><span class="line"><span class="string">()    print(args) --&gt; 一个空元组</span></span><br><span class="line"><span class="string">1    print(num)</span></span><br><span class="line"><span class="string">(2, 3)    print(args) --&gt; 剩余的参数组成的元组！</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>那么这么说来 <code>*</code> 的意思就是“把剩余的所有参数收集 packing 起来”！</p>
<h3 id="关键词参数的收集传递-————-packing"><a href="#关键词参数的收集传递-————-packing" class="headerlink" title="关键词参数的收集传递 ———— **packing"></a>关键词参数的收集传递 ———— **packing</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_kargs</span>(<span class="params">**kargs</span>):</span><br><span class="line">    <span class="built_in">print</span>(kargs)</span><br><span class="line"></span><br><span class="line">print_kargs(x=<span class="number">1</span>, y=<span class="number">2</span>, z=<span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;z&#x27;: 3, &#x27;y&#x27;: 2, &#x27;x&#x27;: 1&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>结果输出了一个字典(dic)，双星号 <code>**</code> 收集了所有关键词参数。</p>
<h3 id="分割参数"><a href="#分割参数" class="headerlink" title="分割参数"></a>分割参数</h3><p><code>*</code> 和 <code>**</code> 也可以在函数调用过程中使用，称之为分割 (unpacking)。</p>
<h4 id="在传递元组时，让元组的每一个元素对应一个位置参数"><a href="#在传递元组时，让元组的每一个元素对应一个位置参数" class="headerlink" title="在传递元组时，让元组的每一个元素对应一个位置参数"></a>在传递元组时，让元组的每一个元素对应一个位置参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_hello</span>(<span class="params">name, sex</span>):</span><br><span class="line">    <span class="built_in">print</span>(name, sex)</span><br><span class="line"></span><br><span class="line">args = (<span class="string">&#x27;Chen&#x27;</span>, <span class="string">&#x27;男&#x27;</span>)</span><br><span class="line">print_hello(*args)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Chen 男</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="在传递词典字典时，让词典的每个键值对作为一个关键字参数传递给函数"><a href="#在传递词典字典时，让词典的每个键值对作为一个关键字参数传递给函数" class="headerlink" title="在传递词典字典时，让词典的每个键值对作为一个关键字参数传递给函数"></a>在传递词典字典时，让词典的每个键值对作为一个关键字参数传递给函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_info</span>(<span class="params">**kargs</span>):</span><br><span class="line">    <span class="built_in">print</span>(kargs)</span><br><span class="line"></span><br><span class="line">kargs = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Chen&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>: <span class="string">&#x27;男&#x27;</span>, <span class="string">&#x27;time&#x27;</span>: <span class="string">&#x27;13&#x27;</span>&#125;</span><br><span class="line">print_info(**kargs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;name&#x27;: &#x27;Chen&#x27;, &#x27;sex&#x27;:&#x27;男&#x27;,&#x27;time&#x27;:&#x27;13&#x27;&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="位置参数、关键词参数（默认参数）、收集参数的混合使用"><a href="#位置参数、关键词参数（默认参数）、收集参数的混合使用" class="headerlink" title="位置参数、关键词参数（默认参数）、收集参数的混合使用"></a>位置参数、关键词参数（默认参数）、收集参数的混合使用</h2><p>基本原则是：先位置参数，默认参数，收集位置参数，收集关键字参数(定义和调用都应遵循)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">name, age, sex=<span class="number">1</span>, *args, **kargs</span>):</span><br><span class="line">    <span class="built_in">print</span>(name, age, sex, args, kargs)</span><br><span class="line"></span><br><span class="line">func(<span class="string">&#x27;Chen&#x27;</span>, <span class="number">25</span>, <span class="number">2</span>, <span class="string">&#x27;Geo&#x27;</span>, <span class="string">&#x27;SR&#x27;</span>, level=<span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Chen 25 2 (&#x27;Geo&#x27;, &#x27;SR&#x27;) &#123;&#x27;level&#x27;: 2&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>总结一下就是，星号 <code>*</code> 只有在定义需要使用不定数目的参数的函数，或者调用“分割”字典和序列时才有必要添加。</p>
<h2 id="各类参数混合使用"><a href="#各类参数混合使用" class="headerlink" title="各类参数混合使用"></a>各类参数混合使用</h2><h3 id="EXAMPLE-1"><a href="#EXAMPLE-1" class="headerlink" title="EXAMPLE 1"></a>EXAMPLE 1</h3><h4 id="Tell-a-story"><a href="#Tell-a-story" class="headerlink" title="Tell a story"></a>Tell a story</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">story1</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Once upon a time, there was a %(job)s called %(name)s.&#x27;</span> % kwargs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">story2</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;In the year of our lord %(year)d, there once lived a %(job)s of a royal line.&#x27;</span> % kwargs</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(story1(name=<span class="string">&#x27;Robin&#x27;</span>, job=<span class="string">&#x27;brave knight&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(story1(job=<span class="string">&#x27;king&#x27;</span>, name=<span class="string">&#x27;Charlie&#x27;</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Once upon a time, there was a brave knight called Robin.</span></span><br><span class="line"><span class="string">Once upon a time, there was a king called Charlie.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">kwargs = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Abel&#x27;</span>, <span class="string">&#x27;job&#x27;</span>: <span class="string">&#x27;bard&#x27;</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(story1(**kwargs))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Once upon a time, there was a bard called Abel.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">kwargs.pop(<span class="string">&#x27;job&#x27;</span>)    <span class="comment"># delete key[&#x27;job&#x27;] --&gt; kwargs = &#123;&#x27;name&#x27;: &#x27;Abel&#x27;,&#125;</span></span><br><span class="line"><span class="built_in">print</span>(story1(job=<span class="string">&#x27;witch&#x27;</span>, **kwargs))    <span class="comment"># redefine key[&#x27;job&#x27;]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Once upon a time, there was a witch called Abel.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(story2(year=<span class="number">1239</span>, job=<span class="string">&#x27;prince&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(story2(job=<span class="string">&#x27;princess&#x27;</span>, year=<span class="number">1239</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">In the year of our lord 1239, there once lived a prince of a royal line.</span></span><br><span class="line"><span class="string">In the year of our lord 1239, there once lived a princess of a royal line.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="EXAMPLE-2"><a href="#EXAMPLE-2" class="headerlink" title="EXAMPLE 2"></a>EXAMPLE 2</h3><h4 id="x-to-the-yth-power"><a href="#x-to-the-yth-power" class="headerlink" title="x to the yth power"></a>x to the yth power</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">power</span>(<span class="params">x, y, *args</span>):</span><br><span class="line">    <span class="keyword">if</span> args:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Received redundant parameters:&#x27;</span>, args)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">pow</span>(x, y)    </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(power(<span class="number">2</span>, <span class="number">3</span>))    <span class="comment"># 2 to the 3rd power</span></span><br><span class="line"><span class="built_in">print</span>(power(<span class="number">3</span>, <span class="number">2</span>))    <span class="comment"># 3 to the 2nd power</span></span><br><span class="line"><span class="built_in">print</span>(power(x=<span class="number">2</span>, y=<span class="number">3</span>))    <span class="comment"># 2 to the 3rd power</span></span><br><span class="line"><span class="built_in">print</span>(power(y=<span class="number">3</span>, x=<span class="number">2</span>))    <span class="comment"># no order for keyword arguments</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">8</span></span><br><span class="line"><span class="string">9</span></span><br><span class="line"><span class="string">8</span></span><br><span class="line"><span class="string">8</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">args = (<span class="number">5</span>,) * <span class="number">2</span>    <span class="comment"># &#x27;(5,) * 2&#x27;: copy elements in the tuple 2 times. The outcome is (5, 5)</span></span><br><span class="line"><span class="built_in">print</span>(power(*args))    <span class="comment"># 5 to the 5th power</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">3125</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(power(<span class="number">2</span>, <span class="number">5</span>, <span class="string">&#x27;Keep on trying&#x27;</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Received redundant parameters: (&#x27;Keep on trying&#x27;,)</span></span><br><span class="line"><span class="string">32    # 2 to the 5th power</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="EXAMPLE-3"><a href="#EXAMPLE-3" class="headerlink" title="EXAMPLE 3"></a>EXAMPLE 3</h3><h4 id="Bulid-a-function-to-realize-range-for-step-gt-0"><a href="#Bulid-a-function-to-realize-range-for-step-gt-0" class="headerlink" title="Bulid a function to realize range() for step&gt;0"></a>Bulid a function to realize range() for step&gt;0</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">interval</span>(<span class="params">start=<span class="number">0</span>, stop=<span class="literal">None</span>, step=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">if</span> stop <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        start, stop = <span class="number">0</span>, start</span><br><span class="line">    result = []</span><br><span class="line">    i = start</span><br><span class="line">    <span class="keyword">while</span> i &lt; stop:</span><br><span class="line">        result.append(i)</span><br><span class="line">        i += step</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(interval(<span class="number">10</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(interval(<span class="number">1</span>, <span class="number">6</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[1, 2, 3, 4, 5]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(interval(<span class="number">3</span>, <span class="number">15</span>, <span class="number">3</span>))</span><br><span class="line">[<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">12</span>]</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="POSTSCRIPT"><a href="#POSTSCRIPT" class="headerlink" title="POSTSCRIPT"></a>POSTSCRIPT</h2><p>在Python 3.X当中加入了”部分剩余参数”的概念。举例如下 :<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a,\*b,c = <span class="built_in">range</span>(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><br>中间的 *b 实际上是收集到了3个参数。</p>
<p>下面这个例子更加直观 :<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func1</span>(<span class="params">x, y, *args, z=<span class="number">1</span></span>):    <span class="comment"># z=1 在函数参数中最后定义</span></span><br><span class="line">    <span class="built_in">print</span>(x, y, args, z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">func1(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1 2 (3, 4, 5) 1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>Python 3.X 当中，默认参数 z=1 在函数参数中最后定义，Python 就会知道除了 传给 x 和 y, 以及 z=1 的部分，其他的剩余参数 (3, 4, 5) 都是传入到 *args 当中。</p>
<font color="red">但在Python 2.X 并没有这一特性，所以 Python 2.X 的 z=1 必须在args和*kwargs这种剩余参数收集之前。</font>

<p>默认参数 z=1 不是在函数参数中最后定义时，情况又是怎样？<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func2</span>(<span class="params">x, y, z=<span class="number">1</span>, *args</span>):</span><br><span class="line">    <span class="built_in">print</span>(x, y, z, args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">func2(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1 2 3 (4, 5)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>3传入到 z ，而 args=(4, 5)</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.jianshu.com/p/61507f60fa29">Python中的 <em>args 和 *</em>kwargs</a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>可变参数</tag>
      </tags>
  </entry>
  <entry>
    <title>chmod &amp; chown</title>
    <url>/2017/12/20/chmod%20&amp;%20chown/</url>
    <content><![CDATA[<p>两种都是关于linux权限的命令。</p>
<span id="more"></span>
<h1 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h1><p> <code>chmod</code> 指令是更改文件读写执行权限的。<br>文件权限可以通过 <code>ls -a</code> 或 <code>ll</code> 来看,在每个文件前有10个字符,第一个是 <code>d</code> 是文件夹,否则为 <code>-</code> 。后面三组 <code>rwx</code> ，分别是读取，写入和执行的权限； 三组分别是用户自己,同组以及其他人的相应 <code>rwx</code> 权限。没有执行权限, 脚本和程序也不能直接跑；没有写权限,就没法生成和保存文件；没有读的权限就连访问都难。一般文件权限是 <code>755</code> ，下面将介绍。</p>
<h2 id="权限有两种表示方式"><a href="#权限有两种表示方式" class="headerlink" title="权限有两种表示方式 :"></a>权限有两种表示方式 :</h2><ul>
<li><code>rwx</code> 方式</li>
<li><p>数字方式</p>
<ul>
<li><code>r</code> 权限代表 <code>1</code></li>
<li><code>w</code> 权限代表 <code>2</code></li>
<li><code>x</code> 权限代表 <code>4</code></li>
<li><p>无权限代表 <code>0</code></p>
<p><code>rwx</code> 权限数字的值累加起来，就是一个 <code>用户/组/其余人</code> 的相应权限，例如 <code>775</code> 代表用户和组具有 <code>rwx</code> 权限，而其他人只有 <code>rx</code> 权限没有写权限。</p>
</li>
</ul>
</li>
</ul>
<h2 id="命令格式-chmod-选项-权限模式-文件"><a href="#命令格式-chmod-选项-权限模式-文件" class="headerlink" title="命令格式 : chmod [选项] 权限模式 文件"></a>命令格式 : chmod [选项] 权限模式 文件</h2><h3 id="选项"><a href="#选项" class="headerlink" title="选项"></a>选项</h3><ul>
<li><code>-c or --changes</code> : 效果类似”-v”参数，但仅汇报更改的部分</li>
<li><code>-f or --quiet</code> : 强制执行,不显示错误信息</li>
<li><code>-R or --recursive</code> : 递归处理，将指令目录下的所有文件及子目录一并处理</li>
<li><code>-v or --verbose</code> : 显示指令执行过程</li>
<li><code>--reference=&lt;参考文件或目录&gt;</code> : 把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同</li>
</ul>
<h3 id="权限模式"><a href="#权限模式" class="headerlink" title="权限模式"></a>权限模式</h3><ul>
<li><code>&lt;权限范围&gt;+&lt;权限值&gt;</code> : 开启权限范围的文件或目录的该选项权限设置</li>
<li><code>&lt;权限范围&gt;-&lt;权限值&gt;</code> : 关闭权限范围的文件或目录的该选项权限设置</li>
<li><code>&lt;权限范围&gt;=&lt;权限值&gt;</code> : 指定权限范围的文件或目录的该选项权限设置</li>
</ul>
<h4 id="权限范围"><a href="#权限范围" class="headerlink" title="权限范围"></a>权限范围</h4><ul>
<li><code>u</code> : <code>User</code> , 即文件或目录的拥有者</li>
<li><code>g</code> : <code>Group</code> , 即文件或目录的所属群组</li>
<li><code>o</code> : <code>Other</code> , 除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围</li>
<li><code>a</code> : <code>All</code> , 即全部的用户，包含拥有者，所属群组以及其他用户</li>
</ul>
<h4 id="权限值"><a href="#权限值" class="headerlink" title="权限值"></a>权限值</h4><ul>
<li><code>r</code> : 读取权限，数字代号为 <code>4</code></li>
<li><code>w</code> : 写入权限，数字代号为 <code>2</code></li>
<li><code>x</code> : 执行或切换权限，数字代号为 <code>1</code></li>
<li><code>-</code> : 不具任何权限，数字代号为 <code>0</code></li>
<li><code>s</code> : 特殊功能说明 : 变更文件或目录的权限</li>
<li>不指明权限范围时默认为 <code>All</code> 所有人</li>
</ul>
<h4 id="两种设置方式："><a href="#两种设置方式：" class="headerlink" title="两种设置方式："></a>两种设置方式：</h4><ol>
<li><code>权限范围+/-/=权限值</code> , 例如 <code>u+x</code> 就是用户增加执行权限；不同组别设置使用 <code>,</code> 分隔，例如 <code>u+wx,g+w,o-wx</code> ； 也可以 <code>ug+wx</code> 写。<code>o=r</code> 就是只有读权限 <code>(r–)</code> ; <code>+x</code> 就是三个组都增加执行权限。</li>
<li>三个数字模式，例如 <code>755</code> 代表用户具有 <code>rwx</code> ，组和其他人有 <code>rx</code> 。</li>
</ol>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">chmod u+x,g+w f01　　#为文件f01设置自己可以执行，组员可以写入的权限 </span><br><span class="line">chmod u=rwx,g=rw,o=r f01  #rwxrw-r--</span><br><span class="line">chmod 764 f01    #rwx-wx--x权限</span><br><span class="line">chmod a+x f01　　#对文件f01的u,g,o都增加可执行属性</span><br><span class="line">chmod -R +x DirName #对整个文件夹及里面内容都增加执行权限</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="chown"><a href="#chown" class="headerlink" title="chown"></a>chown</h1><p>chown指令是更改文件归属的，归属哪个用户，用户组是什么。对应将影响chmod里rwx效果。</p>
<p>用户可以是用户或者是用户D，用户组可以是组名或组id。文件名可以使由空格分开的文件列表，在文件名中可以包含通配符。-R选项后可以对整个文件夹操作。 只有文件主(改变自己的文件)和超级用户(改变他人的)才可以便用该命令。非root管理员慎用。</p>
<h2 id="命令格式-chown-选项-用户-组-文件"><a href="#命令格式-chown-选项-用户-组-文件" class="headerlink" title="命令格式: chown [选项] 用户:组 文件"></a>命令格式: chown [选项] 用户:组 文件</h2><ul>
<li><code>-c或--changes</code> : 效果类似 “-v” 参数，但仅汇报更改的部分</li>
<li><code>-f或--quite或--silent</code> : 强制执行,不显示错误信息</li>
<li><code>-h或--no-dereference</code> : 只对符号连接的文件作修改，而不更改其他任何相关文件</li>
<li><code>-R或--recursive</code> : 递归处理，将指定目录下的所有文件及子目录一并处理</li>
<li><code>-v或--version</code> : 显示指令执行过程</li>
<li><code>--dereference</code> : 效果和“-h”参数相同</li>
<li><code>--help</code> : 在线帮助</li>
<li><code>--reference=&lt;参考文件或目录&gt;</code> : 把指定文件或目录的拥有者与所属群组全部设成和参考文件或目录的拥有者与所属群组相同</li>
<li><code>--version</code> : 显示版本信息。</li>
</ul>
<p>记住 <code>用户:组</code> 的写法就可以了。</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title>expand和repeat区别</title>
    <url>/2020/12/08/expand%E5%92%8Crepeat%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p><code>expand</code> 和 <code>repeat</code> 都是对张量进行扩维，这里记录下使用区别。</p>
<span id="more"></span>
<h1 id="expand"><a href="#expand" class="headerlink" title="expand()"></a>expand()</h1><blockquote>
<p>Returns a new view of the :attr:<code>self</code> tensor with singleton dimensions expanded to a larger size.</p>
</blockquote>
<p>将张量<code>=1</code>的维度进行扩展，<code>&gt;1</code>的维度保持不变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">12</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">a.expand(<span class="number">3</span>,-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[12,  3,  4],
        [12,  3,  4],
        [12,  3,  4]])
</code></pre><h1 id="repeat"><a href="#repeat" class="headerlink" title="repeat()"></a>repeat()</h1><blockquote>
<p>Repeats this tensor along the specified dimensions.</p>
</blockquote>
<p>将张量沿着特定维度进行复制。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">b.repeat(<span class="number">3</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1, 2, 4, 1, 2, 4],
        [4, 5, 6, 4, 5, 6],
        [1, 2, 4, 1, 2, 4],
        [4, 5, 6, 4, 5, 6],
        [1, 2, 4, 1, 2, 4],
        [4, 5, 6, 4, 5, 6]])
</code></pre>]]></content>
      <categories>
        <category>pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>tensor</tag>
      </tags>
  </entry>
  <entry>
    <title>git撤销操作</title>
    <url>/2021/05/10/git%E6%92%A4%E9%94%80%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p><code>git</code>有如下三种状态：</p>
<span id="more"></span>
<img src="/2021/05/10/git%E6%92%A4%E9%94%80%E6%93%8D%E4%BD%9C/git.jpg" class="">
<blockquote>
<ul>
<li>Modified: You have changed the file but have not committed it to your database yet.</li>
<li>Staged: You have marked a modified file in its current version to go into your next commit snapshot.</li>
<li>Committed: The data is safely stored in your local database.</li>
</ul>
</blockquote>
<p>现在主要讲述下<code>git</code>的撤销操作：</p>
<h2 id="discard-changes-in-working-directory"><a href="#discard-changes-in-working-directory" class="headerlink" title="discard changes in working directory"></a>discard changes in working directory</h2><p>前提：<code>&lt;file&gt;</code>已经被添加到暂存区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git restore &lt;file&gt;</span><br></pre></td></tr></table></figure>
<h2 id="working-direction-lt-index"><a href="#working-direction-lt-index" class="headerlink" title="working direction &lt;- index"></a>working direction &lt;- index</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git reset HEAD &lt;file&gt;</span><br></pre></td></tr></table></figure>
<h2 id="index-lt-HEAD"><a href="#index-lt-HEAD" class="headerlink" title="index &lt;- HEAD"></a>index &lt;- HEAD</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git reset --soft HEAD^</span><br></pre></td></tr></table></figure>
<h2 id="working-direction-lt-HEAD"><a href="#working-direction-lt-HEAD" class="headerlink" title="working direction &lt;- HEAD"></a>working direction &lt;- HEAD</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git reset --hard HEAD^</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>VCS</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo支持latex</title>
    <url>/2022/03/08/hexo%E6%94%AF%E6%8C%81latex/</url>
    <content><![CDATA[<p>最近在新电脑上重新搭了个博客，为了使hexo开启latex支持又踩了一次坑，在此记录一下。</p>
<span id="more"></span>
<h2 id="卸载旧版渲染引擎"><a href="#卸载旧版渲染引擎" class="headerlink" title="卸载旧版渲染引擎"></a>卸载旧版渲染引擎</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm uninstall hexo-math --save</span><br></pre></td></tr></table></figure>
<h2 id="安装新引擎"><a href="#安装新引擎" class="headerlink" title="安装新引擎"></a>安装新引擎</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-kramed --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure>
<h2 id="更改库文件"><a href="#更改库文件" class="headerlink" title="更改库文件"></a>更改库文件</h2><h3 id="node-modules-hexo-renderer-kramed-lib-renderer-js"><a href="#node-modules-hexo-renderer-kramed-lib-renderer-js" class="headerlink" title="node_modules/hexo-renderer-kramed/lib/renderer.js"></a><code>node_modules/hexo-renderer-kramed/lib/renderer.js</code></h3><p>将<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">formatText</span>(<span class="params">text</span>) &#123;</span><br><span class="line">    <span class="comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span></span><br><span class="line">    <span class="keyword">return</span> text.<span class="title function_">replace</span>(<span class="regexp">/`\$(.*?)\$`/g</span>, <span class="string">&#x27;$$$$$1$$$$&#x27;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>修改为：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">formatText</span>(<span class="params">text</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="node-modules-kramed-lib-rules-inline-js"><a href="#node-modules-kramed-lib-rules-inline-js" class="headerlink" title="node_modules/kramed/lib/rules/inline.js"></a><code>node_modules/kramed/lib/rules/inline.js</code></h3><p>latex与markdown语法上有语义冲突，hexo默认的转义规则会将一些字符进行转义，所以我们需要对默认的规则进行修改。更改后为：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> inline = &#123;</span><br><span class="line">  <span class="comment">// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line">  <span class="attr">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br><span class="line">  <span class="attr">autolink</span>: <span class="regexp">/^&lt;([^ &gt;]+(@|:\/)[^ &gt;]+)&gt;/</span>,</span><br><span class="line">  <span class="attr">url</span>: noop,</span><br><span class="line">  <span class="attr">html</span>: <span class="regexp">/^&lt;!--[\s\S]*?--&gt;|^&lt;(\w+(?!:\/|[^\w\s@]*@)\b)*?(?:&quot;[^&quot;]*&quot;|&#x27;[^&#x27;]*&#x27;|[^&#x27;&quot;&gt;])*?&gt;([\s\S]*?)?&lt;\/\1&gt;|^&lt;(\w+(?!:\/|[^\w\s@]*@)\b)(?:&quot;[^&quot;]*&quot;|&#x27;[^&#x27;]*&#x27;|[^&#x27;&quot;&gt;])*?&gt;/</span>,</span><br><span class="line">  <span class="attr">link</span>: <span class="regexp">/^!?\[(inside)\]\(href\)/</span>,</span><br><span class="line">  <span class="attr">reflink</span>: <span class="regexp">/^!?\[(inside)\]\s*\[([^\]]*)\]/</span>,</span><br><span class="line">  <span class="attr">nolink</span>: <span class="regexp">/^!?\[((?:\[[^\]]*\]|[^\[\]])*)\]/</span>,</span><br><span class="line">  <span class="attr">reffn</span>: <span class="regexp">/^!?\[\^(inside)\]/</span>,</span><br><span class="line">  <span class="attr">strong</span>: <span class="regexp">/^__([\s\S]+?)__(?!_)|^\*\*([\s\S]+?)\*\*(?!\*)/</span>,</span><br><span class="line">  <span class="comment">// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line">  <span class="attr">em</span>: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br><span class="line">  <span class="attr">code</span>: <span class="regexp">/^(`+)\s*([\s\S]*?[^`])\s*\1(?!`)/</span>,</span><br><span class="line">  <span class="attr">br</span>: <span class="regexp">/^ &#123;2,&#125;\n(?!\s*$)/</span>,</span><br><span class="line">  <span class="attr">del</span>: noop,</span><br><span class="line">  <span class="attr">text</span>: <span class="regexp">/^[\s\S]+?(?=[\\&lt;!\[_*`$]| &#123;2,&#125;\n|$)/</span>,</span><br><span class="line">  <span class="attr">math</span>: <span class="regexp">/^\$\$\s*([\s\S]*?[^\$])\s*\$\$(?!\$)/</span>,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h3 id="node-modules-hexo-renderer-mathjax-mathjax-html"><a href="#node-modules-hexo-renderer-mathjax-mathjax-html" class="headerlink" title="node_modules/hexo-renderer-mathjax/mathjax.html"></a><code>node_modules/hexo-renderer-mathjax/mathjax.html</code></h3><p>将最后一行改为：<br><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="开启mathjax"><a href="#开启mathjax" class="headerlink" title="开启mathjax"></a>开启mathjax</h2><p>打开<font color="red"><strong>主题</strong></font>目录下的 <code>_config.yml</code> 文件，加入如下：<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>在写博客的时候需要开启latex就加上字段说明：<br><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">title: test</span><br><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><br>但每次都这么加显得很麻烦，可以在 <code>scaffolds/post.md</code> 添加如下模版：<br><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">categories: </span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line"><span class="section">toc: true</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><br>这样就无需每次手写了。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/weixin_44191286/article/details/102702479" >Hexo博客中使用Latex<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>软件工具</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title>iOS内存管理</title>
    <url>/2017/08/21/iOS%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<p>现在iOS开发已经是ARC的时代，但是内存管理仍是一个重点关注的问题。它是程序设计中很重要的一部分。程序在运行的过程中消耗内存，运行结束后释放占用的内存。如果程序运行时一直分配内存而不及时释放无用的内存，程序占用的内存会越来越大，直至内存消耗殆尽，程序因无内存可用导致崩溃，这就是所谓的内存泄漏。本文将介绍ObjC的内存管理方式。</p>
<span id="more"></span>
<h2 id="引用计数"><a href="#引用计数" class="headerlink" title="引用计数"></a>引用计数</h2><p>ObjC采用引用计数来进行内存管理:</p>
<ol>
<li>每个对象都有一个关联的整数，称为引用计数器</li>
<li>当代码需要使用该对象时，则将对象的引用计数加1</li>
<li>当代码结束使用该对象时，则将对象的引用计数减1</li>
<li>当引用计数的值变为0时，表示对象没有被任何代码使用，此时对象占用的内存将被释放</li>
</ol>
<p>与之对应的消息发送方法如下:</p>
<ol>
<li>当对象被创建(<code>alloc 、new 、copy</code> 等方法)时，其引用计数初始值为1</li>
<li>给对象发送 <code>retain</code> 消息，其引用计数加1</li>
<li>给对象发送 <code>release</code> 消息，其引用计数减1</li>
<li>当对象引用计数归0时，ObjC给对象发送 <code>dealloc</code> 消息销毁对象</li>
</ol>
<p>下面通过一个例子来说明(关闭Xcode的 Automatic Reference Counting 功能):</p>
<p>场景: 有一个宠物中心（内存），可以派出小动物（对象）陪小朋友们玩耍（对象引用者），现在xiaoming想和小狗一起玩耍。</p>
<p>新建Dog类，重写其创建和销毁的方法</p>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="meta">#import <span class="string">&quot;Dog.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">@implementation</span> <span class="title">Dog</span></span></span><br><span class="line"></span><br><span class="line">- (<span class="keyword">instancetype</span>)init</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">self</span> = [<span class="keyword">super</span> init])</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@&quot;小狗已被派出，%lu&quot;</span>,<span class="keyword">self</span>.retainCount);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">self</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">- (<span class="keyword">void</span>)dealloc</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@&quot;小狗回到宠物中心&quot;</span>);</span><br><span class="line">    [<span class="keyword">super</span> dealloc];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">@end</span></span><br></pre></td></tr></table></figure>
<p>在main方法中创建dog对象，给dog发送消息</p>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="meta">#import <span class="string">&lt;Foundation/Foundation.h&gt;</span></span></span><br><span class="line"><span class="meta">#import <span class="string">&quot;Dog.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> main(<span class="keyword">int</span> argc, <span class="keyword">const</span> <span class="keyword">char</span> * argv[]) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        <span class="comment">//模拟：宠物中心派出小狗</span></span><br><span class="line">        Dog * dog = [[Dog alloc]init];</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//模拟：xiaoming需要和小狗玩耍，需要将其引用计数加1</span></span><br><span class="line">        [dog <span class="keyword">retain</span>];</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//模拟：xiaoming不和小狗玩耍了，需要将其引用计数减1</span></span><br><span class="line">        [dog release];</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//没人需要和小狗玩耍了，将其引用计数减1</span></span><br><span class="line">        [dog release];</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将指针置nil，否则变为野指针</span></span><br><span class="line">        dog = <span class="literal">nil</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b25cf4b2f5204a84591887f6dd0aef0f.png" alt="结果"></p>
<p>可以看到，引用计数帮助宠物中心很好的标记了小狗的使用状态，在完成任务的时候及时收回到宠物中心。</p>
<h3 id="注意事项："><a href="#注意事项：" class="headerlink" title="注意事项："></a>注意事项：</h3><ol>
<li>NSString 引用计数问题</li>
</ol>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSString</span>* str = <span class="string">@&quot;hello world!&quot;</span>;</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;%ld&quot;</span>, str.retainCount);</span><br></pre></td></tr></table></figure>
<p>结果输出为-1，这可以理解为str实际上是一个字符串常量，它存储在常量存储区，是没有引用计数的。</p>
<ol>
<li>赋值不会拥有某个对象</li>
</ol>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">Dog* dog1 = dog;</span><br></pre></td></tr></table></figure>
<font color=red>这里仅仅是指针赋值操作，并不会增加dog的引用计数，需要持有对象必须要发送retain消息</font>

<ol>
<li><p>dealloc<br>由于释放对象是会调用dealloc方法，因此重写dealloc方法来查看对象释放的情况，如果没有调用则会造成内存泄露。在上面的例子中我们通过重写dealloc让小狗被释放的时候打印日志来告诉我们已经完成释放。</p>
</li>
<li><p>在上面的例子中，如果再增加这样一个操作:</p>
</li>
</ol>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="comment">//没人需要和小狗玩耍了，将其引用计数减1</span></span><br><span class="line">[dog release];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;%ld&quot;</span>,dog.retainCount);</span><br></pre></td></tr></table></figure>
<p>会发现引用计数为1，为什么不是0呢？这是因为对引用计数为1的对象release时，系统知道该对象将被回收，就不会再对该对象的引用计数进行减1操作，这样可以增加对象回收的效率。</p>
<p>另外，对已释放的对象发送消息是不可取的，因为对象的内存已被回收，如果发送消息时，该内存已经被其他对象使用了，得到的结果是无法确定的，甚至会造成崩溃。</p>
<h2 id="自动释放池"><a href="#自动释放池" class="headerlink" title="自动释放池"></a>自动释放池</h2><p>当不再使用一个对象时应该将其释放，但是在某些情况下，我们很难理清一个对象什么时候不再使用（比如xiaoming和小狗玩耍结束的时间不确定），这应该如何处理？</p>
<p>ObjC提供autorelease方法来解决这个问题，当给一个对象发送autorelease消息时，方法会在未来某个时间给这个对象发送release消息将其释放，在这个时间段内，对象还是可以使用的。</p>
<h3 id="autorelease-的工作原理"><a href="#autorelease-的工作原理" class="headerlink" title="autorelease 的工作原理"></a>autorelease 的工作原理</h3><p>当对象接收到autorelease消息时，它会被添加到了当前的自动释放池中，当自动释放池被销毁时，会给池里所有的对象发送release消息。这里就引出了自动释放池这个概念，顾名思义，就是一个池，这个池可以容纳对象，而且可以自动释放，这就大大增加了我们处理对象的灵活性。</p>
<p>ObjC提供两种方法创建自动释放池：</p>
<ol>
<li>使用NSAutoreleasePool来创建</li>
</ol>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSAutoreleasePool</span> * pool = [[<span class="built_in">NSAutoreleasePool</span> alloc]init];</span><br><span class="line"><span class="comment">//这里写代码</span></span><br><span class="line">[pool release];</span><br></pre></td></tr></table></figure>
<ol>
<li>使用@autoreleasepool创建</li>
</ol>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line"><span class="comment">//这里写代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>自动释放池创建后，就会成为活动的池子，释放池子后，池子将释放其所包含的所有对象。以上两种方法推荐第二种，因为第二种方法效率更高。</p>
<h4 id="自动释放池什么时候创建"><a href="#自动释放池什么时候创建" class="headerlink" title="自动释放池什么时候创建?"></a>自动释放池什么时候创建?</h4><p>自动释放池的销毁时间是确定的，一般是在程序事件处理之后释放，或者由我们自己手动释放。<br>下面举例说明自动释放池的工作流程：<br>场景：现在xiaoming和xiaohong都想和小狗一起玩耍，但是他们的需求不一样，他们的玩耍时间不一样。</p>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建一个自动释放池</span></span><br><span class="line"><span class="built_in">NSAutoreleasePool</span> * pool = [[<span class="built_in">NSAutoreleasePool</span> alloc] init];</span><br><span class="line"><span class="comment">//模拟：宠物中心派出小狗</span></span><br><span class="line">Dog * dog = [[Dog alloc]init];</span><br><span class="line"></span><br><span class="line"><span class="comment">//模拟：xiaoming需要和小狗玩耍，需要将其引用计数加1</span></span><br><span class="line">[dog <span class="keyword">retain</span>];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line"></span><br><span class="line"><span class="comment">//模拟：xiaohong需要和小狗玩耍，需要将其引用计数加1</span></span><br><span class="line">[dog <span class="keyword">retain</span>];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line"></span><br><span class="line"><span class="comment">//模拟：xiaoming确定不想和小狗玩耍了，需要将其引用计数减1</span></span><br><span class="line">[dog release];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line"></span><br><span class="line"><span class="comment">//模拟：xiaohong不确定何时不想和小狗玩耍了，将其设置为自动释放</span></span><br><span class="line">[dog autorelease];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line"></span><br><span class="line"><span class="comment">//没人需要和小狗玩耍了，将其引用计数减1</span></span><br><span class="line">[dog release];</span><br><span class="line"></span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;释放池子&quot;</span>);</span><br><span class="line">[pool release];</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/08148f61a2a6260ab599542d6a87b8fe.png" alt="结果"></p>
<p>可以看到，当池子释放后，dog对象才被释放，因此在池子释放之前，xiaohong都可以尽情地和小狗玩耍。</p>
<h4 id="使用自动释放池需要注意"><a href="#使用自动释放池需要注意" class="headerlink" title="使用自动释放池需要注意"></a>使用自动释放池需要注意</h4><ol>
<li>自动释放池实质上只是在释放的时候給池中所有对象对象发送release消息，不保证对象一定会销毁，如果自动释放池向对象发送release消息后对象的引用计数仍大于1，对象就无法销毁。</li>
<li>自动释放池中的对象会集中同一时间释放，如果操作需要生成的对象较多占用内存空间大，可以使用多个释放池来进行优化。比如在一个循环中需要创建大量的临时变量，可以创建内部的池子来降低内存占用峰值。</li>
<li><font color=red>autorelease不会改变对象的引用计数</font>

</li>
</ol>
<h4 id="自动释放池的常见问题"><a href="#自动释放池的常见问题" class="headerlink" title="自动释放池的常见问题"></a>自动释放池的常见问题</h4><p>在管理对象释放的问题上，自动帮助我们释放池节省了大量的时间，但是有时候它却未必会达到我们期望的效果，比如在一个循环事件中，如果循环次数较大或者事件处理占用内存较大，就会导致内存占用不断增长，可能会导致不希望看到的后果。</p>
<p>示例代码:<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100000</span>; i ++) &#123;</span><br><span class="line">    <span class="built_in">NSString</span> * log  = [<span class="built_in">NSString</span> stringWithFormat:<span class="string">@&quot;%d&quot;</span>, i];</span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@&quot;%@&quot;</span>, log);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>前面讲过，自动释放池的释放时间是确定的，这个例子中自动释放池会在循环事件结束时释放，那问题来了：在这个十万次的循环中，每次都会生成一个字符串并打印，这些字符串对象都放在池子中并直到循环结束才会释放，因此在循环期间内存不增长。</p>
<p>这类问题的解决方案是在循环中创建新的自动释放池，多少个循环释放一次由我们自行决定。</p>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100000</span>; i ++) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        <span class="built_in">NSString</span> * log  = [<span class="built_in">NSString</span> stringWithFormat:<span class="string">@&quot;%d&quot;</span>, i];</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@&quot;%@&quot;</span>, log);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="iOS的内存管理规则"><a href="#iOS的内存管理规则" class="headerlink" title="iOS的内存管理规则"></a>iOS的内存管理规则</h2><h3 id="基本规则"><a href="#基本规则" class="headerlink" title="基本规则"></a>基本规则</h3><ol>
<li>当你通过new、alloc或copy方法创建一个对象时，它的引用计数为1，当不再使用该对象时，应该向对象发送release或者autorelease消息释放对象</li>
<li>当你通过其他方法获得一个对象时，如果对象引用计数为1且被设置为autorelease，则不需要执行任何释放对象的操作</li>
<li>如果你打算取得对象所有权，就需要保留对象并在操作完成之后释放，且必须保证retain和release的次数对等</li>
</ol>
<p>应用到文章开头的例子中，小朋友每申请一个小狗（生成对象），最后都要归还到宠物中心（释放对象），如果只申请而不归还（对象创建了没有释放），那宠物中心的小狗就会越来越少（可用内存越来越少），到最后一个小狗都没有了（内存被耗尽），其他小朋友就再也没有小狗可申请了（无资源可申请使用），因此，必须要遵守规则：申请必须归还（规则1），申请几个必须归还几个（规则3），如果小狗被设定归还时间则不用小朋友主动归还（规则2）</p>
<h3 id="ARC"><a href="#ARC" class="headerlink" title="ARC"></a>ARC</h3><p>在MRC时代，必须严格遵守以上规则，否则内存问题将成为恶魔一样的存在，然而来到ARC时代，事情似乎变得轻松了，不用再写无止尽的ratain和release似乎让开发变得轻松了，对初学者变得更友好。</p>
<p>ObjC2.0引入了垃圾回收机制，然而由于垃圾回收机制会对移动设备产生某些不好的影响（例如由于垃圾清理造成的卡顿），iOS并不支持这个机制，苹果的解决方案就是ARC（自动引用计数）。</p>
<p>iOS5以后，我们可以开启ARC模式，ARC可以理解成一位管家，这个管家会帮我们向对象发送retain和release语句，不再需要我们手动添加了，我们可以更舒心地创建或引用对象，简化内存管理步骤，节省大量的开发时间。</p>
<p>实际上，ARC不是垃圾回收，也并不是不需要内存管理了，它是隐式的内存管理，编译器在编译的时候会在代码插入合适的ratain和release语句，相当于在背后帮我们完成了内存管理的工作。</p>
<p>下面将自动释放池的例子转化为ARC来看看：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line"></span><br><span class="line">    Dog * dog = [[Dog alloc]init];</span><br><span class="line"></span><br><span class="line">    [xiaoming playWithDog:dog];</span><br><span class="line"></span><br><span class="line">    [xiaohong playWithDog:dog];</span><br><span class="line"></span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@&quot;释放池子&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><ul>
<li>如果你的工程历史比较久，可以将其从MRC转换成ARC，跟上时代的步伐更好地维护</li>
<li>如果你的工程引用了某些不支持ARC的库，可以在Build Phases的Compile Sources将对应的m文件的编译器参数配置为-fno-objc-arc</li>
<li><font color=red>ARC能帮我们简化内存管理问题，但不代表它是万能的，还是有它不能处理的情况，这就需要我们自己手动处理，比如循环引用、非ObjC对象、Core Foundation中的malloc()或者free()等等</font>

</li>
</ul>
<h3 id="ARC的修饰符"><a href="#ARC的修饰符" class="headerlink" title="ARC的修饰符"></a>ARC的修饰符</h3><p>ARC提供四种修饰符，分别是：</p>
<ul>
<li><code>strong</code>, </li>
<li><code>weak</code>, </li>
<li><code>autoreleasing</code>, </li>
<li><code>unsafe_unretained</code></li>
</ul>
<p>下面举例说明：</p>
<ul>
<li><code>_strong</code> : 强引用，持有所指向对象的所有权，无修饰符情况下的默认值。如需强制释放，可置nil。</li>
</ul>
<p>比如我们常用的定时器：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSTimer</span> * timer = [<span class="built_in">NSTimer</span> timerWith...];</span><br></pre></td></tr></table></figure><br>相当于<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSTimer</span> * __<span class="keyword">strong</span> timer = [<span class="built_in">NSTimer</span> timerWith...];</span><br></pre></td></tr></table></figure><br>当不需要使用时，强制销毁定时器<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">[timer invalidate];</span><br><span class="line">timer = <span class="literal">nil</span>;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>_weak</code> : 弱引用，不持有所指向对象的所有权，引用指向的对象内存被回收之后，引用本身会置nil，避免野指针。</li>
</ul>
<p>比如避免循环引用的弱引用声明：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">__<span class="keyword">weak</span> __<span class="keyword">typeof</span>(<span class="keyword">self</span>) weakSelf = <span class="keyword">self</span>;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>__autoreleasing</code> : 自动释放对象的引用，一般用于传递参数</li>
</ul>
<p>比如一个读取数据的方法:<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)loadData:(<span class="built_in">NSError</span> **)error;</span><br></pre></td></tr></table></figure><br>当你调用时会发现这样的提示<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSError</span> * error;</span><br><span class="line">[dataTool loadData:(<span class="built_in">NSError</span> *__autoreleasing *)]</span><br></pre></td></tr></table></figure><br>这时编译器自动帮我们插入以下代码：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSError</span> * error;</span><br><span class="line"><span class="built_in">NSError</span> * __autoreleasing tmpErr = error;</span><br><span class="line">[dataTool loadData:&amp;tmpErr];</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>__unsafe_unretained</code> : 为兼容iOS5以下版本的产物，可以理解成MRC下的weak，现在基本用不到，这里不作描述。</li>
</ul>
<h3 id="属性的内存管理"><a href="#属性的内存管理" class="headerlink" title="属性的内存管理"></a>属性的内存管理</h3><p>ObjC2.0引入了@property，提供成员变量访问方法、权限、环境、内存管理类型的声明，下面主要说明ARC中属性的内存管理。</p>
<p>属性的参数分为三类，基本数据类型默认为(atomic,readwrite,assign)，对象类型默认为(atomic,readwrite,strong)，其中第三个参数就是该属性的内存管理方式修饰，修饰词可以是以下之一：</p>
<ul>
<li><p><code>assign</code> : 一般用来修饰基本数据类型</p>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>, <span class="keyword">assign</span>) <span class="built_in">NSInteger</span> count;</span><br></pre></td></tr></table></figure>
<p>当然也可以修饰ObjC对象，但是不推荐，因为被assign修饰的对象释放后，指针还是指向释放前的内存，在后续操作中可能会导致内存问题引发崩溃。</p>
</li>
<li><p><code>retain</code> : release旧值，再retain新值（引用计数＋1）</p>
</li>
</ul>
<p>retain和strong一样，都用来修饰ObjC对象。使用set方法赋值时，实质上是会先保留新值，再释放旧值，再设置新值，避免新旧值一样时导致对象被释放的的问题。</p>
<p>MRC写法如下：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)setCount:(<span class="built_in">NSObject</span> *)count &#123;</span><br><span class="line">    [count <span class="keyword">retain</span>];</span><br><span class="line">    [_count release];</span><br><span class="line">    _count = count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>ARC写法如下：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)setCount:(<span class="built_in">NSObject</span> *)count &#123;</span><br><span class="line">    _count = count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>copy</code> : release旧值，再copy新值（拷贝内容）</li>
</ul>
<p>一般用来修饰String、Dict、Array等需要保护其封装性的对象，尤其是在其内容可变的情况下，因此会拷贝（深拷贝）一份内容給属性使用，避免可能造成的对源内容进行改动。</p>
<p>使用set方法赋值时，实质上是会先拷贝新值，再释放旧值，再设置新值。<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>, <span class="keyword">copy</span>) <span class="built_in">NSString</span>* name;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>weak</code> : ARC新引入修饰词，可代替assign，比assign多增加一个特性（置nil，见上文）。</li>
</ul>
<p>weak和strong一样用来修饰ObjC对象。使用set方法赋值时，实质上不保留新值，也不释放旧值，只设置新值。</p>
<p>比如常用的代理的声明：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@property</span> (<span class="keyword">weak</span>) <span class="keyword">id</span>&lt;MyDelegate&gt; delegate;</span><br></pre></td></tr></table></figure></p>
<p>Xib控件的引用：<br>@property (weak, nonatomic) IBOutlet UIImageView* productImage;</p>
<ul>
<li><code>strong</code> : ARC新引入修饰词，可代替retain</li>
</ul>
<p>可参照retain，这里不再作描述。</p>
<h3 id="block的内存管理"><a href="#block的内存管理" class="headerlink" title="block的内存管理"></a>block的内存管理</h3><p>iOS中使用block必须自己管理内存，错误的内存管理将导致循环引用等内存泄漏问题，这里主要说明在ARC下block声明和使用的时候需要注意的两点：</p>
<ul>
<li>如果你使用@property去声明一个block的时候，一般使用copy来进行修饰（当然也可以不写，编译器自动进行copy操作），尽量不要使用retain。</li>
</ul>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>, <span class="keyword">copy</span>) <span class="keyword">void</span>(^block)(<span class="built_in">NSData</span>* data);</span><br></pre></td></tr></table></figure>
<ul>
<li>block会对内部使用的对象进行强引用，因此在使用的时候应该确定不会引起循环引用，当然保险的做法就是添加弱引用标记。<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">__<span class="keyword">weak</span> <span class="keyword">typeof</span>(<span class="keyword">self</span>) weakSelf = <span class="keyword">self</span>;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="经典内存泄漏及其解决方案"><a href="#经典内存泄漏及其解决方案" class="headerlink" title="经典内存泄漏及其解决方案"></a>经典内存泄漏及其解决方案</h2><p>虽然ARC好处多多，然而也并无法避免内存泄漏问题，下面介绍在ARC中常见的内存泄漏。</p>
<h3 id="僵尸对象和野指针"><a href="#僵尸对象和野指针" class="headerlink" title="僵尸对象和野指针"></a>僵尸对象和野指针</h3><font color=orange>僵尸对象：内存已经被回收的对象</font>

<font color=orange>野指针：指向僵尸对象的指针，向野指针发送消息会导致崩溃</font>

<p>野指针错误形式在Xcode中通常表现为：Thread 1：EXC_BAD_ACCESS，因为你访问了一块已经不属于你的内存。</p>
<p>例子代码：(没有出现错误的多运行几遍，因为获取野指针指向的结果是不确定的)<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">Dog * dog = [[Dog alloc]init];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;before&quot;</span>);</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;%s&quot;</span>,object_getClassName(dog));</span><br><span class="line">[dog release];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;after&quot;</span>);</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;%s&quot;</span>,object_getClassName(dog));</span><br></pre></td></tr></table></figure><br>运行结果：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2b5c91ee80037ab5ecc3b6309ed5e131.png" alt="结果"></p>
<p>可以看到，当运行到第六行的时候崩溃了，并给出了EXC_BAD_ACCESS的提示。</p>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>对象已经被释放后，应将其指针置为空指针（没有指向任何对象的指针，给空指针发送消息不会报错）。</p>
<p>然而在实际开发中实际遇到EXC_BAD_ACCESS错误时，往往很难定位到错误点，幸好Xcode提供方便的工具給我们来定位及分析错误。</p>
<ul>
<li>在product－scheme－edit scheme－diagnostics中将enable zombie objects勾选上，下次再出现这样的错误就可以准确定位了。</li>
<li>在Xcode－open developer tool－Instruments打开工具集，选择Zombies工具可以对已安装的应用进行僵尸对象检测。</li>
</ul>
<h3 id="循环引用"><a href="#循环引用" class="headerlink" title="循环引用"></a>循环引用</h3><p>循环引用是ARC中最常出现的问题，对于可能引发循环引用的一些原因在有一篇文章<a class="link"   href="http://www.jianshu.com/p/bcc0bcaadd6c" >iOS总结篇：影响控制器正常释放的常见问题<i class="fas fa-external-link-alt"></i></a>中有提及。</p>
<p>一般来讲循环引用也是可以使用工具来检测到的，分为两种：</p>
<ul>
<li>在 product－Analyze 中使用静态分析来检测代码中可能存在循环引用的问题。</li>
<li>在 Xcode－open developer tool－Instruments 打开工具集，选择Leaks工具可以对已安装的应用进行内存泄漏检测，此工具能检测静态分析不会提示，但是到运行时才会出现的内存泄漏问题。</li>
</ul>
<p>Leaks工具虽然强大，但是它不能检测到block循环引用导致的内存泄漏，这种情况一般需要自行排查问题（考验你的基本功时候到了）。</p>
<h3 id="循环中对象占用内存大"><a href="#循环中对象占用内存大" class="headerlink" title="循环中对象占用内存大"></a>循环中对象占用内存大</h3><p>这个问题常见于循环次数较大，循环体生成的对象占用内存较大的情景。</p>
<p>例子代码：我需要10000个演员来打仗<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10000</span>; i ++) &#123;</span><br><span class="line">    Person * soldier = [[Person alloc]init];</span><br><span class="line">    [soldier fight];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>该循环内产生大量的临时对象，直至循环结束才释放，可能导致内存泄漏，解决方法和上文中提到的自动释放池常见问题类似：在循环中创建自己的autoReleasePool，及时释放占用内存大的临时变量，减少内存占用峰值。<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10000</span>; i ++) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        Person * soldier = [[Person alloc]init];</span><br><span class="line">        [soldier fight];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然而有时候autoReleasePool也不是万能的：</p>
<p>例子：假如有2000张图片，每张1M左右，现在需要获取所有图片的尺寸，你会怎么做？<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2000</span>; i ++) &#123;</span><br><span class="line">    <span class="built_in">CGSize</span> size = [<span class="built_in">UIImage</span> imageNamed:[<span class="built_in">NSString</span> stringWithFormat:<span class="string">@&quot;%d.jpg&quot;</span>,i]].size;</span><br><span class="line">    <span class="comment">//add size to array</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>用imageNamed方法加载图片占用Cache的内存，autoReleasePool也不能释放，对此问题需要另外的解决方法，当然保险的当然是双管齐下了:<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2000</span>; i ++) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        <span class="built_in">CGSize</span> size = [<span class="built_in">UIImage</span> imageWithContentsOfFile:filePath].size;</span><br><span class="line">        <span class="comment">//add siez to array</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="无限循环"><a href="#无限循环" class="headerlink" title="无限循环"></a>无限循环</h3><p>无论你出于什么原因，当你启动了一个无限循环的时候，ARC会默认该方法用不会执行完毕，方法里面的对象就永不释放，内存无限上涨，导致内存泄漏。</p>
<p>例子：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;start !&quot;</span>);</span><br><span class="line"><span class="built_in">dispatch_async</span>(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT, <span class="number">0</span>), ^&#123;</span><br><span class="line">    <span class="built_in">BOOL</span> isSucc = <span class="literal">YES</span>;</span><br><span class="line">    <span class="keyword">while</span> (isSucc) &#123;</span><br><span class="line">        [<span class="built_in">NSThread</span> sleepForTimeInterval:<span class="number">1.0</span>];</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@&quot;create an obj&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><br>对于这样的问题还是很好解决的，只要设置相关条件判断就能避免。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>关于iOS内存管理的知识点还有很多，在这里先做基础储备，后续在实战中会继续补充。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="http://www.jianshu.com/p/8b1ed04b3ba9" >iOS内功篇：内存管理<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>Objective-C</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>js变量提升</title>
    <url>/2019/01/16/js%E5%8F%98%E9%87%8F%E6%8F%90%E5%8D%87/</url>
    <content><![CDATA[<h2 id="例1"><a href="#例1" class="headerlink" title="例1"></a>例1</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line">    <span class="keyword">if</span> (!a) &#123;</span><br><span class="line">        <span class="keyword">var</span> a = <span class="number">200</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">f</span>()</span><br><span class="line"><span class="comment">// undefined</span></span><br><span class="line"><span class="comment">// 200</span></span><br></pre></td></tr></table></figure>
<h2 id="例2"><a href="#例2" class="headerlink" title="例2"></a>例2</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    a = <span class="number">200</span>;</span><br><span class="line">    <span class="keyword">return</span> ;</span><br><span class="line">    <span class="keyword">function</span> <span class="title function_">a</span>(<span class="params"></span>) &#123;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">f</span>();</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line"><span class="comment">// 100</span></span><br></pre></td></tr></table></figure>
<p>如果你习惯了强类型语言的编程方式，那么看到上述输出结果你肯定会大吃一惊。</p>
<h1 id="js-作用域"><a href="#js-作用域" class="headerlink" title="js 作用域"></a><code>js</code> 作用域</h1><p>我们来看一下 <code>C++</code> 的一个例子：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">int</span> x = <span class="number">100</span>;</span><br><span class="line">	cout &lt;&lt; x &lt;&lt; endl;</span><br><span class="line">	<span class="keyword">if</span> (<span class="number">1</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="type">int</span> x = <span class="number">200</span>;</span><br><span class="line">		cout &lt;&lt; x &lt;&lt; endl;</span><br><span class="line">	&#125;</span><br><span class="line">	cout &lt;&lt; x &lt;&lt; endl;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 100</span></span><br><span class="line"><span class="comment">// 200</span></span><br><span class="line"><span class="comment">// 100</span></span><br></pre></td></tr></table></figure>
<p>再来看一个 <code>js</code> 的例子：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line"><span class="keyword">if</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> a = <span class="number">200</span>;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line">&#125;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line"><span class="comment">// 100</span></span><br><span class="line"><span class="comment">// 200</span></span><br><span class="line"><span class="comment">// 200</span></span><br></pre></td></tr></table></figure>
<p> <code>if</code> 代码块中的变量覆盖了全局变量。那是因为 <code>js</code> 只有<span style="color: red; font-size: 20px">全局作用域和函数作用域，没有块作用域。</span>块内的变量 <code>x</code> 影响到了全局变量 <code>x</code> 。</p>
<h2 id="js-实现块级作用域效果"><a href="#js-实现块级作用域效果" class="headerlink" title="js 实现块级作用域效果"></a><code>js</code> 实现块级作用域效果</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> x = <span class="number">100</span>;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(x);</span><br><span class="line">    <span class="keyword">if</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        (<span class="keyword">function</span>(<span class="params"></span>) &#123;</span><br><span class="line">            <span class="keyword">var</span> x = <span class="number">200</span>;</span><br><span class="line">            <span class="variable language_">console</span>.<span class="title function_">log</span>(x);</span><br><span class="line">        &#125;());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(x);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 100</span></span><br><span class="line"><span class="comment">// 200</span></span><br><span class="line"><span class="comment">// 100</span></span><br></pre></td></tr></table></figure>
<p>其本质上利用了 <code>js</code> 的函数作用域来模拟实现块级作用域。</p>
<h1 id="Hoisting-in-js"><a href="#Hoisting-in-js" class="headerlink" title="Hoisting in js"></a>Hoisting in <code>js</code></h1><p>在 <code>js</code> 中，变量进入一个作用域有以下方式：</p>
<ul>
<li>变量定义： <code>var a</code></li>
<li>函数形参：函数的形参存在于作用域中—— <code>function f(a, b) &#123;&#125;</code></li>
</ul>
<p>在代码运行前，函数声明和变量定义通常会被解释器移动到其所在作用域的最顶部。如下：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="title function_">test</span>();</span><br><span class="line">    <span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面代码被解释器解释后，将会变成如下形式：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> a;</span><br><span class="line">    <span class="title function_">test</span>();</span><br><span class="line">    a = <span class="number">100</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><span style="color: green; font-size: 20px"> <code>hoisting</code> 只是将变量的定义上升，但变量的赋值并不会上升。</span></p>
<p>再来看一个例子：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="title function_">f1</span>();</span><br><span class="line">    <span class="title function_">f2</span>();</span><br><span class="line">    <span class="keyword">var</span> f1 = <span class="keyword">function</span> <span class="title function_">f1</span>(<span class="params"></span>) &#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;error&quot;</span>);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">function</span> <span class="title function_">f2</span>(<span class="params"></span>) &#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;normal&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">f</span>();</span><br><span class="line"><span class="comment">// TypeError: f1 is not a function</span></span><br><span class="line"><span class="comment">// normal</span></span><br></pre></td></tr></table></figure>
<p>首先 <code>var f1</code> 会上升到函数顶部，但是此时 <code>f1</code> 为 <code>undefined</code> ，所以执行报错。但对于函数 <code>f2</code> ，函数本身也是一种变量，存在变量上升的现象，也会上升到函数顶部，所以 <code>f2()</code> 能顺利进行。</p>
<h1 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h1><p>例1等同于如下代码：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> a;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line">    <span class="keyword">if</span> (!a) &#123;</span><br><span class="line">       a = <span class="number">200</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">f</span>()</span><br><span class="line"><span class="comment">// undefined</span></span><br><span class="line"><span class="comment">// 200</span></span><br></pre></td></tr></table></figure>
<p>例2等同于如下代码：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">function</span> <span class="title function_">a</span>(<span class="params"></span>) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    a = <span class="number">200</span>;</span><br><span class="line">    <span class="keyword">return</span> ;   </span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">f</span>();</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line"><span class="comment">// 100</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://segmentfault.com/a/1190000003114255#articleHeader1" >Javascript作用域和变量提升<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>变量提升</tag>
      </tags>
  </entry>
  <entry>
    <title>logistic回归参数求解推导过程</title>
    <url>/2021/03/23/logistic%E5%9B%9E%E5%BD%92%E5%8F%82%E6%95%B0%E6%B1%82%E8%A7%A3%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>记录一下逻辑回归的参数求解推导过程：</p>
<span id="more"></span>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>线性回归的表达式为：$f(x) = wx+b$，为了消除后面的$b$，令$\theta = [w \quad b], x = [x \quad 1]^T$，则$f(x) = \theta x$</p>
<p>将其转换为逻辑回归模型：$y=\sigma(f({x}))=\sigma\left({\theta} {x}\right)=\frac{1}{1+e^{-{\theta} {x}}}$</p>
<p>我们把单个样本看作一个事件，那么这个事件发生的概率为：</p>
<script type="math/tex; mode=display">
P(y \mid {x})=\left\{\begin{array}{r}
p, y=1 \\
1-p, y=0
\end{array}\right.</script><p>它等价于：$P\left(y_{i} \mid {x}_{i}\right)=p^{y_{i}}(1-p)^{1-y_{i}}$</p>
<p>如果我们采集到了一组数据一共N个，$\left\{\left({x}_{1}, y_{1}\right),\left({x}_{2}, y_{2}\right),\left({x}_{3}, y_{3}\right) \ldots\left({x}_{N}, y_{N}\right)\right\},$ 这个合成在一起的合事件发生的总概率如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P_{total} &= P(y_1|x_1)P(y_2|x_2)P(y_3|x_3) \ldots P(y_N|x_N) \\
&= \prod_{i=1}^{N} p^{y_{i}}(1-p)^{1-y_{i}} \\
F(\theta) &= ln(P_{total}) = \sum_{i=1}^N ln(p^{y_{i}}(1-p)^{1-y_{i}}) \\
&= \sum_{i=1}^N y_ilnp + (1-y_i)ln(1-p) \\
其中 p &= \frac{1}{1+e^{-{\theta} {x}}}
\end{aligned}</script><p>为了符合损失函数的含义，将其定义为：</p>
<script type="math/tex; mode=display">
L(\theta) = -F(\theta)</script><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><script type="math/tex; mode=display">
\frac{\partial L}{\partial \theta} = \frac{\partial L}{\partial p} \times \frac{\partial p}{\partial \theta}</script><p>先求$\frac{\partial p}{\partial \theta}$ :</p>
<script type="math/tex; mode=display">
\begin{aligned}
p' &= (\frac{1}{1+e^{-\theta x}})' \\
&= \frac{-1}{(1+e^{-\theta x})^2} \cdot e^{-\theta x} \cdot -x \\
&= \frac{1}{1+e^{-\theta x}} \cdot \frac{e^{-\theta x}}{1+e^{-\theta x}} \cdot x \\
&= p(1-p)x
\end{aligned}</script><p>求$\frac{\partial L}{\partial \theta}$ :</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla F(\theta) &= \nabla (\sum_{i=1}^N y_ilnp + (1-y_i)ln(1-p)) \\
&= \sum_{i=1}^N y_i \frac{1}{p} p' + (1-y_i)\frac{-1}{1-p}p' \\
&= \sum_{i=1}^N y_i(1-p)x_i - (1-y_i)px_i \\
&= \sum_{i=1}^N (y_i-p) x_i \\
\end{aligned}</script><p>因此 $\frac{\partial L}{\partial \theta} = \sum_{i=1}^N (p-y_i)x_i$</p>
<h2 id="梯度更新"><a href="#梯度更新" class="headerlink" title="梯度更新"></a>梯度更新</h2><p>通过反向传播，$\theta$ 的更新过程如下：</p>
<script type="math/tex; mode=display">
\theta := \theta - \alpha \sum_{i=1}^N (y_i - \frac{1}{1+e^{-\theta x_i}}) x_i</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/44591359" >逻辑回归 logistics regression 公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>picture bed</title>
    <url>/2019/09/19/picture%20bed/</url>
    <content><![CDATA[<p>暂时作为一个图片床吧。。。</p>
<span id="more"></span>
<img src="/2019/09/19/picture%20bed/93.png" class="">
<img src="/2019/09/19/picture%20bed/94.png" class="">
<img src="/2019/09/19/picture%20bed/95.png" class="">
<img src="/2019/09/19/picture%20bed/96.png" class="">
<img src="/2019/09/19/picture%20bed/97.png" class="">
<img src="/2019/09/19/picture%20bed/98.png" class="">
<img src="/2019/09/19/picture%20bed/99.png" class="">
<img src="/2019/09/19/picture%20bed/100.png" class="">
<img src="/2019/09/19/picture%20bed/101.png" class="">
<img src="/2019/09/19/picture%20bed/102.png" class="">
<img src="/2019/09/19/picture%20bed/103.png" class="">
<img src="/2019/09/19/picture%20bed/104.png" class="">
<img src="/2019/09/19/picture%20bed/105.png" class="">
<img src="/2019/09/19/picture%20bed/106.png" class="">
<img src="/2019/09/19/picture%20bed/107.png" class="">
<img src="/2019/09/19/picture%20bed/108.png" class="">
<img src="/2019/09/19/picture%20bed/109.png" class="">
<img src="/2019/09/19/picture%20bed/110.png" class="">
<img src="/2019/09/19/picture%20bed/111.png" class="">
<img src="/2019/09/19/picture%20bed/112.png" class="">
<img src="/2019/09/19/picture%20bed/113.png" class="">
<img src="/2019/09/19/picture%20bed/114.png" class="">
<img src="/2019/09/19/picture%20bed/115.png" class="">
<img src="/2019/09/19/picture%20bed/116.png" class="">
<img src="/2019/09/19/picture%20bed/117.png" class="">
<img src="/2019/09/19/picture%20bed/118.png" class="">
<img src="/2019/09/19/picture%20bed/119.png" class="">
<img src="/2019/09/19/picture%20bed/120.png" class="">
<img src="/2019/09/19/picture%20bed/121.png" class="">
<img src="/2019/09/19/picture%20bed/122.png" class="">
<img src="/2019/09/19/picture%20bed/123.png" class="">
<img src="/2019/09/19/picture%20bed/124.png" class="">
<img src="/2019/09/19/picture%20bed/125.png" class="">
<img src="/2019/09/19/picture%20bed/126.png" class="">
<img src="/2019/09/19/picture%20bed/127.png" class="">
<img src="/2019/09/19/picture%20bed/128.png" class="">
<img src="/2019/09/19/picture%20bed/129.png" class="">
<img src="/2019/09/19/picture%20bed/130.png" class="">
<img src="/2019/09/19/picture%20bed/131.png" class="">
<img src="/2019/09/19/picture%20bed/132.png" class="">
<img src="/2019/09/19/picture%20bed/133.png" class="">
<img src="/2019/09/19/picture%20bed/134.png" class="">
<img src="/2019/09/19/picture%20bed/135.png" class="">
<img src="/2019/09/19/picture%20bed/lcs.jpeg" class="">
<img src="/2019/09/19/picture%20bed/136.png" class="">
]]></content>
      <categories>
        <category>personal</category>
      </categories>
      <tags>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title>productivity tools for windows</title>
    <url>/2019/09/19/productivity%20tools%20for%20windows/</url>
    <content><![CDATA[<p>每次重装系统都要装回一大堆软件，但有些软件可能会忘记，在此记录一下：</p>
<span id="more"></span>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">工具名</th>
<th style="text-align:center">简介</th>
<th style="text-align:center">下载地址</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Chrome</td>
<td style="text-align:center">谷歌浏览器，web开发必备</td>
<td style="text-align:center"><a class="link"   href="https://www.google.com/intl/zh-CN/chrome/" >https://www.google.com/intl/zh-CN/chrome/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">PyCharm</td>
<td style="text-align:center">编写python程序的IDE</td>
<td style="text-align:center"><a class="link"   href="https://www.jetbrains.com/pycharm/download/" >https://www.jetbrains.com/pycharm/download/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">IDEA</td>
<td style="text-align:center">编写java程序的IDE</td>
<td style="text-align:center"><a class="link"   href="https://www.jetbrains.com/idea/download/" >https://www.jetbrains.com/idea/download/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">DataGrid</td>
<td style="text-align:center">数据库管理工具</td>
<td style="text-align:center"><a class="link"   href="https://www.jetbrains.com/datagrip/download/" >https://www.jetbrains.com/datagrip/download/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">Android Studio</td>
<td style="text-align:center">官方安卓开发IDE</td>
<td style="text-align:center"><a class="link"   href="https://developer.android.com/studio" >https://developer.android.com/studio<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">Visual Studio</td>
<td style="text-align:center">宇宙最强IDE</td>
<td style="text-align:center"><a class="link"   href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community&amp;rel=16" >https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community&amp;rel=16<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">VS Code</td>
<td style="text-align:center">微软出品的编辑器，适合做前端以及python开发</td>
<td style="text-align:center"><a class="link"   href="https://code.visualstudio.com/Download" >https://code.visualstudio.com/Download<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">Octave</td>
<td style="text-align:center">仿matlab的数学计算工具</td>
<td style="text-align:center"><a class="link"   href="https://www.gnu.org/software/octave/#install" >https://www.gnu.org/software/octave/#install<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">Office</td>
<td style="text-align:center">办公三件套</td>
<td style="text-align:center">可到各学校软件中心下载</td>
</tr>
<tr>
<td style="text-align:center">Everything</td>
<td style="text-align:center">简洁、快速的文件检索工具</td>
<td style="text-align:center"><a class="link"   href="http://www.voidtools.com/downloads/" >http://www.voidtools.com/downloads/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">typora</td>
<td style="text-align:center">markdown编辑器，所见即所得</td>
<td style="text-align:center"><a class="link"   href="https://typora.io/" >https://typora.io/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">MobaXterm</td>
<td style="text-align:center">服务器远程连接工具</td>
<td style="text-align:center"><a class="link"   href="https://mobaxterm.mobatek.net/download.html" >https://mobaxterm.mobatek.net/download.html<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">Rufus</td>
<td style="text-align:center">简洁轻巧的系统盘制作工具</td>
<td style="text-align:center"><a class="link"   href="https://rufus.ie/" >https://rufus.ie/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">分区助手</td>
<td style="text-align:center">傻瓜式操作的系统分区工具</td>
<td style="text-align:center"><a class="link"   href="https://www.disktool.cn" >https://www.disktool.cn<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">石墨文档</td>
<td style="text-align:center">可多人在线协同编辑的云端Office，支持markdown</td>
<td style="text-align:center"><a class="link"   href="https://shimo.im/download" >https://shimo.im/download<i class="fas fa-external-link-alt"></i></a></td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>personal</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch中的二分类及多分类交叉熵损失函数</title>
    <url>/2020/12/08/pytorch%E4%B8%AD%E7%9A%84%E4%BA%8C%E5%88%86%E7%B1%BB%E5%8F%8A%E5%A4%9A%E5%88%86%E7%B1%BB%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>本文主要记录一下pytorch里面的二分类及多分类交叉熵损失函数的使用。</p>
<span id="more"></span>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">torch.manual_seed(<span class="number">2020</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;torch._C.Generator at 0x7f4e8b3298b0&gt;
</code></pre><h2 id="二分类交叉熵损失函数"><a href="#二分类交叉熵损失函数" class="headerlink" title="二分类交叉熵损失函数"></a>二分类交叉熵损失函数</h2><h4 id="Single"><a href="#Single" class="headerlink" title="Single"></a>Single</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(<span class="built_in">input</span>), target)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">f_output = F.binary_cross_entropy(m(<span class="built_in">input</span>), target)</span><br><span class="line"><span class="built_in">print</span>(f_output)</span><br><span class="line">l_output = nn.BCEWithLogitsLoss()(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(l_output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([ 1.2372, -0.9604,  1.5415], requires_grad=True)
tensor(0.2576, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
tensor(0.2576, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
tensor(0.2576, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;)
</code></pre><h4 id="Batch"><a href="#Batch" class="headerlink" title="Batch"></a>Batch</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">32</span>,<span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">32</span>,<span class="number">5</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(<span class="built_in">input</span>), target)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">f_output = F.binary_cross_entropy(m(<span class="built_in">input</span>), target)</span><br><span class="line"><span class="built_in">print</span>(f_output)</span><br><span class="line">l_output = nn.BCEWithLogitsLoss()(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(l_output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 1.2986,  1.5832, -1.1648,  0.8027, -0.9628],
        [-1.5793, -0.2155,  0.4706, -1.2511,  0.7105],
        [-0.1274, -1.9361,  0.8374,  0.0081, -0.1504],
        [ 0.1521,  1.1443,  0.2171, -1.1438,  0.9341],
        [-3.3199,  1.2998,  0.3918,  0.8327,  1.2411],
        [-0.8507, -0.1016, -1.2434, -0.5755,  0.1871],
        [-0.3064,  1.3751,  1.8478,  0.0326,  0.2032],
        [ 0.1782,  2.3037,  1.5948, -1.4731,  1.5312],
        [-0.9075, -1.7135,  0.4650, -1.7061,  0.0625],
        [-1.1904,  0.1130, -1.6609, -0.2000, -0.1422],
        [ 0.3307, -0.8395, -1.3068, -0.8891,  0.9858],
        [ 0.5484,  0.7461, -1.0738, -2.2162,  0.6801],
        [-0.8803,  0.9934, -1.6438,  0.3860,  0.4111],
        [-1.1078, -0.9629, -0.9534, -0.6207,  0.6885],
        [-0.0175,  1.9496,  0.9740, -0.4687, -0.6127],
        [ 0.3713,  0.8074,  0.3072,  1.1604, -0.2669],
        [-0.1773, -0.2787,  0.1926,  0.7492,  0.7492],
        [-0.3126, -0.3321, -1.7287, -3.0126,  0.1194],
        [ 1.0486, -0.1890, -0.5853,  0.4353,  0.2619],
        [ 1.9726, -0.5510, -0.1826, -0.8600, -0.9906],
        [ 0.7551,  0.8431, -0.8461, -1.2120,  0.2908],
        [-0.0932, -0.7151, -0.0631,  1.7554,  0.7374],
        [-0.1494, -0.6990, -0.1666,  2.0430,  1.3968],
        [ 0.2280, -0.3187,  1.0309, -0.1067,  1.1622],
        [-1.5120, -0.8617,  1.4165, -0.2361, -0.0355],
        [-0.8757, -0.6554,  0.1121, -0.1669, -0.2628],
        [-0.8023,  0.2305, -1.1792,  0.4314, -0.3653],
        [ 0.7487,  0.5358, -0.2677, -0.8128,  0.3029],
        [ 1.4439, -0.5677,  0.5564, -0.2485, -0.3281],
        [-2.0259,  1.1038,  1.0615,  1.7317, -0.0531],
        [ 0.9083, -0.8274,  0.8101, -1.1375, -1.2009],
        [ 0.3300, -0.8760,  1.3459, -1.0209, -0.5313]], requires_grad=True)
tensor(0.8165, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
tensor(0.8165, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
tensor(0.8165, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;)
</code></pre><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><ul>
<li>二分类交叉熵损失函数的input和target的shape是一致的</li>
<li><code>nn.BCELoss()</code> 与 <code>F.binary_cross_entropy</code> 计算结果是等价的，具体两者差距可见<a class="link"   href="https://www.zhihu.com/question/66782101" >PyTorch 中，nn 与 nn.functional 有什么区别？<i class="fas fa-external-link-alt"></i></a></li>
<li><blockquote>
<p><code>nn.BCEWithLogitsLoss</code>: combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability. 至于为什么更稳定，见 <a class="link"   href="https://blog.csdn.net/u010630669/article/details/105599067" >https://blog.csdn.net/u010630669/article/details/105599067<i class="fas fa-external-link-alt"></i></a></p>
</blockquote>
</li>
</ul>
<h2 id="多分类交叉熵损失函数"><a href="#多分类交叉熵损失函数" class="headerlink" title="多分类交叉熵损失函数"></a>多分类交叉熵损失函数</h2><h4 id="Single-1"><a href="#Single-1" class="headerlink" title="Single"></a>Single</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">f_output = F.cross_entropy(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(f_output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(1.7541, grad_fn=&lt;NllLossBackward&gt;)
tensor(1.7541, grad_fn=&lt;NllLossBackward&gt;)
</code></pre><h4 id="Batch-1"><a href="#Batch-1" class="headerlink" title="Batch"></a>Batch</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">32</span>, <span class="number">10</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">32</span>, <span class="number">5</span>, dtype=torch.long).random_(<span class="number">10</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">f_output = F.cross_entropy(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(f_output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(2.7944, grad_fn=&lt;NllLoss2DBackward&gt;)
tensor(2.7944, grad_fn=&lt;NllLoss2DBackward&gt;)
</code></pre><h3 id="Note-1"><a href="#Note-1" class="headerlink" title="Note"></a>Note</h3><ul>
<li><code>nn.CrossEntropyLoss</code> 与 <code>F.cross_entropy</code> 计算结果是等价的。两个函数都结合了 <code>LogSoftmax</code> and <code>NLLLoss</code> 运算</li>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss"><code>nn.CrossEntropyLoss</code></a> 的公式为：<script type="math/tex; mode=display">
\operatorname{loss}(\mathrm{x}, \text { class })=-\log \left(\frac{\exp (\mathrm{x}[\mathrm{class}])}{\sum_{\mathrm{j}} \exp (\mathrm{x}[\mathrm{j}])}\right)=-\mathrm{x}[\mathrm{class}]+\log \left(\sum_{\mathrm{j}} \exp (\mathrm{x}[\mathrm{j}])\right)</script>这与我们平时见到的多分类交叉熵损失函数有点不同，具体的推导过程见<a class="link"   href="https://www.cnblogs.com/marsggbo/p/10401215.html" >Pytorch里的CrossEntropyLoss详解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss" >Docs &gt; torch.nn &gt; CrossEntropyLoss<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html?highlight=bceloss#torch.nn.BCELoss" >Docs &gt; torch.nn &gt; BCELoss<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/marsggbo/p/10401215.html" >Pytorch里的CrossEntropyLoss详解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>scp使用</title>
    <url>/2021/11/09/scp%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>有的时候本地下载好的文件需要上传到服务器上去，但是需要借助第三方软件，显得非常繁琐。因此就用了一下 <code>scp</code> 命令：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp local_file username@ip:remote_folder</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title>shell中2&gt;&amp;1的含义</title>
    <url>/2021/07/16/shell%E4%B8%AD2_&amp;1%E7%9A%84%E5%90%AB%E4%B9%89/</url>
    <content><![CDATA[<p>在做实验的过程中，经常会看到shell脚本里存在 <code>2&gt;&amp;1</code> 的指令组合，有点懵逼，在此记录一下。</p>
<span id="more"></span>
<h2 id="0-1-2"><a href="#0-1-2" class="headerlink" title="0  |  1  |  2"></a><code>0</code>  |  <code>1</code>  |  <code>2</code></h2><p>这三个数字是linux文件描述符。<br>|   数字   |  含义    |<br>| —— | —— |<br>|   0   |    stdin   |<br>|   1   |    stdout  |<br>|   2   |    stderr  |</p>
<p><code>1</code> 和 <code>2</code> 都是输出到屏幕设备上。</p>
<p>我们平时使用的：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> &gt; test.log</span><br></pre></td></tr></table></figure></p>
<p>等价于：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> 1&gt; test.log</span><br></pre></td></tr></table></figure>
<p>注意 <code>1&gt;</code> 是连在一块的。如果分开，那么写入文件的就是 <font color="red">Hello World 1</font> 。</p>
<h2 id="2-gt-amp-1"><a href="#2-gt-amp-1" class="headerlink" title="2&gt;&amp;1"></a><code>2&gt;&amp;1</code></h2><blockquote>
<p>将标准错误输出重定向到标准输出。</p>
</blockquote>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.stdout.write(<span class="string">&quot;this is stdout\n&quot;</span>)</span><br><span class="line">sys.stderr.write(<span class="string">&quot;this is stderr\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python test.py &gt; test.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
<p>表示将标准错误输出重定向到标准输出，标准输出重定向到 <code>test.log</code> 文件中。</p>
<p>在程序执行过程中，我们希望输出保存到文件中，如果有错误的信息也希保存到文件中，那么就使用上面的命令。</p>
<ul>
<li><code>python test.py &gt; test.log 2&gt;1</code> : 那么标准输出将重定向到 <code>test.log</code> ，而错误将输出到<font color="red">名字为 <code>1</code> 的文件中</font>。这里的 <code>&amp;</code> 可以理解为 <code>1</code> 的引用</li>
<li><code>python test.py &gt; test.log 2 &gt;&amp;1</code> : 那么标准输出将重定向到 <code>test.log</code> ，而错误将输出到屏幕上</li>
<li><code>python test.py &gt; test.log 2&gt;&amp; 1</code> : 等价于 <code>python test.py &gt; test.log 2&gt;&amp;1</code></li>
<li><code>python test.py 1&gt; test.log 2&gt; test.log</code> : 会存在如下两个问题：<ul>
<li><code>stdout</code> 会覆盖 <code>stderr</code></li>
<li><code>test.log</code> 会被打开两次，IO效率低下</li>
</ul>
</li>
</ul>
<h3 id="2-gt-amp-1-为什么放在末尾？"><a href="#2-gt-amp-1-为什么放在末尾？" class="headerlink" title="2&gt;&amp;1 为什么放在末尾？"></a><code>2&gt;&amp;1</code> 为什么放在末尾？</h3><p><code>python test.py &gt; test.log 2&gt;&amp;1</code> 从左往右来看 <code>stdin</code> 定向到 <code>test.log</code>，然后 <code>stderr</code> 定向到 <code>stdin</code>，等于说 <code>stderr</code> 也输入到了 <code>test.log</code> 中。</p>
<p>如果放在中间：<code>python test.py 2&gt;&amp;1 &gt;test.log</code> ，<code>stderr</code> 定向到 <code>stdin</code>，但此时 <code>stdin</code> 指向的是屏幕，所以 <code>stderr</code> 会输出到屏幕。执行到 <code>&gt;test.log</code> 的时候，<code>stdin</code> 定向到 <code>test.log</code>。所以 <code>test.log</code> 文件里只有 <code>this is stdout</code>。</p>
<h3 id="简写"><a href="#简写" class="headerlink" title="简写"></a>简写</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python test.py &gt; test.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
<p>可以简写成：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python test.py &gt;&amp; test.log</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/zhaominpro/article/details/82630528" >Linux shell中2&gt;&amp;1的含义解释<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>tmux - 终端复用工具</title>
    <url>/2019/09/12/tmux%20-%20%E7%BB%88%E7%AB%AF%E5%A4%8D%E7%94%A8%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<p>当运行一个web应用时，我们希望在退出登录或关闭终端的时候，web应用仍然能够运行，这时候就要用到 <code>nohup</code> 。<code>nohup</code> 有个缺点就是它会将输出重定向到 <code>nohup.out</code> 文件，虽然也有其他方法能够在终端实时查看 <code>nohup</code> 输出，但重新登录服务器的时候，这些输出将会丢失。</p>
<span id="more"></span>
<p>同样的，当我们想在终端进行其他活动时，就必须打开一个新的终端，这显然不够greek。<br>基于上述两个痛点，tmux就派上用场了。tmux主要有如下三大功能：</p>
<ul>
<li>保护现场：即使命令行的工作只进行到一半，关闭终端后还可以重新进入到操作现场，继续工作。对于ssh远程连接而言，即使网络不稳定也没有关系，掉线后重新连接，可以直奔现场，之前运行中的任务，依旧在跑，就好像从来没有离开过一样；特别是在远程服务器上运行耗时的任务，tmux可以帮你一直保持住会话。如此一来，你就可以随时随地放心地进行移动办公，只要你附近的计算机装有tmux（没有你也可以花几分钟装一个），你就能继续刚才的工作。</li>
<li>分屏：tmux窗口中，新开的pane，默认进入到之前的路径，如果是ssh连接，登录状态也依旧保持着，如此一来，我就可以随意的增删pane，非常灵活。</li>
<li>会话共享：将tmux会话的地址分享给他人，这样他们就可以通过 SSH 接入该会话。</li>
</ul>
<h2 id="session-amp-window-amp-pane"><a href="#session-amp-window-amp-pane" class="headerlink" title="session &amp; window &amp; pane"></a>session &amp; window &amp; pane</h2><ul>
<li>一个tmux session（会话）可以包含多个window（窗口），窗口默认充满会话界面，因此这些窗口中可以运行相关性不大的任务。</li>
<li>一个window又可以包含多个pane（面板），窗口下的面板，都处于同一界面下，这些面板适合运行相关性高的任务，以便同时观察到它们的运行情况。</li>
</ul>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo yum install -y tmux</span><br></pre></td></tr></table></figure>
<h3 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h3><h4 id="新建会话"><a href="#新建会话" class="headerlink" title="新建会话"></a>新建会话</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux new -s <span class="built_in">test</span> <span class="comment"># 新建一个名叫test的会话</span></span><br></pre></td></tr></table></figure>
<h4 id="断开当前会话"><a href="#断开当前会话" class="headerlink" title="断开当前会话"></a>断开当前会话</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux detach</span><br></pre></td></tr></table></figure>
<p>也可以使用快捷键，先按 <code>ctrl + b</code> ，再按 <code>d</code> 。</p>
<h4 id="进入之前的会话"><a href="#进入之前的会话" class="headerlink" title="进入之前的会话"></a>进入之前的会话</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux a -t <span class="built_in">test</span> <span class="comment"># 进入名叫test的会话</span></span><br></pre></td></tr></table></figure>
<h4 id="关闭会话"><a href="#关闭会话" class="headerlink" title="关闭会话"></a>关闭会话</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux kill-session -t <span class="built_in">test</span> <span class="comment"># 关闭test会话</span></span><br><span class="line">tmux kill-server <span class="comment"># 关闭服务器，所有会话都将关闭</span></span><br></pre></td></tr></table></figure>
<h4 id="查看会话"><a href="#查看会话" class="headerlink" title="查看会话"></a>查看会话</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux <span class="built_in">ls</span></span><br></pre></td></tr></table></figure>
<p>也可以使用快捷键 <code>ctrl + b + s</code> ，此时tmux将打开会话列表。按上下键可切换会话，按左右键可收起或展开会话。</p>
<h2 id="tmux快捷键"><a href="#tmux快捷键" class="headerlink" title="tmux快捷键"></a>tmux快捷键</h2><ul>
<li>系统指令</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">前缀</th>
<th style="text-align:center">指令</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>?</code></td>
<td style="text-align:center">显示快捷键帮助文档</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>d</code></td>
<td style="text-align:center">断开当前会话</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>D</code></td>
<td style="text-align:center">选择要断开的会话</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>ctrl+z</code></td>
<td style="text-align:center">挂起当前会话</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>r</code></td>
<td style="text-align:center">强制重载当前会话</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>s</code></td>
<td style="text-align:center">显示会话列表用于选择并切换</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>:</code></td>
<td style="text-align:center">进入命令行模式</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>[</code></td>
<td style="text-align:center">进入复制模式</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>]</code></td>
<td style="text-align:center">粘贴复制模式中复制的文本</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>~</code></td>
<td style="text-align:center">列出提示信息缓存</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>窗口(window)命令</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">前缀</th>
<th style="text-align:center">指令</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>c</code></td>
<td style="text-align:center">新建窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>&amp;</code></td>
<td style="text-align:center">关闭当前窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>0~9</code></td>
<td style="text-align:center">切换到指定窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>p</code></td>
<td style="text-align:center">切换到上一窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>n</code></td>
<td style="text-align:center">切换到下一窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>w</code></td>
<td style="text-align:center">打开窗口列表且切换窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>,</code></td>
<td style="text-align:center">重命名当前窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>.</code></td>
<td style="text-align:center">修改当前窗口编号</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>f</code></td>
<td style="text-align:center">快速定位到窗口</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>面板(pane)指令</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">前缀</th>
<th style="text-align:center">指令</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>&quot;</code></td>
<td style="text-align:center">当前面板上下一分为二，下侧新建面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>%</code></td>
<td style="text-align:center">当前面板左右一分为二，右侧新建面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>x</code></td>
<td style="text-align:center">关闭当前面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>z</code></td>
<td style="text-align:center">最大化当前面板，再重复一次按键后恢复正常</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>!</code></td>
<td style="text-align:center">将当前面板移动到新的窗口打开（原窗口中存在两个及以上面板有效）</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>;</code></td>
<td style="text-align:center">切换到最后一次使用的面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>q</code></td>
<td style="text-align:center">显示面板编号，在编号消失前输入对应的数字可切换到相应的面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>&#123;</code></td>
<td style="text-align:center">向前置换当前面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>&#125;</code></td>
<td style="text-align:center">向后置换当前面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>ctrl+o</code></td>
<td style="text-align:center">顺时针旋转当前窗口中的所有面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center">方向键</td>
<td style="text-align:center">移动光标切换面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>o</code></td>
<td style="text-align:center">选择下一面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>space</code></td>
<td style="text-align:center">在自带的面板布局中循环切换</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>alt+方向键</code></td>
<td style="text-align:center">以5个单元格为单位调整当前面板边缘</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>ctrl+方向键</code></td>
<td style="text-align:center">以1个单元格为单位调整当前面板边缘（Mac下被系统快捷键覆盖）</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>t</code></td>
<td style="text-align:center">显示时钟</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p>关于tmux的高阶应用，如个性化配置、保存会话、会话共享等，请参照：</p>
<ul>
<li><a class="link"   href="http://louiszhai.github.io/2017/09/30/tmux/" >Tmux使用手册<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/kaiye/p/6275207.html" >十分钟学会 tmux<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>v-model &amp; v-bind</title>
    <url>/2018/05/08/v-model%20&amp;%20v-bind/</url>
    <content><![CDATA[<p>记录一下两种指令的用法。</p>
<span id="more"></span>
<h2 id="v-model"><a href="#v-model" class="headerlink" title="v-model"></a>v-model</h2><p>我们可以使用 <code>v-model</code> 指令在 <code>&lt;input&gt;</code> (<code>&lt;input&gt;</code> 标签有多种类型，如 <code>button、select</code> 等等)及 <code>&lt;textarea&gt;</code> 元素上进行双向数据绑定。但 <code>v-model</code> 本质上不过是语法糖。它负责监听用户的输入事件以更新数据，并对一些极端场景进行一些特殊处理。</p>
<p><code>v-model</code> 会忽略所有表单元素的 <code>value</code>、<code>checked</code>、<code>selected</code> 特性的初始值而总是将 Vue 实例的数据作为数据来源。你应该通过 JavaScript 在组件的 <code>data</code>选项中声明初始值：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;ie=edge&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cdn.jsdelivr.net/npm/vue@2.5.16/dist/vue.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>vue<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;app&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">input</span> <span class="attr">v-model</span>=<span class="string">&quot;message&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span>&gt;</span>The input value is : &#123;&#123;message&#125;&#125;<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> app = <span class="keyword">new</span> <span class="title class_">Vue</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">            <span class="attr">el</span>: <span class="string">&#x27;#app&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">            <span class="attr">data</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="attr">message</span>: <span class="string">&#x27;Hello Word!&#x27;</span></span></span><br><span class="line"><span class="language-javascript">            &#125;</span></span><br><span class="line"><span class="language-javascript">        &#125;)</span></span><br><span class="line"><span class="language-javascript">    </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>那么输入框的初始值就是 <font color="green">Hello World!</font> 。</p>
<p>实际上，由于<code>v-model</code> 只是语法糖， <code>&lt;input v-model=&quot;message&quot;&gt;</code> 与下面的两行代码是一致的：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">v-bind:value</span>=<span class="string">&quot;message&quot;</span> <span class="attr">v-on:input</span>=<span class="string">&quot;message = $event.target.value&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">:value</span>=<span class="string">&quot;message&quot;</span> @<span class="attr">input</span>=<span class="string">&quot;message = $event.target.value&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="v-bind"><a href="#v-bind" class="headerlink" title="v-bind"></a>v-bind</h2><p>它的用法是后面加冒号，跟上html元素的属性，例如：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">v-bind:class</span>=<span class="string">&quot;someclass&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>如果不加 <code>v-bind</code> 那么 <code>someclass</code> 就是个常量，没有任何动态数据参与。当加上 <code>v-bind</code> 之后，它的值 <code>someclass</code> 不是字符串，而是vue实例对应的 <code>data.someclass</code> 这个变量。具体传入变量类型可参考 <a class="link"   href="https://cn.vuejs.org/v2/guide/class-and-style.html" >Class与Style绑定<i class="fas fa-external-link-alt"></i></a> 。这非常适合用在通过css来实现动画效果的场合。除了class，其他大部分html原始的属性都可以通过这种方式来绑定，而且为了方便，它可以直接缩写成冒号形式，例如：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> app = <span class="title class_">Vue</span>(&#123;  </span><br><span class="line">    <span class="attr">el</span>: <span class="string">&#x27;#app&#x27;</span>,  </span><br><span class="line">    <span class="attr">template</span>: <span class="string">&#x27;&lt;img :src=&quot;remoteimgurl&quot;&gt;&#x27;</span>,  </span><br><span class="line">    <span class="attr">data</span>: &#123;    <span class="attr">src</span>: <span class="string">&#x27;&#x27;</span>,  &#125;,  </span><br><span class="line">    <span class="title function_">beforeMount</span>(<span class="params"></span>) &#123;    <span class="title function_">fetch</span>(...).<span class="title function_">then</span>(...).<span class="title function_">then</span>(<span class="function"><span class="params">res</span> =&gt;</span> <span class="variable language_">this</span>.<span class="property">src</span> = res.<span class="property">remoteimgurl</span>) &#125;,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>上面这段代码中，默认情况下 <code>data.src</code> 是空字符串，也就说不会有图片显示出来，但是当从远端获取到图片地址之后，更新了 <code>data.src</code>，图片就会显示出来了。</p>
<h2 id="v-bind与v-model区别"><a href="#v-bind与v-model区别" class="headerlink" title="v-bind与v-model区别"></a>v-bind与v-model区别</h2><p>有一些情况我们需要 <code>v-bind</code> 和 <code>v-model</code> 一起使用：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">:value</span>=<span class="string">&quot;name&quot;</span> <span class="attr">v-model</span>=<span class="string">&quot;body&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>data.name</code> 和 <code>data.body</code>，到底谁跟着谁变呢？甚至，它们会不会产生冲突呢？</p>
<p>实际上它们的关系和上面的阐述是一样的，<code>v-bind</code> 产生的效果不含有双向绑定，所以 <code>:value</code> 的效果就是让 input的value属性值等于 <code>data.name</code> 的值，而 <code>v-model</code> 的效果是使input和 <code>data.body</code> 建立双向绑定，因此首先 <code>data.body</code> 的值会给input的value属性，其次，当input中输入的值发生变化的时候，<code>data.body</code> 还会跟着改变。</p>
<p>上文提到过下面两句是等价的：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">v-model</span>=<span class="string">&quot;message&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">v-bind:value</span>=<span class="string">&quot;message&quot;</span> <span class="attr">v-on:input</span>=<span class="string">&quot;message = $event.target.value&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure>
<p>那么 <code>v-model</code> 其实就是 <code>v-bind</code> 和 <code>v-on</code> 的语法糖。</p>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>JavaScript</tag>
        <tag>Vue</tag>
      </tags>
  </entry>
  <entry>
    <title>zsh配置环境变量</title>
    <url>/2022/03/10/zsh%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/</url>
    <content><![CDATA[<p>MacOS现在默认的shell为zsh了，这里以配置node环境变量为例：</p>
<span id="more"></span>
<ol>
<li>打开 <code>~/.zshrc</code></li>
<li>输入如下内容：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">NODE_ENV=~/opt/node/bin</span><br><span class="line">export PATH=$NODE_ENV:$PATH</span><br></pre></td></tr></table></figure></li>
<li><code>source ~/.zshrc</code></li>
</ol>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>如果只是输入：<code>export NODE_ENV=~/opt/node/bin</code> ，那么终端还是不能识别 <code>node</code> 命令，只能输出 <code>echo $NODE_ENV</code> ，必须要把 <code>NODE_ENV</code> 加入到 <code>PATH</code> 中。</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>zsh</tag>
      </tags>
  </entry>
  <entry>
    <title>专用于个人简历的latex模板</title>
    <url>/2022/04/29/%E4%B8%93%E7%94%A8%E4%BA%8E%E4%B8%AA%E4%BA%BA%E7%AE%80%E5%8E%86%E7%9A%84latex%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>一份简历模板，fork自<a class="link"   href="https://github.com/hijiangtao/resume" >hijiangtao/resume<i class="fas fa-external-link-alt"></i></a>。自己随意删改了一些东西，地址为：<a class="link"   href="https://github.com/TransformersWsz/wsz_resume" >resume<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><ul>
<li>将整个项目直接上传到overleaf上，使用xelatex编译</li>
<li>使用本地的texstudio编译</li>
</ul>
<h2 id="预览"><a href="#预览" class="headerlink" title="预览"></a>预览</h2><p><img src="https://cdn.jsdelivr.net/gh/TransformersWsz/image_hosting@master/resume_preview.2ykrizc4yiu0.png" alt="resume_preview"></p>
<h2 id="FontAwesome"><a href="#FontAwesome" class="headerlink" title="FontAwesome"></a>FontAwesome</h2><p><code>resume-zh_CN.tex</code> 已经导入了 <code>fontawesome</code> 包，但只有 <a class="link"   href="https://fontawesome.com/v4/icons/" >v4.5<i class="fas fa-external-link-alt"></i></a> 的图标（不支持 <code>alias</code> 别名）。在网站中查找想使用的图标，然后在 <code>fontawesome.sty</code> 中找到相应的宏, 将其作为普通文本一样使用：<br><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\faGithub</span>  <span class="keyword">\;</span> GitHub @hijiangtao</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>软件工具</category>
      </categories>
      <tags>
        <tag>latex</tag>
        <tag>简历</tag>
      </tags>
  </entry>
  <entry>
    <title>从有序数组中查找不小于（不大于）某数的第一个（最后一个）元素</title>
    <url>/2021/03/12/%E4%BB%8E%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E6%9F%A5%E6%89%BE%E4%B8%8D%E5%B0%8F%E4%BA%8E%EF%BC%88%E4%B8%8D%E5%A4%A7%E4%BA%8E%EF%BC%89%E6%9F%90%E6%95%B0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%EF%BC%88%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%EF%BC%89%E5%85%83%E7%B4%A0/</url>
    <content><![CDATA[<p>记录一下二分查找的变形场景：</p>
<span id="more"></span>
<h2 id="不小于某数的第一个元素"><a href="#不小于某数的第一个元素" class="headerlink" title="不小于某数的第一个元素"></a>不小于某数的第一个元素</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_search_notlessthan_first</span>(<span class="params">arr, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(arr)-<span class="number">1</span></span><br><span class="line">    res = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">        mid = (low+high)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> target &lt;= arr[mid]:</span><br><span class="line">            res = mid</span><br><span class="line">            high = mid-<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            low = mid+<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>如果 <code>res==-1</code> ，说明 <code>any(arr) &lt; target</code></p>
<hr>
<h2 id="不大于某数的最后一个元素"><a href="#不大于某数的最后一个元素" class="headerlink" title="不大于某数的最后一个元素"></a>不大于某数的最后一个元素</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_search_notgreaterthan_last</span>(<span class="params">arr, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(arr)-<span class="number">1</span></span><br><span class="line">    res = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">        mid = (low + high) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> arr[mid] &lt;= target:</span><br><span class="line">            res = mid</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            high = mid - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>如果 <code>res==-1</code> ，说明 <code>any(arr) &gt; target</code></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>二分查找</tag>
      </tags>
  </entry>
  <entry>
    <title>关于mac上outlook2016无法打开的问题</title>
    <url>/2021/10/10/%E5%85%B3%E4%BA%8Emac%E4%B8%8Aoutlook2016%E6%97%A0%E6%B3%95%E6%89%93%E5%BC%80%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>本人安装了校园网站的office2016，word，ppt，excel都能正常打开，但是outlook却遇到了问题：</p>
<font color="red">You'll need to use the latest version of Outlook to use this database</font>

<span id="more"></span>
<p>解决方案如下：</p>
<p><a class="link"   href="https://answers.microsoft.com/en-us/mac/forum/macoffice2016-macoutlook/upgrade-outlook-database/3bc4c6ea-830a-4d1f-805c-a33018d08e2e" >Upgrade Outlook Database?<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>软件工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>Outlook</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2021/04/07/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>决策树的定义和示例见西瓜书P73~P74，下面主要介绍决策树的构造算法：</p>
<span id="more"></span>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>信息熵是衡量样本集合纯度最常用的一种指标。假定当前样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k(k=1,2,\cdots,|\gamma|)$ ，那么 $D$ 的信息熵为：</p>
<script type="math/tex; mode=display">
Ent(D) = - \sum_{k=1}^{|\gamma|} p_k log_2 p_k</script><p>$Ent(D)$ 越小，则 $D$ 纯度越高。</p>
<h2 id="构造算法"><a href="#构造算法" class="headerlink" title="构造算法"></a>构造算法</h2><h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h3><script type="math/tex; mode=display">
Gain(D, a) = Ent(D) - \sum_{v=1}^V \frac{|D^v|}{|D|} Ent(D^v)</script><p>$V$ 表示属性 $a$ 的取值范围。信息增益越大，表示使用属性 $a$ 进行划分所获得的“纯度提升”越大。</p>
<p>根据 ID3 算法的核心思想，只要在每次决策树非叶子节点划分之前，计算出每一个属性所带来的信息增益，选择最大信息增益的属性来划分，就可以让本次划分更优，因此整个 ID3 实际上是一个贪心算法。</p>
<p>以西瓜数据集2.0来说，“纹理”的信息增益最大，于是它被选为划分属性。后续对其它属性进行特征选择。</p>
<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><p>C4.5 对 ID3 算法最大的改进就是在获取最优分类特征的时候，将 ID3 所使用的信息增益换成了信息增益比。</p>
<p>如果我们将每个西瓜样本的“编号”作为候选划分属性，那么它的信息增益最大。因为“编号”将产生17个分支，每个分支仅包含一个样本，这些分支结点的纯度达到最大。但这样的决策树不具有泛化能力，无法对新样本有效预测。</p>
<p>实际上，信息增益准则对取值较多的属性有所偏好，为了减少这种偏好带来的不利影响，C4.5采用“增益率”来选择最优划分属性：</p>
<script type="math/tex; mode=display">
Gain\_ratio = \frac{Gain(D,a)}{IV(a)} \\
IV(a) = -\sum_{v=1}^V \frac{D^v}{D} log_2 \frac{D^v}{D}</script><p>$IV(a)$ 称为属性 $a$ 的“固有值”。$V$ 越大，$IV(a)$ 越大。</p>
<p>需注意的是，增益率对 $V$ 较小的属性 $a$ 有所偏好。因此，C4.5算法并不直接选择增益率最大的候选划分属性，而使用了一个启发式策略：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p>CART决策树使用“基尼指数”来选择划分属性。数据集 $D$ 的纯度可用基尼值来度量：</p>
<script type="math/tex; mode=display">
Gini(D) = \sum_{k=1}^{|\gamma|} \sum_{k' \ne k} p_k p_{k'} = 1 - \sum_{k=1}^{|\gamma|} p_k^2</script><p>直观来说，$Gini(D)$ 反映了从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(D)$ 越小，则数据集 $D$ 的纯度越高。</p>
<p>属性 $a$ 的基尼指数定义为：</p>
<script type="math/tex; mode=display">
Gini\_index(D,a) = \sum_{v=1}^V \frac{|D^v|}{|D|} Gini(D^v)</script><p>在候选属性集合 $A$ 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即：</p>
<script type="math/tex; mode=display">
a_{*} = argmax_{a \in A} Gini\_ index(D,a)</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li>西瓜书P73-P79</li>
<li><a class="link"   href="https://techlog.cn/article/list/10183264#" >决策树的构建算法 — ID3 与 C4.5 算法<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/139523931" >决策树算法—CART分类树算法<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式训练</title>
    <url>/2021/08/08/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<p>在面试中，遇到有些面试官会问分布式训练的有关问题，在此总结一下。</p>
<span id="more"></span>
<p>分布式训练的并行方式主要分如下两种：</p>
<ul>
<li>数据并行：将数据集切分放到各计算节点，每个计算节点的计算内容完全一致，并在多个计算节点之间传递模型参数。数据并行可以解决数据集过大无法在单机高效率训练的问题，也是工业生产中最常用的并行方法。</li>
<li>模型并行：通常指将模型单个算子计算分治到多个硬件设备上并发计算，以达到计算单个算子计算速度的目的。一般会将单个算子的计算，利用模型并行的方式分配在配置相同的几个硬件上，进行模型存储和计算，以保证计算步调一致。</li>
</ul>
<p>这里详细介绍数据并行的两种训练架构：</p>
<ul>
<li>Parameter Server：该架构可以对模型参数的存储进行分布式保存，因此对于存储超大规模模型参数的训练场景十分友好。因此在个性化推荐场景中（任务需要保存海量稀疏特征对应的模型参数）应用广泛。</li>
<li>Collective：多被用于视觉、自然语言处理等需要复杂网络计算（计算密集型）的模型训练任务场景。</li>
</ul>
<h2 id="Parameter-Server"><a href="#Parameter-Server" class="headerlink" title="Parameter Server"></a>Parameter Server</h2><img src="/2021/08/08/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/ps.jpeg" class="">
<p>PS分为两大部分：server group和多个worker group，另外resource manager负责总体的资源分配调度。</p>
<ul>
<li>server group内部包含多个server node，每个server node负责维护一部分参数，server manager负责维护和分配server资源；</li>
<li>每个worker group对应一个application（即一个模型训练任务），worker group之间以及worker group内部的worker node互相之间并不通信，worker node只与server通信。</li>
</ul>
<p>具体的架构详解可见：<a class="link"   href="https://zhuanlan.zhihu.com/p/82116922" >一文读懂「Parameter Server」的分布式机器学习训练原理<i class="fas fa-external-link-alt"></i></a></p>
<p>总结一下Parameter Server实现分布式机器学习模型训练的要点：</p>
<ul>
<li>用异步非阻断式的分布式梯度下降策略替代同步阻断式的梯度下降策略；</li>
<li>实现多server节点的架构，避免了单master节点带来的带宽瓶颈和内存瓶颈；</li>
<li>使用一致性哈希，range pull和range push等工程手段实现信息的最小传递，避免广播操作带来的全局性网络阻塞和带宽浪费。</li>
</ul>
<h2 id="Collective"><a href="#Collective" class="headerlink" title="Collective"></a>Collective</h2><p>主要有 <code>TreeAllReduce</code> 和 <code>RingAllReduce</code> 两种。</p>
<h4 id="TreeAllReduce"><a href="#TreeAllReduce" class="headerlink" title="TreeAllReduce"></a>TreeAllReduce</h4><img src="/2021/08/08/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/TreeAllReduce.jpeg" class="">
<p>该架构已被抛弃，存在如下两个问题：</p>
<ul>
<li>每一轮的训练迭代都需要所有卡都将数据同步完做一次Reduce才算结束。如果并行的卡很多的时候，就涉及到计算快的卡需要去等待计算慢的卡的情况，造成计算资源的浪费。</li>
<li>每次迭代所有的GPU卡都需要针对全部的模型参数跟Reduce卡进行通信，如果参数的数据量大的时候，那么这种通信开销也是非常庞大，而且这种开销会随着卡数的增加而线性增长。</li>
</ul>
<h4 id="RingAllReduce"><a href="#RingAllReduce" class="headerlink" title="RingAllReduce"></a>RingAllReduce</h4><p>与 <code>TreeAllReduce</code> 不同， <code>RingAllreduce</code> 算法的每次通信成本是恒定的，与系统中GPU的数量无关，完全由系统中gpu之间最慢的连接决定。</p>
<img src="/2021/08/08/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/RingAllReduce.png" class="">
<p><code>RingAllReduce</code> 中的GPU排列在一个逻辑环中。 每个GPU应该有一个左邻居和一个右邻居。它只会向其右邻居发送数据，并从其左邻居接收数据。</p>
<p>该算法分两步进行：</p>
<ol>
<li>scatter-reduce：GPU交换数据，使得每个GPU最终得到最终结果的一部分；</li>
<li>all-gather：GPU交换这些块，以便所有GPU最终得到完整的最终结果。</li>
</ol>
<p>具体示例可见：<a class="link"   href="https://blog.csdn.net/lj2048/article/details/108322931" >分布式训练-Ring AllReduce<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://pytorch.org/docs/master/notes/ddp.html#ddp" >DISTRIBUTED DATA PARALLEL<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/wujianming-110117/p/14398483.html" >分布式训练基本原理<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/82116922" >一文读懂「Parameter Server」的分布式机器学习训练原理<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.zhihu.com/question/57799212/answer/292494636" >ring allreduce和tree allreduce的具体区别是什么？<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/lj2048/article/details/108322931" >分布式训练-Ring AllReduce<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>利用Github Action来自动化部署Hexo博客</title>
    <url>/2022/04/24/%E5%88%A9%E7%94%A8Github%20Action%E6%9D%A5%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2Hexo%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p>这两天尝试了使用Github Action来自动化部署博客，踩了一些坑，在此记录一下。</p>
<span id="more"></span>
<h2 id="新建仓库"><a href="#新建仓库" class="headerlink" title="新建仓库"></a>新建仓库</h2><ul>
<li>存放博客源文章的仓库（Source Repo），命名随意</li>
<li>存放编译后生成的静态文件的仓库（Page Repo），命名<code>username.github.io</code></li>
</ul>
<h2 id="配置部署密钥"><a href="#配置部署密钥" class="headerlink" title="配置部署密钥"></a>配置部署密钥</h2><p>利用 <code>ssh-keygen</code> 来生成公钥和私钥：</p>
<ul>
<li><p>私钥放于Source仓库的 <code>Settings -&gt; Secrets -&gt; Actions</code> ，新建一个secret，命名为 <code>HEXO_DEPLOY_PRI</code>：<br><img src="https://cdn.jsdelivr.net/gh/TransformersWsz/image_hosting@master/pri.614aag5in440.png" alt="pri"></p>
</li>
<li><p>公钥放于Page仓库的 <code>Settings -&gt; Deploy Keys</code> ，新建一个deploy key，命名随意：<br><img src="https://cdn.jsdelivr.net/gh/TransformersWsz/image_hosting@master/pub.17ltsxgf236k.jpg" alt="pub"></p>
</li>
</ul>
<h2 id="编写Action"><a href="#编写Action" class="headerlink" title="编写Action"></a>编写Action</h2><p>整个Source仓库的结构如下所示：<br><img src="https://cdn.jsdelivr.net/gh/TransformersWsz/image_hosting@master/tree.72dyffzppe00.jpg" alt="tree"><br>只需要保留源文件就行了，其它的依赖交给Action来安装。</p>
<p>在 <code>.github/workflows</code> 新建 <code>deploy.yml</code> 文件，内容如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># workflow name</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">Hexo</span> <span class="string">Blog</span> <span class="string">CI</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># master branch on push, auto run</span></span><br><span class="line"><span class="attr">on:</span> </span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">main</span></span><br><span class="line">      </span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span> </span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span> </span><br><span class="line">        </span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="comment"># check it to your workflow can access it</span></span><br><span class="line">    <span class="comment"># from: https://github.com/actions/checkout</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">Repository</span> <span class="string">master</span> <span class="string">branch</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/checkout@master</span> </span><br><span class="line">      </span><br><span class="line">    <span class="comment"># from: https://github.com/actions/setup-node  </span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Node.js</span> <span class="number">16.</span><span class="string">x</span> </span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/setup-node@master</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">node-version:</span> <span class="string">&quot;16.14.0&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Cache</span> <span class="string">node</span> <span class="string">modules</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/cache@v1</span>    <span class="comment"># 缓存node_modules，避免每次跑action都要重新下载</span></span><br><span class="line">      <span class="attr">id:</span> <span class="string">cache</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">node_modules</span></span><br><span class="line">        <span class="attr">key:</span> <span class="string">$&#123;&#123;</span> <span class="string">runner.os</span> <span class="string">&#125;&#125;-node-$&#123;&#123;</span> <span class="string">hashFiles(&#x27;**/package-lock.json&#x27;)</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">restore-keys:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          $&#123;&#123; runner.os &#125;&#125;-node-</span></span><br><span class="line"><span class="string"></span>    </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Hexo</span> <span class="string">Dependencies</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        npm install hexo-cli -g</span></span><br><span class="line"><span class="string">        npm install</span></span><br><span class="line"><span class="string"></span>    </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">update</span> <span class="string">mathjax</span>    <span class="comment"># kramed引擎有点问题，将其部分文件替换掉</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp correct/hexo-renderer-kramed/renderer.js node_modules/hexo-renderer-kramed/lib/</span></span><br><span class="line"><span class="string">        cp correct/kramed/inline.js node_modules/kramed/lib/rules/</span></span><br><span class="line"><span class="string">        cp correct/hexo-renderer-mathjax/mathjax.html node_modules/hexo-renderer-mathjax</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Deploy</span> <span class="string">Private</span> <span class="string">Key</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">        <span class="attr">HEXO_DEPLOY_PRIVATE_KEY:</span> <span class="string">$&#123;&#123;</span> <span class="string">secrets.HEXO_DEPLOY_PRI</span> <span class="string">&#125;&#125;</span>    <span class="comment"># 这个就是Source仓库的私钥</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        mkdir -p ~/.ssh/</span></span><br><span class="line"><span class="string">        echo &quot;$HEXO_DEPLOY_PRIVATE_KEY&quot; &gt; ~/.ssh/id_rsa </span></span><br><span class="line"><span class="string">        chmod 600 ~/.ssh/id_rsa</span></span><br><span class="line"><span class="string">        ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts</span></span><br><span class="line"><span class="string"></span>        </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Git</span> <span class="string">Infomation</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">| </span></span><br><span class="line"><span class="string">        git config --global user.name &quot;TransformersWsz&quot;</span></span><br><span class="line"><span class="string">        git config --global user.email &quot;3287124026@qq.com&quot;</span></span><br><span class="line"><span class="string"></span>    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Deploy</span> <span class="string">Hexo</span> </span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        hexo clean</span></span><br><span class="line"><span class="string">        hexo generate </span></span><br><span class="line"><span class="string">        hexo deploy</span></span><br></pre></td></tr></table></figure>
<h3 id="相关字段说明"><a href="#相关字段说明" class="headerlink" title="相关字段说明"></a>相关字段说明</h3><ul>
<li><code>use</code>：引用现有的第三方的action，这样就无需自己写流程了</li>
<li><code>run</code>：运行命令，用法跟linux一致</li>
</ul>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><h3 id="1-自己使用的主题未生效？"><a href="#1-自己使用的主题未生效？" class="headerlink" title="1. 自己使用的主题未生效？"></a>1. 自己使用的主题未生效？</h3><ul>
<li>原因：由于主题是 <code>git clone</code> 下来的，主题目录下生成了 <code>.git</code> 目录，导致和 hexo根目录下 <code>.git</code> 冲突了，commit时没有把主题push上去导致的。</li>
<li>解决办法： 删掉主题下的 <code>.git</code> 文件夹，重新提交，目的是把主题文件夹提交上去（删掉 <code>.git</code> 文件夹后git commit依然没有提交上，需要把主题文件夹剪切出来后 <code>git add &amp;&amp; git commit &amp;&amp; git push</code> 后，再把主题文件夹拷贝回来，再 <code>git add &amp;&amp; git commit &amp;&amp; git push</code> 就可以提交成功了）</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/liuhp123/article/details/114040409" >Github action自动部署Hexo Next<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://sanonz.github.io/2020/deploy-a-hexo-blog-from-github-actions/" >利用 Github Actions 自动部署 Hexo 博客<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://sujie-168.top/2021/05/24/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Github-Actions%E5%AE%9E%E7%8E%B0Hexo%E5%8D%9A%E5%AE%A2%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/" >如何使用Github+Actions实现Hexo博客自动化部署<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>软件工具</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>CI</tag>
      </tags>
  </entry>
  <entry>
    <title>反向传播</title>
    <url>/2019/05/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    <content><![CDATA[<p>误差反向传播算法简称反向传播算法(Back Propagation)。使用反向传播算法的多层感知器又称为BP神经网络。</p>
<span id="more"></span>
<p>BP算法是一个迭代算法，它的基本思想如下：</p>
<ol>
<li>将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。</li>
<li>由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；</li>
<li>在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。</li>
<li>迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。</li>
</ol>
<h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><p>有如下一个神经网络：</p>
<p><img src="https://img-blog.csdnimg.cn/20190518224358906.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70" alt="1.png"></p>
<p>第一层是输入层，包含两个神经元 $i_1$，$i_2$ 和偏置项 $b_1$；第二层是隐藏层，包含两个神经元 $h_1$，$h_2$ 和偏置项 $b_2$；第三层是输出 $o_1$，$o_2$。每条线上标的 $w_i$ 是层与层之间连接的权重。激活函数是 $sigmod$ 函数。我们用 $z$ 表示某神经元的加权输入和；用 $a$ 表示某神经元的输出。</p>
<p>上述各参数赋值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$i_1$</td>
<td style="text-align:center">0.05</td>
</tr>
<tr>
<td style="text-align:center">$i_2$</td>
<td style="text-align:center">0.10</td>
</tr>
<tr>
<td style="text-align:center">$w_1$</td>
<td style="text-align:center">0.15</td>
</tr>
<tr>
<td style="text-align:center">$w_2$</td>
<td style="text-align:center">0.20</td>
</tr>
<tr>
<td style="text-align:center">$w_3$</td>
<td style="text-align:center">0.25</td>
</tr>
<tr>
<td style="text-align:center">$w_4$</td>
<td style="text-align:center">0.30</td>
</tr>
<tr>
<td style="text-align:center">$w_5$</td>
<td style="text-align:center">0.40</td>
</tr>
<tr>
<td style="text-align:center">$w_6$</td>
<td style="text-align:center">0.45</td>
</tr>
<tr>
<td style="text-align:center">$w_7$</td>
<td style="text-align:center">0.50</td>
</tr>
<tr>
<td style="text-align:center">$w_8$</td>
<td style="text-align:center">0.55</td>
</tr>
<tr>
<td style="text-align:center">$b_1$</td>
<td style="text-align:center">0.35</td>
</tr>
<tr>
<td style="text-align:center">$b_2$</td>
<td style="text-align:center">0.60</td>
</tr>
<tr>
<td style="text-align:center">$o_1$</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">$o_2$</td>
<td style="text-align:center">0.99</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Step-1-前向传播"><a href="#Step-1-前向传播" class="headerlink" title="Step 1 前向传播"></a>Step 1 前向传播</h2><h3 id="输入层-—-gt-隐藏层"><a href="#输入层-—-gt-隐藏层" class="headerlink" title="输入层 —-&gt; 隐藏层"></a>输入层 —-&gt; 隐藏层</h3><p>神经元 $h_1$ 的输入加权和：<br><img src="https://img-blog.csdnimg.cn/20190518224522132.PNG" alt="输入加权和"><br>神经元 $h_1$ 的输出 $a_{h1}$ ：</p>
<script type="math/tex; mode=display">
a_{h1} = \frac{1}{1+e^{-z_{h1}}} = \frac{1}{1+e^{-0.3775}} = 0.593269992</script><p>同理可得，神经元 $h_2$ 的输出 $a_{h2}$ ：</p>
<script type="math/tex; mode=display">
a_{h2} = 0.596884378</script><h3 id="隐藏层-—-gt-输出层"><a href="#隐藏层-—-gt-输出层" class="headerlink" title="隐藏层 —-&gt; 输出层"></a>隐藏层 —-&gt; 输出层</h3><p>计算输出层神经元 $o1$ 和 $o2$ 的值：<br><img src="https://img-blog.csdnimg.cn/201905182246056.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70" alt="隐藏层-&gt;输出层"><br>前向传播的过程就结束了，我们得到的输出值是 $[0.751365069, 0.772928465]$ ，与实际值 $[0.01, 0.99]$ 相差还很远。接下来我们对误差进行反向传播，更新权值，重新计算输出。</p>
<h2 id="Step-2-反向传播"><a href="#Step-2-反向传播" class="headerlink" title="Step 2 反向传播"></a>Step 2 反向传播</h2><ol>
<li>计算损失函数：</li>
</ol>
<script type="math/tex; mode=display">
E_{total} = \sum\frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $o_1$ 和 $o_2$ 的损失值，总误差为两者之和：</p>
<script type="math/tex; mode=display">
E_{o_1} = \frac {1}{2}(0.01 - 0.751365069)^2 = 0.274811083 \\
E_{o_2} = \frac {1}{2}(0.99 - 0.772928465)^2 = 0.023560026 \\
E_{total} = E_{o_1} + E_{o_2} = 0.274811083 + 0.023560026 = 0.298371109</script><ol>
<li>隐藏层 —-&gt; 输出层的权值更新</li>
</ol>
<p>以权重参数 $w_5$ 为例，如果我们想知道 $w_5$ 对整体损失产生了多少影响，可以用整体损失对 $w_5$ 求偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w_5} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} }*{ \frac {\partial z_{o_1}} {\partial w_5} }</script><p>下面的图可以更直观了解误差是如何反向传播的：</p>
<p><img src="https://img-blog.csdnimg.cn/20190518224659584.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70" alt="2.png"></p>
<p>我们现在分别来计算每个式子的值：</p>
<p>计算 $\frac {\partial E_{total}} {\partial a_{o_1}}$ ：</p>
<script type="math/tex; mode=display">
E_{total} = \frac {1}{2}(target_{o_1} - a_{o_1})^2 + \frac {1}{2}(target_{o_2} - a_{o_1})^2 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = 2 * \frac {1}{2} (target_{o_1} - a_{o_1})*-1 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = -(target_{o_1} - a_{o_1}) = 0.751365069-0.01=0.741365069 \\</script><p>计算 $\frac {\partial E_{total}} {\partial a_{o_1}}$ ：</p>
<script type="math/tex; mode=display">
a_{o_1} = \frac {1}{1+e^{-z_{o_1}}} \\
\frac {\partial a_{o_1}} {\partial z_{o_1}} = a_{o_1}*(1-a_{o_1}) = 0.751365069*(1-0.751365069) = 0.186815602</script><p>计算 $\frac {\partial z_{o_1}} {\partial w_5}$ ：</p>
<script type="math/tex; mode=display">
z_{o_1} = w_5*a_{h1} + w_6*a_{h2} + b_2*1 \\
\frac {\partial z_{o_1}} {\partial w_5} = a_{h_1} = 0.593269992</script><p>最后三者相乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = 0.741365069*0.186815602*0.593269992 = 0.082167041</script><p>这样我们就算出整体损失 $E_{total}$ 对 $w_5$ 的偏导值。</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1}) * a_{h_1}</script><p>针对上述公式，为了表达方便，使用 $\delta_{o_1}$ 来表示输出层的误差：</p>
<script type="math/tex; mode=display">
\delta_{o_1} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} } = \frac {\partial E_{total}} {\partial z_{o_1}} \\
\delta_{o_1} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1})</script><p>因此整体损失 $E_{total}$ 对 $w_5$ 的偏导值可以表示为：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}}{\partial w_5} = \delta_{o_1}*a_{h_1}</script><p>最后我们来更新 $w_5$ 的值：</p>
<script type="math/tex; mode=display">
w_5^+ = w_5 - \eta * \frac {\partial E_{total}} {\partial w_5} = 0.4 - 0.5*0.082167041 = 0.35891648 \qquad \eta: 学习率</script><p>同理可更新 $w_6, w_7, w_8$ ：</p>
<script type="math/tex; mode=display">
w_6^+ = 0.408666186 \\
w_7^+ = 0.511301270 \\
w_8^+ = 0.561370121</script><ol>
<li>隐藏层 —-&gt; 隐藏层的权值更新：</li>
</ol>
<p>计算 $\frac {\partial E_{total}} {\partial w_1}$ 与上述方法类似，但需要注意下图：</p>
<p><img src="https://img-blog.csdnimg.cn/20190518224736113.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70" alt="3.png"></p>
<p>计算 $\frac {\partial E_{total}} {\partial a_{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = \frac {\partial E_{o_1}} {\partial a_{h_1}} + \frac {\partial E_{o_2}} {\partial a_{h_1}}</script><p>先计算 $\frac {\partial E_{o_1}} {\partial a_{h_1}}$ ：</p>
<p><img src="https://img-blog.csdnimg.cn/20190518224838154.PNG" alt="o1h1"></p>
<p>同理可得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{o_2}} {\partial a_{h_1}} = -0.019049119</script><p>两者相加得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = 0.055399425 - 0.019049119 = 0.036350306</script><p>计算 $\frac {a_{h_1}} {z_{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {a_{h_1}} {z_{h_1}} = a_{h_1} * (1-a_{h_1}) = 0.593269992*(1-0.593269992) = 0.2413007086</script><p>计算 $\frac {\partial z_{h_1}} {\partial w_1}$</p>
<script type="math/tex; mode=display">
\frac {\partial z_{h_1}} {\partial w_1} = i_1 = 0.05</script><p>最后三者相互乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_1} = 0.036350306 * 0.2413007086 * 0.05 = 0.000438568</script><p>为了简化公式，用 $\delta_{h_1}$ 表示隐藏层单元 $h_1$ 的误差： </p>
<p><img src="https://img-blog.csdnimg.cn/20190518225048268.PNG" alt="simplify"></p>
<p>最后更新 $w_1$ 的权值：</p>
<script type="math/tex; mode=display">
w_1^+ = w_1 - \eta * \frac {\partial E_{total}} {\partial w_1} = 0.15 - 0.5*0.000438568 = 0.149780716</script><p>同理，更新 $w_2, w_3, w_4$ 权值：</p>
<script type="math/tex; mode=display">
w_2^+ = 0.19956143 \\
w_3^+ = 0.24975114 \\
w_4^+ = 0.29950229</script><p>这样，反向传播算法就完成了，最后我们再把更新的权值重新计算，不停地迭代。在这个例子中第一次迭代之后，总误差 $E_{total}$ 由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为$[0.015912196,0.984065734](原输入为[0.01,0.99]$ ，证明效果还是不错的。</p>
<h1 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h1><p><img src="https://img-blog.csdnimg.cn/20190518225132914.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70" alt="4.png"></p>
<h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$n_l$</td>
<td style="text-align:center">网络层数</td>
</tr>
<tr>
<td style="text-align:center">$y_j$</td>
<td style="text-align:center">输出层第 $j$ 类标签</td>
</tr>
<tr>
<td style="text-align:center">$S_l$</td>
<td style="text-align:center">第 $l$ 层神经元个数（不包括偏置项）</td>
</tr>
<tr>
<td style="text-align:center">$g(x)$</td>
<td style="text-align:center">激活函数</td>
</tr>
<tr>
<td style="text-align:center">$w_{ij}^{l}$</td>
<td style="text-align:center">第 $l-1$ 层的第 $j$ 个神经元连接到第 $l$ 层第 $i$ 个神经元的权重</td>
</tr>
<tr>
<td style="text-align:center">$b_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的偏置</td>
</tr>
<tr>
<td style="text-align:center">$z_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输入加权和</td>
</tr>
<tr>
<td style="text-align:center">$a_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输出（激活值）</td>
</tr>
<tr>
<td style="text-align:center">$\delta_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元产生的错误</td>
</tr>
</tbody>
</table>
</div>
<h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><h3 id="基本公式"><a href="#基本公式" class="headerlink" title="基本公式"></a>基本公式</h3><p><img src="https://img-blog.csdnimg.cn/20190518225230965.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70" alt="basic expresssion"></p>
<h3 id="梯度方向传播公式推导"><a href="#梯度方向传播公式推导" class="headerlink" title="梯度方向传播公式推导"></a>梯度方向传播公式推导</h3><h4 id="初始条件"><a href="#初始条件" class="headerlink" title="初始条件"></a>初始条件</h4><p><img src="https://img-blog.csdnimg.cn/20190518225308345.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70" alt="6"></p>
<h4 id="递推公式"><a href="#递推公式" class="headerlink" title="递推公式"></a>递推公式</h4><p><img src="https://img-blog.csdnimg.cn/20190518225340289.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70" alt="7"></p>
<h2 id="反向传播伪代码"><a href="#反向传播伪代码" class="headerlink" title="反向传播伪代码"></a>反向传播伪代码</h2><ol>
<li>输入训练集。</li>
<li>对于训练集的每个样本 $\vec x$ ，设输入层对应的激活值为 $a^l$ ：<ul>
<li>前向传播：$z^l = w^l*a^{l-1}+b^l, a^l = g(z^l)$</li>
<li>计算输出层产生的误差：$\delta^L = \frac {\partial J(\theta)} {\partial a^L} \odot g’(z^L)$</li>
<li>反向传播错误：$\delta^l = ((w^{l+1})^T*\delta^{l+1}) \odot g’(z^l)$</li>
</ul>
</li>
<li>使用梯度下降训练参数：<ul>
<li>$w^l \dashrightarrow w^l - \frac {\alpha} {m} \sum_x\delta^{x, l}*(a^{x, l-1})^T$</li>
<li>$b^l \dashrightarrow  b^l - \frac {\eta} {m} \sum_x\delta^{x, l}$</li>
</ul>
</li>
</ol>
<h1 id="交叉熵损失函数推导"><a href="#交叉熵损失函数推导" class="headerlink" title="交叉熵损失函数推导"></a>交叉熵损失函数推导</h1><p>对于多分类问题，$softmax$ 函数可以将神经网络的输出变成一个概率分布。它只是一个额外的处理层，下图展示了加上了 $softmax$ 回归的神经网络结构图：</p>
<p><img src="https://img-blog.csdnimg.cn/20190518225454143.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70" alt="softmax"></p>
<p>递推公式仍然和上述递推公式保持一致。初始条件如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20190518225634644.PNG" alt="8"></p>
<p>$softmax$ 偏导数计算：</p>
<script type="math/tex; mode=display">
\frac {\partial y_j^p} {\partial a_i^{nl}} =
\left\{
\begin{aligned}
-y_i^p*y_j^p \qquad i \neq j \\
y_i^p*(1-y_i^p) i = j
\end{aligned}
\right.</script><h2 id="推导过程-1"><a href="#推导过程-1" class="headerlink" title="推导过程"></a>推导过程</h2><p><img src="https://img-blog.csdnimg.cn/20190518225717464.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYW5zZm9ybWVyX1dTWg==,size_16,color_FFFFFF,t_70" alt="9"></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/charlotte77/p/5629865.html?tdsourcetag=s_pcqq_aiomsg" >一文弄懂神经网络中的反向传播法——BackPropagation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/u014313009/article/details/51039334" >反向传播算法（过程及公式推导）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/nowgood/p/backprop2.html?tdsourcetag=s_pcqq_aiomsg" >反向传播公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>四种常见的POST提交数据方式</title>
    <url>/2017/12/15/%E5%9B%9B%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84POST%E6%8F%90%E4%BA%A4%E6%95%B0%E6%8D%AE%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<p>HTTP/1.1 协议规定的 HTTP 请求方法有 <code>OPTIONS、GET、HEAD、POST、PUT、DELETE、TRACE、CONNECT</code> 这几种。其中 POST 一般用来向服务端提交数据，本文主要讨论 POST 提交数据的几种方式。</p>
<span id="more"></span>
<p>我们知道，HTTP 协议是以 ASCII 码传输，建立在 TCP/IP 协议之上的应用层规范。规范把 HTTP 请求分为三个部分：<font color="green">状态行、请求头、消息主体</font>。类似于下面这样：<br><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">method</span>&gt;</span> <span class="tag">&lt;<span class="name">request-URL</span>&gt;</span> <span class="tag">&lt;<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">headers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">entity-body</span>&gt;</span></span><br></pre></td></tr></table></figure><br>协议规定 POST 提交的数据必须放在消息主体（entity-body）中，但协议并没有规定数据必须使用什么编码方式。实际上，开发者完全可以自己决定消息主体的格式，只要最后发送的 HTTP 请求满足上面的格式就可以。</p>
<p>但是，数据发送出去，还要服务端解析成功才有意义。一般服务端语言如 php、python 等，以及它们的 framework，都内置了自动解析常见数据格式的功能。服务端通常是根据请求头（headers）中的 Content-Type 字段来获知请求中的消息主体是用何种方式编码，再对主体进行解析。所以说到 POST 提交数据方案，包含了 Content-Type 和消息主体编码方式两部分。下面就正式开始介绍它们。</p>
<h2 id="application-x-www-form-urlencoded"><a href="#application-x-www-form-urlencoded" class="headerlink" title="application/x-www-form-urlencoded"></a><font color="blue">application/x-www-form-urlencoded</font></h2><p>这应该是最常见的 POST 提交数据的方式了。浏览器的原生 <code>&lt;form&gt;</code> 表单，如果不设置 <code>enctype</code> 属性，那么最终就会以 <code>application/x-www-form-urlencoded</code> 方式提交数据。请求类似于下面这样（无关的请求头在本文中都省略掉了）：<br><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">POST http://www.example.com HTTP/1.1</span><br><span class="line">Content-Type: application/x-www-form-urlencoded;charset=utf-8</span><br><span class="line"></span><br><span class="line">title=test&amp;sub%5B%5D=1&amp;sub%5B%5D=2&amp;sub%5B%5D=3</span><br></pre></td></tr></table></figure><br>首先，<code>Content-Type</code> 被指定为 <code>application/x-www-form-urlencoded</code>；其次，提交的数据按照 <code>key1=val1&amp;key2=val2</code> 的方式进行编码，key 和 val 都进行了 URL 转码。大部分服务端语言都对这种方式有很好的支持。例如 PHP 中，<code>$_POST[&#39;title&#39;]</code> 可以获取到 title 的值，<code>$_POST[&#39;sub&#39;]</code> 可以得到 sub 数组。</p>
<p>很多时候，我们用 Ajax 提交数据时，也是使用这种方式。例如 JQuery 和 QWrap 的 Ajax，Content-Type 默认值都是<code>application/x-www-form-urlencoded;charset=utf-8</code>。</p>
<h2 id="multipart-form-data"><a href="#multipart-form-data" class="headerlink" title="multipart/form-data"></a><font color="blue">multipart/form-data</font></h2><p>这又是一个常见的 POST 数据提交的方式。我们使用表单上传文件时，必须让 <code>&lt;form&gt;</code> 表单的 <code>enctype</code> 等于 <code>multipart/form-data</code>。直接来看一个请求示例：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">POST http://www.example.com HTTP/1.1</span><br><span class="line">Content-Type:multipart/form-data; boundary=----WebKitFormBoundaryrGKCBY7qhFd3TrwA</span><br><span class="line"></span><br><span class="line">------WebKitFormBoundaryrGKCBY7qhFd3TrwA</span><br><span class="line">Content-Disposition: form-data; name=&quot;text&quot;</span><br><span class="line"></span><br><span class="line">title</span><br><span class="line">------WebKitFormBoundaryrGKCBY7qhFd3TrwA</span><br><span class="line">Content-Disposition: form-data; name=&quot;file&quot;; filename=&quot;chrome.png&quot;</span><br><span class="line">Content-Type: image/png</span><br><span class="line"></span><br><span class="line">PNG ... content of chrome.png ...</span><br><span class="line">------WebKitFormBoundaryrGKCBY7qhFd3TrwA--</span><br></pre></td></tr></table></figure>
<p>这个例子稍微复杂点。首先生成了一个 <code>boundary</code> 用于分割不同的字段，为了避免与正文内容重复，<code>boundary</code> 很长很复杂。然后 <code>Content-Type</code> 里指明了数据是以 <code>multipart/form-data</code> 来编码，本次请求的 <code>boundary</code> 是什么内容。消息主体里按照字段个数又分为多个结构类似的部分，每部分都是以 <code>--boundary</code> 开始，紧接着是内容描述信息，然后是回车，最后是字段具体内容（文本或二进制）。如果传输的是文件，还要包含文件名和文件类型信息。消息主体最后以 <code>--boundary--</code> 标示结束。关于 <code>multipart/form-data</code> 的详细定义，请前往 <a class="link"   href="http://www.ietf.org/rfc/rfc1867.txt" >rfc1867<i class="fas fa-external-link-alt"></i></a> 查看。</p>
<p>这种方式一般用来上传文件，各大服务端语言对它也有着良好的支持。</p>
<p>上面提到的这两种 POST 数据的方式，都是浏览器原生支持的，而且现阶段标准中原生 <code>&lt;form&gt;</code> 表单也只支持这两种方式（通过 <code>&lt;form&gt;</code> 元素的 <code>enctype</code> 属性指定，默认为 <code>application/x-www-form-urlencoded</code>。其实 <code>enctype</code> 还支持 <code>text/plain</code>，不过用得非常少）。</p>
<p>随着越来越多的 Web 站点，尤其是 WebApp，全部使用 Ajax 进行数据交互之后，我们完全可以定义新的数据提交方式，给开发带来更多便利。</p>
<h2 id="application-json"><a href="#application-json" class="headerlink" title="application/json"></a><font color="blue">application/json</font></h2><p><code>application/json</code> 这个 <code>Content-Type</code> 作为响应头大家肯定不陌生。实际上，现在越来越多的人把它作为请求头，用来告诉服务端消息主体是序列化后的 JSON 字符串。由于 JSON 规范的流行，除了低版本 IE 之外的各大浏览器都原生支持 <code>JSON.stringify</code>，服务端语言也都有处理 JSON 的函数，使用 JSON 不会遇上什么麻烦。</p>
<p>JSON 格式支持比键值对复杂得多的结构化数据，这一点也很有用。记得我几年前做一个项目时，需要提交的数据层次非常深，我就是把数据 JSON 序列化之后来提交的。不过当时我是把 JSON 字符串作为 val，仍然放在键值对里，以 <code>x-www-form-urlencoded</code> 方式提交。</p>
<p>Google 的 <a class="link"   href="http://angularjs.org/" >AngularJS<i class="fas fa-external-link-alt"></i></a> 中的 Ajax 功能，默认就是提交 JSON 字符串。例如下面这段代码：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> data = &#123;<span class="string">&#x27;title&#x27;</span>:<span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;sub&#x27;</span> : [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]&#125;;</span><br><span class="line">$http.<span class="title function_">post</span>(url, data).<span class="title function_">success</span>(<span class="keyword">function</span>(<span class="params">result</span>) &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<p>最终发送的请求是：<br><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">POST http<span class="punctuation">:</span><span class="comment">//www.example.com HTTP/1.1 </span></span><br><span class="line">Content-Type<span class="punctuation">:</span> application/json;charset=utf<span class="number">-8</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;title&quot;</span><span class="punctuation">:</span><span class="string">&quot;test&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sub&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><br>这种方案，可以方便的提交复杂的结构化数据，特别适合 <code>RESTful</code> 的接口。各大抓包工具如 Chrome 自带的开发者工具、Firebug、Fiddler，都会以树形结构展示 JSON 数据，非常友好。但也有些服务端语言还没有支持这种方式，例如 php 就无法通过 <code>$_POST</code> 对象从上面的请求中获得内容。这时候，需要自己动手处理下：在请求头中 <code>Content-Type</code> 为 <code>application/json</code> 时，从 <code>php://input</code> 里获得原始输入流，再 <code>json_decode</code> 成对象。一些 php 框架已经开始这么做了。</p>
<p>当然 <a class="link"   href="http://angularjs.org/" >AngularJS<i class="fas fa-external-link-alt"></i></a> 也可以配置为使用 <code>x-www-form-urlencoded</code> 方式提交数据。如有需要，可以参考 <a class="link"   href="http://victorblog.com/2012/12/20/make-angularjs-http-service-behave-like-jquery-ajax/" >这篇文章<i class="fas fa-external-link-alt"></i></a>。</p>
<h2 id="text-xml"><a href="#text-xml" class="headerlink" title="text/xml"></a><font color="blue">text/xml</font></h2><p>我的博客之前提到过 <a class="link"   href="http://www.imququ.com/post/64.html" >XML-RPC (XML Remote Procedure Call)<i class="fas fa-external-link-alt"></i></a>。它是一种使用 HTTP 作为传输协议，XML 作为编码方式的远程调用规范。典型的 XML-RPC 请求是这样的：<br><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">POST http://www.example.com HTTP/1.1 </span><br><span class="line">Content-Type: text/xml</span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">methodCall</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">methodName</span>&gt;</span>examples.getStateName<span class="tag">&lt;/<span class="name">methodName</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">params</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span><span class="tag">&lt;<span class="name">i4</span>&gt;</span>41<span class="tag">&lt;/<span class="name">i4</span>&gt;</span><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">param</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">params</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">methodCall</span>&gt;</span></span><br></pre></td></tr></table></figure><br>XML-RPC 协议简单、功能够用，各种语言的实现都有。它的使用也很广泛，如 WordPress 的 <a class="link"   href="http://codex.wordpress.org/XML-RPC_WordPress_API" >XML-RPC Api<i class="fas fa-external-link-alt"></i></a>，搜索引擎的 <a class="link"   href="http://help.baidu.com/question?prod_en=master&amp;class=476&amp;id=1000423" >ping 服务<i class="fas fa-external-link-alt"></i></a> 等等。JavaScript 中，也有 <a class="link"   href="http://plugins.jquery.com/xmlrpc/" >现成的库<i class="fas fa-external-link-alt"></i></a> 支持以这种方式进行数据交互，能很好的支持已有的 XML-RPC 服务。不过，我个人觉得 XML 结构还是过于臃肿，一般场景用 JSON 会更灵活方便。</p>
<hr>
<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><ul>
<li><a class="link"   href="https://imququ.com/post/four-ways-to-post-data-in-http.html" >四种常见的 POST 提交数据方式<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>POST</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title>寻找两个正序数组的中位数</title>
    <url>/2021/03/12/%E5%AF%BB%E6%89%BE%E4%B8%A4%E4%B8%AA%E6%AD%A3%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/</url>
    <content><![CDATA[<p>这道题题目描述很简单，但却是leetcode <code>hard</code>难度。如果用传统的二分查找方法来做，那么边界情况将非常多。</p>
<span id="more"></span>
<p>本题将寻找两个有序数组的中位数看作是从两个有序数组中查找第<code>k</code>小元素，具体讲解见：<a class="link"   href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by-w-2/" >详细通俗的思路分析，多解法（解法三）<i class="fas fa-external-link-alt"></i></a>。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMedianSortedArrays</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">getKthElement</span>(<span class="params">k</span>):</span><br><span class="line">            idx1, idx2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                <span class="keyword">if</span> idx1 == m:</span><br><span class="line">                    <span class="keyword">return</span> nums2[idx2+k-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> idx2 == n:</span><br><span class="line">                    <span class="keyword">return</span> nums1[idx1+k-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> k == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="built_in">min</span>(nums1[idx1], nums2[idx2])</span><br><span class="line">                </span><br><span class="line">                newidx1 = <span class="built_in">min</span>(idx1 + k//<span class="number">2</span> -<span class="number">1</span>, m-<span class="number">1</span>)</span><br><span class="line">                newidx2 = <span class="built_in">min</span>(idx2 + k//<span class="number">2</span> -<span class="number">1</span>, n-<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> nums1[newidx1] &gt;= nums2[newidx2]:</span><br><span class="line">                    k -= newidx2-idx2+<span class="number">1</span></span><br><span class="line">                    idx2 = newidx2+<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    k -= newidx1-idx1+<span class="number">1</span></span><br><span class="line">                    idx1 = newidx1+<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        m = <span class="built_in">len</span>(nums1)</span><br><span class="line">        n = <span class="built_in">len</span>(nums2)</span><br><span class="line">        <span class="keyword">if</span> (m+n)%<span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> getKthElement((m+n)//<span class="number">2</span>+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (getKthElement((m+n)//<span class="number">2</span>) + getKthElement( (m+n)//<span class="number">2</span>+<span class="number">1</span> )) / <span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>二分查找</tag>
      </tags>
  </entry>
  <entry>
    <title>常用排序算法的比较</title>
    <url>/2021/04/09/%E5%B8%B8%E7%94%A8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83/</url>
    <content><![CDATA[<p>记录一下各种常见排序算法的比较。</p>
<span id="more"></span>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">思想</th>
<th style="text-align:center">最好时间复杂度</th>
<th style="text-align:center">最坏时间复杂度</th>
<th>平均时间复杂度</th>
<th style="text-align:center">空间复杂度</th>
<th style="text-align:center">是否稳定</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">冒泡排序</td>
<td style="text-align:center">两两比较相邻记录的关键字，如果反序则交换，直到没有反序的记录为止</td>
<td style="text-align:center">$O(n)$</td>
<td style="text-align:center">$O(n^2)$</td>
<td>$O(n^2)$</td>
<td style="text-align:center">$O(1)$</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">插入排序</td>
<td style="text-align:center">把$n$个待排序的元素看成为一个有序表和一个无序表。开始时有序表中只包含1个元素，无序表中包含有$n-1$个元素，排序过程中每次从无序表中取出第一个元素，将它插入到有序表中的适当位置，使之成为新的有序表，重复$n-1$次可完成排序过程</td>
<td style="text-align:center">$O(n)$</td>
<td style="text-align:center">$O(n^2)$</td>
<td>$O(n^2)$</td>
<td style="text-align:center">$O(1)$</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">简单选择排序</td>
<td style="text-align:center">通过$n-i$次关键字之间的比较，从$n-i+1$个记录中选择关键字最小的记录，并和第$i(1 \le i \le n)$个记录交换之</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">$O(n^2)$</td>
<td>$O(n^2)$</td>
<td style="text-align:center">$O(1)$</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">归并排序</td>
<td style="text-align:center">分治算法，是建立在归并操作上的一种有效的排序算法。常用的2路归并排序假设初始序列有$n$个记录，可以看成是$n$个长度为1的子序列，进行两两归并，可以得到$\frac{n}{2}$个长度为2的子序列；再两两归并,直到得到一个长度为$n$的有序序列为止</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">$O(nlogn)$</td>
<td>$O(nlogn)$</td>
<td style="text-align:center">$O(n)$</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">堆排序</td>
<td style="text-align:center">把待排序的序列构造成一个大顶堆，此时序列的最大值就是队顶元素，把该元素放在最后，然后对剩下的$n-1$个元素继续构造大顶堆，直到排序完成</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">$O(nlogn)$</td>
<td>$O(nlogn)$</td>
<td style="text-align:center">$O(1)$</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">快速排序</td>
<td style="text-align:center">通过一趟排序将待排记录分割成独立的两部分，其中一部分的记录都比另一部分小，然后再分别对这两个部分进行快速排序，最终实现整个序列的排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">$O(n^2)$</td>
<td>$O(nlogn)$</td>
<td style="text-align:center">$O(logn)$</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>常见NLP面试问答</title>
    <url>/2021/03/30/%E5%B8%B8%E8%A7%81NLP%E9%9D%A2%E8%AF%95%E9%97%AE%E7%AD%94/</url>
    <content><![CDATA[<p>NLP面试中的经典八股文。</p>
<span id="more"></span>
<h2 id="1-HMM-vs-MEMM-vs-CRF"><a href="#1-HMM-vs-MEMM-vs-CRF" class="headerlink" title="1. HMM vs MEMM vs CRF"></a>1. HMM vs MEMM vs CRF</h2><h4 id="HMM-gt-MEMM"><a href="#HMM-gt-MEMM" class="headerlink" title="HMM -&gt; MEMM"></a>HMM -&gt; MEMM</h4><p>HMM模型中存在两个假设：</p>
<ol>
<li>输出观察值之间严格独立。MEMM解决了HMM输出独立性假设的问题。因为HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。</li>
<li>状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。</li>
</ol>
<h4 id="MEMM-gt-CRF"><a href="#MEMM-gt-CRF" class="headerlink" title="MEMM -&gt; CRF:"></a>MEMM -&gt; CRF:</h4><ul>
<li>CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。</li>
<li>HMM、MEMM属于有向图，所以考虑了x与y的影响，但没将x当做整体考虑进去（这点问题应该只有HMM）。CRF属于无向图，没有这种依赖性，克服此问题。</li>
</ul>
<hr>
<h2 id="2-常见的几种优化器"><a href="#2-常见的几种优化器" class="headerlink" title="2. 常见的几种优化器"></a>2. 常见的几种优化器</h2><ol>
<li>SGD</li>
</ol>
<script type="math/tex; mode=display">
\theta \leftarrow \theta-\eta \nabla_{\theta} J(\theta)</script><p>$\eta$ 是学习率，$J(\theta)$ 是损失函数</p>
<ol>
<li>Momentum</li>
</ol>
<script type="math/tex; mode=display">
\begin{array}{l}
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J(\theta) \\
\theta=\theta-v_{t}
\end{array}</script><p>当我们将一个小球从山上滚下来时，没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。<br>加入的这一项，可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。</p>
<p><strong>超参数设定值:  一般 γ 取值 0.9 左右。</strong></p>
<ol>
<li>Nesterov</li>
</ol>
<script type="math/tex; mode=display">
\begin{array}{l}
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right) \\
\theta=\theta-v_{t}
\end{array}</script><p>用 $\theta-\gamma v_{t-1}$ 来近似当做参数下一步会变成的值，则在计算梯度时，不是在当前位置，而是未来的位置上。</p>
<ol>
<li>AdaGrad</li>
</ol>
<p>这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性。</p>
<script type="math/tex; mode=display">
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i}</script><p>其中 $g_{t,i}$ 是 $t$ 时刻参数 $\theta_{i}$ 的梯度，$G_{t, ii}$ (对角矩阵 $G_t$ 的 $(i,i)$ 元素)就是 $t$ 时刻参数 $\theta_i$ 的梯度平方和。</p>
<p>超参数设定值：一般 $\eta$ 选取0.01</p>
<ol>
<li>RMSprop</li>
</ol>
<p>RMSprop 都是为了解决 Adagrad 学习率急剧下降问题的：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
E\left[g^{2}\right]_{t}=0.9 E\left[g^{2}\right]_{t-1}+0.1 g_{t}^{2} \\
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t}
\end{array}</script><p>使用的是指数加权平均，旨在消除梯度下降中的摆动，与Momentum的效果一样，某一维度的导数比较大，则指数加权平均就大，某一维度的导数比较小，则其指数加权平均就小，这样就保证了各维度导数都在一个量级，进而减少了摆动。允许使用一个更大的学习率 $\eta$ 。</p>
<p>超参数设定值： $\gamma$ 为 0.9，$\eta$ 为 0.001</p>
<ol>
<li>Adam</li>
</ol>
<p>相当于 RMSprop + Momentum：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
m_{t}=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\
v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}
\end{array}</script><p>除了像 momentum 一样保持了过去梯度 $m_t$ 的指数衰减平均值， 也像Adadelta 和 RMSprop 一样存储了过去梯度的平方 $v_t$ 的指数衰减平均值。</p>
<p>如果 $m_t$ 和 $v_t$ 都被初始化为0，那么它们会向0偏置，要做偏差纠正。通过计算偏差校正后的 $m_t$ 和 $v_t$ 来抵消这些偏差：</p>
<script type="math/tex; mode=display">
\hat{m}_t = \frac{m_t} {1-\beta_1^t} \\
\hat{v}_t = \frac{v_t} {1-\beta_2^t}</script><p>梯度更新规则：</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon} \hat{m}_t</script><p>超参数设定值： $\beta_1$ 为 0.9，$\beta_2$ 为0.999，$\epsilon$ 为 $10^{-8}$</p>
<hr>
<h2 id="3-Self-Attention添加head数量是否会增加计算复杂度？"><a href="#3-Self-Attention添加head数量是否会增加计算复杂度？" class="headerlink" title="3. Self-Attention添加head数量是否会增加计算复杂度？"></a>3. Self-Attention添加head数量是否会增加计算复杂度？</h2><p>不会。self-attention的时间复杂度为$O(n^2 \times d)$，$n$ 为序列长度，$d$ 为维度。假设分成 $h$ 个头，那么张量shape为 $h \times n \times m$ 。其中 $d = h \times m$ 。每个头做self-attention的时间复杂度为 $O(n^2 \times m)$ ，那么 $h$ 个头的总时间复杂度为 $O(h \times n^2 \times m) = O(n^2 \times d)$ 。因此增加头的数量不会导致计算复杂度增加。</p>
<hr>
<h2 id="4-L1和L2正则化"><a href="#4-L1和L2正则化" class="headerlink" title="4. L1和L2正则化"></a>4. L1和L2正则化</h2><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><ul>
<li>优点：输出具有稀疏性，即产生一个稀疏模型，进而可以用于特征选择；一定程度上，L1可以防止过拟合</li>
<li>缺点：非稀疏情况下计算效率低</li>
</ul>
<h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><ul>
<li>优点：计算效率高（因为存在解析解）；可以防止模型过拟合</li>
<li>缺点：非稀疏输出；无特征选择</li>
</ul>
<hr>
<h2 id="5-方差与偏差"><a href="#5-方差与偏差" class="headerlink" title="5. 方差与偏差"></a>5. 方差与偏差</h2><blockquote>
<p>偏差：用所有可能的训练数据集训练出的所有模型的输出的平均值与真实模型的输出值之间的差异。<br>方差：不同的训练数据集训练出的模型输出值之间的差异。</p>
</blockquote>
<ul>
<li>欠拟合：高偏差，低方差</li>
<li>过拟合：高偏差，高方差</li>
</ul>
<hr>
<h2 id="6-正负样例分布不均衡解决办法"><a href="#6-正负样例分布不均衡解决办法" class="headerlink" title="6. 正负样例分布不均衡解决办法"></a>6. 正负样例分布不均衡解决办法</h2><ol>
<li>过采样与欠采样：<ul>
<li>过抽样：通过增加分类中少数类样本的数量来实现样本均衡</li>
<li>欠抽样：通过减少分类中多数类样本的数量来实现样本均衡</li>
</ul>
</li>
<li>通过正负样本的惩罚权重解决样本不均衡：对于分类中不同样本数量的类别分别赋予不同的权重，一般是小样本量类别权重高，大样本量类别权重低。</li>
</ol>
<hr>
<h2 id="7-词汇表太大，softmax计算如何优化？"><a href="#7-词汇表太大，softmax计算如何优化？" class="headerlink" title="7. 词汇表太大，softmax计算如何优化？"></a>7. 词汇表太大，softmax计算如何优化？</h2><p>Hierarchical Softmax根据单词出现的频率来构建一颗霍夫曼树。树的叶子结点代表一个单词，在每一个非叶子节点处都需要作一次二分类，走左边的概率和走右边的概率，这里用逻辑回归的公式表示：</p>
<ul>
<li>正类别：$\sigma\left(X_{i} \theta\right)=\frac{1}{1+e^{-x_{i} \theta}}$</li>
<li>负类别：$1-\sigma\left(X_{i} \theta\right)$<br>每个词都会有一条路径，根据训练样本的特征向量 $X_i$ 预测目标label词 $Y_i$ 的概率为：<script type="math/tex; mode=display">
P\left(Y_{i} \mid X_{i}\right)=\prod_{j=2}^{l} P\left(d_{j} \mid X_{i}, \theta_{j-1}\right) \\
P\left(d_{j} \mid X_{i}, \theta_{j-1}\right)=\left\{\begin{array}{ll}
\sigma\left(X_{i} \theta\right), & \text { if } \mathrm{d_j}=1 \\
1-\sigma\left(X_{i} \theta\right), & \text { if } \mathrm{d_j}=0
\end{array}\right.</script>详细见：</li>
<li><a class="link"   href="https://www.cnblogs.com/eniac1946/p/8818892.html" >层次softmax函数（hierarchical softmax）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/56139075" >Hierarchical Softmax（层次Softmax）<i class="fas fa-external-link-alt"></i></a><h2 id="8-LSTM如何解决梯度弥散或爆炸？"><a href="#8-LSTM如何解决梯度弥散或爆炸？" class="headerlink" title="8. LSTM如何解决梯度弥散或爆炸？"></a>8. LSTM如何解决梯度弥散或爆炸？</h2>LSTM的介绍见：<a class="link"   href="https://zhuanlan.zhihu.com/p/44124492" >LSTM：RNN最常用的变体<i class="fas fa-external-link-alt"></i></a><br>梯度问题见：</li>
<li><a class="link"   href="https://www.cnblogs.com/bonelee/p/10475453.html" >LSTM如何解决梯度消失或爆炸的？<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.zhihu.com/question/34878706" >LSTM如何来避免梯度弥散和梯度爆炸？<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="9-简述EM算法的流程"><a href="#9-简述EM算法的流程" class="headerlink" title="9. 简述EM算法的流程"></a>9. 简述EM算法的流程</h2><p>输入：观察数据 $x=\left(x^{(1)}, x^{(2)}, \ldots x^{(m)}\right),$ 联合分布 $p(x, z ; \theta),$ 条件分布 $p(z \mid x ; \theta),$ 最大迭代次数 $J$</p>
<ol>
<li>随机初始化模型参数 $\theta$ 的初值 $\theta^{0}$</li>
<li>$for \quad j \quad from \quad 1 \quad to \quad j$:<br>a) E步。计算联合分布的条件概率期望：<script type="math/tex; mode=display">
\begin{array}{c}
Q_{i}\left(z^{(i)}\right)=P\left(z^{(i)} \mid x^{(i)}, \theta^{j}\right) \\
L\left(\theta, \theta^{j}\right)=\sum_{i=1}^{m} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log P\left(x^{(i)}, z^{(i)} ; \theta\right)
\end{array}</script>b) M步。极大化 $L\left(\theta, \theta^{j}\right),$ 得到 $\theta^{j+1}$ :<script type="math/tex; mode=display">
\theta^{j+1}=\underset{\theta}{\arg \max } L\left(\theta, \theta^{j}\right)</script>c) 如果 $\theta^{j+1}$ 收敛, 则算法结束。否则继续回到步骤 a) 进行E步迭代<br>输出：模型参数 $\theta$ 。<h3 id="具体示例可见："><a href="#具体示例可见：" class="headerlink" title="具体示例可见："></a>具体示例可见：</h3></li>
</ol>
<ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/36331115" >人人都懂EM算法<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/v_JULY_v/article/details/81708386" >如何通俗理解EM算法<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/6912636.html" >EM算法原理总结<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="10-LR、SVM、决策树的对比"><a href="#10-LR、SVM、决策树的对比" class="headerlink" title="10. LR、SVM、决策树的对比"></a>10. LR、SVM、决策树的对比</h2><h3 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li>实现简单高效</li>
<li>对观测样本概率输出<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4></li>
<li>特征空间太大时表现不太好</li>
<li>对于非线性特征须要作特征变换</li>
<li>需要额外添加正则项<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4></li>
<li>能够处理高维特征 </li>
<li>自带正则项</li>
<li>使用核函数轻松应对非线性特征空间</li>
<li>分类面不依赖于全部数据<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4></li>
<li>核函数选择较难</li>
<li>样本量非常大，核函数映射维度非常高时，计算量过大</li>
<li>对缺失数据敏感</li>
</ol>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ol>
<li>决策过程直观</li>
<li>可以处理非线性特征<h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4></li>
<li>容易过拟合</li>
<li>无法输出概率，只能输出分类结果</li>
</ol>
<hr>
<h2 id="11-SVM常用核函数"><a href="#11-SVM常用核函数" class="headerlink" title="11. SVM常用核函数"></a>11. SVM常用核函数</h2><ol>
<li>线性核函数</li>
<li>多项式核函数</li>
<li>高斯核函数</li>
<li>sigmoid核函数</li>
</ol>
<p>详细见：<a class="link"   href="https://blog.csdn.net/batuwuhanpei/article/details/52354822" >svm常用核函数<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="12-k-means与EM联系与区别"><a href="#12-k-means与EM联系与区别" class="headerlink" title="12. k-means与EM联系与区别"></a>12. k-means与EM联系与区别</h2><blockquote>
<p>两者都是无监督学习。</p>
</blockquote>
<p>k-means可以看成是两阶段的：</p>
<ul>
<li>第一阶段，确定每一个样本所属的聚类，在这个过程中，聚类的中心保持不变。可以看作EM的E步。</li>
<li>第二阶段，确定聚类中心，在这个过程中，每一个样本所属的类别保持不变。可以看作EM的M步。</li>
</ul>
<p>EM算法和K-Means算法的迭代过程比较类似，不同的是K-Means算法中每次对参数的更新是硬猜测，而EM中每次对参数的更新是软猜测；相同的是，两个算法都可能得到局部最优解，采用不同的初始参数迭代会有利于得到全局最优解。</p>
<p>详细见：</p>
<ul>
<li><a class="link"   href="https://www.jianshu.com/p/2c42c567e893" >机器学习笔记11: K-Means算法和EM算法<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/youyouzaLearn/p/9471409.html" >k-Means与EM之间的关系<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="13-Xavier原理"><a href="#13-Xavier原理" class="headerlink" title="13. Xavier原理"></a>13. Xavier原理</h2><blockquote>
<p>为了使得网络中信息更好的流动，每一层输出的方差应该尽量相等。</p>
</blockquote>
<p>先贴结论：</p>
<script type="math/tex; mode=display">
w \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_{i n}+n_{\text {out }}}}, \frac{\sqrt{6}}{\sqrt{n_{\text {in }}+n_{\text {out }}}}\right]</script><p>具体的公式推导见：</p>
<ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/27919794" >深度前馈网络与Xavier初始化原理
<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/VictoriaW/article/details/73000632" >深度学习之参数初始化（一）——Xavier初始化<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.jianshu.com/p/f2d800388d1c" >一文搞懂深度网络初始化（Xavier and Kaiming initialization）<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="14-目标检测综述"><a href="#14-目标检测综述" class="headerlink" title="14. 目标检测综述"></a>14. 目标检测综述</h2><h3 id="Two-stage方法"><a href="#Two-stage方法" class="headerlink" title="Two-stage方法"></a>Two-stage方法</h3><h4 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h4><ol>
<li>通过Selective Search（SS）方法筛选出一些备选的区域框（Region proposal）；</li>
<li>CNN提取特征，SVM分类；</li>
<li>分类完成后，对bbox进行回归，修正bbox中的坐标的值，得到更精确的bbox。</li>
</ol>
<h4 id="SPP-net"><a href="#SPP-net" class="headerlink" title="SPP-net"></a>SPP-net</h4><ul>
<li>R-CNN中，每个区域都要过一次CNN 提取特征。而SPP-net中，一张图片只需要过一次CNN，特征提取是针对整张图进行的，候选区域的框定以及特征向量化是在CNN的feature map层面进行的。</li>
<li>提出自适应池化的方法，它分别对输入的feature map（可以由不定尺寸的输入图像进CNN得到，也可由region proposal 框定后进CNN 得到）进行多个尺度（实际上就是改变pooling 的size 和stride）的池化，分别得到特征，并进行向量化后拼接起来。无需像R-CNN一样对所有的Region proposal进行缩放得到相同的大小。</li>
</ul>
<h4 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h4><ul>
<li>提出了ROI pooling 的结构，实际上就是一种特殊的SPP（相当于SPP 的金字塔层数设置为了1，即只计算一次池化）。</li>
<li>将最终的SVM分类去掉了，直接做成了端到端的一个网络结构。对这个网络进行多任务训练，即分类和回归，得到物体类别和bbox的位置。</li>
</ul>
<h4 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h4><p>提出RPN网络：利用一个与检测器共享部分权重的RPN 网络来直接对图片生成候选框，然后基于RPN 得到的候选框进行分类和位置回归：</p>
<blockquote>
<p>定义anchor box 的尺寸（scale）和比例（aspect ratio）。按上图，预先定义了k个anchor box。在实际的RPN网络实现中，共采用了3个不同的scale（大中小）和3种不同的比例（宽中窄）。然后通过组合，得到了9个anchor box，即 $k=9$ 。在训练RPN的过程中，对于每个feature map上的像素点，都生成 $k$ 个anchor box 的预测。由于预测需要有两个输出用来分类（前景/背景），以及4个用来定位 $(x, y, w, h)$ ，所以RPN的分类层生成的是 $2k$ 维度的向量，RPN的回归层生成的是 $4k$ 维度的向量。</p>
</blockquote>
<h3 id="One-stage方法"><a href="#One-stage方法" class="headerlink" title="One-stage方法"></a>One-stage方法</h3><h4 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h4><p>YOLO的过程如下：首先，将整个图像分成 $S \times S$ 的小格子（cell），对于每个格子，分别预测B 个bbox，以及C个类别的条件概率（注意是条件概率，即已经确定有目标的情况下，该目标属于哪个类别的概率，因此不需要对每个bbox分别预测类别，每个格子只预测一个概率向量即可）。每个bbox都有5个变量，分别是四个描述位置坐标的值，以及一个objectness，即是否有目标（相当于RPN 网络里的那个前景/背景预测）。这样一来，每个格子需要输出 $5B+C$ 维度的向量，因此，CNN最终的输出的tensor的形态为 $S \times S \times (5B + C)$ 。</p>
<p>YOLO的训练过程如下：首先，对于每个GT bbox，找到它的中心位置，该中心位置所在的cell负责该物体的预测。因此，对于该cell 中的输出，其objectness应该尽可能的增加，同时其位置坐标尽可能拟合GTbbox（注意，由于每个cell可以输出多个备选的bbox，因此这里需要选择和GT最相近的那个预测的bbox进行调优）。另外，根据其实际的类别，对类别概率向量进行优化，使其输出真实的类别。对于不负责任何类别的那些cell 的预测值，不必进行优化。</p>
<h4 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h4><p>SSD 也是一种one-stage的直接检测的模型。它相比起YOLO v1主要的改进点在于两个方面：</p>
<ol>
<li>利用了先验框（Prior Box）的方法，预先给定scale 和aspect ratio，实际上就是之前Faster R-CNN 中的anchor box的概念。</li>
<li>多尺度（multi-scale）预测，即对CNN输出的后面的多个不同尺度的feature map 都进行预测。</li>
</ol>
<h4 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h4><ol>
<li>对所有卷积层增加了BN层。</li>
<li>用高分辨率的图片fine-tune 网络10个epoch。</li>
<li>通过k-means进行聚类，得到 $k$ 个手工选择的先验框（Prior anchor box）。这里的聚类用到的距离函数为 $1 - IoU$ ，这个距离函数可以很直接地反映出IoU 的情况。</li>
<li>直接预测位置坐标。之前的坐标回归实际上回归的不是坐标点，而是需要对预测结果做一个变换才能得到坐标点，即 $x = tx \times wa − xa$ （纵坐标同理），其中 $tx$ 为预测的直接结果。从该变换的形式可以看出，对于坐标点的预测不仅和直接预测位置结果相关，还和预测的宽和高也相关。因此，这样的预测方式可以使得任何anchor box可以出现在图像中的任意位置，导致模型可能不稳定。在YOLO v2 中，中心点预测结果为相对于该cell的角点的坐标（0-1 之间）。</li>
<li>多尺度训练（随机选择一个缩放尺度）、跳连层（paththrough layer）将前面的fine-grained特征直接拼接到后面的feature map 中。</li>
</ol>
<h4 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h4><p>通过将所有scale 的feature map 进行打通和结合，兼顾了速度和准确率。</p>
<p>FPN的block 结构分为两个部分：一个自顶向下通路（top-down pathway），另一个是侧边通路（lateral pathway）。所谓自顶向下通路，具体指的是上一个小尺寸的feature map（语义更高层）做2倍上采样，并连接到下一层。而侧边通路则指的是下面的feature map（高分辨率低语义）先利用一个1x1 的卷积核进行通道压缩，然后和上面下来的采样后结果进行合并。合并方式为逐元素相加（element-wise addition）。合并之后的结果在通过一个3x3的卷积核进行处理，得到该scale下的feature map。</p>
<h4 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h4><p>RetinaNet 的最大的贡献不在于网络结构，而是在于提出了一个one-stage 检测的重要的问题，及其对应的解决方案。这个问题就是one-stage 为何比two-stage 的准确率低，两者的区别在哪里？解决方案就是平衡正负样本+平衡难易样本的focal loss。</p>
<h4 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h4><p>本模型将实例分割（instance segmentation）与目标检测（object detection）两个任务相结合，并在两个任务上都达到了SOTA。</p>
<p>整个过程的pipeline 如下：首先，输入图片，根据RoI进行RoIAlign操作，得到该区域内的特征，然后将该特征feature map 进行逐点sigmoid（pixel-wise sigmoid），用于产生mask。另外，还有两个支路用于分类和回归。</p>
<h4 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h4><p>YOLO v3 是针对YOLO模型的又一次改进版本，是一个incremental improvement，并无太大创新，基本都是一些调优和trick。主要包括以下几个方面。</p>
<ol>
<li><p>用单类别的binary logistic 作为分类方式，代替全类别的softmax（和mask R-CNN 的mask 生成方式类似）。这样的好处在于可以处理有些数据集中有目标重叠的情况。</p>
</li>
<li><p>YOLO v3采用了FPN网络做预测，并且沿用了k-means聚类选择先验框，v3中选择了9个prior box，并选择了三个尺度。</p>
</li>
<li><p>backbone做了改进，从darknet-19变成了darknet-53，darknet-53除了3x3和1x1的交替以外，还加入了residual方法，因此层数得到扩展。</p>
</li>
</ol>
<h3 id="参考自"><a href="#参考自" class="headerlink" title="参考自"></a>参考自</h3><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/Hh5EioN_pVnstfHcR777VQ" >从R-CNN到YOLO，2020 图像目标检测算法综述<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="15-过拟合的原因"><a href="#15-过拟合的原因" class="headerlink" title="15. 过拟合的原因"></a>15. 过拟合的原因</h2><ul>
<li>训练集的数量和模型的复杂度不匹配，比如训练集太小或者模型太复杂</li>
<li>训练集和测试集分布不一致</li>
<li>训练集的噪声样本太多，导致模型只学习到了噪声特征，反而忽略了真实的输入输出关系</li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>常见的离散型分布律</title>
    <url>/2019/06/30/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%A6%BB%E6%95%A3%E5%9E%8B%E5%88%86%E5%B8%83%E5%BE%8B/</url>
    <content><![CDATA[<p>在学习一些算法的过程中，总是会遇到各种各样的离散型概率分布。对于它们的分布律不够熟悉，在此记录一下。</p>
<span id="more"></span>
<h2 id="退化分布"><a href="#退化分布" class="headerlink" title="退化分布"></a>退化分布</h2><blockquote>
<p>设 $X$ 是随机变量，$a$ 是常数，若：</p>
<script type="math/tex; mode=display">
P\lbrace X = a \rbrace = 1</script><p>则称 $X$ 服从退化分布。</p>
</blockquote>
<p>退化分布某种程度上已经丧失了随机性，就像随机事件里的不可能事件和必然事件。我们可以将退化分布理解为分布的某种极端。</p>
<h2 id="两点分布"><a href="#两点分布" class="headerlink" title="两点分布"></a>两点分布</h2><blockquote>
<p>设 $X$ 是随机变量，$0 &lt; p &lt; 1$ 是常数，$q = 1 - p$，若：</p>
<script type="math/tex; mode=display">
P \lbrace X = 1 \rbrace = p, \quad P \lbrace X = 0 \rbrace = 1 - p = q</script><p>则称 $X$ 服从参数为 $p$ 的两点分布($0-1$ 分布)</p>
</blockquote>
]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>分布律</tag>
        <tag>离散型变量</tag>
      </tags>
  </entry>
  <entry>
    <title>扔鸡蛋问题</title>
    <url>/2021/08/25/%E6%89%94%E9%B8%A1%E8%9B%8B%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>这是一道非常经典的google面试题，在此记录一下。</p>
<span id="more"></span>
<p>具体案例引导可见：<a class="link"   href="https://blog.csdn.net/qq249356520/article/details/89207891" >扔鸡蛋问题（四种解法）<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>这里介绍动态规划的解法：</p>
<p>我们假设 $F(K,N)$ 表示有 $K$ 个鸡蛋、$N$ 层楼，测出其摔碎临界点所需的最少次数，那么有如下状态转移公式：</p>
<script type="math/tex; mode=display">
F(K, N) = 1 + min_{1 \leq i \leq N} max(F(K, N-i), F(K-1, i-1))</script><ul>
<li>$F(K, N-i)$ : 如果第一个鸡蛋在第 $i$ 层没有摔碎，那么我们还有 $K$ 个鸡蛋以及剩余 $N-i$ 个楼层测试</li>
<li>$F(K-1, i-1)$ : 如果第一个鸡蛋在第 $i$ 层摔碎，那么我们还有 $K-1$ 个鸡蛋以及剩余 $i-1$ 个楼层测试</li>
<li>取两者最坏情况，再取所有情况中最小的值，表示最少测试次数。</li>
</ul>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>具体编程实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">superEggDrop</span>(<span class="params">self, K: <span class="built_in">int</span>, N: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [ [<span class="number">0</span>]*(N+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(K+<span class="number">1</span>) ]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">1</span>][i] = i</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, K+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">                min_drop = N</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">                    tmp_max = <span class="built_in">max</span>(dp[k-<span class="number">1</span>][i-<span class="number">1</span>], dp[k][n-i])</span><br><span class="line">                    min_drop = <span class="built_in">min</span>(min_drop, <span class="number">1</span>+tmp_max)</span><br><span class="line">                dp[k][n] = min_drop</span><br><span class="line">        <span class="keyword">return</span> dp[K][N]</span><br></pre></td></tr></table></figure>
<p>上述代码在<a class="link"   href="https://leetcode-cn.com/problems/super-egg-drop/" >leetcode<i class="fas fa-external-link-alt"></i></a>上超时了，复制粘贴了官方的代码ac的。</p>
<hr>
<h2 id="参考自"><a href="#参考自" class="headerlink" title="参考自"></a>参考自</h2><ul>
<li><a class="link"   href="https://leetcode-cn.com/problems/super-egg-drop/" >887. 鸡蛋掉落<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://baike.baidu.com/item/%E6%89%94%E9%B8%A1%E8%9B%8B%E9%97%AE%E9%A2%98/24626883?fr=aladdin" >扔鸡蛋问题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq249356520/article/details/89207891" >扔鸡蛋问题（四种解法）<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>智力题</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>查看github仓库创建时间</title>
    <url>/2022/03/17/%E6%9F%A5%E7%9C%8Bgithub%E4%BB%93%E5%BA%93%E5%88%9B%E5%BB%BA%E6%97%B6%E9%97%B4/</url>
    <content><![CDATA[<p>github是没有直接的图形化界面来显示仓库的最早创建时间的，我们可以通过调用api的形式来查看，格式如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">https://api.github.com/repos/&#123;username&#125;/&#123;reponame&#125;</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><ul>
<li>确定 <code>username</code> 和 <code>reponame</code> ：<br><img src="https://cdn.jsdelivr.net/gh/TransformersWsz/image_hosting@master/1.5wtegw7m1ow0.webp" alt="1"></li>
<li>在终端中输入：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -k  https://api.github.com/repos/bojone/bert4keras | jq . | grep created_at</span><br></pre></td></tr></table></figure>
<ul>
<li>结果如下：<br><img src="https://cdn.jsdelivr.net/gh/TransformersWsz/image_hosting@master/case.2krf83ywy1g0.webp" alt="2"></li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><p><a class="link"   href="https://blog.csdn.net/Jack_lzx/article/details/117480746" >一键查看GitHub仓库的创建日期<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://github.com/bojone/bert4keras" >bojone/bert4keras: keras implement of transformers for humans<i class="fas fa-external-link-alt"></i></a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>软件工具</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>海量数据处理面试题</title>
    <url>/2021/08/10/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<p>百度三面考到了海量数据处理题，真的是血泪教训，在此记录一下。</p>
<span id="more"></span>
<h2 id="1-海量日志数据，提取出某日访问百度次数最多的那个IP"><a href="#1-海量日志数据，提取出某日访问百度次数最多的那个IP" class="headerlink" title="1. 海量日志数据，提取出某日访问百度次数最多的那个IP"></a>1. 海量日志数据，提取出某日访问百度次数最多的那个IP</h2><p>可以采用映射的方法，比如模1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map进行频率统计，然后再找出频率最大的几个）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。</p>
<h2 id="2-有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。"><a href="#2-有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。" class="headerlink" title="2. 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。"></a>2. 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。</h2><ol>
<li>顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。 </li>
<li>找一台内存在2G左右的机器，依次用hash_map(query, query_count)来统计每个query出现的次数。</li>
<li>利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件。</li>
<li>对这10个文件进行归并排序（内排序与外排序相结合）。</li>
</ol>
<h2 id="3-给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？"><a href="#3-给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？" class="headerlink" title="3. 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？"></a>3. 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？</h2><ol>
<li>遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。</li>
<li>遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小文件（a0 vs b0,a1 vs b1,…,a999 vs b999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。</li>
<li>求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。</li>
</ol>
<h2 id="4-腾讯面试题：给40亿个不重复的unsigned-int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？"><a href="#4-腾讯面试题：给40亿个不重复的unsigned-int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？" class="headerlink" title="4. 腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？"></a>4. 腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？</h2><h4 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h4><p>申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。</p>
<h4 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h4><p>我们把40亿个数中的每一个用32位的二进制来表示假设这40亿个数开始放在一个文件中。</p>
<p>然后将这40亿个数分成两类: 1.最高位为0 2.最高位为1，并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=20亿，而另一个&gt;=20亿（相当于折半）；与要查找的数的最高位比较并接着进入相应的文件再查找。</p>
<p>再然后把这个文件为又分成两类: 1.次最高位为0 2.次最高位为1，并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=10亿，而另一个&gt;=10亿（相当于折半）；与要查找的数的次最高位比较并接着进入相应的文件再查找。以此类推，就可以找到了,而且时间复杂度为O(logn)。</p>
<hr>
<p>其它的场景及解决方案可继续参考：<a class="link"   href="https://zhuanlan.zhihu.com/p/341386422" >十道海量数据处理面试题与十个方法大总结<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>大数据</tag>
        <tag>topK</tag>
      </tags>
  </entry>
  <entry>
    <title>用Sql Server编写一个存储过程</title>
    <url>/2017/06/06/%E7%94%A8Sql%20Server%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>今天数据库上机要求编写一个存储过程来体会sql server的可编程性。</p>
<span id="more"></span>
<h2 id="题目如下："><a href="#题目如下：" class="headerlink" title="题目如下："></a>题目如下：</h2><blockquote>
<p>数据库中有一张表 student, 有两列分别是xh varchar(10), xm  varchar(50)，xh是主码。 现在要求编写一个存储过程，传入两个用分号分隔的字符串（如xhStr=’01;02;03;04’, xmStr=’张三;李斯;王五;赵六’, 其中字符串的长度不限，里面的分号数目也不限，由用户传入）, 存储过程完成如下功能：<br>把对应的两个字符串中的分号前面的字符提取，插入到student表对应的xh和xm列中。<br>注意：需要判断传入的字符串中分号数目是否一致，否则不让插入需要判断学号是否存在，如果存在，就不插入，而是更新姓名。</p>
</blockquote>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--下面是定义函数（计算某字符在字符串中出现的次数）</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">function</span> CalcCounts</span><br><span class="line">(	</span><br><span class="line">	<span class="variable">@searchstr</span> <span class="type">varchar</span>(max),</span><br><span class="line">	<span class="variable">@valuestr</span> <span class="type">varchar</span>(max)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">returns</span> <span class="type">int</span></span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@index</span> <span class="type">int</span></span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@count</span> <span class="type">int</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">set</span> <span class="variable">@index</span> <span class="operator">=</span> charindex(<span class="variable">@valuestr</span>,<span class="variable">@searchstr</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">set</span> <span class="variable">@count</span> <span class="operator">=</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">	while <span class="variable">@index</span> <span class="operator">&gt;</span> <span class="number">0</span></span><br><span class="line">	<span class="keyword">begin</span></span><br><span class="line">		<span class="keyword">set</span> <span class="variable">@count</span> <span class="operator">=</span> <span class="variable">@count</span><span class="operator">+</span><span class="number">1</span></span><br><span class="line">		<span class="keyword">set</span> <span class="variable">@searchstr</span> <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@searchstr</span>,<span class="variable">@index</span><span class="operator">+</span>len(<span class="variable">@valuestr</span>),len(<span class="variable">@searchstr</span>))</span><br><span class="line">		<span class="keyword">set</span> <span class="variable">@index</span> <span class="operator">=</span> charindex(<span class="variable">@valuestr</span>,<span class="variable">@searchstr</span>,<span class="number">0</span>)</span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> <span class="variable">@count</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--编写存储过程</span></span><br><span class="line"><span class="keyword">create</span> proc say_hello</span><br><span class="line">	<span class="variable">@xhstr</span> <span class="type">varchar</span>(max),</span><br><span class="line">	<span class="variable">@valuestr</span> <span class="type">varchar</span>(max),</span><br><span class="line">	<span class="variable">@xmstr</span> <span class="type">varchar</span>(max)</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@xhindex</span> <span class="type">int</span></span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@xmindex</span> <span class="type">int</span></span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@indexcount</span> <span class="type">int</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@xm</span>_toinsert <span class="type">varchar</span>(<span class="number">50</span>)</span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@subxh</span>_front <span class="type">varchar</span>(<span class="number">10</span>)</span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@subxm</span>_front <span class="type">varchar</span>(<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">	if(dbo.CalcCounts(<span class="variable">@xhstr</span>,<span class="variable">@valuestr</span>)<span class="operator">=</span> dbo.CalcCounts(<span class="variable">@xmstr</span>,<span class="variable">@valuestr</span>))</span><br><span class="line">	<span class="keyword">begin</span></span><br><span class="line">		print(<span class="string">&#x27;分号一致，可以插入&#x27;</span>)</span><br><span class="line">		<span class="keyword">set</span> <span class="variable">@indexcount</span> <span class="operator">=</span> dbo.CalcCounts(<span class="variable">@xhstr</span>,<span class="variable">@valuestr</span>)</span><br><span class="line"></span><br><span class="line">		while <span class="variable">@indexcount</span> <span class="operator">&gt;=</span> <span class="number">0</span></span><br><span class="line">		<span class="keyword">begin</span></span><br><span class="line"></span><br><span class="line">			if <span class="variable">@indexcount</span> <span class="operator">=</span> <span class="number">0</span></span><br><span class="line">			<span class="keyword">begin</span></span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@subxh</span>_front <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xhstr</span>,<span class="number">1</span>,len(<span class="variable">@xhstr</span>))</span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@subxm</span>_front <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xmstr</span>,<span class="number">1</span>,len(<span class="variable">@xmstr</span>))</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">			<span class="keyword">else</span></span><br><span class="line">			<span class="keyword">begin</span></span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@xhindex</span> <span class="operator">=</span> charindex(<span class="variable">@valuestr</span>,<span class="variable">@xhstr</span>,<span class="number">1</span>)</span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@xmindex</span> <span class="operator">=</span> charindex(<span class="variable">@valuestr</span>,<span class="variable">@xmstr</span>,<span class="number">1</span>)</span><br><span class="line">		</span><br><span class="line">				<span class="comment">--截取xh待插入部分</span></span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@subxh</span>_front <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xhstr</span>,<span class="number">1</span>,<span class="variable">@xhindex</span><span class="number">-1</span>)</span><br><span class="line">			</span><br><span class="line">				<span class="comment">--截取xm待插入部分</span></span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@subxm</span>_front <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xmstr</span>,<span class="number">1</span>,<span class="variable">@xmindex</span><span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">				<span class="comment">--截取字符串后面部分</span></span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@xhstr</span> <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xhstr</span>,<span class="variable">@xhindex</span><span class="operator">+</span><span class="number">1</span>,len(<span class="variable">@xhstr</span>))</span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@xmstr</span> <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xmstr</span>,<span class="variable">@xmindex</span><span class="operator">+</span><span class="number">1</span>,len(<span class="variable">@xmstr</span>))</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line">			</span><br><span class="line">			<span class="comment">--执行插入过程</span></span><br><span class="line">			<span class="keyword">select</span> <span class="variable">@xm</span>_toinsert <span class="operator">=</span> xm <span class="keyword">from</span> student <span class="keyword">where</span> xh <span class="operator">=</span> <span class="variable">@subxh</span>_front</span><br><span class="line">			if <span class="variable">@xm</span>_toinsert <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span></span><br><span class="line">			<span class="keyword">begin</span></span><br><span class="line">				<span class="keyword">update</span> student <span class="keyword">set</span> xm <span class="operator">=</span> <span class="variable">@subxm</span>_front <span class="keyword">where</span> xh <span class="operator">=</span> <span class="variable">@subxh</span>_front</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">			<span class="keyword">else</span></span><br><span class="line">			<span class="keyword">begin</span></span><br><span class="line">				<span class="keyword">insert</span> <span class="keyword">into</span> student <span class="keyword">values</span>(<span class="variable">@subxh</span>_front,<span class="variable">@subxm</span>_front)</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">			<span class="keyword">set</span> <span class="variable">@indexcount</span> <span class="operator">=</span> <span class="variable">@indexcount</span><span class="number">-1</span></span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	<span class="keyword">begin</span></span><br><span class="line">		print(<span class="string">&#x27;分号不一致，无法插入&#x27;</span>)</span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>本道编程题较为基础，算是练一下手了！</p>
]]></content>
      <categories>
        <category>DataBase</category>
      </categories>
      <tags>
        <tag>Sql Server</tag>
        <tag>存储过程</tag>
      </tags>
  </entry>
  <entry>
    <title>策略梯度</title>
    <url>/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/</url>
    <content><![CDATA[<p>在看师兄的论文时，里面涉及到强化学习的 <strong>Policy Gradient</strong> 。看了网上好多博客，觉得公式推导太复杂了，断断续续地持续了三周。今天静下心来看了一遍，发现没有那么难，果然做学术还是不能浮躁啊！</p>
<span id="more"></span>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p> 强化学习是机器学习的一个分支，但是它与我们常见监督式学习不太一样。从学习方式上讲强化学习更加接近人类的学习，例如当你接触一款新的电子游戏的时候，虽然看不懂屏幕的提示，但是经过自己的摸索也能掌握游戏方法，这个摸索的过程其实就是通过试错逐渐了解游戏规则的学习过程。同样，强化学习也是通过一系列的尝试并根据得到的反馈不断调整自己的行为来学习陌生的对象。 </p>
<p>强化学习主要包括如下几个部分：</p>
<img src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/composition.png" class="">
<ul>
<li><strong>主体（Agent）</strong>： 指能够通过动作与环境交互的对象，强化学习中主体通常是运行中的算法，比如在游戏中的主体是用于控制本方球拍的算法。</li>
<li><strong>环境（Environment）</strong>： 指主体动作作用的对象， 比如游戏本身。 </li>
<li><strong>动作（Action）</strong>: 指所有可能作用于环境上的操作，比如游戏中算法控制球拍上下移动。 </li>
<li><strong>状态（State）</strong>: 指可被主体感知的关于环境的信息，比如游戏中屏幕显示的球和球拍的位置以及移动方向和速度信息。 </li>
<li><strong>奖励（Reward）</strong>: 指由环境回馈给主体的描述上一个动作效果的信息，比如游戏中球拍动作导致双方的得分变化。 </li>
</ul>
<p>强化学习的过程是一个通过和环境交互获得反馈，再根据反馈调整动作以期使总奖励最大化的过程，这个是一个多步 (multi timestep) 的交互的过程，每一步交互都会影响其后的所有步骤。强化学习中的一次交互是指主体对环境施加一个动作，环境的状态发生改变并且回馈给主体一个奖励（奖励既可以是正向的，如本方得分增加；也可以是负向的，如对方得分增加）。强化学习的目标就是寻找一个最优的策略使得整个学习过程（从开始状态到终结状态）获得的奖励最大化。</p>
<p>在实现上，强化学习是一个通过多个轮次逐渐优化算法的参数从而增强学习效果的过程，每个轮次包含两部分：前向反馈（feed forward）和反向传播（back propagation）。处于初始状态的主体根据算法的当前参数生成动作作用于环境，环境返回给主体新的状态和对动作的奖励，在轮次结束后算法通过汇总所有在本轮收集到的反馈调整算法的参数开始下一轮的学习，直到学习的效果不再增强。</p>
<p>强化学习包括了一系列不同的算法（如下图），其中比较常见的是基于值（Value-based）的方法和基于策略（Policy-based）的方法。这两类方法各有特点，适用于解决不同的问题。一般来说，基于值的方法适用于比较简单（状态空间比较小）的问题，它有较高的数据利用率并且能稳定收敛；而基于策略的方法适用于复杂问题，但是高方差是这类方法会存在的一个比较明显的问题。</p>
<img src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/category.png" class="">
<h2 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p> 基于值的方法一般是确定性的，给定一个状态就能计算出每种可能动作的奖励（确定值），但这种确定性的方法恰恰无法处理一些现实的问题，比如玩 100 把石头剪刀布的游戏，最好的解法是随机的使用石头、剪刀和布并尽量保证这三种手势出现的概率一样，因为任何一种手势的概率高于其他手势都会被对手注意到并使用相应的手势赢得游戏。 </p>
<p>策略梯度正是为了解决上面的问题产生的，而它的秘密武器就是随机（Stochastic）。首先随机能提供非确定的结果，但这种非确定的结果并不是完全的随意而是服从某种概率分布的随机，策略梯度不计算奖励（reward）而是使用概率选择动作，这样就避免了因为计算奖励而维护状态表。策略梯度的基本原理是通过反馈调整策略，具体来说就是在得到正向奖励时，增加相应的动作的概率；得到负向的奖励时，降低相应动作的概率。下面左图中的绿点表示获得正向奖励的动作，右图表示更新后的策略，可以发现产生正向奖励的区域的概率都增加了（离圆心的距离更近）。</p>
<img src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/justify.png" class="">
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li><p><strong>对象系统</strong>：策略梯度的学习对象，这个对象即可以是一个系统，比如汽车或一个游戏，也可以是一个对手，比如势头剪刀布的游戏对手或者一个职业的围棋手。</p>
</li>
<li><p><strong>Policy（策略）</strong>：$\pi_\theta(a|s)$ 表示在状态 $s$ 和参数 $\theta$ 条件下发生 $a$ 动作的概率。</p>
</li>
<li><p><strong>Episode（轮次）</strong>：表示从起始状态开始使用某种策略产生动作与对象系统交互，直到某个终结状态结束。比如在围棋游戏中的一个轮次就是从棋盘中的第一个落子开始直到对弈分出胜负，或者自动驾驶的轮次指从汽车启动一直到顺利抵达指定的目的地，当然撞车或者开进水塘也是种不理想的终结状态。</p>
</li>
<li><p><strong>Trajectory（轨迹 $\tau$ ）</strong>：表示在 PG 一个轮次的学习中状态 $s$ ，动作 $a$ 和奖励 $r$ 的顺序排列。由于策略产生的是非确定的动作，同一个策略在多个轮次可以产生多个不同的轨迹。$\tau=(s_1, a_1, \dots, s_t, a_t)$</p>
</li>
<li><p><strong>轮次奖励 $\sum r(\tau)$ </strong>：表示在一个轮次中依次动作产生的奖励的总和。 因此在实现中对每个策略会求多个轮次的平均值。</p>
</li>
</ul>
<p>策略梯度的学习是一个策略的优化过程，最开始随机的生成一个策略，当然这个策略对对象系统一无所知，所以用这个策略产生的动作会从对象系统那里很可能会得到一个负面奖励，这个过程就好像在PONG游戏中我们对飞来的乒乒球无动于衷而导致对方的得分增加。为了击败对手我们需要逐渐的改变策略，使得本方的比分增加。策略梯度在一轮的学习中使用同一个策略直到该轮结束，通过梯度上升改变策略并开始下一轮学习，如此往复直到轮次累计奖励不再增长停止。 </p>
<h3 id="理论推导"><a href="#理论推导" class="headerlink" title="理论推导"></a>理论推导</h3><p>首先需要将策略参数话 $\pi(a|s,\theta)=\pi_\theta(a|s)$ ，从轨迹 $\tau$ 中直接找到策略上升的方向，定义这条轨迹在策略 $\pi_\theta$ 下出现的概率为： </p>
<script type="math/tex; mode=display">
p_\theta(\tau) = p_\theta(s_1, a_1, \dots, s_T, a_T) = p(s_1) \prod_{t=1}^{T} \pi_\theta(a_t|s_t) p(s_{t+1} | s_t, a_t)</script><img src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/trajectory.jpg" class="">
<p>我们需要定义长期汇报 $J(\theta)$ ，目标最大化它。过程如下：</p>
<img src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/derive.png" class="">
<p>由于 $\nabla_{\theta} J(\theta)$ 无法直接求出，因此采用蒙特卡洛采样法来近似求解。然后根据梯度上升公式更新参数 $\theta$ 直至收敛，流程如下：</p>
<img src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/procedure.png" class="">
<p>从机器学习的原理的角度来看，策略梯度和传统的监督式学习的学习过程还是比较相似的，每轮次都由前向反馈和反向传播构成，前向反馈负责计算目标函数，反向传播负责更新算法的参数，依此进行多轮次的学习指导学习效果稳定收敛。唯一不同的是，监督式学习的目标函数相对直接，即目标值和真实值的差，这个差值通过一次前向反馈就能得到；而策略梯度的目标函数源自轮次内所有得到的奖励，并且需要进行一定的数学转换才能计算，另外由于用抽样模拟期望，也需要对同一套参数进行多次抽样来增加模拟的准确性。 </p>
<h3 id="缺陷及改进"><a href="#缺陷及改进" class="headerlink" title="缺陷及改进"></a>缺陷及改进</h3><p>我们把 $R(\tau^n)$ 看作是 $\sum_{t=1}^T \nabla_\theta log \pi_\theta (a_t|s_t)$ 的权重，这样会存在两个问题：</p>
<ul>
<li>$R(\tau^n)$  能始终为正，也就是会导致所有策略都会增强，而我们的初衷是降低表现差的行动的概率，提升表现好的行动的概率。</li>
<li>$R(\tau^n) = \sum_{t=1}^T r_t$ ，对于序列中的每一时间段的元组 $(s_t, a_t)$ 只能影响 $t$ 时刻之后的回报，不能影响之前的回报。</li>
</ul>
<p>针对上述两个问题，解决方案如下：</p>
<ul>
<li>引入基线：权重项变为 $R(\tau^n)-b$ ，通常 $b=E[R(\tau^n)]$ ，表示对所有轨迹的累计回报求平均。引入 $b$ 不会对 $\nabla_\theta J(\theta)$ 产生影响，证明如下：</li>
</ul>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\nabla_\theta J(\theta) &= \sum_\tau [R(\tau)-b] \nabla_\theta p_\theta(\tau) \\
                        &= \sum_\tau R(\tau) \nabla_\theta p_\theta(\tau) - b\sum_\tau \nabla_\theta p_\theta(\tau) \\
                        &= \sum_\tau R(\tau) \nabla_\theta p_\theta(\tau) - b\sum_\tau \nabla_\theta 1 \\
                        &= \sum_\tau R(\tau) \nabla_\theta p_\theta(\tau) \\
\end{aligned}
\end{equation}</script><ul>
<li>减少无效元素：权重项变为 $\sum_{t’=t}^T \gamma^{t’-t}r_t$ ，$\gamma$ 表示衰减系数，该式表示只计算t时刻之后的回报，即未来不影响过去。 </li>
</ul>
<p>综上改进后的式子为 $\nabla_\theta J(\theta) \approx \frac {1} {N} \sum_{n=1}^N [(\sum_{t=1}^T \nabla_\theta log \pi_\theta (a_t^n|s_t^n))(\sum_{t’=t}^T \gamma^{t’-t}r_t^n - b)] $</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p> 策略梯度基本靠“猜”。这里的猜不是瞎猜，而是用随机（Stochastic）的方式控制动作的产生进而影响策略的变化，随机既保证了非确定性又能通过控制概率避免完全盲目，是策略梯度解决复杂问题的核心和基础。然而双刃剑的另一面是，”猜“造成了策略梯度方差大、收敛慢的缺点，这是源于策略梯度为了避免遍历所有状态而不得不付出的代价，无法完全避免。 但是瑕不掩瑜，策略梯度除了理论上的处理复杂问题的优势，在实践应用中也有明显的优势，那就是它可以仅靠与目标系统交互进行学习，而不需要标签数据，可以节省了大量的人力。 目前层出不穷的 variance reduction 的方法也证明了人们不仅没有因为策略梯度的缺点放弃它，反而正在通过不断的改进使其扬长避短，发扬光大。 </p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.ibm.com/developerworks/cn/analytics/library/ba-lo-deep-introduce-policy-gradient/index.html" >Machine learning and gaming<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/55298602" >策略梯度理解及代码实现<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://karpathy.github.io/2016/05/31/rl/" >Deep Reinforcement Learning: Pong from Pixels<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Policy Gradient</tag>
      </tags>
  </entry>
  <entry>
    <title>装饰器模式</title>
    <url>/2017/10/24/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<p>学习一下python装饰器模式的概念与基本使用。</p>
<span id="more"></span>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>由于函数也是对象，而且函数对象可以被赋值给变量。所以，通过变量也能调用该函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">now</span>():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Time is 2017-10-23&quot;</span></span><br><span class="line"></span><br><span class="line">f = now</span><br><span class="line">f()</span><br></pre></td></tr></table></figure>
<p>运行结果输出为: <font color="red">“Time is 2017-10-23”</font></p>
<p>现在，假设我们要增强 <code>now()</code> 函数的功能。比如，在函数调用前后自动打印日志，但又不希望修改 <code>now()</code> 函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”(Decorator)。</p>
<p>本质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator，可以定义如下:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args,**kw</span>):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;call %s():&#x27;</span> % func.__name__</span><br><span class="line">        <span class="keyword">return</span> func(*args,**kw)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@log</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">now</span>():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Time is 2017-10-23&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    now()</span><br></pre></td></tr></table></figure><br>运行结果如下:</p>
<img src="/2017/10/24/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/1.png" class="">
<p>观察上面的log，因为它是一个decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助Python的@语法，把decorator置于函数的定义处:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@log</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">now</span>():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Time is 2017-10-23&quot;</span></span><br></pre></td></tr></table></figure><br>调用 <code>now()</code> 函数，不仅会运行 <code>now()</code> 函数本身，还会在 <code>now()</code> 函数前打印一行日志。</p>
<p>把 <code>@log</code> 放到 <code>now()</code> 函数的定义处，相当于执行了语句:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">now = log(now)</span><br></pre></td></tr></table></figure><br>由于 <code>log()</code> 是一个decorator，返回一个函数，所以原来的 <code>now()</code> 函数依然存在，只是现在同名的now变量指向了新的函数，于是调用 <code>now()</code> 将执行新函数，即在 <code>log()</code> 函数中返回的 <code>wrapper()</code> 函数。</p>
<p><code>wrapper()</code> 函数的参数 <code>(*args,**kw)</code>，因此， <code>wrapper()</code> 函数可以接受任意参数的调用。在 <code>wrapper()</code> 函数内，首先打印日志，再紧接着调用原始函数。</p>
<p>如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写起来会更复杂。比如，要自定义log的文本:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&#x27;%s %s():&#x27;</span> % (text, func.__name__)</span><br><span class="line">            <span class="keyword">return</span> func(*args, **kw)</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br></pre></td></tr></table></figure><br>这个3层嵌套的decorator用法如下:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@log(<span class="params"><span class="string">&#x27;execute&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">now</span>():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Time is 2017-10-23&quot;</span></span><br></pre></td></tr></table></figure><br>执行结果如下:</p>
<img src="/2017/10/24/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/2.png" class="">
<p>和两层嵌套的decorator相比，3层嵌套的效果是这样的:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">now = log(<span class="string">&#x27;execute&#x27;</span>)(now)</span><br></pre></td></tr></table></figure><br>我们来剖析上面的语句，首先执行 <code>log(&#39;execute&#39;)</code> ，返回的是 <code>decorator</code> 函数，再调用返回的函数，参数是 <code>now</code> 函数，返回值最终是 <code>wrapper</code> 函数。</p>
<p>以上两种decorator的定义都没有问题，但还差最后一步。因为我们讲了函数也是对象，它有 <code>__name__</code> 等属性，但你去看经过decorator装饰之后的函数，它们的 <code>__name__</code> 已经从原来的 <code>now</code> 变成了 <code>wrapper</code></p>
<hr>
<h2 id="装饰器的那些坑"><a href="#装饰器的那些坑" class="headerlink" title="装饰器的那些坑"></a>装饰器的那些坑</h2><h3 id="位置错误的代码"><a href="#位置错误的代码" class="headerlink" title="位置错误的代码"></a>位置错误的代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">html_tags</span>(<span class="params">tag_name</span>):</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;begin outer function.&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper_</span>(<span class="params">func</span>):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;begin of inner wrapper function.&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            content = func(*args, **kwargs)</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;&lt;&#123;tag&#125;&gt;&#123;content&#125;&lt;/&#123;tag&#125;&gt;&quot;</span>.<span class="built_in">format</span>(tag=tag_name, content=content)</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;end of inner wrapper function.&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;end of outer function&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> wrapper_</span><br><span class="line"></span><br><span class="line"><span class="meta">@html_tags(<span class="params"><span class="string">&#x27;b&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hello</span>(<span class="params">name=<span class="string">&#x27;Toby&#x27;</span></span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello &#123;&#125;!&#x27;</span>.<span class="built_in">format</span>(name)</span><br><span class="line"></span><br><span class="line">hello()</span><br><span class="line">hello()</span><br></pre></td></tr></table></figure>
<p>在装饰器中我在各个可能的位置都加上了print语句，用于记录被调用的情况。你知道他们最后打印出来的顺序吗？如果你心里没底，那么最好不要在装饰器函数之外添加逻辑功能，否则这个装饰器就不受你控制了。以下是输出结果：</p>
<img src="/2017/10/24/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/3.png" class="">
<h3 id="错误的函数签名和文档"><a href="#错误的函数签名和文档" class="headerlink" title="错误的函数签名和文档"></a>错误的函数签名和文档</h3><p>装饰器装饰过的函数看上去名字没变，其实已经变了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">logging</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;print log before a function.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;[DEBUG] &#123;&#125;: enter &#123;&#125;()&quot;</span>.<span class="built_in">format</span>(datetime.now(), func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@logging</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">say</span>(<span class="params">something</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;say something&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;say &#123;&#125;!&quot;</span>.<span class="built_in">format</span>(something)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> say.__name__  <span class="comment"># wrapper</span></span><br></pre></td></tr></table></figure>
<p>为什么会这样呢？只要你想想装饰器的语法糖@代替的东西就明白了。@等同于这样的写法。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">say = logging(say)</span><br></pre></td></tr></table></figure><br><code>logging</code> 其实返回的函数名字刚好是 <code>wrapper</code> ，那么上面的这个语句刚好就是把这个结果赋值给 <code>say</code>， <code>say</code> 的 <code>__name__</code> 自然也就是 <code>wrapper</code> 了。不仅仅是 <code>name</code>，其他属性也都是来自 <code>wrapper</code> ，比如 <code>doc</code> ，<code>source</code> 等等。</p>
<p>使用标准库的 <code>functools.wraps</code>，可以基本解决这个问题。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logging</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;print log before a function.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;[DEBUG] &#123;&#125;: enter &#123;&#125;()&quot;</span>.<span class="built_in">format</span>(datetime.now(), func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@logging</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">say</span>(<span class="params">something</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;say something&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;say &#123;&#125;!&quot;</span>.<span class="built_in">format</span>(something)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> say.__name__  <span class="comment"># say</span></span><br><span class="line"><span class="built_in">print</span> say.__doc__ <span class="comment"># say something</span></span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a class="link"   href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017451662295584" >装饰器<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://segmentfault.com/a/1190000007321935" >详解Python的装饰器<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>高阶函数</tag>
      </tags>
  </entry>
  <entry>
    <title>记一次JavaWeb的开发经历</title>
    <url>/2019/12/17/%E8%AE%B0%E4%B8%80%E6%AC%A1JavaWeb%E7%9A%84%E5%BC%80%E5%8F%91%E7%BB%8F%E5%8E%86/</url>
    <content><![CDATA[<p>这两周断断续续地帮老东家做了一个很low的网站，总体来说感触还挺多的。</p>
<span id="more"></span>
<h2 id="回忆"><a href="#回忆" class="headerlink" title="回忆"></a>回忆</h2><p>还记得大三的时候跟着<a class="link"   href="https://blog.trickotreat.cn/" >学长<i class="fas fa-external-link-alt"></i></a>后面学SSM（Spring + SpringMVC + MyBatis），光环境搭建就花了一周的时间。<code>web.xml</code> 的网站配置，Mybatis的集成配置，Bean对象的生成配置，SQLMapper的CURD编写，基本上全在死磕xml，头都搞炸了。网上介绍Spring时说是JavaWeb的轻量级框架，这还轻量？好吧，我可能没接触过Struts2\JSF等重量级框架吧。在学了一段时间后，没学出头绪就放弃了。</p>
<p>到了大三下，经典boy推荐了springboot，当时尝了下鲜，真的惊呆了，0 xml配置。直接使用IDEA提供的脚手架就能运行了，再稍微配置下数据库的连接信息就能开发web了，开发体验好到爆炸。但后来因为没有实际需求，也没有用它来搞事情。</p>
<p>到了大四，去了一家外企实习（上述所说的老东家）。10月份入职，正赶上一个创新项目结点，要赶着做一款蓝牙通信的iOS APP。谈到做这个app真的是一把辛酸泪（此处省去1万字）。老外要查看月活量，因此需要一个website来进行可视化展示。正好就用上了springboot，前端用的echarts库，花了一下午做好了，成就感满满。看到特效这么炫酷的图表，老外连说cool😎。</p>
<p>后来部门就顺着这个思路给员工做一个工时可视化的网站。前端仍然是传统的那一套Bootstrap+Jquery+ECharts，说到这三个（尤其前两个），真的是太羞愧，互联网行业基本上没人用这些了。后端采用了流行的基于js的express框架。总体上来说，上手快，爽是爽，但基于事件驱动的编程仍给人一种无规范、无管理的感觉。</p>
<h2 id="现在"><a href="#现在" class="headerlink" title="现在"></a>现在</h2><p>如今读研了，学习重点偏研究性质，工程开发搁在了一边。由于有之前的样板代码，这两周做的这个网站就照搬了，但在细节地方提升了不少。主要有如下几点：</p>
<ul>
<li>网络异常判断：老东家的wifi访问外部网站特别慢，以至于发送一次请求会导致圈圈转半天。这里设置了时间限制，若超时则取消请求。</li>
<li>限流：为了防止用户手速过快，操作按钮连点多次以至发送多次网络请求，这里采用的方法就是按钮点击后将其设置为不可点击状态，直至后端返回数据。</li>
<li>404和500处理：两个页面就直接显示<code>404 Not Found</code> 和 <code>500 Internal Error</code> 。为了将这两端文字在网页正中央水平垂直居中，花了不少时间，请教了其他人，真是羞耻。</li>
<li>在线状态的判定：如果用户没有登录，则重定向至登录界面。</li>
<li>Docker打包：win10上开发使用的是jdk13，目前docker仓库里还没有openjdk13，因此就不放到docker里运行了。至于mysql数据库，给大家一个忠告，千万不要在宿主服务器上安装mysql！！！不管是yum源安装，还是手动安装。一旦mysql服务宕掉，那么你很有可能无法重启mysql服务，你会遇到各种各样的报错。虽说网上的解决方案有很多，但很少能帮到你的，反正我是不知道那些博主是怎么解决掉的。因此将mysql放到docker里运行是最安全，服务的关闭和启动都非常地容易。</li>
<li>数据库的定时备份：由于本人只有一台服务器，搞不了网上那么高大上的mysql集群。对于数据的可用性，只能定时备份了。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>不管做的什么项目，成就感都不是很高，甚至挫败感很强。主要有如下几点原因：</p>
<h4 id="前端"><a href="#前端" class="headerlink" title="前端"></a>前端</h4><p>我前端采用的是low得不能再low的技术栈。jquery写起来虽然很爽，但是业务逻辑一多，一个页面洋洋洒洒写下来起码几千行代码，又臭又长。第一感觉就是可读性和可维护性极差。</p>
<p>暑假用过<code>vue</code>来做前后端分离，当时的感觉就是前端不仅仅只是单纯的页面了，而是一个系统的工程。一个小小的按钮都是封装好的组件。视图也是直接与数据绑定的，当后端数据来了，我们可以直接修改数据就能更新视图了，再也不需要用jquery去手动操作dom了，确实很爽。另外，<code>vue</code>脚手架生成的目录结构就很工程化，便于维护。但工程化了，要学的东西就变多了，比如<code>vuex</code>，<code>webpack</code>，<code>babel</code>，<code>ssr</code>，<code>sass</code>等一系列工具链，怪不得说学前端太累了。</p>
<h4 id="后端"><a href="#后端" class="headerlink" title="后端"></a>后端</h4><p>后端的业务逻辑无非就是增删查改，没什么太难的。而且像mybatis这样的orm框架，我没用到它多少特性，全部是手写sql，这样简单直观，但维护性上不强。</p>
<hr>
<font color="red">总的来说，我所用到的东西全是用的别人的，全是套的框架。我的水平仅局限于能用就行，完全没有深入理解过这些框架，更别提造轮子了。</font>

<font color="red">我是一个活在框架下的码农，想提升自身竞争力，给自己增值，还有很长的路要走。。。</font>

<h4 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h4><p>到现在我还会时不时地想起老东家，虽说前一个月很苦逼，但剩下的时间真的很舒服，你有大量的时间去学想学的东西，同事之间很和谐，公司人文关怀很到位，年轻漂亮妹子很多。突然想起了我那一段“伤心”的往事。不说了，这公司就是 <font color="green"><strong>BOSCH</strong></font> .</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>程序人生</tag>
      </tags>
  </entry>
</search>

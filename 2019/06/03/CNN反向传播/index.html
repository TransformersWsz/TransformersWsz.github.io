<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Swift">
    
    <title>
        
            CNN反向传播 |
        
        Swift&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/huoyin.png">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.xml"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":false},"style":{"primary_color":"#0066CC","avatar":"/images/atom.svg","favicon":"/images/huoyin.png","article_img_align":"center","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":true,"scale":true},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving !"},"scroll":{"progress_bar":{"enable":false},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 5.4.1"></head>


<body>
<div class="progress-bar-container">
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                Swift&#39;s Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/links"
                            >
                                LINKS
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                ABOUT
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/tags">TAGS</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/links">LINKS</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">CNN反向传播</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/atom.svg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Swift</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2019-06-03 22:49:21</span>
        <span class="mobile">2019-06-03 22:49</span>
    </span>
    
        <span class="article-categories article-meta-item">
            <i class="fas fa-folder"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/Machine-Learning/">Machine Learning</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/Algorithm/">Algorithm</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/Neural-Networks/">Neural Networks</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <p>深度神经网络(DNN)反向传播的公式推导可以参考之前的博客：<a class="link"   target="_blank" rel="noopener" href="https://transformerswsz.github.io/2019/05/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/" >反向传播<i class="fas fa-external-link-alt"></i></a></p>
<span id="more"></span>
<p>要套用DNN的反向传播算法到CNN，有几个问题需要解决：</p>
<ul>
<li>池化层没有激活函数，我们可以令池化层的激活函数为 $g(z) = z$，即激活后输出本身，激活函数的导数为1。</li>
<li>池化层在前向传播的时候，对输入矩阵进行了压缩，我们需要反向推导出 $\delta^{l-1}$，这个方法与DNN完全不同。</li>
<li>卷积层通过张量卷积，或者说若干个矩阵卷积求和得到当前层的输出，而DNN的全连接层是直接进行矩阵乘法而得到当前层的输出。我们需要反向推导出 $\delta^{l-1}$，计算方法与DNN也不同。</li>
<li>对于卷积层，由于 $W$ 使用的是卷积运算，那么从 $\delta^l$ 推导出该层的filter的 $W, b$ 方式也不同。</li>
</ul>
<p>在研究过程中，需要注意的是，由于卷积层可以有多个卷积核，各个卷积核的处理方法是完全相同且独立的，为了简化算法公式的复杂度，我们下面提到卷积核都是卷积层中若干卷积核中的一个。</p>
<p>下面将对上述问题进行逐一分析：</p>
<h1 id="已知池化层的-delta-l-，推导上一隐藏层的-delta-l-1"><a href="#已知池化层的-delta-l-，推导上一隐藏层的-delta-l-1" class="headerlink" title="已知池化层的 $\delta^l$，推导上一隐藏层的 $\delta^{l-1 }$"></a>已知池化层的 $\delta^l$，推导上一隐藏层的 $\delta^{l-1 }$</h1><p>在前向传播算法时，池化层一般我们会用MAX或者Average对输入进行池化，池化的区域大小已知。现在我们反过来，要从缩小后的误差 $ \delta^l $，还原前一次较大区域对应的误差。</p>
<p>在反向传播时，我们首先会把 $ \delta^l $ 的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是MAX，则把 $ \delta^l $ 的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是Average，则把 $ \delta^l $ 的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做 $upsample$。</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>假设池化区域为2*2，步长为2，$\delta^l$ 的第k个子矩阵为：</p>
<script type="math/tex; mode=display">
\delta_k^l = \left(
                \begin{array}{ccc}
                    2 & 8 \\
                    4 & 6
                \end{array}
             \right)</script><p>我们先将 $ \delta_k^l $ 还原，即变成：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    0 & 0 & 0 & 0 \\
    0 & 2 & 8 & 0 \\
    0 & 4 & 6 & 0 \\
    0 & 0 & 0 & 0 \\
    \end{array}
\right)</script><p>如果是MAX，假设我们之前在前向传播时记录的最大值位置分别是左上、右下、右上、左下，则转换后的矩阵为：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    2 & 0 & 0 & 0 \\
    0 & 0 & 0 & 8 \\
    0 & 4 & 0 & 0 \\
    0 & 0 & 6 & 0 \\
    \end{array}
\right)</script><p>如果是Average，转换后的矩阵为：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    0.5 & 0.5 & 2 & 2 \\
    0.5 & 0.5 & 2 & 2 \\
    1 & 1 & 1.5 & 1.5 \\
    1 & 1 & 1.5 & 1.5 \\
    \end{array}
\right)</script><p>这样我们就得到了上一层 $ \frac {\partial J(W, b)} {\partial a_k^{l-1}} $ ，要得到 $\delta_k^{l-1}$ ：</p>
<script type="math/tex; mode=display">
\delta_k^{l-1} = (\frac {\partial a_k^{l-1}} {\partial z_k^{l-1}})^T \frac {\partial J(W, b)} {\partial a_k^{l-1}} = upsample(\delta_k^l) \odot \sigma'(z_k^{l-1})</script><p>其中，$upsample$ 函数完成了池化误差矩阵放大与误差重新分配的逻辑。</p>
<p>对于张量 $\delta^l$ ，我们有：</p>
<script type="math/tex; mode=display">
\delta^{l-1} = upsample(\delta^l) \odot \sigma'(z^{l-1})</script><h2 id="已知卷积层的-delta-l-，推导上一层隐藏层的-delta-l-1"><a href="#已知卷积层的-delta-l-，推导上一层隐藏层的-delta-l-1" class="headerlink" title="已知卷积层的 $\delta^l$，推导上一层隐藏层的 $\delta^{l-1}$"></a>已知卷积层的 $\delta^l$，推导上一层隐藏层的 $\delta^{l-1}$</h2><p>在DNN中，我们知道 $\delta^{l-1}$ 和 $\delta^l$ 的递推关系为：</p>
<script type="math/tex; mode=display">
\delta^{l-1} = \frac {\partial J(W, b)} {\partial z^{l-1}} = (\frac {\partial z^l} {\partial z^{l-1}})^T \frac {\partial J(W, b)} {\partial z^l} = (\frac {\partial z^l} {\partial z^{l-1}})^T \delta^l</script><p>注意到 $z^l$ 和 $z^{l-1}$ 的关系为：</p>
<script type="math/tex; mode=display">
z^l = a^{l-1}*W^l + b^l = \sigma(z^{l-1})*W^l + b^l</script><p>因此我们有：</p>
<script type="math/tex; mode=display">
\delta^{l-1} = (\frac {\partial z^l} {\partial z^{l-1}})^T \delta^l = \delta^l * rot180(W^l) \odot \sigma'(z^{l-1})</script><p>这里的式子其实和DNN的类似，区别在于对于含有卷积的式子求导时，卷积核被旋转了180度。即式子中的 $rot180()$，翻转180度的意思是上下翻转一次，接着左右翻转一次。在DNN中这里只是矩阵的转置。那么为什么呢？由于这里都是张量，直接推演参数太多了。我们以一个简单的例子说明为啥这里求导后卷积核要翻转。</p>
<p>假设我们 $l-1$ 层的输出 $a^{l-1}$ 是一个3*3的矩阵，第 $l$ 层的卷积核 $W^l$ 是一个2*2矩阵，步幅为1，则输出 $z^l$ 是一个 2*2的矩阵，这里 $b^l$ 简化为0，则有：</p>
<script type="math/tex; mode=display">
a^{l-1} * W^l = z^l</script><p>我们列出 $a, W, z$ 的矩阵表达式如下：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{ccc}
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} \\
    \end{array}
\right) * 
\left(
    \begin{array}{cc}
    w_{11} & w_{12} \\
    w_{21} & w_{22} \\
    \end{array}
\right)
= 
\left(
    \begin{array}{cc}
    z_{11} & z_{12} \\
    z_{21} & z_{22} \\
    \end{array}
\right)</script><p>根据卷积得出：</p>
<script type="math/tex; mode=display">
z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22} \\
z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} + a_{23}w_{22} \\
z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} + a_{32}w_{22} \\
z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} + a_{33}w_{22} \\</script><p>接着我们模拟反向求导：</p>
<script type="math/tex; mode=display">
\bigtriangledown a^{l-1} = \frac {\partial J(W, b)} {\partial a^{l-1}} = (\frac {\partial z^l} {\partial a^{l-1}})^T \frac {\partial J(W, b)} {\partial z^l} = (\frac {\partial z^l} {\partial a^{l-1}})^T \delta^l</script><p>从上式可以看出，对于 $ a^{l-1} $ 的梯度误差 $\bigtriangledown a^{l-1} $ ，等于第 $l$ 层的梯度误差乘以 $\frac {\partial z^l} {\partial a^{l-1}}$ ，而 $\frac {\partial z^l} {\partial a^{l-1}}$ 对应上面的例子中相关联的 $w$ 的值。假设 $z$ 矩阵对应的反向传播误差是 $\delta_{11}, \delta_{12}, \delta_{21}, \delta_{22} $ 组成的2*2矩阵，则利用上面梯度的式子和4个等式，我们可以分别写出 $\bigtriangledown a^{l-1}$ 的9个标量的梯度。</p>
<p>比如对于 $a_{11}$ 的梯度，由于在4个等式中 $a_{11}$ 只和 $z_{11}$ 有乘积关系，从而我们有：</p>
<script type="math/tex; mode=display">
\bigtriangledown a_{11} = w_{11}\delta_{11}</script><p>对于 $a_{12}$ 的梯度，由于在4个等式中 $a_{12}$ 和 $z_{11}, z_{12}$ 有乘积关系，从而我们有：</p>
<script type="math/tex; mode=display">
\bigtriangledown a_{12} = w_{11}\delta_{12} + w_{12}\delta_{11}</script><p>同理可得：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\bigtriangledown a_{13} &= w_{12}\delta_{12} \\
\bigtriangledown a_{21} &= w_{11}\delta_{21} + w_{21}\delta_{11} \\
\bigtriangledown a_{22} &= w_{11}\delta_{22} + w_{12}\delta_{21} + w_{21}\delta_{12} + w_{22}\delta_{11} \\
\bigtriangledown a_{23} &= w_{12}\delta_{22} + w_{22}\delta_{12} \\
\bigtriangledown a_{31} &= w_{21}\delta_{21} \\
\bigtriangledown a_{32} &= w_{21}\delta_{22} + w_{22}\delta_{21}\\
\bigtriangledown a_{33} &= w_{22}\delta_{22} \\
\end{aligned}
\end{equation}</script><p>这上面9个式子其实可以用一个矩阵卷积的形式表示，即：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    0 & 0 & 0 & 0 \\
    0 & \delta_{11} & \delta_{12} & 0 \\
    0 & \delta_{21} & \delta_{22} & 0 \\
    0 & 0 & 0 & 0
    \end{array}
\right) * 
\left(
    \begin{array}{cc}
    w_{22} & w_{21} \\
    w_{12} & w_{11} \\
    \end{array}
\right)
= 
\left(
    \begin{array}{cc}
    \bigtriangledown a_{11} & \bigtriangledown a_{12} & \bigtriangledown a_{13} \\
    \bigtriangledown a_{21} & \bigtriangledown a_{22} & \bigtriangledown a_{23} \\
    \bigtriangledown a_{31} & \bigtriangledown a_{32} & \bigtriangledown a_{33} 
    \end{array}
\right)</script><p>为了符合梯度计算，我们在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差。这个例子直观的介绍了为什么对含有卷积的式子反向传播时，卷积核要翻转180度的原因。</p>
<p>以上就是卷积层的误差反向传播过程。</p>
<h2 id="已知卷积层的-delta-l-，推导该层的-W-b-的梯度"><a href="#已知卷积层的-delta-l-，推导该层的-W-b-的梯度" class="headerlink" title="已知卷积层的 $\delta^l$，推导该层的 $W, b$ 的梯度"></a>已知卷积层的 $\delta^l$，推导该层的 $W, b$ 的梯度</h2><p>对于全连接层，可以按DNN的反向传播算法求该层 $W, b$ 的梯度，而池化层并没有 $W, b$ ,也不用求 $W, b$ 的梯度。只有卷积层的 $W, b$ 需要求出。</p>
<p>注意到卷积层 $z$ 和 $W, b$ 的关系为：</p>
<script type="math/tex; mode=display">
z^l = a^{l-1} * W^l + b</script><p>因此我们有：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W^l} = a^{l-1} * \delta^l</script><p>注意到此时卷积核并没有反转，主要是此时是层内的求导，而不是反向传播到上一层的求导。具体过程我们可以分析一下。</p>
<p>这里举一个简化的例子，这里输入是矩阵，不是张量，那么对于第 $l$ 层，某个卷积核矩阵 $W$ 的导数可以表示如下：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W_{pq}^l} = \sum_i \sum_j(\delta_{ij}^l a_{i+p-1, j+q-1}^{l-1})</script><p>　那么根据上面的式子，我们有：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W_{11}^l} = a_{11}\delta_{11} + a_{12}\delta_{12} + a_{21}\delta_{21} + a_{22}\delta_{22} \\

\frac {\partial J(W, b)} {\partial W_{12}^l} = a_{12}\delta_{11} + a_{13}\delta_{12} + a_{22}\delta_{21} + a_{23}\delta_{22} \\

\frac {\partial J(W, b)} {\partial W_{13}^l} = a_{13}\delta_{11} + a_{14}\delta_{12} + a_{23}\delta_{21} + a_{24}\delta_{22} \\
...... \\
\frac {\partial J(W, b)} {\partial W_{33}^l} = a_{33}\delta_{11} + a_{34}\delta_{12} + a_{43}\delta_{21} + a_{44}\delta_{22} \\</script><p>最终我们可以一共得到9个式子。整理成矩阵形式后可得：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W^l} = 
\left(
    \begin{array}{cccc}
    a_{11} & a_{12} & a_{13} & a_{14} \\
    a_{21} & a_{22} & a_{23} & a_{24} \\
    a_{31} & a_{32} & a_{33} & a_{34} \\
    a_{41} & a_{42} & a_{43} & a_{44}
    \end{array}
\right)
*
\left(
    \begin{array}{cccc}
    \delta_{11} & \delta_{12} \\
    \delta_{21} & \delta_{22}
    \end{array}
\right)</script><p>从而可以清楚的看到这次我们为什么没有反转的原因。</p>
<p>而对于 $b$，则稍微有些特殊，因为 $ \delta^l $ 是高维张量，而 $b$ 只是一个向量，不能像DNN那样直接和 $ \delta^l $ 相等。通常的做法是将 $ \delta^l $ 的各个子矩阵的项分别求和，得到一个误差向量，即为 $b$ 的梯度：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial b^l} = \sum_{u, v}(\delta^l)_{u, v}</script><h2 id="CNN反向传播算法总结"><a href="#CNN反向传播算法总结" class="headerlink" title="CNN反向传播算法总结"></a>CNN反向传播算法总结</h2><p>现在我们总结下CNN的反向传播算法，以最基本的批量梯度下降法为例来描述反向传播算法。</p>
<p>输入：m个图片样本，CNN模型的层数L和所有隐藏层的类型，对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。对于池化层，要定义池化区域大小k和池化标准（MAX或Average），对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。梯度学习率 $\alpha$,最大迭代次数MAX与停止迭代阈值 $\epsilon$</p>
<p>输出：CNN模型各隐藏层与输出层的 $W, b$</p>
<ol>
<li>初始化各隐藏层与输出层的各 $W, b$ 的值为一个随机值。</li>
<li>for iter to 1 to MAX:<ol>
<li>for i =1 to m：<ol>
<li>将CNN输入 $a^1$ 设置为 $x_i$ 对应的张量</li>
<li>for l = 2 to L-1，根据下面3种情况进行前向传播算法计算：<ul>
<li>如果当前是全连接层：则有 $a^{i, l} = \sigma(z^{i, l}) = \sigma(W^l a^{i, l-1} + b^l)  $</li>
<li>如果当前是卷积层：则有 $a^{i, l} = \sigma(z^{i, l}) = \sigma(W^l * a^{i, l-1} + b^l)  $</li>
<li>如果当前是池化层：则有 $a^{i, l} = pool(a^{i, l-1})$</li>
</ul>
</li>
<li>对于输出层第L层：$a^{i, L} = softmax(z^{i, L}) = softmax(W^L a^{i, L-1} + b^L)$</li>
<li>通过损失函数计算输出层的 $\delta^{i, L}$</li>
<li>for l = L-1 to 2, 根据下面3种情况进行进行反向传播算法计算：<ul>
<li>如果当前是全连接层：$ \delta^{i, l} = (W^{l+1})^T \delta^{i, l+1} \odot \sigma’(z^{i, l}) $</li>
<li>如果当前是卷积层：$ \delta^{i, l} = \delta^{i, l+1} * rot180(W^{l+1}) \odot \sigma’(z^{i, l}) $</li>
<li>如果当前是池化层：$ \delta^{i, l} = upsample(\delta^{i, l+1}) \odot \sigma’(z^{i, l}) $</li>
</ul>
</li>
</ol>
</li>
<li>for l = 2 to L，根据下面2种情况更新第 $l$ 层的 $W^l, b^l$：<ul>
<li>如果当前是全连接层：$W^l = W^l - \alpha \sum_{i=1}^m \delta^{i, l}(a^{i, l-1})^T, b^l = b^l - \alpha \sum_{i=1}^m \delta^{i, l} $</li>
<li>如果当前是卷积层，对于每一个卷积核有：$ W^l = W^l - \alpha \sum_{i=1}^m \delta^{i, l} * a^{i, l-1}, b^l = b^l - \alpha \sum_{i=1}^m \sum_{u, v}(\delta^{i, l})_{u, v} $</li>
</ul>
</li>
<li>如果所有的 $W, b$ 的变化值都小于停止迭代阈值 $\epsilon$，则跳出迭代循环到步骤3。</li>
</ol>
</li>
<li>输出各隐藏层与输出层的线性关系系数矩阵 $W$ 和偏置量 $b$ 。</li>
</ol>
<hr>
<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6494810.html?tdsourcetag=s_pcqq_aiomsg" >卷积神经网络(CNN)反向传播算法<i class="fas fa-external-link-alt"></i></a></li>
</ul>

        </div>

        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/Algorithm/">#Algorithm</a>&nbsp;
                    </li>
                
                    <li class="tag-item">
                        <a href="/tags/Neural-Networks/">#Neural Networks</a>&nbsp;
                    </li>
                
            </ul>
        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2019/06/22/SVM/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">SVM</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2019/06/02/CNN/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">CNN</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2017</span>
              -
            
            2022&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Swift</a>
        </div>
        
            <script async  src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
                    <span id="busuanzi_container_site_pv">
                        Totalview&nbsp;<span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B7%B2%E7%9F%A5%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84-delta-l-%EF%BC%8C%E6%8E%A8%E5%AF%BC%E4%B8%8A%E4%B8%80%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84-delta-l-1"><span class="nav-number">1.</span> <span class="nav-text">已知池化层的 $\delta^l$，推导上一隐藏层的 $\delta^{l-1 }$</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.1.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B7%B2%E7%9F%A5%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84-delta-l-%EF%BC%8C%E6%8E%A8%E5%AF%BC%E4%B8%8A%E4%B8%80%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84-delta-l-1"><span class="nav-number">1.2.</span> <span class="nav-text">已知卷积层的 $\delta^l$，推导上一层隐藏层的 $\delta^{l-1}$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B7%B2%E7%9F%A5%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84-delta-l-%EF%BC%8C%E6%8E%A8%E5%AF%BC%E8%AF%A5%E5%B1%82%E7%9A%84-W-b-%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="nav-number">1.3.</span> <span class="nav-text">已知卷积层的 $\delta^l$，推导该层的 $W, b$ 的梯度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93"><span class="nav-number">1.4.</span> <span class="nav-text">CNN反向传播算法总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AC%E8%BD%BD"><span class="nav-number">1.5.</span> <span class="nav-text">转载</span></a></li></ol></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>

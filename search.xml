<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2022年新高考1卷17题解析</title>
    <url>/2022/06/09/2022%E5%B9%B4%E6%96%B0%E9%AB%98%E8%80%831%E5%8D%B717%E9%A2%98%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>听说今年的高考数学题很难，今天有空看了几题，以17题练个手。廉颇老矣，尚能饭否？！</p>
<span id="more"></span>
<img   src="/2022/06/09/2022%E5%B9%B4%E6%96%B0%E9%AB%98%E8%80%831%E5%8D%B717%E9%A2%98%E8%A7%A3%E6%9E%90/17.png"  class="17">
<h2 id="1"><a href="#1" class="headerlink" title="(1)"></a>(1)</h2><script type="math/tex; mode=display">
\begin{aligned}
C_n &= \frac{S_n}{a_n} \\
C_1 &= \frac{a_1}{a_1} = 1 \\
C_n &= 1+(n-1) \frac{1}{3} = \frac{n+2}{3} \\
S_n &= \frac{n+2}{3} a_n \\
S_{n-1} &= \frac{n+1}{3} a_{n-1} \\
S_n - S_{n-1} &= \frac{n+2}{3} a_n - \frac{n+1}{3}  a_{n-1} \\
a_n &=  \frac{n+2}{3}  a_n - \frac{n+1}{3}  a_{n-1} \\
3a_n &= (n+2)a_n - (n+1)a_{n-1} \\
(n-1)a_{n} &= (n+1)a_{n-1} \\
\frac{a_{n}}{a_{n-1}} &= \frac{n+1}{n-1} \\
\frac{a_{n-1}}{a_{n-2}} &= \frac{n}{n-2} \\
\dots \\
\frac{a_{2}}{a_{1}} &= \frac{3}{1} \\

\frac{a_{n}}{a_{n-1}} \cdot \frac{a_{n-1}}{a_{n-2}} \cdot \frac{a_{2}}{a_{1}} &= \frac{n+1}{n-1} \cdot \frac{n}{n-2} \cdots \frac{3}{1} \\
a_n &= \frac{\frac{(n+1)!}{n!}}{\frac{2}{n}} \\
&= \frac{n(n+1)}{2}

\end{aligned}</script><h2 id="2"><a href="#2" class="headerlink" title="(2)"></a>(2)</h2><script type="math/tex; mode=display">
\begin{aligned}
\frac{1}{a_n} &= \frac{2}{n(n+1)} \\
&= 2(\frac{1}{n} - \frac{1}{n+1}) \\

\frac{1}{a_1} + \cdots + \frac{1}{a_n} &= 2(1-\frac{1}{2} + \frac{1}{2}-\frac{1}{3} + \cdots + \frac{1}{n} - \frac{1}{n+1}) < 2

\end{aligned}</script>]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>高考</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>A BetterTransformer for Fast Transformer Inference</title>
    <url>/2022/10/09/A-BetterTransformer-for-Fast-Transformer-Inference/</url>
    <content><![CDATA[<p>PyTorch 1.12版本对 <code>torch.nn.TransformerEncoder</code> 进行了专项优化，用户无须调整模型结构即可大幅提升Transformer推理性能。具体介绍如下：</p>
<span id="more"></span>
<h2 id="性能提升"><a href="#性能提升" class="headerlink" title="性能提升"></a>性能提升</h2><p>BetterTransformer在CPU和GPU上都能获得加速，官方主要实现了两种优化：</p>
<ul>
<li>混合内核并结合多个独立算子来更高效地实现Transformer</li>
<li>利用输入的稀疏性来避免在padding上的冗余运算</li>
</ul>
<h2 id="向后兼容"><a href="#向后兼容" class="headerlink" title="向后兼容"></a>向后兼容</h2><p>用户无须调整之前PyTorch版本的代码，安装新的1.12版本即可。</p>
<h2 id="使用条件"><a href="#使用条件" class="headerlink" title="使用条件"></a>使用条件</h2><ul>
<li>模型使用了 <code>TransformerEncoder</code>、<code>TransformerEncoderLayer</code>、 <code>MultiheadAttention</code> 模块</li>
<li>必须在推理场景下：<ul>
<li><code>model.eval()</code></li>
<li><code>torch.no_grad</code></li>
</ul>
</li>
<li>为了对输入稀疏性优化，在实例化 <code>TransformerEncoder</code> 并传入 <code>src_key_padding_mask</code> 的时候，设置 <code>enable_nested_tensor</code></li>
</ul>
<h3 id="加速效果"><a href="#加速效果" class="headerlink" title="加速效果"></a>加速效果</h3><img   src="/2022/10/09/A-BetterTransformer-for-Fast-Transformer-Inference/f1.jpg"  class="f1">
<p>该实验加速受益于算子优化。</p>
<img   src="/2022/10/09/A-BetterTransformer-for-Fast-Transformer-Inference/f2.jpg"  class="f2">
<p>随着数据量的加大，算子优化提升有限，此时开启稀疏优化可以极大提升模型性能。</p>
<h3 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h3><p><a class="link"   href="https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html" >FAST TRANSFORMER INFERENCE WITH BETTER TRANSFORMER<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a>未来工作</h2><ul>
<li>将推理优化扩展到训练优化</li>
<li>将编码器优化扩展到解码器优化</li>
<li>将BetterTransformer应用到FairSeq、HuggingFace等库</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference//" >A BetterTransformer for Fast Transformer Inference<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGB：用扩散模型颠覆传统自动出价范式</title>
    <url>/2026/02/24/AIGB%EF%BC%9A%E7%94%A8%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E9%A2%A0%E8%A6%86%E4%BC%A0%E7%BB%9F%E8%87%AA%E5%8A%A8%E5%87%BA%E4%BB%B7%E8%8C%83%E5%BC%8F/</url>
    <content><![CDATA[<p>这是阿里巴巴在广告自动出价的一篇工作，用扩散模型颠覆了传统深度rl出价的范式，取得了线上线下的巨大收益。</p>
<span id="more"></span>
<h2 id="一、自动出价问题定义"><a href="#一、自动出价问题定义" class="headerlink" title="一、自动出价问题定义"></a>一、自动出价问题定义</h2><p>在广告竞价中，广告主需要在有限预算下，为每个展示机会出价，最大化总价值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underset{o_i}{\text{maximize}} & \sum_i o_i v_i \\
\text{s.t.} & \sum_i o_i c_i \leq B \\
& \frac{\sum_i c_{ij}o_i}{\sum_i p_{ij}o_i} \leq C_j, \quad \forall j \\
& o_i \in \{0,1\}, \forall i
\end{aligned}</script><p>理论上的最优出价形式为：</p>
<script type="math/tex; mode=display">
b_i^* = \lambda_0 v_i + \sum_{j=1}^J \lambda_j p_{ij}</script><p>其中 $\lambda_j$ 是需要动态调整的出价参数。</p>
<h2 id="二、传统DRL出价-VS-AIGB"><a href="#二、传统DRL出价-VS-AIGB" class="headerlink" title="二、传统DRL出价 VS AIGB"></a>二、传统DRL出价 VS AIGB</h2><p><strong>马尔可夫假设的问题</strong>：传统RL假设下一状态只取决于当前状态和动作：</p>
<script type="math/tex; mode=display">P(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},...) = P(s_{t+1}|s_t,a_t)</script><p><strong>但论文的统计分析发现</strong>：随着历史序列长度增加，与下一状态的相关系数显著上升。</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.9rjxqqfgz6.webp"  alt="correlation"></p>
<p>这说明<strong>历史信息</strong>对预测未来状态很重要，但MDP假设丢弃了这些信息。</p>
<p>针对DRL的缺陷，AIGB直接建模总收益与整个状态轨迹的关联性：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方面</th>
<th>传统RL的MDP缺陷</th>
<th>AIGB的全局建模如何解决</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>状态转移假设</strong></td>
<td>只依赖当前状态</td>
<td>建模整个轨迹分布</td>
</tr>
<tr>
<td><strong>长期依赖</strong></td>
<td>误差累积</td>
<td>一次生成整个序列</td>
</tr>
<tr>
<td><strong>稀疏回报</strong></td>
<td>难以学习</td>
<td>直接以最终收益为条件</td>
</tr>
<tr>
<td><strong>环境随机性</strong></td>
<td>单步预测不稳</td>
<td>全局模式更鲁棒</td>
</tr>
<tr>
<td><strong>约束满足</strong></td>
<td>难控制</td>
<td>条件生成保证</td>
</tr>
</tbody>
</table>
</div>
<h2 id="三、AIGB范式：从”逐步决策”到”全局生成”"><a href="#三、AIGB范式：从”逐步决策”到”全局生成”" class="headerlink" title="三、AIGB范式：从”逐步决策”到”全局生成”"></a>三、AIGB范式：从”逐步决策”到”全局生成”</h2><h3 id="3-1-核心思想转变"><a href="#3-1-核心思想转变" class="headerlink" title="3.1 核心思想转变"></a>3.1 核心思想转变</h3><div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>传统RL</th>
<th>AIGB（全局生成）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>建模对象</strong></td>
<td>单步转移 $P(s_{t+1} \mid s_t,a_t)$</td>
<td>整个轨迹 $p(x_0(\tau) \mid y(\tau))$</td>
</tr>
<tr>
<td><strong>优化目标</strong></td>
<td>最大化累计奖励</td>
<td>最大化条件似然</td>
</tr>
<tr>
<td><strong>决策方式</strong></td>
<td>逐步决策（online）</td>
<td>全局规划后执行(Planning&amp;Control)</td>
</tr>
</tbody>
</table>
</div>
<h3 id="3-2-整体框架"><a href="#3-2-整体框架" class="headerlink" title="3.2 整体框架"></a>3.2 整体框架</h3><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.2vfa6jytaj.webp"  alt="model"></p>
<ol>
<li><strong>Planning生成整条轨迹</strong>：用扩散模型生成整个未来状态轨迹</li>
<li><strong>Control生成出价动作</strong>：用逆动力学模型反推出当前动作，逼近规划轨迹</li>
</ol>
<h2 id="四、DiffBid：扩散出价模型详解"><a href="#四、DiffBid：扩散出价模型详解" class="headerlink" title="四、DiffBid：扩散出价模型详解"></a>四、DiffBid：扩散出价模型详解</h2><h3 id="4-1-问题建模"><a href="#4-1-问题建模" class="headerlink" title="4.1 问题建模"></a>4.1 问题建模</h3><p>将自动出价建模为条件概率问题：</p>
<script type="math/tex; mode=display">
\max_{\theta}\mathbb{E}_{\tau\sim D}[\log p_{\theta}(x_0(\tau)|\boldsymbol{y}(\tau))]</script><p>其中：</p>
<ul>
<li>$x_0(\tau)$：完整状态轨迹 $[s_1, s_2, …, s_T]$，$s_t$包含剩余预算、预算消耗速度等等</li>
<li>$\boldsymbol{y}(\tau)$：轨迹属性，包含总收益、约束条件等等</li>
</ul>
<h3 id="4-2-扩散过程设计"><a href="#4-2-扩散过程设计" class="headerlink" title="4.2 扩散过程设计"></a>4.2 扩散过程设计</h3><h4 id="前向加噪"><a href="#前向加噪" class="headerlink" title="前向加噪"></a>前向加噪</h4><script type="math/tex; mode=display">
q(x_k(\tau)|x_{k-1}(\tau)) = \mathcal{N}(x_k(\tau);\sqrt{1-\beta_k}x_{k-1}(\tau),\beta_k I)</script><h4 id="反向去噪"><a href="#反向去噪" class="headerlink" title="反向去噪"></a>反向去噪</h4><ol>
<li>预测噪音：<script type="math/tex; mode=display">
\hat{\epsilon}_k = \epsilon_{\theta}(x_k(\tau),k) + \omega (\epsilon_{\theta}(x_k(\tau),\boldsymbol{y}(\tau),k) - \epsilon_{\theta}(x_k(\tau),k))</script></li>
<li>去噪生成下一状态轨迹：<script type="math/tex; mode=display">
\boldsymbol{x}_{k-1}(\tau) \sim \mathcal{N}\left(\boldsymbol{x}_{k-1}(\tau) \mid \boldsymbol{\mu}_\theta\left(x_k(\tau), \boldsymbol{y}(\tau), k\right), \Sigma_\theta\left(\boldsymbol{x}_k(\tau), k\right)\right)</script></li>
</ol>
<h3 id="4-3-逆动力学：从未来状态反推动作"><a href="#4-3-逆动力学：从未来状态反推动作" class="headerlink" title="4.3 逆动力学：从未来状态反推动作"></a>4.3 逆动力学：从未来状态反推动作</h3><script type="math/tex; mode=display">
\hat{\boldsymbol{a}}_t = f_{\phi}(s_{t-L:t}, s_{t+1}')</script><p>根据历史状态和预测的下一个目标状态，直接生成当前应采取的最优出价动作，即$\lambda_0, \lambda_1, \dots,\lambda_J$</p>
<h3 id="4-4-训练loss"><a href="#4-4-训练loss" class="headerlink" title="4.4 训练loss"></a>4.4 训练loss</h3><script type="math/tex; mode=display">
\mathcal{L}(\theta,\phi) = \mathbb{E}_{k,\tau \in \mathcal{D}}\left[||\epsilon -\epsilon_{\theta}(x_k(\tau),\boldsymbol{y}(\tau),k)||^2\right] + \mathbb{E}_{(s_{t-L:t},a_t,s_{t+1}')\in \mathcal{D}}\left[||\boldsymbol{a}_t - f_{\phi}(s_{t-L:t},s_{t+1}')||^2\right]</script><h3 id="4-5-线上推理流程"><a href="#4-5-线上推理流程" class="headerlink" title="4.5 线上推理流程"></a>4.5 线上推理流程</h3><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.60us5huy4x.webp"  alt="inference"></p>
<ul>
<li>每次时间步$t$<strong>重新生成整个未来轨迹</strong>，即$t-1$生成的$x_0(\tau)$与$t$生成的$x_0(\tau)$无关</li>
<li>根据历史状态和预测下一状态，用idm来生成出价动作</li>
<li>每步解码都注入历史状态保证已发生的不变</li>
</ul>
<h2 id="五、实验结果"><a href="#五、实验结果" class="headerlink" title="五、实验结果"></a>五、实验结果</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.8l0mi4wchh.webp"  alt="experiment"><br>DiffBid在各数据集上都取得了sota，并大幅领先所有baseline。</p>
<h2 id="六、FAQ"><a href="#六、FAQ" class="headerlink" title="六、FAQ"></a>六、FAQ</h2><ol>
<li>AIGB是根据马尔科夫假设单步去噪的，它是如何体现全局建模的？</li>
</ol>
<p>实际上这两者并不冲突，每步diffusion去噪是生成整个状态轨迹。而全局建模是指在单条状态轨迹中，所有历史状态均对最终收益产生直接影响，有如下两点体现：</p>
<ul>
<li>建模MLE：$\max_{\theta}\mathbb{E}_{\tau\sim D}[\log p_{\theta}(x_0(\tau)|\boldsymbol{y}(\tau))]$</li>
<li>历史状态和预测状态影响出价动作：$\hat{\boldsymbol{a}}_t = f_{\phi}(s_{t-L:t}, s_{t+1}’)$</li>
</ul>
<ol>
<li>diffusion需要多步去噪，线上RT高如何解决？<br><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.3rbrm0ep1e.webp"  alt="online"></li>
</ol>
<p>论文里也提到了这个问题，推理耗时与去噪步数成正比。对于文生图模型，为确保图片质量，步数会非常大，但对于出价问题，较小的步数已经能保证较好的实验效果，且自动出价对实时性要求不高。</p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
        <tag>Bidding</tag>
      </tags>
  </entry>
  <entry>
    <title>AUC &amp; GAUC</title>
    <url>/2023/07/27/AUC-GAUC/</url>
    <content><![CDATA[<p>这两种指标常用于衡量模型性能的好坏。</p>
<span id="more"></span>
<h2 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h2><p>AUC用于表达模型区分正负样本的能力，即正样本大于负样本的概率。</p>
<p>假设有 $M$ 个正样本，$N$ 个负样本，将模型对他们的预测概率从小到大排序，$rank_i$ 表示第 $i$ 个正样本的排序序号，那么比它还小的负样本个数为 $rank_i - i$，则AUC计算公式如下：</p>
<script type="math/tex; mode=display">
AUC=\frac{\sum_{i \in \text { 正样本集合 }} \operatorname{rank}_i-\frac{M(1+M)}{2}}{M \times N}</script><h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h4><p>如果有多个正样本得分相等，那么正样本数保持不变，每一个相等auc的正样本的rank就取它们的平均值。具体示例见：<a class="link"   href="https://blog.csdn.net/pearl8899/article/details/126129148" >AUC的两种计算方式<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="GAUC"><a href="#GAUC" class="headerlink" title="GAUC"></a>GAUC</h2><p>AUC反应了模型整体的排序能力，但在搜广推领域，例如CTR是以用户维度，衡量单个用户对广告的点击预测。用户之间差异大，比如网赚用户点击率高，高价值人群点击率低。</p>
<p>因此，阿里团队提出了新指标GAUC：</p>
<script type="math/tex; mode=display">
\mathrm{GAUC}=\frac{\sum_{i=1}^n w_i * \mathrm{AUC}_i}{\sum_{i=1}^n w_i}=\frac{\sum_{i=1}^n \text { impression }_i * \mathrm{AUC}_i}{\sum_{i=1}^n \text { impression }_i}</script><p>$n$ 表示用户数，$w_i$ 可以是该用户的展现数或者点击数，$AUC_i$ 表示模型在该用户上的AUC表现。</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>有甲和乙两个用户，共有5个样本，其中+表示正样本，-表示负样本，我们把5个样本按照模型A预测的score从小到大排序，得到 甲-，甲+，乙-，甲+，乙+；那假如有另一个模型B，把这5个样本根据score从小到大排序后，得到 甲-，甲+，甲+，乙-，乙+</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th style="text-align:center">样本1</th>
<th style="text-align:center">样本2</th>
<th style="text-align:center">样本3</th>
<th style="text-align:center">样本4</th>
<th style="text-align:center">样本5</th>
<th style="text-align:center">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型A</td>
<td style="text-align:center">甲-</td>
<td style="text-align:center">甲+</td>
<td style="text-align:center">乙-</td>
<td style="text-align:center">甲+</td>
<td style="text-align:center">乙+</td>
<td style="text-align:center">$\frac{1+2+2}{2*3} = 0.833$</td>
</tr>
<tr>
<td>模型B</td>
<td style="text-align:center">甲-</td>
<td style="text-align:center">甲+</td>
<td style="text-align:center">甲+</td>
<td style="text-align:center">乙-</td>
<td style="text-align:center">乙+</td>
<td style="text-align:center">$\frac{1+1+2}{2*3} = 0.667$</td>
</tr>
</tbody>
</table>
</div>
<p>单看AUC，模型A的表现优于模型B。但是从实际情况来看：</p>
<ul>
<li>对于用户甲，模型A的$AUC_{甲}=\frac{1+1}{2}=1, AUC_{乙}=\frac{1}{1}=1$</li>
<li>对于用户甲，模型B的$AUC_{甲}=\frac{1+1}{2}=1, AUC_{乙}=\frac{1}{1}=1$</li>
</ul>
<p>所以从实际情况来看，模型B的效果和模型A应该是一样好的，这和实际的auc的结果矛盾。</p>
<p>每个用户的广告列表是个性化的，不同用户的排序结果不好直接比较，这可能导致全局auc并不能反映真实情况，应采用GAUC。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/88708071" >AUC 和 gauc ks<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.zhihu.com/question/39840928/answer/241440370" >如何理解机器学习和统计中的AUC？ - 无涯的回答<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>AUC</tag>
        <tag>GAUC</tag>
      </tags>
  </entry>
  <entry>
    <title>AdaBoost</title>
    <url>/2021/07/08/AdaBoost/</url>
    <content><![CDATA[<p><code>AdaBoost</code> 是属于 <code>boosting</code> 的一种经典算法。</p>
<span id="more"></span>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><img   src="/2021/07/08/AdaBoost/1.png"  class="">
<p>AdaBoost算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目 $T$ ，最终将这 $T$ 个弱学习器通过集合策略进行整合，得到最终的强学习器。</p>
<h2 id="AdaBoost分类算法流程"><a href="#AdaBoost分类算法流程" class="headerlink" title="AdaBoost分类算法流程"></a>AdaBoost分类算法流程</h2><p>输入样本集 $\boldsymbol{T}=\left\{\left(x, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots\left(x_{m}, y_{m}\right)\right\}$ ，类别为$\{ -1, +1 \}$，弱分类器迭代次数 $K$。输出为最终的强分类器 $f(x)$ 。</p>
<ol>
<li><p>初始化样本集权重为：</p>
<script type="math/tex; mode=display">
D(1)=\left(w_{11}, w_{12}, \ldots w_{1 m}\right) ; \quad w_{1 i}=\frac{1}{m} ; \quad i=1,2 \ldots m</script></li>
<li><p>对于 $k=1, 2, \dots, K$ ：</p>
<ol>
<li><p>使用具有权重 $D_k$ 的样本集来训练数据，得到弱分类器 $G_k(x)$</p>
</li>
<li><p>计算 $G_k(x)$ 的分类误差率：</p>
<script type="math/tex; mode=display">
e_{k}=P\left(G_{k}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{m} w_{k i} I\left(G_{k}\left(x_{i}\right) \neq y_{i}\right)</script></li>
<li><p>计算弱分类器的权重系数：</p>
<script type="math/tex; mode=display">
\alpha_{k}=\frac{1}{2} \log \frac{1-e_{k}}{e_{k}}</script><p>从上式可以看出，如果分类误差率 $e_k$ 越大，则对应的弱分类器权重系数 $\alpha_{k}$ 越小。即误差率小的弱分类器权重系数越大。</p>
</li>
<li><p>更新样本集的权重分布：</p>
<script type="math/tex; mode=display">
w_{k+1, i}=\frac{w_{k i}}{Z_{K}} \exp \left(-\alpha_{k} y_{i} G_{k}\left(x_{i}\right)\right) \quad i=1,2, \ldots m</script><p>其中 $Z_K$ 是归一化因子：</p>
<script type="math/tex; mode=display">
Z_{k}=\sum_{i=1}^{m} w_{k i} \exp \left(-\alpha_{k} y_{i} G_{k}\left(x_{i}\right)\right)</script><p>从上式可以看出，如果第 $i$ 个样本分类错误，则 $y_{i} G_{k} &lt; 0$ ，导致样本的权重在第 $k+1$ 个弱分类器中增大；如果分类正确，则权重在第 $k+1$ 个弱分类器中减少。</p>
</li>
</ol>
</li>
<li><p>构建最终分类器：</p>
<script type="math/tex; mode=display">
f(x)=\operatorname{sign}\left(\sum_{k=1}^{K} \alpha_{k} G_{k}(x)\right)</script></li>
</ol>
<p>对于Adaboost多元分类算法，其原理和二元分类类似。最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数：</p>
<script type="math/tex; mode=display">
\alpha_{k}=\frac{1}{2} \log \frac{1-e_{k}}{e_{k}}+\log (R-1)</script><p>其中 $R$ 为类别数。如果 $R=2$ ，那么上式即是二分类的弱分类器系数。</p>
<h2 id="AdaBoost回归算法流程"><a href="#AdaBoost回归算法流程" class="headerlink" title="AdaBoost回归算法流程"></a>AdaBoost回归算法流程</h2><p>输入样本集 $\boldsymbol{T}=\left\{\left(x, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots\left(x_{m}, y_{m}\right)\right\}$ ，弱分类器迭代次数 $K$。输出为最终的强分类器 $f(x)$ 。</p>
<ol>
<li><p>初始化样本集权重为：</p>
<script type="math/tex; mode=display">
D(1)=\left(w_{11}, w_{12}, \ldots w_{1 m}\right) ; \quad w_{1 i}=\frac{1}{m} ; \quad i=1,2 \ldots m</script></li>
<li><p>对于 $k=1, 2, \dots, K$ ：</p>
<ol>
<li><p>使用具有权重 $D_k$ 的样本集来训练数据，得到弱分类器 $G_k(x)$</p>
</li>
<li><p>计算训练集上的最大误差：</p>
<script type="math/tex; mode=display">
E_{k}=\max \left|y_{i}-G_{k}\left(x_{i}\right)\right| i=1,2 \ldots m</script></li>
<li><p>计算每个样本的相对误差：</p>
<ul>
<li>线性误差：$e_{k i}=\frac{\left|y_{i}-G_{k}\left(x_{i}\right)\right|}{E_{k}}$</li>
<li>平方误差：$e_{k i}=\frac{(y_{i}-G_{k}(x_{i}))^2}{E_{k}^2}$</li>
<li>指数误差：$e_{k i}=1-\exp \left(\frac{-\left|y_{i}-G_{k}\left(x_{i}\right)\right|}{E_{k}}\right)$</li>
</ul>
</li>
<li><p>计算回归误差率：</p>
<script type="math/tex; mode=display">
e_k = \sum_{i=1}^m w_{ki} e_{ki}</script></li>
<li><p>计算弱学习器的权重系数：</p>
<script type="math/tex; mode=display">
a_k = \frac{1-e_k}{e_k}</script></li>
<li><p>更新样本集的权重分布：</p>
<script type="math/tex; mode=display">
w_{k+1, i}=\frac{w_{k i}}{Z_{k}} \alpha_{k}^{1-e_{k i}}</script><p>其中 $Z_K$ 是归一化因子：</p>
<script type="math/tex; mode=display">
Z_{k}=\sum_{i=1}^{m} w_{k i} \alpha_{k}^{1-e_{k i}}</script></li>
</ol>
</li>
<li><p>构建最终强学习器：</p>
<script type="math/tex; mode=display">
f(x)=G_{k^{*}}(x)</script></li>
</ol>
<p>即取所有 $ ln \frac{1}{\alpha_{k}}, k=1,2, \ldots . K$ 的中位数值对于序号 $k^{*}$ 对应的弱学习器。</p>
<h2 id="AdaBoost正则化"><a href="#AdaBoost正则化" class="headerlink" title="AdaBoost正则化"></a>AdaBoost正则化</h2><p>为了防止AdaBoost过拟合，我们加入正则化项：</p>
<script type="math/tex; mode=display">
f_{k}(x)=f_{k-1}(x)+v \alpha_{k} G_{k}(x), \quad 0 < v <= 1</script><p>与GBDT类似，该正则化项称作学习率。对于同样的训练集学习效果，较小的 $v$ 意味着我们需要更多的弱学习器的迭代次数。</p>
<h2 id="AdaBoost总结"><a href="#AdaBoost总结" class="headerlink" title="AdaBoost总结"></a>AdaBoost总结</h2><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>Adaboost作为分类器时，分类精度很高</li>
<li>在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活</li>
<li>不容易发生过拟合</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/px_528/article/details/72963977" >Adaboost入门教程——最通俗易懂的原理介绍（图文实例）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/6133937.html" >集成学习之Adaboost算法原理小结<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>An Embarrassingly Easy but Strong Baseline for Nested Named Entity Recognition</title>
    <url>/2022/09/20/An-Embarrassingly-Easy-but-Strong-Baseline-for-Nested-Named-Entity-Recognition/</url>
    <content><![CDATA[<p>现有片段分类方法采用Biaffine Decoder得到一个评分矩阵 $n \times n \times T$ ，$n$ 表示序列长度，$T$ 表示实体类别数量。本文沿用了片段分类的方法，但作者发现评分矩阵的临近片段具有非常明显的空间关联，如下图所示：</p>
<span id="more"></span>
<img   src="/2022/09/20/An-Embarrassingly-Easy-but-Strong-Baseline-for-Nested-Named-Entity-Recognition/example.png"  class="example">
<p>作者将评分矩阵抽象成了一张图像，维度为 $n \times n \times r$ ，$r$ 相当于通道数。然后利用CNN来建模空间依赖关系：</p>
<img   src="/2022/09/20/An-Embarrassingly-Easy-but-Strong-Baseline-for-Nested-Named-Entity-Recognition/model.jpg"  class="model">
<p>该方法十分简单有效，在三个数据集上都略胜sota一筹：</p>
<img   src="/2022/09/20/An-Embarrassingly-Easy-but-Strong-Baseline-for-Nested-Named-Entity-Recognition/table1.png"  class="table1">
<img   src="/2022/09/20/An-Embarrassingly-Easy-but-Strong-Baseline-for-Nested-Named-Entity-Recognition/table2.png"  class="table2">
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/z8CgjK57nN4bFlKa-UUQWg" >​复旦大学邱锡鹏组：CNN-NER——极其简单有效的嵌套命名实体识别方法<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://arxiv.org/abs/2208.04534" >An Embarrassingly Easy but Strong Baseline for Nested Named Entity Recognition<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/yhcc/CNN_Nested_NER" >yhcc/CNN_Nested_NER<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention Sink</title>
    <url>/2024/05/12/Attention-Sink/</url>
    <content><![CDATA[<p>论文发现自回归LLM存在的一个有趣现象：对于输入文本最靠前的少量几个token，无论它们在语义上与语言建模任务的相关性如何，大量的注意力分数都会分配给他们，如下图所示：</p>
<span id="more"></span>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.8dwop86zjw.png"  alt="Sink"></p>
<p>模型的前两层还能保持attention score更多分配给当前token附近位置的特性，而在其他层，靠前的几个token都会接受到大量的注意力。尽管这些token在语义上很可能并没有什么重要性，但它们却聚集了大量的注意力分数。</p>
<p>出现这个现象的原因就是softmax操作。softmax要求所有上下文token的注意力分数加起来等于1。因此，即使当前token跟前面的其他token都没有语义相关性，模型仍然需要将多余的注意力值分配到前面的某些token，以使得总和为1。</p>
<p>为什么最开头的几个初始token就会承担“接收多余的、不需要的注意力”的任务？最简单的原因就是，对于自回归语言建模，初始token对所有后续token都是可见的，这使得它们更容易被训练成attention sink。</p>
<p>上面这个解释还只是猜想，于是论文做了一个实验来验证这个猜想：把初始的4个token都换成没有重要实际语义的换行符号<code>\n</code>，结果发现模型依然会把大量的注意力分配给这些token，这就说明attention sink这个现象和内容语义无关，而只和这些token所在的位置相关。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/ogrWe61JZz64FXTcBfVN5Q" >大模型推理窗口-从有限到无限大<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT</title>
    <url>/2019/07/28/BERT/</url>
    <content><![CDATA[<p><code>BERT</code> 的两阶段如下所示：</p>
<span id="more"></span>
<img   src="/2019/07/28/BERT/1.png"  class="">
<h2 id="Comparision-Of-Models"><a href="#Comparision-Of-Models" class="headerlink" title="Comparision Of Models"></a>Comparision Of Models</h2><img   src="/2019/07/28/BERT/2.png"  class="">
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" >A Neural Probabilistic Language Model<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://arxiv.org/pdf/1810.04805.pdf" >BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT、RoBerta、XLNet、ALBERT对比</title>
    <url>/2021/03/25/BERT%E3%80%81RoBerta%E3%80%81XLNet%E3%80%81ALBERT%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>记录一下BERT变体的比较。</p>
<span id="more"></span>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>BERT堆叠了多层Transformer的Encoder模块，设计了两个任务来完成预训练：</p>
<ul>
<li>Masked LM：随机mask掉15%的token，其中80%替换为[MASK]，10%替换为其它token，10%保留原单词。</li>
<li>Next Sentence Prediction(NSP)：从训练集中抽取A和B句，50%为A的下一句，50%为其它句子。</li>
</ul>
<h2 id="RoBerta"><a href="#RoBerta" class="headerlink" title="RoBerta"></a>RoBerta</h2><h4 id="静态Mask-VS-动态Mask"><a href="#静态Mask-VS-动态Mask" class="headerlink" title="静态Mask VS 动态Mask"></a>静态Mask VS 动态Mask</h4><ul>
<li>静态Mask：BERT对每一个序列随机选择15%的tokens替换成[MASK]，而一旦被选中，之后的N个epoch就不能再改变。</li>
<li>动态Mask：RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Mask，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。</li>
</ul>
<h4 id="NSP-VS-w-o-NSP"><a href="#NSP-VS-w-o-NSP" class="headerlink" title="NSP VS w/o NSP"></a>NSP VS w/o NSP</h4><p>RoBerta去除了NSP任务，每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL - SENTENCES），而原来的Bert每次只输入两个句子。</p>
<h4 id="hyper-parameter"><a href="#hyper-parameter" class="headerlink" title="hyper-parameter"></a>hyper-parameter</h4><ul>
<li>更大的batch_size</li>
<li>更多的数据</li>
<li>更高的学习率</li>
<li>更长时间的训练</li>
</ul>
<h2 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h2><p><strong>AR LM</strong>：利用上下文单词预测下一个单词的一种模型。但是在这里，上下文单词被限制在两个方向，要么向前，要么向后。</p>
<p><strong>AE LM</strong>：从损坏的输入中重建原始数据的一种模型。它可以同时在向前向后两个方向看到上下文。</p>
<p>BERT存在的问题：</p>
<ul>
<li>掩码导致的微调差异：预训练阶段因为采取引入[Mask]标记来Mask掉部分单词的训练模式，而Fine-tuning阶段是看不到这种被强行加入的Mask标记的，所以两个阶段存在使用模式不一致的情形，这可能会带来一定的性能损失。</li>
<li>预测的标记彼此独立：Bert在第一个预训练阶段，假设句子中多个单词被Mask掉，这些被Mask掉的单词之间没有任何关系，是条件独立的，而有时候这些单词之间是有关系的，XLNet则考虑了这种关系。</li>
</ul>
<p>XLNet在输入侧维持表面的X句子单词顺序，在Transformer内部，看到的已经是被重新排列组合后的顺序，是通过Attention Mask来实现的。从X的输入单词里面，也就是Ti的上文和下文单词中，随机选择i-1个，放到Ti的上文位置中，把其它单词的输入通过Attention Mask隐藏掉，于是就能够达成我们期望的目标。</p>
<h3 id="双流自注意力机制"><a href="#双流自注意力机制" class="headerlink" title="双流自注意力机制"></a>双流自注意力机制</h3><img   src="/2021/03/25/BERT%E3%80%81RoBerta%E3%80%81XLNet%E3%80%81ALBERT%E5%AF%B9%E6%AF%94/1.png"  class="">
<ul>
<li>content stream self-attention $h_{\theta}\left(\mathbf{x}_{\mathbf{z}_{\leq t}}\right)$：标准的Transformer计算，能同时接触到单词x的特征信息和位置信息。</li>
<li>query stream self-attention $g_{\theta}\left(\mathbf{x}_{\mathbf{z}_{&lt;t}}, z_{t}\right)$：只能接触到单词x的位置信息。</li>
</ul>
<p>计算过程如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&g_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=g_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z}<t}^{(m-1)} ; \theta\right), \quad\left(\text { query stream: use } z_{t} \text { but cannot see } x_{z_{t}}\right)\\
&h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\mathbf{h}_{\mathrm{z}<t}^{(m-1)} ; \theta\right), \quad\left(\text { content stream: use both } z_{t} \text { and } x_{z_{t}}\right)
\end{aligned}</script><h4 id="其它改进措施："><a href="#其它改进措施：" class="headerlink" title="其它改进措施："></a>其它改进措施：</h4><ul>
<li>引入Transformer-XL：相对位置编码以及分段RNN机制。解决Transformer对长文档应用不友好的问题。</li>
<li>使用更多更高质量的数据。</li>
</ul>
<h2 id="ALBert"><a href="#ALBert" class="headerlink" title="ALBert"></a>ALBert</h2><h4 id="词嵌入向量参数的因式分解-Factorized-embedding-parameterization"><a href="#词嵌入向量参数的因式分解-Factorized-embedding-parameterization" class="headerlink" title="词嵌入向量参数的因式分解(Factorized embedding parameterization)"></a>词嵌入向量参数的因式分解(<strong>Factorized embedding parameterization</strong>)</h4><script type="math/tex; mode=display">
V \times H > V \times E + E \times H</script><p>在BERT、XLNet中，词表的embedding size(E)和transformer层的hidden size(H)是等同的，所以E=H。但实际上词库的大小一般都很大，这就导致模型参数个数就会变得很大。为了解决这些问题他们提出了一个基于factorization的方法。</p>
<h4 id="跨层参数共享-Cross-layer-parameter-sharing"><a href="#跨层参数共享-Cross-layer-parameter-sharing" class="headerlink" title="跨层参数共享(Cross-layer parameter sharing)"></a>跨层参数共享(Cross-layer parameter sharing)</h4><p>每一层的Transformer可以共享参数，这样一来参数的个数不会以层数的增加而增加。</p>
<h4 id="段落连续性任务-Inter-sentence-coherence-loss"><a href="#段落连续性任务-Inter-sentence-coherence-loss" class="headerlink" title="段落连续性任务(Inter-sentence coherence loss)"></a>段落连续性任务(Inter-sentence coherence loss)</h4><p>后续的研究表示NSP过于简单，性能不可靠。使用段落连续性任务。正例，使用从一个文档中连续的两个文本段落；负例，使用从一个文档中连续的两个文本段落，但位置调换了。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/70257427" >XLNet:运行机制及和Bert的异同比较<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://hackerxiaobai.github.io/2019/10/10/Bert-XLNet-RoBerta-ALBert/" >Bert XLNet RoBerta ALBert<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/84559048" >从BERT, XLNet, RoBERTa到ALBERT<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT面试要点</title>
    <url>/2021/03/21/BERT%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/</url>
    <content><![CDATA[<p>BERT的模型结构如下图所示：</p>
<span id="more"></span>
<img   src="/2021/03/21/BERT%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/1.png"  class="">
<h2 id="1-两个预训练任务"><a href="#1-两个预训练任务" class="headerlink" title="1. 两个预训练任务"></a>1. 两个预训练任务</h2><h4 id="Task1：Masked-Language-Model"><a href="#Task1：Masked-Language-Model" class="headerlink" title="Task1：Masked Language Model"></a>Task1：Masked Language Model</h4><p>MLM是指在训练的时候随机从输入语料上mask掉一些单词，然后通过上下文来预测该单词。在BERT的实验中，15%的Token会被随机Mask掉。这其中的80%会直接替换为[Mask]，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token。</p>
<p>这么做的原因：如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词。加入随机Token的原因是因为Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ‘hairy’。至于单词带来的负面影响，因为一个单词被随机替换掉的概率只有15%*10% =1.5%，这个负面影响其实是可以忽略不计的。</p>
<h4 id="Task2：Next-Sentence-Prediction"><a href="#Task2：Next-Sentence-Prediction" class="headerlink" title="Task2：Next Sentence Prediction"></a>Task2：Next Sentence Prediction</h4><p>NSP的任务是判断句子B是否是句子A的下文。如果是的话输出’IsNext’，否则输出’NotNext’。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从语料中提取的，它们的关系是NotNext的。这个关系保存在[CLS]符号中。</p>
<h2 id="2-BERT的输入与输出"><a href="#2-BERT的输入与输出" class="headerlink" title="2. BERT的输入与输出"></a>2. BERT的输入与输出</h2><h4 id="输入："><a href="#输入：" class="headerlink" title="输入："></a>输入：</h4><ul>
<li>Token Embeddings: 通过查询词表将文本中的每个字转换为一维向量；</li>
<li>Segmentation Embeddings: 对应BERT里面的下一句的预测任务，所以会有两句拼接起来，上句与下句，上句有上句段向量，下句则有下句段向量，也就是图中A与B；</li>
<li>Postion Embeddings: 由于self-attention不能记住文本的时序信息，所以需要加入位置编码。BERT通过初始化参数矩阵来学习位置信息。</li>
</ul>
<h4 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h4><ul>
<li>输入各字对应的融合全文语义信息后的向量表示</li>
<li>CLS:  编码了整个句子语义的特征向量</li>
</ul>
<h2 id="3-BERT的局限性"><a href="#3-BERT的局限性" class="headerlink" title="3. BERT的局限性"></a>3. BERT的局限性</h2><ul>
<li>BERT 在第一个预训练阶段，假设句子中多个单词被 Mask 掉，这些被 Mask 掉的单词之间没有任何关系，是条件独立的，然而有时候这些单词之间是有关系的，比如”New York is a city”，假设我们 Mask 住”New”和”York”两个词，那么给定”is a city”的条件下”New”和”York”并不独立，因为”New York”是一个实体，看到”New”则后面出现”York”的概率要比看到”Old”后面出现”York”概率要大得多。</li>
<li>BERT 的在预训练时会出现特殊的[MASK]，但是它在下游的 fine-tune 中不会出现，这就出现了预训练阶段和 fine-tune 阶段不一致的问题。其实这个问题对最后结果产生多大的影响也是不够明确的，因为后续有许多 BERT 相关的预训练模型仍然保持了[MASK]标记，也取得了很大的结果，而且很多数据集上的结果也比 BERT 要好。但是确确实实引入[MASK]标记，也是为了构造自编码语言模型而采用的一种折中方式。</li>
</ul>
<h2 id="4-ELMo、OpenAI-GPT、BERT区别"><a href="#4-ELMo、OpenAI-GPT、BERT区别" class="headerlink" title="4. ELMo、OpenAI GPT、BERT区别"></a>4. ELMo、OpenAI GPT、BERT区别</h2><ul>
<li>特征提取器：ELMo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，并且Transformer并行能力强。elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层。</li>
<li>单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。</li>
<li>GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，使用了完整句子。</li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>面试</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>BOSCH实习总结</title>
    <url>/2020/01/05/BOSCH%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>之前满怀赤诚之心写的总结，还在公司组会的时候读了出来（羞耻），在这儿记录一下吧。等我工作了，再回首看时，一定会显得非常幼稚。</p>
<span id="more"></span>
<p>来这儿实习四个多月了，一年的三分之一，时间确实过得快。我觉得我非常幸运，投对了部门。首先大家都很优秀，都很年轻，有活力，有干劲。同事间氛围很和谐。其次，对于我个人而言，也经历了很大的锻炼，增长了见识。跟其他部门的实习生比，我很庆幸很少去做搬东西、拿快递这样无聊的杂活。如果我这四个月都是做这种事，我真的要疯了，太浪费时间了。</p>
<p>在这段实习期间，我主要做了两个项目。一个是RFID的iOS app，一个是工时可视化的网站。一开始进部门，正好就碰到了RFID的项目。简历上虽说做过iOS的app，但那基本上只是demo，不够系统，也没给别人用过。当大光头突然说我们这个系统12月3号要给客户使用的时候，大家都在吐槽时间不够，一个月之内不可能完成等等。其实说实话，那个时候我还是暗自窃喜的，因为时间太仓促，我app做不成的话也情有可原，自然也不用背锅了。可大家还是争分夺秒地在赶这个项目。肖老板、子轩、沈宋衍每天都会检查我做app的进度以及下一步的任务。讨论的时候，我当时真的好惊讶：哇，沈宋衍思维逻辑怎么这么清晰，每次都能把讨论的情况梳理得井井有条。外企员工能力确实不一样。由于这个app是用新的技术写的，摸着石头过河，每天也会踩坑，也不知道能否做出来，就是硬着头皮做。写代码写到一两点很常见（可能是我太菜了）。当然实习这么累说没有抱怨肯定是不可能的。在12月4号的时候，子轩带着ivy还有我去工厂拿标签，这一天我像打了鸡血一样，在工厂狂剪了两千多个标签😂。工厂的米饭很好吃，也算体验了把出差的感觉。最后到deadline的时候，我们居然把项目做出来了。自豪感还是有的。整个过程中，感受最深的就是老板对我的信任（把这么紧急的项目给实习生做，这个问题之前也问过老板）、子轩对我的关照、沈宋衍超强的能力。</p>
<p>另一个项目就是工时可视化的网站。这是我师傅王政提出的一个idea。如果说上一个app是被动接受任务，那么这一个就是我主动愿意做的。因为它可以切实解决工作中的一些问题。之前部门会议也分享了这个网站。现在已经上线了，大家可以去用。如果大家喜欢用、觉得还不错的话，可以给我提一些feedback，我会继续维护下去。这个是无偿的，毕竟现在还在上学，没有那么强的劳资观念，能从项目实践中成长才是最重要的。师傅，你的每一次good都是对我莫大的鼓励。要是三年之后大家还在用的话，那我就可以很自豪地在简历上写开发了一个工时系统，部门员工至今已使用三年。如果大家不喜欢用的话，那也没关系。这个系统的有些功能肯定还是实用的，比如那四个炫酷的图表可以直接用在工作总结中。定时邮件提醒，这个也很方便，这种机械化的工作就不用嘉静去做了。实习要是能留下点实用的东西，那就是我感到最幸福的事了。</p>
<p>非项目方面，最要感谢嘉静和秋晶了。其实很多问题我都可以自己去解决的，像一些申请流程之类的。但我总是很急躁，懒得花时间在上面。感谢她们不厌其烦地指导我。</p>
<p>博世真的是一家培养你的公司。在这里你可以尽情地学到自己想学的东西。workshop、部门的分享会议等等都是很好的机会。之前emm1参加了那个拾败的活动，每个人都介绍了自己的失败经历（包括我跟一个妹子要微信失败的经历），然后大家从失败经历中总结经验、继续前行。哇，这种活动真的是太充满人文关怀了。还有一些其他的，例如心理咨询热线、员工反馈箱等等。不敢想象国内公司会有这样的机制。当然，在博世，你做什么事情都是要走流程的，这个流程可能会很长。而我性子就比较急，特别反感和害怕这种流程，昨天还吐槽了离个职怎么还这么麻烦。</p>
<p>在博世实习，也相当于接触了社会嘛，肯定也会考虑自己的未来规划和发展。本科生涯即将结束，研究生生涯即将在另一个地方、另一所学校开始。苏州北南京南，不同的路上领略不同的风景。研究生的三年更是坐冷板凳的三年，熬夜熬到两点多该是家常便饭，这一点在本科阶段已经适应了，那种深更半夜搜索枯肠而不可得的痛苦、大冬天晚上一个人在自习教室写着代码看着书的孤独又要重新尝一遍。三年之后出来，或许我就熬不动夜了。找工作、买房买车、关注柴米油盐的社会生活开始了。我爸妈是笃定让我去上海工作的，我暑假夏令营的时候去过一次上海，地铁挤了两趟才挤上去，当时印象就不好了。在上海，这么拥挤的城市，这么高的房价，这么快的生活节奏，我是有点恐惧的。IoT时代的到来，互联网寒冬是否继续都是未知数。我也不知道能否找到合适的工作。今年我看了一些秋招和春招的就业情况，就拿苏大来说，大部分人找的工作并不理想，包括研究生，商院的出来进四大也就那么点工资。我问有些研究生这么低的工资，怎么活的下去。他们调侃说穷人有穷人的活法，人家买的大一点的房子，你就只能买小一点的。人家买的贵一点的车，你就只能买便宜一点地。确实很有道理。总之，对于未来我是偏悲观的。而我现在能做的就是继续去学习，比我优秀的人太多太多了，去向他们学习。</p>
<p>回到这四个月的实习，我其实并没有做太多事情，大家都是做硬件，我也帮不上什么忙，还拿着工资，心里是有点惭愧的。我在最后只能送上自己衷心的祝愿。希望肖老板继续保持年轻、帅气，你是我心目中的男神，很有外企员工的气质；希望子轩能早点掌握编程、精通C语言；希望沈宋衍继续保持极强的编程能力、头发也越来越多；希望师傅王政继续保持那种干劲和活力；希望朱莉继续保持开朗的性格，你是我心目中的女硬件工程师（在我心中不是所有人都可以称为工程师的）；希望欢姐继续保持善良友爱的性格，从你的电话中能感觉到你是个好母亲；希望祝寅在博世能够实现自己的价值，平时跟你聊的话题很多，也很投机；希望peter头发越来越多；希望博士能继续研究出新的算法；希望kevin继续带领着aeemm前进；希望三位女生继续保持年轻漂亮；希望嘉静能够推进我那个网站；希望ivy保养好头发；希望秋晶学业有成；希望新来的实习生无论在编程能力上还是性格上都比我好的多的多。希望大家，在博世，拥抱更好的自己。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>程序人生</tag>
      </tags>
  </entry>
  <entry>
    <title>BST &amp; AVL &amp; RBT</title>
    <url>/2021/08/23/BST%20&amp;%20AVL%20&amp;%20RBT/</url>
    <content><![CDATA[<p>记录各种变体树：</p>
<span id="more"></span>
<h2 id="BST"><a href="#BST" class="headerlink" title="BST"></a>BST</h2><img   src="/2021/08/23/BST%20&%20AVL%20&%20RBT/BST.png"  class="">
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ul>
<li>根节点的值大于左子树包含的节点的值</li>
<li>根节点的值小于右子树包含的节点的值</li>
<li>左右子树都是BST</li>
</ul>
<h4 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h4><p>假设当前节点为 <code>cur</code> ，待插入节点为 <code>node</code> ，根节点为 <code>root</code> ，分如下四种情况：</p>
<ol>
<li><code>root == None</code>: <code>root=node</code></li>
<li><code>cur.val == node.val</code>: 不做任何处理</li>
<li><code>cur.val &gt; node.val</code>:<ul>
<li><code>if cur.left == None</code>: <code>cur.left = node</code></li>
<li><code>if cur.left != None</code>: 递归左子树</li>
</ul>
</li>
<li><code>cur.val &lt; node.val</code>:<ul>
<li><code>if cur.right == None</code>: <code>cur.right = node</code></li>
<li><code>if cur.right != None</code>: 递归右子树</li>
</ul>
</li>
</ol>
<h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><p>分如下三种情况：</p>
<ol>
<li>删除节点为叶子节点：直接删除</li>
<li>删除节点只有一个子节点：删除节点的父节点指向其唯一的那个子节点</li>
<li>删除节点有两个子节点：选择后继节点（右子树的最小节点）来顶替其位置，然后删除后继节点</li>
</ol>
<h2 id="AVL"><a href="#AVL" class="headerlink" title="AVL"></a>AVL</h2><img   src="/2021/08/23/BST%20&%20AVL%20&%20RBT/AVL.png"  class="">
<blockquote>
<p>平衡因子：树中某结点其左子树的高度和右子树的高度之差</p>
</blockquote>
<h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><ul>
<li>特殊的BST，树中任意一个节点的平衡因子绝对值小于等于1</li>
</ul>
<p>AVL的插入和删除时间复杂度均为 $O(log_2 n)$ ，$n$ 为树中节点个数。</p>
<p>AVL树大部分操作都和BST树相同, 只有在插入删除结点时, 有可能造成AVL树失去平衡, <strong>而且只有那些在被插入/删除结点到根节点的路径上的结点有可能出现失衡, 因为只有那些结点的子树结构发生了变化</strong>。</p>
<p>这时我们需要一些操作来把树恢复平衡，这些操作叫做AVL树的旋转：</p>
<ul>
<li>LL</li>
<li>RR</li>
<li>LR</li>
<li>RL</li>
</ul>
<p>具体操作可见 <a class="link"   href="https://blog.csdn.net/weixin_36888577/article/details/87211314?utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.base" >平衡二叉树(AVL树)的平衡原理以及插入,删除操作<i class="fas fa-external-link-alt"></i></a> 和 <a class="link"   href="https://www.cnblogs.com/magic-sea/p/11992833.html" >AVL树的插入和删除<i class="fas fa-external-link-alt"></i></a></p>
<h4 id="插入-1"><a href="#插入-1" class="headerlink" title="插入"></a>插入</h4><ul>
<li>当插入新结点导致不平衡时, 我们需要找到距离新节点最近的不平衡结点为轴来转动AVL树来达到平衡</li>
</ul>
<h4 id="删除-1"><a href="#删除-1" class="headerlink" title="删除"></a>删除</h4><ul>
<li>AVL删除节点的操作与和BST一样, 不同的是删除一个结点有可能引起父结点失衡。与插入不同，除了在父节点处旋转外，可能必须在父节点的祖先处再进行旋转。因此，我们必须继续追踪路径，直到到达根为止。</li>
</ul>
<h2 id="RBT"><a href="#RBT" class="headerlink" title="RBT"></a>RBT</h2><img   src="/2021/08/23/BST%20&%20AVL%20&%20RBT/RBT.jpeg"  class="">
<ul>
<li>一棵含有 $n$ 个节点的红黑树的高度至多为 $2log(n+1)$</li>
<li>RBT的插入和删除时间复杂度均为 $O(log_2 n)$ ，$n$ 为树中节点个数</li>
</ul>
<h4 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h4><p>RBT也是一种特殊的BST，此外它还有如下五个特性：</p>
<ol>
<li>每个节点要么黑色，要么红色</li>
<li>根节点为黑色</li>
<li>每个叶子节点为黑色（这里叶子节点专指值为None的节点）</li>
<li>如果一个节点为红色，那么它的子节点必为黑色</li>
<li>任意一节点到每个叶子节点的路径上都包含相同数量的黑色节点</li>
</ol>
<h4 id="插入、删除"><a href="#插入、删除" class="headerlink" title="插入、删除"></a>插入、删除</h4><p>RBT的插入和删除情况较为复杂，具体案例可见 <a class="link"   href="http://www.360doc.com/content/19/0311/07/25472797_820646156.shtml" >什么是红黑树？面试必问！<i class="fas fa-external-link-alt"></i></a> 和 <a class="link"   href="https://www.cnblogs.com/skywang12345/p/3245399.html" >红黑树(一)之 原理和算法详细介绍<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="RBT相比于BST、AVL有什么优缺点"><a href="#RBT相比于BST、AVL有什么优缺点" class="headerlink" title="RBT相比于BST、AVL有什么优缺点"></a>RBT相比于BST、AVL有什么优缺点</h2><ul>
<li><p>RBT是牺牲了严格的高度平衡的优越条件为代价，它只要求部分地达到平衡要求，降低了对旋转的要求，从而提高了性能。红黑树能够以 $O(log_2 n)$ 的时间复杂度进行搜索、插入、删除操作。此外，由于它的设计，任何不平衡都会在三次旋转之内解决。当然，还有一些更好的，但实现起来更复杂的数据结构能够做到一步旋转之内达到平衡，但红黑树能够给我们一个比较“便宜”的解决方案。</p>
</li>
<li><p>相比于BST，因为红黑树可以能确保树的最长路径不大于两倍的最短路径的长度，所以可以看出它的查找效果是有最低保证的。在最坏的情况下也可以保证 $O(logn)$ 的，这是要好于二叉查找树的。因为二叉查找树最坏情况可以让查找达到 $O(n)$ 。</p>
</li>
<li><p>RBT的算法时间复杂度和AVL相同，但统计性能比AVL更高。AVL在插入和删除中所做的后期维护操作会比RBT要耗时好多，但是他们的查找效率都是 $O(logn)$ ，所以RBT应用还是高于AVL的。实际上插入AVL和RBT的速度取决于你所插入的数据。如果你的数据分布较好,则比较宜于采用AVL(例如随机产生系列数)，但是如果你想处理比较杂乱的情况，则RBT是比较快的。</p>
</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/c_living/article/details/81021510" >BST（二叉搜索树）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/weixin_36888577/article/details/87211314?utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.base" >平衡二叉树(AVL树)的平衡原理以及插入,删除操作<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_25343557/article/details/89110319" >详细图文——AVL树<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/magic-sea/p/11992833.html" >AVL树的插入和删除<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://www.360doc.com/content/19/0311/07/25472797_820646156.shtml" >什么是红黑树？面试必问！<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/skywang12345/p/3245399.html" >红黑树(一)之 原理和算法详细介绍<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/wuchanming/p/4444961.html" >面试题——轻松搞定面试中的红黑树问题<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title>Bagging &amp; RF</title>
    <url>/2021/07/06/Bagging%20&amp;%20RF/</url>
    <content><![CDATA[<p>相对于 <code>boosting</code> ，<code>bagging</code> 更好理解一点。</p>
<span id="more"></span>
<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>算法流程如下：</p>
<ol>
<li>从原始样本中使用Bootstraping方法有放回地随机抽取 $n$ 个训练样本，共进行 $k$ 轮抽取，得到 $k$ 个训练集；</li>
<li>对于 $k$ 个训练集，分别训练出 $k$ 个模型；</li>
<li>在对预测输出进行结合时：<ul>
<li>分类：简单投票法</li>
<li>回归：简单平均法</li>
</ul>
</li>
</ol>
<h2 id="RF"><a href="#RF" class="headerlink" title="RF"></a>RF</h2><p>RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。</p>
<p>传统决策树在选择划分属性时是在当前结点的属性集合（假设有 $d$ 个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 $k$ 个属性的子集，然后再从这个子集中选择一个最优属性用于划分。$k$ 控制了随机性的引入程度：</p>
<ul>
<li>$k=d$ ：基决策树的构建与传统决策树相同；</li>
<li>$k=1$ ：随机选择一个属性进行划分；</li>
</ul>
<p>RF简单、容易实现、计算开销小，但是性能却非常强大。</p>
<h2 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h2><h4 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a>平均法</h4><p>对于输出值连续型，最常用的结合策略是使用平均法：</p>
<ul>
<li>简单平均法：</li>
</ul>
<script type="math/tex; mode=display">
H(x) = \frac{1}{T} \sum_{i=1}^T h_i(x)</script><ul>
<li>加权平均法：</li>
</ul>
<script type="math/tex; mode=display">
H(x) = \sum_{i=1}^T w_i h_i(x), \quad \sum_{i=1}^T w_i = 1</script><p>$w_i$ 一般是从训练数据中学习而得。</p>
<p>在个体学习器性能相差较大时使用加权平均法，反之使用简单平均法。</p>
<h4 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a>投票法</h4><p>对于分类任务来说，假设有 $\{ c_1, c_2, \dots, c_N \}$ 个类别。最常用的结合策略是使用投票法：</p>
<ul>
<li>绝对多数投票法：</li>
</ul>
<script type="math/tex; mode=display">
H(x) = 
\begin{cases}
c_j,& if \  \sum_{i=1}^T h_i^j(x) > 0.5\sum_{k=1}^N \sum_{i=1}^T h_i^k(x)；\\
reject, & otherwise
\end{cases}</script><p>即某类别得票过半数，则预测为该类别；否则拒绝预测。</p>
<ul>
<li>相对多数投票法：</li>
</ul>
<script type="math/tex; mode=display">
H(x) = c_{argmax_j} \sum_{i=1}^T h_i^j(x)</script><p>即预测为得票最多的类别，若同时有多个类别获得最高票，则从中随机选择一个类别。</p>
<ul>
<li>加权投票法：</li>
</ul>
<script type="math/tex; mode=display">
H(x) = c_{argmax_j} \sum_{i=1}^T w_i h_i^j(x), \quad \sum_{i=1}^T w_i = 1</script><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p>当训练数据很多时，一种更为强大的结合策略是使用”学习法“，即通过另一个学习来结合，典型代表就是Stacking。这里把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器。</p>
<p>Stacking先从初始数据集训练出初级学习器，然后”生成“一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。算法流程如下：</p>
<img   src="/2021/07/06/Bagging%20&%20RF/1.jpg"  class="">
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>C++学习笔记之多态</title>
    <url>/2021/06/01/C++%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%A4%9A%E6%80%81/</url>
    <content><![CDATA[<blockquote>
<p>多态性：相同对象收到不同消息或不同对象收到相同消息时产生不用的实现动作。</p>
</blockquote>
<span id="more"></span>
<p>多态有两种类型：</p>
<ul>
<li>编译时多态性（静态多态）：通过重载函数实现。</li>
<li>运行时多态性（动态多态）：通过虚函数实现。</li>
</ul>
<h2 id="多态与非多态"><a href="#多态与非多态" class="headerlink" title="多态与非多态"></a>多态与非多态</h2><p>实质区别就是函数地址是早绑定还是晚绑定。</p>
<ul>
<li>如果函数的调用，在编译期间就可以确定函数的调用地址，并生产代码，是静态的，就是说地址是早绑定的。</li>
<li>如果函数调用的地址不能在编译期间确定，需要在运行时才确定，这就属于晚绑定。</li>
</ul>
<h2 id="多态的目的"><a href="#多态的目的" class="headerlink" title="多态的目的"></a>多态的目的</h2><ul>
<li>封装：代码模块化。继承：可以扩展已存在的代码。两者目的都是为了代码重用。</li>
<li>多态：接口重用。不论传递过来的究竟是类的哪个对象，函数都能够通过同一个接口调用到适应各自对象的实现方法。</li>
</ul>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>声明基类类型的指针，利用该指针指向任意一个子类对象，调用相应的虚函数，可以根据指向的子类的不同而实现不同的方法。如果没有使用虚函数的话，即没有利用C++多态性，则利用基类指针调用相应的函数的时候，将总被限制在基类函数本身，而无法调用到子类中被重写过的函数。因为没有多态性，函数调用的地址将是固定的，因此将始终调用到同一个函数，这就无法实现“一个接口，多种方法”的目的了。</p>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span> </span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Shape</span> &#123;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="type">int</span> width;</span><br><span class="line">    <span class="type">int</span> height;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Shape</span>(<span class="type">int</span> a = <span class="number">0</span>, <span class="type">int</span> b = <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        width = a;</span><br><span class="line">        height = b;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">area</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Parent class area : &quot;</span> &lt;&lt; <span class="number">-1</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Rectangle</span> : <span class="keyword">public</span> Shape &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Rectangle</span>(<span class="type">int</span> a = <span class="number">0</span>, <span class="type">int</span> b = <span class="number">0</span>) : <span class="built_in">Shape</span>(a,b) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">area</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Rectangle class area: &quot;</span> &lt;&lt; width * height &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Triangle</span> : <span class="keyword">public</span> Shape &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Triangle</span>(<span class="type">int</span> a = <span class="number">0</span>, <span class="type">int</span> b = <span class="number">0</span>) : <span class="built_in">Shape</span>(a, b) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">area</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Triangle class area: &quot;</span> &lt;&lt; width * height / <span class="number">2</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 程序的主函数</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Shape* shape;</span><br><span class="line">    <span class="function">Rectangle <span class="title">rec</span><span class="params">(<span class="number">10</span>, <span class="number">7</span>)</span></span>;</span><br><span class="line">    <span class="function">Triangle  <span class="title">tri</span><span class="params">(<span class="number">10</span>, <span class="number">5</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 存储矩形的地址</span></span><br><span class="line">    shape = &amp;rec;</span><br><span class="line">    <span class="comment">// 调用矩形的求面积函数 area</span></span><br><span class="line">    shape-&gt;<span class="built_in">area</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 存储三角形的地址</span></span><br><span class="line">    shape = &amp;tri;</span><br><span class="line">    <span class="comment">// 调用三角形的求面积函数 area</span></span><br><span class="line">    shape-&gt;<span class="built_in">area</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/37340242" >C++ 多态 - 知乎 (zhihu.com)<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.runoob.com/cplusplus/cpp-polymorphism.html" >C++ 多态 | 菜鸟教程 (runoob.com)<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>C++常见面试题</title>
    <url>/2021/06/02/C++%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<p>记录一下C++常见的面试八股文。</p>
<span id="more"></span>
<h2 id="C-内存分配有哪几种方式？"><a href="#C-内存分配有哪几种方式？" class="headerlink" title="C++内存分配有哪几种方式？"></a>C++内存分配有哪几种方式？</h2><ul>
<li>从静态存储区分配：内存在程序编译的时候已经分配好，这块内存在整个程序的运行期间都存在，例如全局变量、静态变量。</li>
<li>在栈上创建：在执行函数时，函数内局部变量的存储单元都可以在栈上创建，函数执行结束时这些存储单元自动释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存有限。</li>
<li>在堆上分配：程序在运行的时候用malloc或new申请任意多的内存，由程序员手动free或delete来释放内存。动态内存的生存期有开发者自己决定，使用非常灵活，但也会导致内存泄漏的问题。</li>
</ul>
<h2 id="struct和class的异同？"><a href="#struct和class的异同？" class="headerlink" title="struct和class的异同？"></a>struct和class的异同？</h2><ul>
<li>同：struct和class定义类，都可以继承。</li>
<li>异：struct的默认继承权限和默认访问权限是public，而class的默认继承权限和默认访问权限是private。</li>
</ul>
<h2 id="重载和重写的区别？"><a href="#重载和重写的区别？" class="headerlink" title="重载和重写的区别？"></a>重载和重写的区别？</h2><ul>
<li>重载：是指同一可访问区内被声明的几个具有不同参数列（参数的类型，个数，顺序不同）的同名函数，根据参数列表确定调用哪个函数，重载不关心函数返回类型。</li>
<li>重写：指派生类中存在重新定义的函数。其函数名，参数列表，返回值类型，所有都必须同基类中被重写的函数一致。只有函数体不同（花括号内），派生类调用时会调用派生类的重写函数，不会调用被重写函数。重写的基类中被重写的函数必须有 <code>virtual</code> 修饰。</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>C++智能指针详解</title>
    <url>/2022/09/12/C++%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>了解Objective-C/Swift的程序员应该知道<a class="link"   href="https://transformerswsz.github.io/2017/08/21/iOS%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/" >引用计数<i class="fas fa-external-link-alt"></i></a>的概念。引用计数这种计数是为了防止内存泄露而产生的。 基本想法是对于动态分配的对象，进行引用计数，每当增加一次对同一个对象的引用，那么引用对象的引用计数就会增加一次， 每删除一次引用，引用计数就会减一，当一个对象的引用计数减为零时，就自动删除指向的堆内存。</p>
<span id="more"></span>
<p>在传统C++中，程序员需要手动释放资源，经常忘记去释放资源而导致泄露。通常的做法是对于一个对象而言，我们在构造函数的时候申请空间，而在析构函数（在离开作用域时调用）的时候释放空间， 也就是我们常说的 RAII资源获取即初始化技术。</p>
<p>传统C++里我们使用<code>new</code>和<code>delete</code>去申请和释放资源，两者必须配对写，<code>new</code>会返回一个裸指针，即<code>Object *</code>这种形式。而C++11引入了智能指针的概念，使用了引用计数的想法，让程序员不再需要关心手动释放内存。 这些智能指针包括：</p>
<ul>
<li><code>shared_ptr</code></li>
<li><code>unique_ptr</code></li>
<li><code>weak_ptr</code></li>
</ul>
<p>使用它们需要包含头文件<code>memory</code>，下面进行详细介绍。</p>
<h2 id="shared-ptr"><a href="#shared-ptr" class="headerlink" title="shared_ptr"></a><code>shared_ptr</code></h2><p><code>shared_ptr</code>是一种智能指针，它能够记录多少个<code>shared_ptr</code>共同指向一个对象，从而消除显式的调用<code>delete</code>，当引用计数变为零的时候就会将对象自动删除。</p>
<ul>
<li><code>make_shared</code>用来消除显式的使用<code>new</code>，它会分配创建传入参数中的对象，并返回这个对象类型的<code>shared_ptr</code>指针。</li>
<li><code>shared_ptr</code>可以通过<code>get()</code>方法来获取原始指针，通过<code>reset()</code>来减少一个引用计数， 并通过<code>use_count()</code>来查看一个对象的引用计数。</li>
</ul>
<p>下面是关于 <code>shared_ptr</code> 的示例：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">auto</span> p = <span class="built_in">make_shared</span>&lt;<span class="type">int</span>&gt;(<span class="number">10</span>);</span><br><span class="line">	(*p)++;</span><br><span class="line">	cout &lt;&lt; *p &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">auto</span> p1 = p;</span><br><span class="line">	<span class="keyword">auto</span> p2 = p1;</span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p count: &quot;</span> &lt;&lt; p.<span class="built_in">use_count</span>() &lt;&lt; endl;	<span class="comment">// 3</span></span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p1 count: &quot;</span> &lt;&lt; p1.<span class="built_in">use_count</span>() &lt;&lt; endl;	<span class="comment">// 3</span></span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p2 count: &quot;</span> &lt;&lt; p2.<span class="built_in">use_count</span>() &lt;&lt; endl;	<span class="comment">// 3</span></span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;---------------&quot;</span> &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">	p1.<span class="built_in">reset</span>();</span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p count: &quot;</span> &lt;&lt; p.<span class="built_in">use_count</span>() &lt;&lt; endl;	<span class="comment">// 2</span></span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p1 count: &quot;</span> &lt;&lt; p1.<span class="built_in">use_count</span>() &lt;&lt; endl;	<span class="comment">// 0</span></span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p2 count: &quot;</span> &lt;&lt; p2.<span class="built_in">use_count</span>() &lt;&lt; endl;	<span class="comment">// 2</span></span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;---------------&quot;</span> &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">	p.<span class="built_in">reset</span>();</span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p count: &quot;</span> &lt;&lt; p.<span class="built_in">use_count</span>() &lt;&lt; endl;	<span class="comment">// 0</span></span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p1 count: &quot;</span> &lt;&lt; p1.<span class="built_in">use_count</span>() &lt;&lt; endl;	<span class="comment">// 0</span></span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p2 count: &quot;</span> &lt;&lt; p2.<span class="built_in">use_count</span>() &lt;&lt; endl;	<span class="comment">// 1</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li>占用内存高：因为除了要管理一个裸指针外，还要维护一个引用计数器。</li>
<li>原子操作性能低：虑到线程安全问题，引用计数的增减必须是原子操作。而原子操作一般情况下都比非原子操作慢。</li>
</ul>
<h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><ul>
<li>通常使用在共享权不明的场景，有可能多个对象同时管理同一个内存。</li>
<li>对象的延迟销毁，当一个对象的析构非常耗时，甚至影响到了关键线程的速度。可以使用 <code>BlockingQueue&lt;shared_ptr&lt;void&gt;&gt;</code>将对象转移到另外一个线程中释放，从而解放关键线程（陈硕-《Linux多线程服务器端编程》）。</li>
</ul>
<h2 id="unique-ptr"><a href="#unique-ptr" class="headerlink" title="unique_ptr"></a><code>unique_ptr</code></h2><p><code>unique_ptr</code>是一种独占的智能指针，它禁止其他智能指针与其共享同一个对象，从而保证代码的安全。<code>unique_ptr</code>只支持移动，不支持赋值：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">unique_ptr&lt;<span class="type">int</span>&gt; pointer = <span class="built_in">make_unique</span>&lt;<span class="type">int</span>&gt;(<span class="number">10</span>);</span><br><span class="line">unique_ptr&lt;<span class="type">int</span>&gt; pointer2 = pointer; <span class="comment">// 非法</span></span><br><span class="line">unique_ptr&lt;<span class="type">int</span>&gt; pointer3 = <span class="built_in">move</span>(pointer); <span class="comment">// 合法</span></span><br></pre></td></tr></table></figure></p>
<p>下面是关于 <code>unique_ptr</code> 的示例：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Foo</span> &#123;</span><br><span class="line">	<span class="built_in">Foo</span>() &#123; cout &lt;&lt; <span class="string">&quot;construct Foo&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">	~<span class="built_in">Foo</span>() &#123; cout &lt;&lt; <span class="string">&quot;delete Foo&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">	<span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">int</span> i)</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;point&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; is not null. Here is Foo:foo&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">const</span> Foo&amp; foo, <span class="type">int</span> i)</span> </span>&#123;</span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;point&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; call outer function&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="function">unique_ptr&lt;Foo&gt; <span class="title">p1</span><span class="params">(make_unique&lt;Foo&gt;())</span></span>;</span><br><span class="line">	<span class="comment">// p1 不空, 输出</span></span><br><span class="line">	<span class="keyword">if</span> (p1)</span><br><span class="line">	&#123;</span><br><span class="line">		p1-&gt;<span class="built_in">foo</span>(<span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="function">unique_ptr&lt;Foo&gt; <span class="title">p2</span><span class="params">(move(p1))</span></span>;</span><br><span class="line">		<span class="comment">// p2 不空, 输出</span></span><br><span class="line">		<span class="built_in">f</span>(*p2, <span class="number">2</span>);</span><br><span class="line">		<span class="comment">// p2 不空, 输出</span></span><br><span class="line">		<span class="keyword">if</span> (p2) p2-&gt;<span class="built_in">foo</span>(<span class="number">2</span>);</span><br><span class="line">		<span class="comment">// p1 为空, 无输出</span></span><br><span class="line">		<span class="keyword">if</span> (p1) p1-&gt;<span class="built_in">foo</span>(<span class="number">1</span>);</span><br><span class="line">		p1 = <span class="built_in">move</span>(p2);</span><br><span class="line">		<span class="comment">// p2 为空, 无输出</span></span><br><span class="line">		<span class="keyword">if</span> (p2) p2-&gt;<span class="built_in">foo</span>(<span class="number">2</span>);</span><br><span class="line">		cout &lt;&lt; <span class="string">&quot;p2 被销毁&quot;</span> &lt;&lt; endl;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// p1 不空, 输出</span></span><br><span class="line">	<span class="keyword">if</span> (p1) p1-&gt;<span class="built_in">foo</span>(<span class="number">1</span>);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h4><ul>
<li><code>unique_ptr</code>在默认情况下和裸指针的大小是一样的。所以内存上没有任何的额外消耗，性能是最优的。</li>
</ul>
<h4 id="使用场景-1"><a href="#使用场景-1" class="headerlink" title="使用场景"></a>使用场景</h4><ul>
<li>忘记<code>delete</code>:<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Widget</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="built_in">Widget</span>() &#123;&#125;</span><br><span class="line">	~<span class="built_in">Widget</span>() &#123;&#125;</span><br><span class="line">	<span class="function"><span class="type">void</span> <span class="title">do_something</span><span class="params">()</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		cout &lt;&lt; <span class="string">&quot;Here is Widget!&quot;</span> &lt;&lt; endl;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Box</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="built_in">Box</span>(): <span class="built_in">w</span>(<span class="keyword">new</span> <span class="built_in">Widget</span>())</span><br><span class="line">	&#123;&#125;</span><br><span class="line">	~<span class="built_in">Box</span>()</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="comment">/*delete w;*/</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="type">void</span> <span class="title">call_widget</span><span class="params">()</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		w-&gt;<span class="built_in">do_something</span>();</span><br><span class="line">	&#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">	Widget* w;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
如果因为一些原因，<code>w</code>必须建立在堆上。如果用裸指针管理<code>w</code>，那么需要在析构函数中 <code>delete w</code>，但程序员容易漏写该语句，造成内存泄漏。</li>
</ul>
<p>如果按照<code>unique_ptr</code>的写法，不用在析构函数手动<code>delete</code>属性。当对象析构时，属性<code>w</code>将会自动释放内存。</p>
<ul>
<li>异常安全<br>假如我们在一段代码中，需要创建一个对象，处理一些事情后返回，返回之前将对象销毁，如下所示：<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">process</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Widget* w = <span class="keyword">new</span> <span class="built_in">Widget</span>();</span><br><span class="line">    w-&gt;<span class="built_in">do_something</span>(); <span class="comment">// 可能会发生异常</span></span><br><span class="line">    <span class="keyword">delete</span> w;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
在正常流程下，我们会在函数末尾<code>delete</code>创建的对象<code>w</code>，正常调用析构函数，释放内存。</li>
</ul>
<p>但是如果<code>w-&gt;do_something()</code>发生了异常，无法执行到<code>delete w</code>。此时就会发生内存泄漏。<br>我们当然可以使用<code>try…catch</code>捕捉异常，在 <code>catch</code>里面执行<code>delet</code>，但是这样代码上并不美观，也容易漏写。</p>
<p>如果我们用<code>unique_ptr</code>，那么这个问题就迎刃而解了。无论代码怎么抛异常，在<code>unique_ptr</code>离开函数作用域的时候，内存就将会自动释放。</p>
<h2 id="weak-ptr"><a href="#weak-ptr" class="headerlink" title="weak_ptr"></a><code>weak_ptr</code></h2><p>看如下代码；<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">A</span>;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">B</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">A</span> &#123;</span><br><span class="line">    shared_ptr&lt;B&gt; pointer;</span><br><span class="line">    ~<span class="built_in">A</span>() &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;A 被销毁&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">B</span> &#123;</span><br><span class="line">    shared_ptr&lt;A&gt; pointer;</span><br><span class="line">    ~<span class="built_in">B</span>() &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;B 被销毁&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> a = <span class="built_in">make_shared</span>&lt;A&gt;();</span><br><span class="line">    <span class="keyword">auto</span> b = <span class="built_in">make_shared</span>&lt;B&gt;();</span><br><span class="line">    a-&gt;pointer = b;</span><br><span class="line">    b-&gt;pointer = a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>运行结果是 A, B 都不会被销毁，这是因为 a,b 内部的 pointer 同时又引用了 a,b，这使得 a,b 的引用计数均变为了 2，而离开作用域时，a,b 智能指针被析构，却只能造成这块区域的引用计数减一，这样就导致了 a,b 对象指向的内存区域引用计数不为零，而外部已经没有办法找到这块区域了，也就造成了内存泄露，如图所示：</p>
<img   src="/2022/09/12/C++%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88%E8%AF%A6%E8%A7%A3/weak1.png"  class="weak1">
<p>解决这个问题的办法就是使用弱引用指针<code>weak_ptr</code>，它是一种弱引用（相比较而言<code>shared_ptr</code>就是一种强引用）。弱引用不会引起引用计数增加，当换用弱引用时候，最终的释放流程如图下图所示：</p>
<img   src="/2022/09/12/C++%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88%E8%AF%A6%E8%A7%A3/weak2.png"  class="weak2">
<p>在上图中，最后一步只剩下 B，而 B 并没有任何智能指针引用它，因此这块内存资源也会被释放。</p>
<p><code>weak_ptr</code>没有<code>*</code>运算符和<code>-&gt;</code>运算符，所以不能够对资源进行操作，它可以用于检查<code>shared_ptr</code>是否存在，其<code>expired()</code>方法能在资源未被释放时，会返回<code>false</code>，否则返回<code>true</code>；除此之外，它也可以用于获取指向原始对象的<code>shared_ptr</code>指针，其<code>lock()</code>方法在原始对象未被释放时，返回一个指向原始对象的<code>shared_ptr</code> 指针，进而访问原始对象的资源，否则返回<code>nullptr</code>。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在日常使用中，<code>unique_ptr</code>使用频率最高，<code>weak_ptr</code>最低，需要避免循环引用的情况。当然，还是要根据具体的业务场景和性能要求来选择哪种指针。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cyhone.com/articles/right-way-to-use-cpp-smart-pointer/" >C++ 智能指针的正确使用方式<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://changkun.de/modern-cpp/zh-cn/05-pointers/#5-4-std-weak-ptr" >智能指针与内存管理<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN</title>
    <url>/2019/06/02/CNN/</url>
    <content><![CDATA[<blockquote>
<p><a class="link"   href="https://en.wikipedia.org/wiki/Convolutional_neural_network?spm=a2c4e.11153940.blogcont637953.7.471b45b8P0daTl" >卷积神经网络（Convolutional Neural Networks）<i class="fas fa-external-link-alt"></i></a>是一种深度学习模型或类似于人工神经网络的多层感知器，常用来分析视觉图像。CNN在图像分类数据集上有非常突出的表现。</p>
</blockquote>
<span id="more"></span>
<h1 id="DNN与CNN"><a href="#DNN与CNN" class="headerlink" title="DNN与CNN"></a>DNN与CNN</h1><p>下图为DNN：</p>
<img   src="/2019/06/02/CNN/DNN.jpg"  class="">
<p>下图为CNN：</p>
<img   src="/2019/06/02/CNN/CNN.jpg"  class="">
<p>虽然两张图的结构直观上差异较大，但实际上它们的整体架构是非常相似的。</p>
<ul>
<li>CNN通过一层一层的节点组织起来。</li>
<li>和DNN一样，CNN的每一个节点都是一个神经元。</li>
<li>CNN的输入输出与DNN基本一致。以图像分类为例，CNN的输入层就是图像的原始像素，而输出层中的每一个节点代表了不同类别的可信度。DNN中的损失函数以及参数的优化过程也适用于CNN。</li>
</ul>
<h1 id="CNN结构"><a href="#CNN结构" class="headerlink" title="CNN结构"></a>CNN结构</h1><p>使用DNN处理图像的最大问题在于全连接层的参数太多。对于MNIST数据，每一张图片的大小为28*28*1，1表示图像是黑白的，只有一个彩色通道。假设第一层隐藏层的节点数为500个，那么一个全连接层的神经网络会有28*28*500+500=392500个参数。如果图片采取更大的规格，比如有RGB三个彩色通道，那么参数数量更是巨大。过多的参数会导致计算速度减慢以及过拟合。而CNN可以有效的减少参数的个数。下图是具体的CNN结构图：</p>
<img   src="/2019/06/02/CNN/CNN_detail.jpg"  class="">
<p>在CNN的前几层中，每一层的节点都被组织成一个三维矩阵。比如将输入的图片组织成一个32*32*3的三维矩阵。从上图中可以看出CNN的前几层中每个节点只和上一层中部分的节点相连。CNN主要由以下5中结构组成：</p>
<h2 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h2><p>输入层是整个神经网络的输入，在处理图像的CNN中，它一般代表了一张图片的像素矩阵。在上图的最左侧的三维矩阵就可以代表一张图片。三维矩阵的长和宽代表了图像的大小，而三维矩阵的深度代表了图像的色彩通道。比如黑白图片的深度为1，而在RGB色彩模式下，图像的深度为3。</p>
<h2 id="卷积层-Convolution-Layer"><a href="#卷积层-Convolution-Layer" class="headerlink" title="卷积层(Convolution Layer)"></a>卷积层(Convolution Layer)</h2><p>卷积层是CNN最重要的部分。和传统全连接层不同，卷积层中每一个节点的输入只是上一层神经网络的一小块。下图为卷积层的过滤器(filter)或者内核(kernel)：</p>
<img   src="/2019/06/02/CNN/filter.jpg"  class="">
<p>filter可以将当前层神经网络上的一个子节点矩阵转化为下一层神经网络上的一个单位节点矩阵（长宽均为1，但深度不限）。filter的尺寸指的是filter输入节点矩阵的大小，通常有3*3或5*5。filter处理的矩阵深度和当前层神经网络节点矩阵（输入节点矩阵）的深度是一致的，而filter的深度指的是输出单位节点的深度。</p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>下图展示了如何通过filter将一个2*2*3的节点矩阵转化为一个1*1*5的单位节点矩阵。</p>
<img   src="/2019/06/02/CNN/g0.png"  class="">
<p>一个filter的前向传播过程和全连接层相似，它总共需要2*2*3*5+5个参数，+5表示偏置项参数的个数。假设使用$w_{x,y,z}^i$ 表示对于输出单位节点矩阵中的第i个节点，filter输入节点 $(x, y, z)$ 的权重，使用 $b^i$ 表示第i个输出节点对应的偏置项参数，那么单位矩阵中的第i个节点的取值 $g(i)$ 为：</p>
<script type="math/tex; mode=display">
g(i) = f(\sum_{x=1}^2\sum_{y=1}^2\sum_{z=1}^3 a_{x, y, z}*w_{x, y, z}^i + b^i)</script><p>其中 $a_{x, y, z}$ 为filter节点 $(x, y, z)$ 的取值，$f$ 采用ReLU作为激活函数。上图展示了 $g(0)$ 的计算过程。每一个二维矩阵表示三维矩阵在某一个深度上的取值。</p>
<p>卷积层结构的前向传播就是通过将一个filter从神经网络当前层的左上角移动到右下角（即滑过整个图像），并在移动过程中重复上述运算：</p>
<img   src="/2019/06/02/CNN/slide.gif"  class="">
<h3 id="调整输出矩阵大小"><a href="#调整输出矩阵大小" class="headerlink" title="调整输出矩阵大小"></a>调整输出矩阵大小</h3><h4 id="全0填充"><a href="#全0填充" class="headerlink" title="全0填充"></a>全0填充</h4><p>为了避免卷积层前向传播过程中节点矩阵的尺寸的变化，可以在当前矩阵的边界上加入全0填充。这样可以使得卷积层前向传播结果矩阵的大小和当前层矩阵保持一致：</p>
<img   src="/2019/06/02/CNN/padding.jpg"  class="">
<h4 id="步长"><a href="#步长" class="headerlink" title="步长"></a>步长</h4><p>下图显示了filter步长为2且使用全0填充时，卷积层前向传播的过程：</p>
<img   src="/2019/06/02/CNN/stride.jpg"  class="">
<h4 id="输出矩阵的大小"><a href="#输出矩阵的大小" class="headerlink" title="输出矩阵的大小"></a>输出矩阵的大小</h4><p>宽度：</p>
<script type="math/tex; mode=display">
out_w = \frac {W - F_w + P} {S} + 1</script><p>高度：</p>
<script type="math/tex; mode=display">
out_h = \frac {H - F_h + P} {S} + 1</script><p>深度有人工指定。</p>
<ul>
<li>$W$：输入图像的宽度</li>
<li>$H$：输入图像的高度</li>
<li>$F_w$：filter的宽度</li>
<li>$F_h$：filter的高度</li>
<li>$P$：全0填充的宽度</li>
<li>$S$：移动步幅</li>
</ul>
<h3 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h3><p>当前卷积层中所有过滤器的参数是共享的，这样就可以巨幅减少神经网络上的参数。假设输入层矩阵的的维度为32*32*3，第一层卷积层filter尺寸为5*5，深度为16，那么这个卷积层的参数个数为5*5*3*16+16=1216个。如果使用全连接层，那么全连接层的参数个数为32*32*3*500=1536000个。相比之下，卷积层的参数个数要远远小于全连接层。卷积层的参数个数与图片的大小无关，它只和filter的尺寸、深度以及当前输入层的深度有关。这使得CNN可以很好地扩展到更大的图像数据上。</p>
<h2 id="池化层-Pooling-Layer"><a href="#池化层-Pooling-Layer" class="headerlink" title="池化层(Pooling Layer)"></a>池化层(Pooling Layer)</h2><p><font color="red">池化层不会改变三维矩阵的深度，但是它可以缩小矩阵的大小</font>。通过池化层，可以进一步缩小最后全连接层中节点的个数，从而达到减少整个神经网络参数的目的。使用池化层既可以加快计算速度也可以防止过拟合。</p>
<p>池化层filter的计算不是节点的加权和，而是采用最大值或者平均值计算。使用最大值操作的池化层被称之为最大池化层（max pooling），这是被使用得最多的池化层结构。使用平均值操作的池化层被称之为平均池化层（average pooling）。</p>
<p>池化层前向传播的过程也是通过移动一个类似filter的结构完成的。与卷积层的filter类似，池化层的filter也需要人工设定filter的尺寸、全0填充以及filter的步长。卷积层和池化层的filter移动方式也是相似的，唯一的区别在卷积层使用的filter是横跨整个深度的，而池化层使用的filter只影响一个深度上的节点。所以池化层的filter除了在长和宽两个维度移动外，它还需要在深度这个维度移动。如下图所示：</p>
<img   src="/2019/06/02/CNN/pooling.jpg"  class="">
<h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>在经过多轮卷积层和池化层的处理之后，在CNN的最后一般会由1到2个全连接层来给出最后的分类结果。经过几轮卷积层和池化层的处理之后，可以认为图像中的信息已经被抽象成了信息含量更高的特征。我们可以将卷积层和池化层看成自动图像特征提取的过程。在提取完成之后，仍然需要使用全连接层来完成分类任务。</p>
<h2 id="Softmax层"><a href="#Softmax层" class="headerlink" title="Softmax层"></a>Softmax层</h2><p>通过Softmax层，可以得到当前样例属于不同种类的概率分布问题。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN反向传播</title>
    <url>/2019/06/03/CNN%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    <content><![CDATA[<p>深度神经网络(DNN)反向传播的公式推导可以参考之前的博客：<a class="link"   href="https://transformerswsz.github.io/2019/05/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/" >反向传播<i class="fas fa-external-link-alt"></i></a></p>
<span id="more"></span>
<p>要套用DNN的反向传播算法到CNN，有几个问题需要解决：</p>
<ul>
<li>池化层没有激活函数，我们可以令池化层的激活函数为 $g(z) = z$，即激活后输出本身，激活函数的导数为1。</li>
<li>池化层在前向传播的时候，对输入矩阵进行了压缩，我们需要反向推导出 $\delta^{l-1}$，这个方法与DNN完全不同。</li>
<li>卷积层通过张量卷积，或者说若干个矩阵卷积求和得到当前层的输出，而DNN的全连接层是直接进行矩阵乘法而得到当前层的输出。我们需要反向推导出 $\delta^{l-1}$，计算方法与DNN也不同。</li>
<li>对于卷积层，由于 $W$ 使用的是卷积运算，那么从 $\delta^l$ 推导出该层的filter的 $W, b$ 方式也不同。</li>
</ul>
<p>在研究过程中，需要注意的是，由于卷积层可以有多个卷积核，各个卷积核的处理方法是完全相同且独立的，为了简化算法公式的复杂度，我们下面提到卷积核都是卷积层中若干卷积核中的一个。</p>
<p>下面将对上述问题进行逐一分析：</p>
<h1 id="已知池化层的-delta-l-，推导上一隐藏层的-delta-l-1"><a href="#已知池化层的-delta-l-，推导上一隐藏层的-delta-l-1" class="headerlink" title="已知池化层的 $\delta^l$，推导上一隐藏层的 $\delta^{l-1 }$"></a>已知池化层的 $\delta^l$，推导上一隐藏层的 $\delta^{l-1 }$</h1><p>在前向传播算法时，池化层一般我们会用MAX或者Average对输入进行池化，池化的区域大小已知。现在我们反过来，要从缩小后的误差 $ \delta^l $，还原前一次较大区域对应的误差。</p>
<p>在反向传播时，我们首先会把 $ \delta^l $ 的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是MAX，则把 $ \delta^l $ 的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是Average，则把 $ \delta^l $ 的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做 $upsample$。</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>假设池化区域为2*2，步长为2，$\delta^l$ 的第k个子矩阵为：</p>
<script type="math/tex; mode=display">
\delta_k^l = \left(
                \begin{array}{ccc}
                    2 & 8 \\
                    4 & 6
                \end{array}
             \right)</script><p>我们先将 $ \delta_k^l $ 还原，即变成：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    0 & 0 & 0 & 0 \\
    0 & 2 & 8 & 0 \\
    0 & 4 & 6 & 0 \\
    0 & 0 & 0 & 0 \\
    \end{array}
\right)</script><p>如果是MAX，假设我们之前在前向传播时记录的最大值位置分别是左上、右下、右上、左下，则转换后的矩阵为：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    2 & 0 & 0 & 0 \\
    0 & 0 & 0 & 8 \\
    0 & 4 & 0 & 0 \\
    0 & 0 & 6 & 0 \\
    \end{array}
\right)</script><p>如果是Average，转换后的矩阵为：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    0.5 & 0.5 & 2 & 2 \\
    0.5 & 0.5 & 2 & 2 \\
    1 & 1 & 1.5 & 1.5 \\
    1 & 1 & 1.5 & 1.5 \\
    \end{array}
\right)</script><p>这样我们就得到了上一层 $ \frac {\partial J(W, b)} {\partial a_k^{l-1}} $ ，要得到 $\delta_k^{l-1}$ ：</p>
<script type="math/tex; mode=display">
\delta_k^{l-1} = (\frac {\partial a_k^{l-1}} {\partial z_k^{l-1}})^T \frac {\partial J(W, b)} {\partial a_k^{l-1}} = upsample(\delta_k^l) \odot \sigma'(z_k^{l-1})</script><p>其中，$upsample$ 函数完成了池化误差矩阵放大与误差重新分配的逻辑。</p>
<p>对于张量 $\delta^l$ ，我们有：</p>
<script type="math/tex; mode=display">
\delta^{l-1} = upsample(\delta^l) \odot \sigma'(z^{l-1})</script><h2 id="已知卷积层的-delta-l-，推导上一层隐藏层的-delta-l-1"><a href="#已知卷积层的-delta-l-，推导上一层隐藏层的-delta-l-1" class="headerlink" title="已知卷积层的 $\delta^l$，推导上一层隐藏层的 $\delta^{l-1}$"></a>已知卷积层的 $\delta^l$，推导上一层隐藏层的 $\delta^{l-1}$</h2><p>在DNN中，我们知道 $\delta^{l-1}$ 和 $\delta^l$ 的递推关系为：</p>
<script type="math/tex; mode=display">
\delta^{l-1} = \frac {\partial J(W, b)} {\partial z^{l-1}} = (\frac {\partial z^l} {\partial z^{l-1}})^T \frac {\partial J(W, b)} {\partial z^l} = (\frac {\partial z^l} {\partial z^{l-1}})^T \delta^l</script><p>注意到 $z^l$ 和 $z^{l-1}$ 的关系为：</p>
<script type="math/tex; mode=display">
z^l = a^{l-1}*W^l + b^l = \sigma(z^{l-1})*W^l + b^l</script><p>因此我们有：</p>
<script type="math/tex; mode=display">
\delta^{l-1} = (\frac {\partial z^l} {\partial z^{l-1}})^T \delta^l = \delta^l * rot180(W^l) \odot \sigma'(z^{l-1})</script><p>这里的式子其实和DNN的类似，区别在于对于含有卷积的式子求导时，卷积核被旋转了180度。即式子中的 $rot180()$，翻转180度的意思是上下翻转一次，接着左右翻转一次。在DNN中这里只是矩阵的转置。那么为什么呢？由于这里都是张量，直接推演参数太多了。我们以一个简单的例子说明为啥这里求导后卷积核要翻转。</p>
<p>假设我们 $l-1$ 层的输出 $a^{l-1}$ 是一个3*3的矩阵，第 $l$ 层的卷积核 $W^l$ 是一个2*2矩阵，步幅为1，则输出 $z^l$ 是一个 2*2的矩阵，这里 $b^l$ 简化为0，则有：</p>
<script type="math/tex; mode=display">
a^{l-1} * W^l = z^l</script><p>我们列出 $a, W, z$ 的矩阵表达式如下：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{ccc}
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} \\
    \end{array}
\right) * 
\left(
    \begin{array}{cc}
    w_{11} & w_{12} \\
    w_{21} & w_{22} \\
    \end{array}
\right)
= 
\left(
    \begin{array}{cc}
    z_{11} & z_{12} \\
    z_{21} & z_{22} \\
    \end{array}
\right)</script><p>根据卷积得出：</p>
<script type="math/tex; mode=display">
z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22} \\
z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} + a_{23}w_{22} \\
z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} + a_{32}w_{22} \\
z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} + a_{33}w_{22} \\</script><p>接着我们模拟反向求导：</p>
<script type="math/tex; mode=display">
\bigtriangledown a^{l-1} = \frac {\partial J(W, b)} {\partial a^{l-1}} = (\frac {\partial z^l} {\partial a^{l-1}})^T \frac {\partial J(W, b)} {\partial z^l} = (\frac {\partial z^l} {\partial a^{l-1}})^T \delta^l</script><p>从上式可以看出，对于 $ a^{l-1} $ 的梯度误差 $\bigtriangledown a^{l-1} $ ，等于第 $l$ 层的梯度误差乘以 $\frac {\partial z^l} {\partial a^{l-1}}$ ，而 $\frac {\partial z^l} {\partial a^{l-1}}$ 对应上面的例子中相关联的 $w$ 的值。假设 $z$ 矩阵对应的反向传播误差是 $\delta_{11}, \delta_{12}, \delta_{21}, \delta_{22} $ 组成的2*2矩阵，则利用上面梯度的式子和4个等式，我们可以分别写出 $\bigtriangledown a^{l-1}$ 的9个标量的梯度。</p>
<p>比如对于 $a_{11}$ 的梯度，由于在4个等式中 $a_{11}$ 只和 $z_{11}$ 有乘积关系，从而我们有：</p>
<script type="math/tex; mode=display">
\bigtriangledown a_{11} = w_{11}\delta_{11}</script><p>对于 $a_{12}$ 的梯度，由于在4个等式中 $a_{12}$ 和 $z_{11}, z_{12}$ 有乘积关系，从而我们有：</p>
<script type="math/tex; mode=display">
\bigtriangledown a_{12} = w_{11}\delta_{12} + w_{12}\delta_{11}</script><p>同理可得：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\bigtriangledown a_{13} &= w_{12}\delta_{12} \\
\bigtriangledown a_{21} &= w_{11}\delta_{21} + w_{21}\delta_{11} \\
\bigtriangledown a_{22} &= w_{11}\delta_{22} + w_{12}\delta_{21} + w_{21}\delta_{12} + w_{22}\delta_{11} \\
\bigtriangledown a_{23} &= w_{12}\delta_{22} + w_{22}\delta_{12} \\
\bigtriangledown a_{31} &= w_{21}\delta_{21} \\
\bigtriangledown a_{32} &= w_{21}\delta_{22} + w_{22}\delta_{21}\\
\bigtriangledown a_{33} &= w_{22}\delta_{22} \\
\end{aligned}
\end{equation}</script><p>这上面9个式子其实可以用一个矩阵卷积的形式表示，即：</p>
<script type="math/tex; mode=display">
\left(
    \begin{array}{cccc}
    0 & 0 & 0 & 0 \\
    0 & \delta_{11} & \delta_{12} & 0 \\
    0 & \delta_{21} & \delta_{22} & 0 \\
    0 & 0 & 0 & 0
    \end{array}
\right) * 
\left(
    \begin{array}{cc}
    w_{22} & w_{21} \\
    w_{12} & w_{11} \\
    \end{array}
\right)
= 
\left(
    \begin{array}{cc}
    \bigtriangledown a_{11} & \bigtriangledown a_{12} & \bigtriangledown a_{13} \\
    \bigtriangledown a_{21} & \bigtriangledown a_{22} & \bigtriangledown a_{23} \\
    \bigtriangledown a_{31} & \bigtriangledown a_{32} & \bigtriangledown a_{33} 
    \end{array}
\right)</script><p>为了符合梯度计算，我们在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差。这个例子直观的介绍了为什么对含有卷积的式子反向传播时，卷积核要翻转180度的原因。</p>
<p>以上就是卷积层的误差反向传播过程。</p>
<h2 id="已知卷积层的-delta-l-，推导该层的-W-b-的梯度"><a href="#已知卷积层的-delta-l-，推导该层的-W-b-的梯度" class="headerlink" title="已知卷积层的 $\delta^l$，推导该层的 $W, b$ 的梯度"></a>已知卷积层的 $\delta^l$，推导该层的 $W, b$ 的梯度</h2><p>对于全连接层，可以按DNN的反向传播算法求该层 $W, b$ 的梯度，而池化层并没有 $W, b$ ,也不用求 $W, b$ 的梯度。只有卷积层的 $W, b$ 需要求出。</p>
<p>注意到卷积层 $z$ 和 $W, b$ 的关系为：</p>
<script type="math/tex; mode=display">
z^l = a^{l-1} * W^l + b</script><p>因此我们有：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W^l} = a^{l-1} * \delta^l</script><p>注意到此时卷积核并没有反转，主要是此时是层内的求导，而不是反向传播到上一层的求导。具体过程我们可以分析一下。</p>
<p>这里举一个简化的例子，这里输入是矩阵，不是张量，那么对于第 $l$ 层，某个卷积核矩阵 $W$ 的导数可以表示如下：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W_{pq}^l} = \sum_i \sum_j(\delta_{ij}^l a_{i+p-1, j+q-1}^{l-1})</script><p>　那么根据上面的式子，我们有：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W_{11}^l} = a_{11}\delta_{11} + a_{12}\delta_{12} + a_{21}\delta_{21} + a_{22}\delta_{22} \\

\frac {\partial J(W, b)} {\partial W_{12}^l} = a_{12}\delta_{11} + a_{13}\delta_{12} + a_{22}\delta_{21} + a_{23}\delta_{22} \\

\frac {\partial J(W, b)} {\partial W_{13}^l} = a_{13}\delta_{11} + a_{14}\delta_{12} + a_{23}\delta_{21} + a_{24}\delta_{22} \\
...... \\
\frac {\partial J(W, b)} {\partial W_{33}^l} = a_{33}\delta_{11} + a_{34}\delta_{12} + a_{43}\delta_{21} + a_{44}\delta_{22} \\</script><p>最终我们可以一共得到9个式子。整理成矩阵形式后可得：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial W^l} = 
\left(
    \begin{array}{cccc}
    a_{11} & a_{12} & a_{13} & a_{14} \\
    a_{21} & a_{22} & a_{23} & a_{24} \\
    a_{31} & a_{32} & a_{33} & a_{34} \\
    a_{41} & a_{42} & a_{43} & a_{44}
    \end{array}
\right)
*
\left(
    \begin{array}{cccc}
    \delta_{11} & \delta_{12} \\
    \delta_{21} & \delta_{22}
    \end{array}
\right)</script><p>从而可以清楚的看到这次我们为什么没有反转的原因。</p>
<p>而对于 $b$，则稍微有些特殊，因为 $ \delta^l $ 是高维张量，而 $b$ 只是一个向量，不能像DNN那样直接和 $ \delta^l $ 相等。通常的做法是将 $ \delta^l $ 的各个子矩阵的项分别求和，得到一个误差向量，即为 $b$ 的梯度：</p>
<script type="math/tex; mode=display">
\frac {\partial J(W, b)} {\partial b^l} = \sum_{u, v}(\delta^l)_{u, v}</script><h2 id="CNN反向传播算法总结"><a href="#CNN反向传播算法总结" class="headerlink" title="CNN反向传播算法总结"></a>CNN反向传播算法总结</h2><p>现在我们总结下CNN的反向传播算法，以最基本的批量梯度下降法为例来描述反向传播算法。</p>
<p>输入：m个图片样本，CNN模型的层数L和所有隐藏层的类型，对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。对于池化层，要定义池化区域大小k和池化标准（MAX或Average），对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。梯度学习率 $\alpha$,最大迭代次数MAX与停止迭代阈值 $\epsilon$</p>
<p>输出：CNN模型各隐藏层与输出层的 $W, b$</p>
<ol>
<li>初始化各隐藏层与输出层的各 $W, b$ 的值为一个随机值。</li>
<li>for iter to 1 to MAX:<ol>
<li>for i =1 to m：<ol>
<li>将CNN输入 $a^1$ 设置为 $x_i$ 对应的张量</li>
<li>for l = 2 to L-1，根据下面3种情况进行前向传播算法计算：<ul>
<li>如果当前是全连接层：则有 $a^{i, l} = \sigma(z^{i, l}) = \sigma(W^l a^{i, l-1} + b^l)  $</li>
<li>如果当前是卷积层：则有 $a^{i, l} = \sigma(z^{i, l}) = \sigma(W^l * a^{i, l-1} + b^l)  $</li>
<li>如果当前是池化层：则有 $a^{i, l} = pool(a^{i, l-1})$</li>
</ul>
</li>
<li>对于输出层第L层：$a^{i, L} = softmax(z^{i, L}) = softmax(W^L a^{i, L-1} + b^L)$</li>
<li>通过损失函数计算输出层的 $\delta^{i, L}$</li>
<li>for l = L-1 to 2, 根据下面3种情况进行进行反向传播算法计算：<ul>
<li>如果当前是全连接层：$ \delta^{i, l} = (W^{l+1})^T \delta^{i, l+1} \odot \sigma’(z^{i, l}) $</li>
<li>如果当前是卷积层：$ \delta^{i, l} = \delta^{i, l+1} * rot180(W^{l+1}) \odot \sigma’(z^{i, l}) $</li>
<li>如果当前是池化层：$ \delta^{i, l} = upsample(\delta^{i, l+1}) \odot \sigma’(z^{i, l}) $</li>
</ul>
</li>
</ol>
</li>
<li>for l = 2 to L，根据下面2种情况更新第 $l$ 层的 $W^l, b^l$：<ul>
<li>如果当前是全连接层：$W^l = W^l - \alpha \sum_{i=1}^m \delta^{i, l}(a^{i, l-1})^T, b^l = b^l - \alpha \sum_{i=1}^m \delta^{i, l} $</li>
<li>如果当前是卷积层，对于每一个卷积核有：$ W^l = W^l - \alpha \sum_{i=1}^m \delta^{i, l} * a^{i, l-1}, b^l = b^l - \alpha \sum_{i=1}^m \sum_{u, v}(\delta^{i, l})_{u, v} $</li>
</ul>
</li>
<li>如果所有的 $W, b$ 的变化值都小于停止迭代阈值 $\epsilon$，则跳出迭代循环到步骤3。</li>
</ol>
</li>
<li>输出各隐藏层与输出层的线性关系系数矩阵 $W$ 和偏置量 $b$ 。</li>
</ol>
<hr>
<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/6494810.html?tdsourcetag=s_pcqq_aiomsg" >卷积神经网络(CNN)反向传播算法<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>COBRA详解</title>
    <url>/2025/04/03/COBRA%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>这是一篇生成式推荐用于召回场景的工作，其建模范式仍旧是输入端根据用户行为序列构造prompt，输出端预测next item。该工作巧妙地将稀疏ID与稠密向量表征级联融合起来，达到了SOTA水平。</p>
<span id="more"></span>
<h2 id="传统方法对比"><a href="#传统方法对比" class="headerlink" title="传统方法对比"></a>传统方法对比</h2><div class="table-container">
<table>
<thead>
<tr>
<th>方案类型</th>
<th>核心技术</th>
<th>局限性</th>
</tr>
</thead>
<tbody>
<tr>
<td>纯文本+LLM</td>
<td>直接使用广告文本特征</td>
<td>输入过长，资源消耗大</td>
</tr>
<tr>
<td>短语表征</td>
<td>关键词压缩表达</td>
<td>信息丢失严重</td>
</tr>
<tr>
<td>稠密表征+对比学习</td>
<td>端到端向量编码</td>
<td>建模复杂度高，缺少兴趣探索</td>
</tr>
<tr>
<td>稀疏ID生成</td>
<td>RQ-VAE量化技术</td>
<td>信息损失导致细粒度捕捉弱</td>
</tr>
</tbody>
</table>
</div>
<h2 id="COBRA介绍"><a href="#COBRA介绍" class="headerlink" title="COBRA介绍"></a>COBRA介绍</h2><p><strong>稀疏ID可以唯一表示item，有很好的区分性，但丧失了对item的细粒度信息刻画。纯文本可以准确可以item属性，但构造成prompt太长，套入到LLM中会导致资源消耗过大</strong>。那么如何结合两者的优点呢？</p>
<p>COBRA首先根据codebook生成item的稀疏ID，<strong>该ID可以理解为item的大类别。既不过于精细，像unique id，又不过于宽泛</strong>。然后将ID序列输入到Transformer Decoder中预测稠密向量。</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.wiqfmnvsa.webp"  alt="model"></p>
<h4 id="离线训练"><a href="#离线训练" class="headerlink" title="离线训练"></a>离线训练</h4><p>两个预测任务的损失函数如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text {sparse }}=-\sum_{t=1}^{T-1} \log \left(\frac{\exp \left(z_{t+1}^{I D_{t+1}}\right)}{\sum_{j=1}^C \exp \left(z_{t+1}^j\right)}\right) \\

\left.\left.\mathcal{L}_{\text {dense }}=-\sum_{t=1}^{T-1} \log \frac{\exp \left(\cos \left(\hat{\mathbf{v}}_{t+1} \cdot \mathbf{v}_{t+1}\right)\right)}{\sum_{\text {item }_j \in \text { Batch }} \exp \left(\operatorname { c o s } \left(\hat{\mathbf{v}}_{t+1}, \mathbf{v}_{\text {item }}^j\right.\right.} \mathbf{}\right)\right)</script><p>ID预测就是经典的多分类任务，dense vector就是经典的对比学习任务。</p>
<h4 id="在线推理"><a href="#在线推理" class="headerlink" title="在线推理"></a>在线推理</h4><ol>
<li><p>稀疏ID生成：decoder根据beam search生成top $M$个ID，每个ID有其得分<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.64e12ncrd7.webp"  alt="id gen"></p>
</li>
<li><p>稠密向量生成：根据每个稀疏ID继续生成dense vector，然后检索出同一个ID下的跟vector相似的top $N$个候选item<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.9dd4zb21qb.webp"  alt="vector gen"></p>
</li>
<li><p>最终召回候选集生成：为了兼顾多样性（即不同ID）以及准确性（即同一ID下的候选item），联合打分取top $K$个item召回<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.45uxxdi6n.webp"  alt="recall"></p>
</li>
</ol>
<h4 id="在离线实验结果"><a href="#在离线实验结果" class="headerlink" title="在离线实验结果"></a>在离线实验结果</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.51ebrrvbkk.webp"  alt="offline"></p>
<p>在公共数据集上，离线指标提升很明显。在A/B实验上，转化率和收入也在咔咔涨，就不细说了。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://arxiv.org/pdf/2503.02453" >Sparse Meets Dense: Unified Generative Recommendations with Cascaded Sparse-Dense Representations<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/32AWMSGdwlA5W7rWQG-Plw" >一篇论文，看见百度广告推荐系统在大模型时代的革新<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Generative Recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF</title>
    <url>/2019/07/07/CRF/</url>
    <content><![CDATA[<p>条件随机场(Conditional Random Fields)是给定一组输入序列条件下另一组输出序列的概率分布模型，在NLP中应用很广泛。</p>
<span id="more"></span>
<h1 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h1><p>假设我们有Bob一天从早到晚的一系列照片，Bob想考考我们，要我们猜这一系列的每张照片对应的活动，比如: 工作的照片，吃饭的照片，唱歌的照片等等。一个比较直观的办法就是，我们找到Bob之前的日常生活的一系列照片，然后找Bob问清楚这些照片代表的活动标记，这样我们就可以用监督学习的方法来训练一个分类模型，比如逻辑回归，接着用模型去预测这一天的每张照片最可能的活动标记。</p>
<p>这种办法虽然是可行的，但是却忽略了一个重要的问题，就是这些照片之间的顺序其实是有很大的时间顺序关系的，而用上面的方法则会忽略这种关系。比如我们现在看到了一张Bob闭着嘴的照片，那么这张照片我们怎么标记Bob的活动呢？比较难去打标记。但是如果我们有Bob在这一张照片前一点点时间的照片的话，那么这张照片就好标记了。如果在时间序列上前一张的照片里Bob在吃饭，那么这张闭嘴的照片很有可能是在吃饭咀嚼。而如果在时间序列上前一张的照片里Bob在唱歌，那么这张闭嘴的照片很有可能是在唱歌。</p>
<p>为了让我们的分类器表现的更好，在标记数据的时候，可以考虑相邻数据的标记信息。这一点，是普通的分类器难以做到的。而这一块，也是CRF比较擅长的地方。</p>
<p>在实际应用中，自然语言处理中的词性标注(POS Tagging)就是非常适合CRF使用的地方。词性标注的目标是给出一个句子中每个词的词性（名词，动词，形容词等）。而这些词的词性往往和上下文的词的词性有关，因此，使用CRF来处理是很适合的。</p>
<h1 id="MRF"><a href="#MRF" class="headerlink" title="MRF"></a>MRF</h1><p>随机场是由若干个位置组成的整体，当给每一个位置中按照某种分布随机赋予一个值之后，其全体就叫做随机场。举个词性标注的例子：假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词…)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。</p>
<p>马尔可夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅与它相邻的位置的赋值有关。就拿上面的例子来说，我们假设所有词的词性只和它相邻的词的词性有关，这个随机场就特化成MRF。比如第三个词的词性除了与自己本身的位置有关外，只与第二个词和第四个词的词性相关。</p>
<h1 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h1><p>CRF是MRF的特例，它假设MRF中只有 $X$ 和 $Y$ 两种变量。$X$ 一般是给定的，而 $Y$ 是在给定 $X$ 的条件下的输出。这样MRF就特化成了CRF。在十个词的句子词性标注中，$X$ 是词，$Y$ 是词性。因此，如果我们假设它是一个MRF，那么它也是CRF。</p>
<p>我们用准确的数学语言来描述：</p>
<p>设 $X$ 和 $Y$ 是随机变量，$P(Y|X)$ 是给定 $X$ 时 $Y$ 的条件概率分布。若 $Y$ 构成MRF，则称条件概率分布 $P(Y|X)$ 是CRF。</p>
<h1 id="Linear-CRF"><a href="#Linear-CRF" class="headerlink" title="Linear CRF"></a>Linear CRF</h1><p>设 $X = (X_1, X_2, \dots , X_n), Y = (Y_1, Y_2, \dots, Y_n)$ 均为线性链的随机变量序列。在给定随机变量序列 $X$ 的情况下，随机变量 $Y$ 的条件概率分布 $P(Y|X)$ 满足马尔可夫性：</p>
<script type="math/tex; mode=display">
P(Y_i | X, Y_1, Y_2, \dots , Y_n) = P(Y_i | X, Y_{i-1}, Y_{i+1})</script><p>则称 $P(Y|X)$ 为线性链条件随机场。</p>
<h2 id="参数化形式"><a href="#参数化形式" class="headerlink" title="参数化形式"></a>参数化形式</h2><p>我们通过特征函数及其权重系数来将Linear CRF转化为机器学习模型。</p>
<p>特征函数分为两类，一类是定义在 $Y$ 节点上的状态特征函数，这类特征函数只与当前节点有关，记为：</p>
<script type="math/tex; mode=display">
s_l(y_i, x, i), \quad l=1, 2, \dots, L</script><p>$i$ 是当前节点在序列的位置，$L$ 表示当前节点的状态特征函数的个数。</p>
<p>另一类是定义在 $Y$ 上下文的转移特征函数，这类特征函数只和当前节点和上一个节点有关，记为：</p>
<script type="math/tex; mode=display">
t_k(y_{i-1}, y_i, x, i), \quad k = 1, 2, \dots, K</script><p> $i$ 是当前节点在序列的位置，$K$ 表示当前节点的转移特征函数的个数。</p>
<p>无论是状态特征函数还是转移特征函数，它们的取值只能是0或1。即满足特征条件或不满足特征条件。同时，我们可以为每个特征函数赋予一个权值，用以表达我们对这个特征函数的信任度。假设 $t_k$ 的权重系数是 $\lambda_k$ ，$s_l$ 权重系数是 $\mu_l$ , 由此得到Linear CRF的参数化形式：</p>
<script type="math/tex; mode=display">
P(y|x) = \frac {1} {Z(x)} e^{\sum_{i, k} \lambda_k t_k(y_{i-1}, y_i, x, i) + \sum_{i, l} \mu_l s_l (y_i, x, i)}</script><p>其中，$Z(x)$ 为规范化因子：</p>
<script type="math/tex; mode=display">
Z(x) = \sum_y e^{\sum_{i, k} \lambda_k t_k(y_{i-1}, y_i, x, i) + \sum_{i, l} \mu_l s_l (y_i, x, i)}</script><p>每个特征函数定义了一个Linear CRF的规则，其系数定义了这个规则的可信度。两者一起构成了Linear CRF的条件概率分布。</p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>这里举一个词性标注的例子。假设输入的都是三个词的句子，即 $X = (X_1, X_2, X_3)$，输出的词性标记为 $Y = (Y_1, Y_2, Y_3)$，其中 $Y \in {\lbrace   1(名词), 2(动词) \rbrace}$。这里只标记出取值为1的特征函数：</p>
<script type="math/tex; mode=display">
t_1 = t_1(y_{i-1}=1, y_i=2, x, i), \quad i = 2, 3, \quad \lambda_1=1 \\
t_2 = t_2(y_1=1, y_2=1, x, 2), \quad \lambda_2 = 0.5 \\
t_3 = t_3(y_2=2, y_3=1, x, 3), \quad \lambda_3 = 1 \\
t_4 = t_4(y_1=2, y_2=1, x, 2), \quad \lambda_4 = 1 \\
t_5 = t_5(y_2=2, y_3=2, x, 3), \quad \lambda_5 = 0.2 \\
s_1 = s_1(y_1=1, x, 1), \quad \mu_1=1 \\
s_2 = s_2(y_i=2, x, i), \quad i=1, 2, \quad \mu_2 = 0.5 \\
s_3 = s_3(y_i=1, x, i), \quad i=2, 3 \quad \mu_3 = 0.8 \\
s_4 = s_4(y_3=2, x, 3) \quad \mu_4 = 0.5</script><p>求标记(1, 2, 2)的概率。</p>
<p>根据上述参数化公式我们有：</p>
<script type="math/tex; mode=display">
P(y|x) \propto e^{\sum_{i, k} \lambda_k t_k(y_{i-1}, y_i, x, i) + \sum_{i, l} \mu_l s_l (y_i, x, i)}</script><p>代入(1, 2, 2)得到：</p>
<script type="math/tex; mode=display">
P(y_1=1, y_2=2, y_3=2|x) \propto e^{3.2}</script><h2 id="简化形式"><a href="#简化形式" class="headerlink" title="简化形式"></a>简化形式</h2><p>我们用 $s_l$ 表示状态特征函数，用 $t_k$ 表示转移特征函数，同时也使用了不同的符号表示权重系数，导致表示起来非常麻烦。这里我们简化一下表示形式。</p>
<p>假设在某节点有 $K_1$ 个状态特征函数和 $K_2$ 个转移特征函数。我们用一个特征函数 $f_k(y_{i-1}, y_i, x, i)$ 来统一表示：</p>
<script type="math/tex; mode=display">
f_k(y_{i-1}, y_i, x, i) = \begin{cases}
t_k(y_{i-1}, y_i, x, i) \quad k=1, 2, \dots, K_1 \\
s_l(y_i, x, i) \quad k = K_1+l, \quad l=1, 2, \dots, K2
\end{cases}</script><p>同时我们也统一 $f_k(y_{i-1}, y_i, x, i)$ 对应的权重系数 $w_k$ 如下：</p>
<script type="math/tex; mode=display">
w_k = \begin{cases}
\lambda_k \quad k = 1, 2, \dots, K_1 \\
\mu_l \quad k = K_1 + l, \quad l = 1, 2, \dots, K_2
\end{cases}</script><p>最终Linear CRF的参数化形式简化如下：</p>
<script type="math/tex; mode=display">
P_w(y|x) = \frac {e^{\sum_{k=1}^K w_k f_k(y_{i-1}, y_i, x, i)}} {\sum_y e^{\sum_{k=1}^K w_k f_k(y_{i-1}, y_i, x, i)}}</script><p>其中，$y$ 表示一条输出序列，如上例的(1, 2, 2)</p>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/7048333.html" >条件随机场CRF(一)从随机场到线性链条件随机场<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.zhihu.com/question/53458773/answer/554436625" >条件随机场（CRF）和隐马尔科夫模型（HMM）最大区别在哪里？CRF的全局最优体现在哪里？<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>MRF</tag>
        <tag>Linear CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF损失函数与Viterbi算法</title>
    <url>/2021/03/24/CRF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8EViterbi%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>CRF考虑到了输出层面的关联性，如下图所示：</p>
<span id="more"></span>
<img   src="/2021/03/24/CRF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8EViterbi%E7%AE%97%E6%B3%95/1.png"  class="">
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>时间步 $t$ 输出的标签值由两部分组成：</p>
<ul>
<li>发射分数：$ h(y_t;X) $</li>
<li>转移分数：$ g(y_t;y_{t-1}) $</li>
</ul>
<p>一条路径标识为 $y_1, y_2, \dots , y_n$ 的概率为：</p>
<script type="math/tex; mode=display">
P(y_1, y_2, \dots, y_n | X) = \frac{1}{Z(X)} e^{h(y_1;x)+\sum_{i=2}^{n}g(y_i;y_{i-1})+h(y_i;X)}</script><p>其中 $Z(X)$ 为归一化因子。在 CRF 模型中，由于我们只考虑了临近标签的联系（马尔可夫假设），因此我们可以递归地算出归一化因子，这使得原来是指数级的计算量降低为线性级别。</p>
<p>具体来说，我们将计算到时刻 $t$ 的归一化因子记为 $Z_t$，并将它分为 $k$ 个部分：</p>
<script type="math/tex; mode=display">
Z_t = Z_t^1 + Z_t^2 + \cdots + Z_t^k</script><p>上式分别是截止到当前时刻 $t$ 中、以标签 $1,2,\cdots, k$ 为终点的所有路径的得分指数和。那么，我们可以递归地计算：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
Z_{t+1}^{(1)}=\left(Z_{t}^{(1)} G_{11}+Z_{t}^{(2)} G_{21}+\cdots+Z_{t}^{(k)} G_{k 1}\right) H_{t+1}(1 \mid X) \\
Z_{t+1}^{(2)}=\left(Z_{t}^{(1)} G_{12}+Z_{t}^{(2)} G_{22}+\cdots+Z_{t}^{(k)} G_{k 2}\right) H_{t+1}(2 \mid X) \\
\vdots \\
Z_{t+1}^{(k)}=\left(Z_{i}^{(1)} G_{1 k}+Z_{t}^{(2)} G_{2 k}+\cdots+Z_{t}^{(k)} G_{k k}\right) H_{t+1}(k \mid X)
\end{array}</script><p>其中$G_{ij} = e^{g(y_j;y_i)}, H(y_{t+1}|X)=e^{h(y_{t+1}|X)}$，上式简写成矩阵形式为：</p>
<script type="math/tex; mode=display">
Z_{t+1} = Z_tG \otimes H_{t+1}</script><p>为了符合损失函数的含义，将其定义为：</p>
<script type="math/tex; mode=display">
Loss = -logP(y_1, y_2, \dots, y_n | X)</script><h2 id="viterbi-算法"><a href="#viterbi-算法" class="headerlink" title="viterbi 算法"></a>viterbi 算法</h2><p>有了损失函数后，就可以通过反向传播结合梯度下降来求解最优参数。</p>
<p>序列标注的目标是找出一条概率最高的路径。假设整个网络的宽度为 $k$，网络长度为 $N$ ，按照穷举法求最佳路径的时间复杂度为 $O(k^N)$，但CRF采用了马尔可夫假设，因此可以使用动态规划来求解，时间复杂度优化到 $O(N \times k^2)$。</p>
<p>具体示例可见：<a class="link"   href="https://www.zhihu.com/question/20136144" >如何通俗地讲解 viterbi 算法？<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.jiqizhixin.com/articles/2018-05-23-3" >简明条件随机场CRF介绍 | 附带纯Keras实现<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_16949707/article/details/107812643" >CRF条件随机场loss函数与维特比算法理解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>Viterbi</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>CUDA编程模型</title>
    <url>/2024/02/28/CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/GPU%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B.mcrgzor0hj4.png"  alt="CUDA"></p>
<p>参照数学坐标系，grid的规格是 $(4,3)$ ，block的规格是 $(3,2)$</p>
<span id="more"></span>
<p>对于CUDA编程模型，本质上还是要掌握并行编程思想。每一个矩阵元素运算，都是由一条线程执行。我们要做的就是找到线程坐标位置及其对应的矩阵元素，然后执行计算逻辑。</p>
<p>下面是一个二维矩阵相加示例：</p>
<p><code>cudastart.h</code><br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">ifndef</span> CUDASTART_H</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CUDASTART_H</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(call)\</span></span><br><span class="line"><span class="meta">&#123;\</span></span><br><span class="line"><span class="meta">  const cudaError_t <span class="keyword">error</span>=call;\</span></span><br><span class="line"><span class="meta">  <span class="keyword">if</span>(<span class="keyword">error</span>!=cudaSuccess)\</span></span><br><span class="line"><span class="meta">  &#123;\</span></span><br><span class="line"><span class="meta">      printf(<span class="string">&quot;ERROR: %s:%d,&quot;</span>,__FILE__,__LINE__);\</span></span><br><span class="line"><span class="meta">      printf(<span class="string">&quot;code:%d,reason:%s\n&quot;</span>,<span class="keyword">error</span>,cudaGetErrorString(<span class="keyword">error</span>));\</span></span><br><span class="line"><span class="meta">      exit(1);\</span></span><br><span class="line"><span class="meta">  &#125;\</span></span><br><span class="line"><span class="meta">&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> _WIN32</span></span><br><span class="line"><span class="meta">#	<span class="keyword">include</span> <span class="string">&lt;windows.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#	<span class="keyword">include</span> <span class="string">&lt;sys/time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">double</span> <span class="title function_">cpuSecond</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">timeval</span> <span class="title">tp</span>;</span></span><br><span class="line">  gettimeofday(&amp;tp,<span class="literal">NULL</span>);</span><br><span class="line">  <span class="keyword">return</span>((<span class="type">double</span>)tp.tv_sec+(<span class="type">double</span>)tp.tv_usec*<span class="number">1e-6</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">initialData</span><span class="params">(<span class="type">float</span>* ip,<span class="type">int</span> size)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">time_t</span> t;</span><br><span class="line">  srand((<span class="type">unsigned</span> )time(&amp;t));</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;i++)</span><br><span class="line">  &#123;</span><br><span class="line">    ip[i]=(<span class="type">float</span>)(rand()&amp;<span class="number">0xffff</span>)/<span class="number">1000.0f</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">initDevice</span><span class="params">(<span class="type">int</span> devNum)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">int</span> dev = devNum;</span><br><span class="line">  cudaDeviceProp deviceProp;</span><br><span class="line">  CHECK(cudaGetDeviceProperties(&amp;deviceProp,dev));</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Using device %d: %s\n&quot;</span>,dev,deviceProp.name);</span><br><span class="line">  CHECK(cudaSetDevice(dev));</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">checkResult</span><span class="params">(<span class="type">float</span> * hostRef,<span class="type">float</span> * gpuRef,<span class="type">const</span> <span class="type">int</span> N)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">double</span> epsilon=<span class="number">1.0E-8</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N;i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">abs</span>(hostRef[i]-gpuRef[i])&gt;epsilon)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;Results don\&#x27;t match!\n&quot;</span>);</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;%f(hostRef[%d] )!= %f(gpuRef[%d])\n&quot;</span>,hostRef[i],i,gpuRef[i],i);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Check result success!\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure></p>
<p><code>sum_martix.cu</code><br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cudastart.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// CPU对照组，用于对比加速比</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sumMatrix2DonCPU</span><span class="params">(<span class="type">float</span> *MatA, <span class="type">float</span> *MatB, <span class="type">float</span> *MatC, <span class="type">int</span> nx, <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> *a = MatA;</span><br><span class="line">    <span class="type">float</span> *b = MatB;</span><br><span class="line">    <span class="type">float</span> *c = MatC;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; ny; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nx; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            c[i] = a[i] + b[i];</span><br><span class="line">        &#125;</span><br><span class="line">        c += nx;</span><br><span class="line">        b += nx;</span><br><span class="line">        a += nx;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 核函数，每一个线程计算矩阵中的一个元素。</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">sumMatrix</span><span class="params">(<span class="type">float</span> *MatA, <span class="type">float</span> *MatB, <span class="type">float</span> *MatC, <span class="type">int</span> nx, <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ix = threadIdx.x + blockDim.x * blockIdx.x;</span><br><span class="line">    <span class="type">int</span> iy = threadIdx.y + blockDim.y * blockIdx.y;</span><br><span class="line">    <span class="comment">// 找到该线程的坐标位置</span></span><br><span class="line">    <span class="type">int</span> idx = ix + iy * nx;</span><br><span class="line">    <span class="keyword">if</span> (ix &lt; nx &amp;&amp; iy &lt; ny)</span><br><span class="line">    &#123;</span><br><span class="line">        MatC[idx] = MatA[idx] + MatB[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 主函数</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 设备初始化</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;strating...\n&quot;</span>);</span><br><span class="line">    initDevice(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输入二维矩阵，4096*4096，单精度浮点型。</span></span><br><span class="line">    <span class="type">int</span> nx = <span class="number">1</span> &lt;&lt; <span class="number">12</span>;</span><br><span class="line">    <span class="type">int</span> ny = <span class="number">1</span> &lt;&lt; <span class="number">13</span>;</span><br><span class="line">    <span class="type">int</span> nBytes = nx * ny * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Malloc，开辟主机内存</span></span><br><span class="line">    <span class="type">float</span> *A_host = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    <span class="type">float</span> *B_host = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    <span class="type">float</span> *C_host = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    <span class="type">float</span> *C_from_gpu = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    initialData(A_host, nx * ny);</span><br><span class="line">    initialData(B_host, nx * ny);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// cudaMalloc，开辟设备内存</span></span><br><span class="line">    <span class="type">float</span> *A_dev = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">float</span> *B_dev = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">float</span> *C_dev = <span class="literal">NULL</span>;</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;A_dev, nBytes));</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;B_dev, nBytes));</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;C_dev, nBytes));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输入数据从主机内存拷贝到设备内存</span></span><br><span class="line">    CHECK(cudaMemcpy(A_dev, A_host, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line">    CHECK(cudaMemcpy(B_dev, B_host, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 二维线程块，32×32</span></span><br><span class="line">    dim3 <span class="title function_">block</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span>;</span><br><span class="line">    <span class="comment">// 二维线程网格，128×128</span></span><br><span class="line">    dim3 <span class="title function_">grid</span><span class="params">((nx - <span class="number">1</span>) / block.x + <span class="number">1</span>, (ny - <span class="number">1</span>) / block.y + <span class="number">1</span>)</span>;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;grid.x %d, grid.y %d\n&quot;</span>, grid.x, grid.y);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试GPU执行时间</span></span><br><span class="line">    <span class="type">double</span> gpuStart = cpuSecond();</span><br><span class="line">    <span class="comment">// 将核函数放在线程网格中执行</span></span><br><span class="line">    sumMatrix&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A_dev, B_dev, C_dev, nx, ny);</span><br><span class="line">    CHECK(cudaDeviceSynchronize());</span><br><span class="line">    <span class="type">double</span> gpuTime = cpuSecond() - gpuStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;GPU Execution Time: %f sec\n&quot;</span>, gpuTime);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 在CPU上完成相同的任务</span></span><br><span class="line">    cudaMemcpy(C_from_gpu, C_dev, nBytes, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="type">double</span> cpuStart = cpuSecond();</span><br><span class="line">    sumMatrix2DonCPU(A_host, B_host, C_host, nx, ny);</span><br><span class="line">    <span class="type">double</span> cpuTime = cpuSecond() - cpuStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;CPU Execution Time: %f sec\n&quot;</span>, cpuTime);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检查GPU与CPU计算结果是否相同</span></span><br><span class="line">    CHECK(cudaMemcpy(C_from_gpu, C_dev, nBytes, cudaMemcpyDeviceToHost));</span><br><span class="line">    checkResult(C_host, C_from_gpu, nx * ny);</span><br><span class="line"></span><br><span class="line">    cudaFree(A_dev);</span><br><span class="line">    cudaFree(B_dev);</span><br><span class="line">    cudaFree(C_dev);</span><br><span class="line">    <span class="built_in">free</span>(A_host);</span><br><span class="line">    <span class="built_in">free</span>(B_host);</span><br><span class="line">    <span class="built_in">free</span>(C_host);</span><br><span class="line">    <span class="built_in">free</span>(C_from_gpu);</span><br><span class="line">    cudaDeviceReset();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>编译 <code>sum_martix.cu</code> 文件并执行程序：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvcc -o sum_matrix sum_martix.cu &amp;&amp; ./sum_matrix </span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/34587739" >CUDA编程入门极简教程<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_43715171/article/details/121794135" >cuda中threadIdx、blockIdx、blockDim和gridDim的使用<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/97192227" >CUDA编程入门（三）从矩阵加法例程上手CUDA编程<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>CVR预估中的多任务学习</title>
    <url>/2023/03/13/CVR%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>介绍一下阿里巴巴在CVR领域多任务学习的三篇论文：</p>
<span id="more"></span>
<h2 id="ESMM"><a href="#ESMM" class="headerlink" title="ESMM"></a>ESMM</h2><p>经典之作，不解释了。公式如下：</p>
<script type="math/tex; mode=display">
\underbrace{p(y=1, z=1 \mid \boldsymbol{x})}_{p C T C V R}=\underbrace{p(y=1 \mid \boldsymbol{x})}_{p C T R} \times \underbrace{p(z=1 \mid y=1, \boldsymbol{x})}_{p C V R}</script><img   src="/2023/03/13/CVR%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/esmm.png"  class="esmm">
<img   src="/2023/03/13/CVR%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/esm_result.png"  class="esm_result">
<h2 id="ESM2"><a href="#ESM2" class="headerlink" title="ESM2"></a>ESM2</h2><p>用户点击商品后，除了直接购买外，还会有加入购物车等行为，这些对产生转化也有决定性的影响。因此建模这些行为序列也是多任务学习的重要方向。</p>
<img   src="/2023/03/13/CVR%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/esm2_example.png"  class="esm2_example">
<img   src="/2023/03/13/CVR%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/esm2_model.png"  class="esm2_model">
<img   src="/2023/03/13/CVR%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/esm2_result.png"  class="esm2_result">
<h2 id="HM3"><a href="#HM3" class="headerlink" title="HM3"></a>HM3</h2><p>该工作认为ESM2中引入的用户行为是宏观行为。在这些宏观行为之外，还可以观察到很多微观行为（比如在点击商品后，查看商品大图、查看评论、咨询客服等等），这些微观行为也可以辅助进行cvr预估。</p>
<img   src="/2023/03/13/CVR%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/hm_example.png"  class="hm_example">
<img   src="/2023/03/13/CVR%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/hm_model.png"  class="hm_model">
<img   src="/2023/03/13/CVR%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/hm_result.png"  class="hm_result">
<hr>
<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/RqZzwwJNF1b2Rad1PAtbAA" >CVR预估中的多任务学习三部曲<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>Multi-Task</tag>
        <tag>CVR</tag>
      </tags>
  </entry>
  <entry>
    <title>DANN &amp; GRL</title>
    <url>/2024/09/24/DANN-GRL/</url>
    <content><![CDATA[<p>域自适应是指在目标域与源域的数据分布不同但任务相同下的迁移学习，从而将模型在源域上的良好性能迁移到目标域上，极大地缓解目标域标签缺失严重导致模型性能受损的问题。</p>
<p>介绍一篇经典工作 <a class="link"   href="https://proceedings.mlr.press/v37/ganin15.pdf" >DANN<i class="fas fa-external-link-alt"></i></a> ：</p>
<span id="more"></span>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.8ojnugoi4m.webp"  alt="model"></p>
<p>在训练阶段需要预测如下两个任务：</p>
<ul>
<li>实现源域数据集准确分类，即图像分类误差的最小化，这与正常分类任务保持一致</li>
<li>实现源域和目标域准确分类，即域分类器的误差最小化。而特征提取器的目标是最大化域分类误差，使得域分类器无法分辨数据是来自源域还是目标域，从而让特征提取器学习到域不变特征(domain-invariant)。也就是说特征提取器和域分类器的目标是相反的<ul>
<li>本质上就是让特征提取器不要过拟合源域，要学习出源域和目标域的泛化特征</li>
<li>这两个网络对抗训练，DANN通过GRL层使特征提取器更新的梯度与域判别器的梯度相反，构造出了类似于GAN的对抗损失，又通过该层避免了GAN的两阶段训练过程，提升模型训练稳定性</li>
</ul>
</li>
</ul>
<h2 id="GRL"><a href="#GRL" class="headerlink" title="GRL"></a>GRL</h2><p>GRL是作用在特征提取器上的，对其参数梯度取反。</p>
<p>具体实现如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ReverseLayerF</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, x, alpha</span>):</span><br><span class="line">        ctx.alpha = alpha</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x.view_as(x)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        output = grad_output.neg() * ctx.alpha</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, <span class="literal">None</span></span><br></pre></td></tr></table></figure></p>
<p>调用如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_data, alpha</span>):</span><br><span class="line">    input_data = input_data.expand(input_data.data.shape[<span class="number">0</span>], <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    feature = self.feature(input_data)</span><br><span class="line">    feature = feature.view(-<span class="number">1</span>, <span class="number">50</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">    reverse_feature = ReverseLayerF.apply(feature, alpha)</span><br><span class="line">    class_output = self.class_classifier(feature)</span><br><span class="line">    domain_output = self.domain_classifier(reverse_feature)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> class_output, domain_output</span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/109051269" >【深度域自适应】一、DANN与梯度反转层（GRL）详解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/zengjichuan/DANN" >zengjichuan/DANN<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Gradient Reversal</tag>
        <tag>Domain Adaptation</tag>
      </tags>
  </entry>
  <entry>
    <title>DCN</title>
    <url>/2025/03/02/DCN/</url>
    <content><![CDATA[<p>DCN是DeepFM的升级版，后者是只能做二阶交叉特征，随着阶数上升，模型复杂度大幅提高，且FM网络层较浅，表达能力有限。google团队通过构建深度交叉网络来自动进行特征的高阶交叉，且时空复杂度均为线性增长，极大提升了模型性能。</p>
<span id="more"></span>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>整体网络结构跟DeepFM类似：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.9kgbmcdyez.webp"  alt="model"></p>
<p>特征交叉细节：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.8oju6w5gkd.webp"  alt="cross"></p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>代码其实非常简单：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_net</span>(<span class="params">self, inputs</span>):</span><br><span class="line">    <span class="comment"># 进行特征交叉时的x_0一直没有变，变的是x_l和每一层的权重</span></span><br><span class="line">    x_0 = inputs <span class="comment"># B x dims </span></span><br><span class="line">    x_l = x_0</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.layer_nums):</span><br><span class="line">        <span class="comment"># 将x_l的第一个维度与w[i]的第0个维度计算点积</span></span><br><span class="line">        xl_w = tf.tensordot(x_l, self.W[i], axes=(<span class="number">1</span>, <span class="number">0</span>)) <span class="comment"># B, </span></span><br><span class="line">        xl_w = tf.expand_dims(xl_w, axis=-<span class="number">1</span>) <span class="comment"># 在最后一个维度上添加一个维度 # B x 1</span></span><br><span class="line">        cross = tf.multiply(x_0, xl_w) <span class="comment"># B x dims</span></span><br><span class="line">        x_l = cross + self.b[i] + x_l</span><br><span class="line">    <span class="keyword">return</span> x_l</span><br></pre></td></tr></table></figure>
<p>这里的 <code>cross</code> 其实是相当于学习残差。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>就随便看看吧，baselines提到了FM、LR，但只字不提跟它们的性能比较，无语。。。（Wide&amp;Deep依赖于大量人工先验来选择交叉特征，DCN只跟自动交叉特征的方法比，例如FM等）</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.lvve45ssr.webp"  alt="logloss"></p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.64dzu9ezcs.webp"  alt="parameter"></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://github.com/datawhalechina/fun-rec/blob/master/docs/ch02/ch2.2/ch2.2.2/DCN.md" >DCN.md<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://dl.acm.org/doi/pdf/10.1145/3124749.3124754" >Deep &amp; Cross Network for Ad Click Predictions<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>Cross-Features</tag>
      </tags>
  </entry>
  <entry>
    <title>DIN解读</title>
    <url>/2023/02/12/DIN%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>传统的Embedding&amp;MLP架构将用户特征编码进一个固定长度的向量。当推出一个商品时，该架构无法捕捉用户丰富的历史行为中的多样性兴趣与该商品的关联。阿里妈妈团队提出了DIN网络进行改进，主要有如下两点创新：</p>
<span id="more"></span>
<ul>
<li>引入注意力机制来捕捉历史行为与当前商品的关联。用NMT的话来说，上文不同的单词对当前待生成的单词贡献不同，贡献高的应该赋予更大的权重，否则赋小</li>
<li>设计两种训练技巧来帮助训练大规模稀疏神经网络：<ul>
<li>mini-batch aware正则化</li>
<li>自适应激活函数</li>
</ul>
</li>
</ul>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><img   src="/2023/02/12/DIN%E8%A7%A3%E8%AF%BB/1.png"  class="DIN">
<h4 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h4><script type="math/tex; mode=display">
\boldsymbol{v}_U(A)=f\left(\boldsymbol{v}_A, \boldsymbol{e}_1, \boldsymbol{e}_2, \ldots, \boldsymbol{e}_H\right)=\sum_{j=1}^H a\left(\boldsymbol{e}_j, \boldsymbol{v}_A\right) \boldsymbol{e}_j=\sum_{j=1}^H \boldsymbol{w}_j \boldsymbol{e}_j</script><p>需要注意的是，DIN舍弃了 $\sum_{i}w_i = 1$ 这个限制。主要是为了突出用户对历史兴趣的强烈程度，比如用户历史中对电子产品很感兴趣，那么他这类兴趣的得分就很高，其它兴趣得分则很小，兴趣差异得到放大。</p>
<h4 id="两种训练技巧"><a href="#两种训练技巧" class="headerlink" title="两种训练技巧"></a>两种训练技巧</h4><ul>
<li><p>mini-batch aware正则化：L2正则化是对模型所有的参数进行约束，训练成本高，而工业界推荐系统常常是大规模的稀疏网络。DIN团队对L2正则进行了近似计算，这样就能降低训练成本：</p>
<script type="math/tex; mode=display">
L_2(\mathbf{W}) \approx \sum_{j=1}^K \sum_{m=1}^B \frac{\alpha_{m j}}{n_j}\left\|\boldsymbol{w}_j\right\|_2^2</script></li>
<li><p>自适应激活函数：传统的激活函数在特定点处会出现突变，泛化性不好。DIN团队进行了改进：</p>
</li>
</ul>
<img   src="/2023/02/12/DIN%E8%A7%A3%E8%AF%BB/dice.png"  class="Dice">
<script type="math/tex; mode=display">
f(s)=p(s) \cdot s+(1-p(s)) \cdot \alpha s, \quad p(s)=\frac{1}{1+e^{-\frac{s-E[s]}{\sqrt{\operatorname{Var}[s]+\epsilon}}}}</script><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><img   src="/2023/02/12/DIN%E8%A7%A3%E8%AF%BB/result.png"  class="result">
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>CTR</tag>
        <tag>Recommender Systems</tag>
      </tags>
  </entry>
  <entry>
    <title>DPO讲解</title>
    <url>/2023/12/18/DPO%E8%AE%B2%E8%A7%A3/</url>
    <content><![CDATA[<p>PPO算法的pipeline冗长，涉及模型多，资源消耗大，且训练极其不稳定。DPO是斯坦福团队基于PPO推导出的优化算法，去掉了RW训练和RL环节，只需要加载一个推理模型和一个训练模型，直接在偏好数据上进行训练即可：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.suiljdpc9dc.png"  alt="DPO"></p>
<span id="more"></span>
<p>损失函数如下(顿时清爽简洁了不少)：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right]</script><p>DPO在理解难度、实现难度和资源占用都非常友好，想看具体的公式推导见：</p>
<p><a class="link"   href="https://zhuanlan.zhihu.com/p/653975451" >[论文笔记]DPO：Direct Preference Optimization: Your Language Model is Secretly a Reward Model<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://arxiv.org/pdf/2305.18290.pdf">Direct Preference Optimization:<br>Your Language Model is Secretly a Reward Model</a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/642569664" >DPO: Direct Preference Optimization 论文解读及代码实践<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>DPO</tag>
        <tag>RM</tag>
      </tags>
  </entry>
  <entry>
    <title>DQN</title>
    <url>/2024/12/09/DQN/</url>
    <content><![CDATA[<p>最近我组有同学在探索用RL落地营销场景的可能性，借此机会学习下RL。</p>
<span id="more"></span>
<h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>Q-learning算法以表格的方式存储了每个状态下所有动作值的表格。表格中的每一个动作价值表示在状态下选择动作然后继续遵循某一策略预期能够得到的期望回报。Q值的更新公式如下：</p>
<script type="math/tex; mode=display">
Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma \max _a Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right]</script><p>当 $Q(s,a)$ 不再显著变化时，算法收敛。其中：</p>
<ul>
<li>$S$：当前状态</li>
<li>$A$：当前动作</li>
<li>$\alpha$：学习率​​（0 &lt; $\alpha$ ≤ 1）。控制新信息覆盖旧信息的程度，类似于动量更新。接近0表示保守（不学习新经验），接近1表示激进（快速接受新经验）</li>
<li>$R$：即时奖励，也就是$R(S,A)$，在当前状态下采取当前行动所获得的奖励</li>
<li>$\gamma$：​​折扣因子​​（0 ≤ $\gamma$ &lt; 1）。衡量未来奖励的重要性。接近0表示“目光短浅”，只在乎眼前奖励；接近1表示“目光长远”，非常重视长期价值</li>
<li>$S’$：下一状态</li>
<li>TD target - $Q(S,A)$：时序差分误差<ul>
<li>TD target = $R+\gamma \max _a Q\left(S^{\prime}, A^{\prime}\right)$：这是一个更接近“真实”长期得分的估值</li>
</ul>
</li>
</ul>
<p>伪代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义环境参数</span></span><br><span class="line">GRID_SIZE = <span class="number">4</span>  <span class="comment"># 网格世界大小 (4x12)</span></span><br><span class="line">NUM_ACTIONS = <span class="number">4</span>  <span class="comment"># 动作数量: 0:上, 1:右, 2:下, 3:左</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Q-Learning参数</span></span><br><span class="line">ALPHA = <span class="number">0.1</span>    <span class="comment"># 学习率</span></span><br><span class="line">GAMMA = <span class="number">0.99</span>   <span class="comment"># 折扣因子</span></span><br><span class="line">EPSILON = <span class="number">0.1</span>  <span class="comment"># 探索概率 (10%的概率随机探索)</span></span><br><span class="line">EPISODES = <span class="number">500</span> <span class="comment"># 训练轮数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化Q表，形状为 (状态数量, 动作数量)</span></span><br><span class="line">q_table = np.zeros((GRID_SIZE * <span class="number">12</span>, NUM_ACTIONS))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(EPISODES):</span><br><span class="line">        state = <span class="number">36</span>  <span class="comment"># 起始状态 (左下角 S)</span></span><br><span class="line">        total_reward = <span class="number">0</span></span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            <span class="comment"># ε-greedy策略: 大部分时间选择最优动作，小部分时间随机探索</span></span><br><span class="line">            <span class="keyword">if</span> np.random.uniform(<span class="number">0</span>, <span class="number">1</span>) &lt; EPSILON:</span><br><span class="line">                action = np.random.randint(NUM_ACTIONS)  <span class="comment"># 随机探索</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                action = np.argmax(q_table[state])  <span class="comment"># 选择当前Q值最大的动作，从而计算出当前的奖励</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 执行动作，得到新状态和奖励</span></span><br><span class="line">            next_state, reward, done = take_action(state, action)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Q-Learning更新公式</span></span><br><span class="line">            old_value = q_table[state, action]</span><br><span class="line">            next_max = np.<span class="built_in">max</span>(q_table[next_state])</span><br><span class="line">            </span><br><span class="line">            new_value = old_value + ALPHA * (reward + GAMMA * next_max - old_value)</span><br><span class="line">            q_table[state, action] = new_value</span><br><span class="line"></span><br><span class="line">            state = next_state</span><br><span class="line">            total_reward += reward</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每50轮打印一次进度</span></span><br><span class="line">        <span class="keyword">if</span> episode % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Episode: <span class="subst">&#123;episode&#125;</span>, Total Reward: <span class="subst">&#123;total_reward&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境交互函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">take_action</span>(<span class="params">state, action</span>):</span><br><span class="line">    row, col = state // <span class="number">12</span>, state % <span class="number">12</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义动作效果</span></span><br><span class="line">    <span class="keyword">if</span> action == <span class="number">0</span>:    row -= <span class="number">1</span>  <span class="comment"># 上</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">1</span>:  col += <span class="number">1</span>  <span class="comment"># 右</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">2</span>:  row += <span class="number">1</span>  <span class="comment"># 下</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">3</span>:  col -= <span class="number">1</span>  <span class="comment"># 左</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确保不走出边界</span></span><br><span class="line">    row = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">min</span>(row, GRID_SIZE - <span class="number">1</span>))</span><br><span class="line">    col = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">min</span>(col, <span class="number">11</span>))</span><br><span class="line">    </span><br><span class="line">    new_state = row * <span class="number">12</span> + col</span><br><span class="line">    reward = -<span class="number">1</span>  <span class="comment"># 每走一步的代价</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 检查是否到达终点或掉下悬崖</span></span><br><span class="line">    <span class="keyword">if</span> new_state == <span class="number">47</span>:  <span class="comment"># 终点(G)</span></span><br><span class="line">        reward = <span class="number">100</span></span><br><span class="line">        done = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="number">37</span> &lt;= new_state &lt;= <span class="number">46</span>:  <span class="comment"># 悬崖区域</span></span><br><span class="line">        reward = -<span class="number">100</span></span><br><span class="line">        new_state = <span class="number">36</span>  <span class="comment"># 回到起点</span></span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> new_state, reward, done</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行训练</span></span><br><span class="line">train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看学习结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training completed!&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Final Q-table:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(q_table)</span><br></pre></td></tr></table></figure></p>
<p>然而，这种用表格存储动作价值的做法只在环境的状态和动作都是离散的，并且空间都比较小的情况下适用。当动作或状态数量巨大时，Q-Learning已捉襟见肘。</p>
<h2 id="Deep-Q-Learning"><a href="#Deep-Q-Learning" class="headerlink" title="Deep Q-Learning"></a>Deep Q-Learning</h2><p>Deep Q-Learning将Q-Table的更新问题变成一个函数拟合问题，即<code>q=dnn(s,a)</code>，相近的状态得到相近的输出动作。但DL与RL结合会存在如下四个问题：</p>
<ol>
<li>DL需要大量带标签的样本进行监督学习，但RL只有reward返回值<ul>
<li>解决方案：使用reward构造标签</li>
</ul>
</li>
<li>DL的样本独立，但RL前后state状态相关<ul>
<li>解决方案：使用Experience Replay</li>
</ul>
</li>
<li>DL目标分布固定，但RL的分布一直变化。比如你玩一个游戏，一个关卡和下一个关卡的状态分布是不同的，所以训练好了前一个关卡，下一个关卡又要重新训练<ul>
<li>解决方案：使用Experience Replay</li>
</ul>
</li>
<li>使用非线性网络表示值函数时出现不稳定等问题，因为预估Q值和Target Q值都是由神经网络输出的<ul>
<li>解决方案：使用更新较慢的目标网络预估Target Q</li>
</ul>
</li>
</ol>
<p>整体的算法流程如下：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/dqn.175fokezb5.webp"  alt="DQN"></p>
<h4 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h4><p>使用经验回放有两点好处：</p>
<ul>
<li>提高样本利用效率，也避免了灾难性遗忘</li>
<li>使样本满足独立假设。在MDP中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。</li>
</ul>
<h4 id="Target-Network"><a href="#Target-Network" class="headerlink" title="Target Network"></a>Target Network</h4><p>DQN算法最终更新的目标是让$Q_\theta(s,a)$逼近$r+\gamma \max _{a^{\prime}} Q_\theta\left(s^{\prime}, a^{\prime}\right)$，但由于TD误差目标本身就包含神经网络的输出，因此在更新网络参数的同时目标也在不断地改变，这非常容易造成神经网络训练的不稳定性。为了解决这一问题，DQN便使用了目标网络（target network）的思想：既然训练过程中Q网络的不断更新会导致目标不断发生改变，不如暂时先将TD目标中的Q网络固定住。为了实现这一思想，我们需要利用两套Q网络。</p>
<ol>
<li>原来的训练网络$Q_\theta(s,a)$，用于计算原来的损失函数中的项$\frac{1}{2}\left[Q_\theta(s, a)-\left(r+\gamma \max _{a^{\prime}} Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)\right)\right]^2$中的$Q_\theta(s,a)$，并且使用正常梯度下降方法来进行更新。</li>
<li>目标网络$Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right)$，用于计算原先损失函数中的$(r+\gamma \max _{a^{\prime}} Q_{\theta^{-}}\left(s^{\prime}, a^{\prime}\right))$项，其中$\theta^{-}$表示目标网络中的参数。如果两套网络的参数随时保持一致，则仍为原先不够稳定的算法。为了让更新目标更稳定，目标网络并不会每一步都更新。具体而言，目标网络使用训练网络的一套较旧的参数，训练网络$Q_\theta(s,a)$在训练中的每一步都会更新，而目标网络的参数每隔$C$步才会与训练网络同步一次，即$\theta^{-} \leftarrow \theta$。这样做使得目标网络相对于训练网络更加稳定。</li>
</ol>
<h4 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h4><p>具体的交互环境搭建和DQN代码见：<a class="link"   href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py" >5_Deep_Q_Network<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://cloud.tencent.com/developer/article/1092239" >实战深度强化学习DQN-理论和实践<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow" >Simple Reinforcement learning tutorials<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN" >什么是DQN<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm" >The Deep Q-Learning Algorithm<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" >Human-level control through deep reinforcement learning<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://hrl.boyuai.com/chapter/2/dqn%E7%AE%97%E6%B3%95" >第7章 DQN 算法<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Q-Learning</tag>
        <tag>Deep Q-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>DSSM双塔特征交互</title>
    <url>/2024/07/09/DSSM%E5%8F%8C%E5%A1%94%E7%89%B9%E5%BE%81%E4%BA%A4%E4%BA%92/</url>
    <content><![CDATA[<p>传统的DSSM双塔无法在早期进行user和item侧的特征交互，这在一定程度上降低了模型性能。我们想要对双塔模型进行细粒度的特征交互，同时又不失双塔模型离线建向量索引的解耦性。下面介绍两篇这方面的工作。</p>
<span id="more"></span>
<h2 id="美团-Dual-Augmented-Two-tower"><a href="#美团-Dual-Augmented-Two-tower" class="headerlink" title="美团-Dual Augmented Two-tower"></a>美团-Dual Augmented Two-tower</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.2h86s5t6wx.png"  alt="meituan"></p>
<ul>
<li>在user和item的特征侧分别引入可学习的特征向量</li>
<li>当label=1的时候，user的$a_u$去学习item正样本的输出表征，从而实现隐式特征交互；item侧亦如此</li>
</ul>
<p>损失函数如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{loss}_u & =\frac{1}{T} \sum_{(u, v, y) \in \mathcal{T}}\left[y \mathbf{a}_u+(1-y) \mathbf{p}_v-\mathbf{p}_v\right]^2 \\
\operatorname{loss}_v & =\frac{1}{T} \sum_{(u, v, y) \in \mathcal{T}}\left[y \mathrm{a}_v+(1-y) \mathrm{p}_u-\mathbf{p}_u\right]^2 \\

y &\in \{0,1\}
\end{aligned}</script><ul>
<li>$p_u$ 和 $p_v$ 梯度冻结，不进行更新</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>这种方式引入的交叉特征实际是非常“粗粒度”和“高阶”的，即携带的信息仅仅是对方tower最后输出的表征，对方tower在编码这段表征时，也仅仅只利用了fake的emb和tower本身的输入特征的交互。</p>
<h2 id="百度-I3-Retriever"><a href="#百度-I3-Retriever" class="headerlink" title="百度-I3 Retriever"></a>百度-I3 Retriever</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.2obenlycwe.webp"  alt="RankNet"></p>
<ul>
<li>在doc侧设计一个轻量的query生成模块，利用doc侧特征作为输入，去fake一个query侧表征，去重构出query侧的输入特征。当然需要注意的是，也仅仅是在正样本上执行重构loss</li>
<li>doc侧与生成的query进行特征交互</li>
<li>交互完的doc侧与query侧对比学习</li>
</ul>
<p>重构损失函数如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}_r=-\sum_{w_i \in \mathbf{q}} \mathbf{y}_{w_i} \log \left(\mathbf{W}^{R_{\mathbb{K}}}(\mathbf{p})_q\right)</script><p>对比损失函数如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}_c=-\log \frac{\exp \left(S\left(\mathbf{q}, \mathbf{p}_{+}\right)\right)}{\exp \left(S\left(\mathbf{q}, \mathbf{p}_{+}\right)\right)+\sum_{\mathbf{p}-\in \mathcal{N}_{-}} \exp \left(S\left(\mathbf{q}, \mathbf{p}_{-}\right)\right)},</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/674703530?utm_medium=social&amp;utm_psn=1777428140744691712&amp;utm_source=wechat_session" >CIKM2023 | 突破双塔: 生成式交互的向量化召回<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>Feature Interaction</tag>
        <tag>Dual Tower</tag>
      </tags>
  </entry>
  <entry>
    <title>DataLoader中sampler参数介绍</title>
    <url>/2021/12/24/DataLoader%E4%B8%ADsampler%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p><code>Sampler</code> 决定了 <code>Dataset</code> 的采样顺序。</p>
<span id="more"></span>
<h2 id="DataLoader-Sampler-DataSet-关系"><a href="#DataLoader-Sampler-DataSet-关系" class="headerlink" title="DataLoader | Sampler | DataSet 关系"></a><code>DataLoader</code> | <code>Sampler</code> | <code>DataSet</code> 关系</h2><img   src="/2021/12/24/DataLoader%E4%B8%ADsampler%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D/relation.png"  class="">
<ul>
<li><code>Sampler</code> : 提供数据集中元素的索引</li>
<li><code>DataSet</code> : 根据 <code>Sampler</code> 提供的索引来检索数据</li>
<li><code>DataLoader</code> : 批量加载数据用于后续的训练和测试</li>
</ul>
<h2 id="Sampler"><a href="#Sampler" class="headerlink" title="Sampler"></a><code>Sampler</code></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Sampler</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Base class for all Samplers.</span></span><br><span class="line"><span class="string">    Every Sampler subclass has to provide an __iter__ method, providing a way</span></span><br><span class="line"><span class="string">    to iterate over indices of dataset elements, and a __len__ method that</span></span><br><span class="line"><span class="string">    returns the length of the returned iterators.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_source</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<p>PyTorch官网已经实现了多种 <code>Sampler</code> :</p>
<h3 id="SequentialSampler"><a href="#SequentialSampler" class="headerlink" title="SequentialSampler"></a><code>SequentialSampler</code></h3><blockquote>
<p>若 <code>shuffle=False</code> ，且未指定 <code>sampler</code> ，默认使用</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SequentialSampler</span>(<span class="title class_ inherited__">Sampler</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Samples elements sequentially, always in the same order.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_source</span>):</span><br><span class="line">        self.data_source = data_source</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(self.data_source)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data_source)</span><br></pre></td></tr></table></figure>
<h3 id="RandomSampler"><a href="#RandomSampler" class="headerlink" title="RandomSampler"></a><code>RandomSampler</code></h3><blockquote>
<p>若 <code>shuffle=True</code> ，且未指定 <code>sampler</code> ，默认使用</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RandomSampler</span>(<span class="title class_ inherited__">Sampler</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Samples elements randomly. If without replacement, then sample from a shuffled dataset.</span></span><br><span class="line"><span class="string">    If with replacement, then user can specify ``num_samples`` to draw.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">        replacement (bool): samples are drawn with replacement if ``True``, default=``False``</span></span><br><span class="line"><span class="string">        num_samples (int): number of samples to draw, default=`len(dataset)`. This argument</span></span><br><span class="line"><span class="string">            is supposed to be specified only when `replacement` is ``True``.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_source</span>):</span><br><span class="line">        self.data_source = data_source</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        n = <span class="built_in">len</span>(self.data_source)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randperm(n).tolist())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br></pre></td></tr></table></figure>
<h3 id="BatchSampler"><a href="#BatchSampler" class="headerlink" title="BatchSampler"></a><code>BatchSampler</code></h3><blockquote>
<p>like <code>sampler</code>, but returns a batch of indices at a time. Mutually exclusive with <code>batch_size</code>, <code>shuffle</code>, <code>sampler</code>, and <code>drop_last</code></p>
</blockquote>
<ul>
<li>在 <code>DataLoader</code> 中设置 <code>batch_sampler=batch_sampler</code> 的时候，上面四个参数都必须是默认值。也很好理解，每次采样返回一个batch，那么 <code>batch_size</code> 肯定为 <code>1</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchSampler</span>(<span class="title class_ inherited__">Sampler</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Wraps another sampler to yield a mini-batch of indices.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sampler (Sampler): Base sampler.</span></span><br><span class="line"><span class="string">        batch_size (int): Size of mini-batch.</span></span><br><span class="line"><span class="string">        drop_last (bool): If ``True``, the sampler will drop the last batch if</span></span><br><span class="line"><span class="string">            its size would be less than ``batch_size``</span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))</span></span><br><span class="line"><span class="string">        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))</span></span><br><span class="line"><span class="string">        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sampler, batch_size, drop_last</span>):</span><br><span class="line">        self.sampler = sampler</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.drop_last = drop_last</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        batch = []</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> self.sampler:</span><br><span class="line">            batch.append(idx)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(batch) == self.batch_size:</span><br><span class="line">                <span class="keyword">yield</span> batch</span><br><span class="line">                batch = []</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch) &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> self.drop_last:</span><br><span class="line">            <span class="keyword">yield</span> batch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.drop_last:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(self.sampler) // self.batch_size</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (<span class="built_in">len</span>(self.sampler) + self.batch_size - <span class="number">1</span>) // self.batch_size</span><br></pre></td></tr></table></figure>
<ul>
<li>可以看到在构造 <code>BatchSampler</code> 实例的时候，需要传入一个sampler作为实参</li>
</ul>
<hr>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><p>最近看到一篇推文，分享了一个使模型训练速度提升20%的Trick—<a class="link"   href="https://mp.weixin.qq.com/s/xGvaW87UQFjetc5xFmKxWg" >BlockShuffle<i class="fas fa-external-link-alt"></i></a> 。fork了原作者的代码，并自定义了 <code>batch_sampler</code> ，源码见：<a class="link"   href="https://github.com/TransformersWsz/BlockShuffleTest" >TransformersWsz/BlockShuffleTest<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/xGvaW87UQFjetc5xFmKxWg" >一个使模型训练速度提升20%的Trick—BlockShuffle<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.zdaiot.com/MLFrameworks/Pytorch/Pytorch%20DataLoader%E8%AF%A6%E8%A7%A3/" >Pytorch DataLoader详解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://PyTorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader" >torch.utils.data — PyTorch 1.10.1 documentation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.codeleading.com/article/79575865698/" >PyTorch中用Mnist数据集dataloader 自定义batchsampler - 代码先锋网 (codeleading.com)<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.tqwba.com/x_d/jishu/415752.html" >PyTorch 实现一个自定义的dataloader，每个batch都可以实现类别数量均衡 (tqwba.com)<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/76893455" >一文弄懂Pytorch的DataLoader, DataSet, Sampler之间的关系<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>DataLoader</tag>
        <tag>sampler</tag>
      </tags>
  </entry>
  <entry>
    <title>Dataset</title>
    <url>/2019/07/28/Dataset/</url>
    <content><![CDATA[<p>Here are some detailed descriptions of datasets for NLP experiments:</p>
<span id="more"></span>
<h2 id="MNLI"><a href="#MNLI" class="headerlink" title="MNLI"></a>MNLI</h2><p>Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classification task. Given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to the first one.</p>
<h2 id="QQP"><a href="#QQP" class="headerlink" title="QQP"></a>QQP</h2><p>Quora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent.</p>
<h2 id="QNLI"><a href="#QNLI" class="headerlink" title="QNLI"></a>QNLI</h2><p>Question Natural Language Inference is a version of the Stanford Question Answering Dataset which has been converted to a binary classification task. The positive examples are (question, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.</p>
<h2 id="SST-2"><a href="#SST-2" class="headerlink" title="SST-2"></a>SST-2</h2><p>The Stanford Sentiment Treebank is a binary single-sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment.</p>
<h2 id="CoLA"><a href="#CoLA" class="headerlink" title="CoLA"></a>CoLA</h2><p>The Corpus of Linguistic Acceptability is a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically “acceptable” or not.</p>
<h2 id="STS-B"><a href="#STS-B" class="headerlink" title="STS-B"></a>STS-B</h2><p>The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources. They were annotated with a score from 1<br>to 5 denoting how similar the two sentences are in terms of semantic meaning.</p>
<h2 id="MRPC"><a href="#MRPC" class="headerlink" title="MRPC"></a>MRPC</h2><p>Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.</p>
<h2 id="RTE"><a href="#RTE" class="headerlink" title="RTE"></a>RTE</h2><p>Recognizing Textual Entailment is a binary entailment task similar to MNLI, but with<br>much less training data.</p>
<h2 id="SQuAD-v1-1"><a href="#SQuAD-v1-1" class="headerlink" title="SQuAD v1.1"></a>SQuAD v1.1</h2><p>The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs. Given a question and a passage from Wikipedia containing the answer, the task is to<br>predict the answer text span in the passage.</p>
<h2 id="SQuAD-v2-0"><a href="#SQuAD-v2-0" class="headerlink" title="SQuAD v2.0"></a>SQuAD v2.0</h2><p>The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic.</p>
<h2 id="SWAG"><a href="#SWAG" class="headerlink" title="SWAG"></a>SWAG</h2><p>The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference. Given a sentence, the task is to choose the most plausible continuation among four choices.</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Data</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepWalk解读</title>
    <url>/2022/09/09/DeepWalk%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>转载自：<a class="link"   href="https://github.com/dsgiitr/graph_nets/blob/master/DeepWalk/DeepWalk_Blog%2BCode.ipynb" >dsgiitr/graph_nets<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<p>As a part of this blog series and continuing with the tradition of extracting useful graph features by considering the topology of the network graph using machine learning, this blog deals with Deep Walk. This is a simple unsupervised online learning approach, very similar to language modelling used in NLP, where the goal is to generate word embeddings. In this case, generalizing the same concept, it simply tries to learn latent representations of nodes/vertices of a given graph. These graph embeddings which capture neighborhood similarity and community membership can then be used for learning downstream tasks on the graph. </p>
<span id="more"></span>
<img   src="/2022/09/09/DeepWalk%E8%A7%A3%E8%AF%BB/karate_to_embedding.jpg"  class="Input Graph to Embdeddings">
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Assume a setting, given a graph G where you wish to convert the nodes into embedding vectors and the only information about a node are the indices of the nodes to which it is connected (adjacency matrix). Since there is no initial feature matrix corresponding to the data, we will construct a feature matrix which will have all the randomly selected nodes. There can be multiple methods to select these but here we will be assuming that they are normally sampled (though it won’t make much of a difference even if they are taken from some other distribution).</p>
<h2 id="Random-Walks"><a href="#Random-Walks" class="headerlink" title="Random Walks"></a>Random Walks</h2><p>Random walk rooted at vertex $v_i$ as $W_{v_i}$. It is a stochastic process with random variables ${W^1}_{v_i}$, ${W^2}_{v_i}$, $. . .$, ${W^k}_{v_i}$ such that ${W^{k+1}}{v_i}$ is a vertex chosen at random from the neighbors of vertex $v_k$. Random Walk distances are good features for many problems. We’ll be discussing how these short random walks are analogous to the sentences in the language modelling setting and how we can carry the concept of context windows to graphs as well.</p>
<h2 id="What-is-Power-Law"><a href="#What-is-Power-Law" class="headerlink" title="What is Power Law?"></a>What is Power Law?</h2><p>A scale-free network is a network whose degree distribution follows a power law, at least asymptotically. That is, the fraction $P(k)$ of nodes in the network having $k$ connections to other nodes goes for large values of $k$ as<br>$P(k) \sim k^{-\gamma}$ where $k=2,3$ etc.</p>
<img   src="/2022/09/09/DeepWalk%E8%A7%A3%E8%AF%BB/Power_Law_Graph.gif"  class="Power Law Graph">
<p>The network of global banking activity with nodes representing the absolute size of assets booked in the respective jurisdiction and the edges between them the exchange of financial assets, with data taken from the IMF is a scale free network and follows Power Law. We can then see clearly how a very few core nodes dominate this network, there are approximately 200 countries in the world but these 19 largest jurisdictions in terms of capital together are responsible for over 90% of the assets.</p>
<img   src="/2022/09/09/DeepWalk%E8%A7%A3%E8%AF%BB/Power_Law_Example.jpg"  class="Input Graph to Embdeddings">
<p>These highly centralized networks are more formally called scale free or power law networks, that describe a power or exponential relationship between the degree of connectivity a node has and the frequency of its occurrence. <a class="link"   href="https://www.youtube.com/watch?v=qmCrtuS9vtU" >More<i class="fas fa-external-link-alt"></i></a> about centralized networks and power law.</p>
<h3 id="Why-is-it-important-here"><a href="#Why-is-it-important-here" class="headerlink" title="Why is it important here?"></a>Why is it important here?</h3><p>Social networks, including collaboration networks, computer networks, financial networks and Protein-protein interaction networks are some examples of networks claimed to be scale-free.</p>
<p>According to the authors, “If the degree distribution of a connected graph follows a power law (i.e. scale-free), we observe that the frequency which vertices appear in the short random walks will also follow a power-law distribution. Word frequency in natural language follows a similar distribution, and techniques from language modeling account for this distributional behavior.”</p>
<img   src="/2022/09/09/DeepWalk%E8%A7%A3%E8%AF%BB/NLP_vs_Graph.jpg"  class="NLP vs Graph Random Walks Power Law D">
<p><em>$(a)$ comes from a series of short random walks on a scale-free graph, and $(b)$ comes from the text of 100,000 articles from the English Wikipedia.</em></p>
<h2 id="Intuition-with-SkipGram"><a href="#Intuition-with-SkipGram" class="headerlink" title="Intuition with SkipGram"></a>Intuition with SkipGram</h2><p>Think about the below unrelated problem for now:-</p>
<p>Given, some english sentences (could be any other language, doesn’t matter) you need to find a vector corresponding to each word appearing at least once in the sentence such that the words having similar meaning appear close to each other in their vector space, and the opposite must hold for words which are dissimilar.</p>
<p>Suppose the sentences are</p>
<ol>
<li>Hi, I am Bert.</li>
<li>Hello, this is Bert.</li>
</ol>
<p>From the above sentences you can see that 1 and 2 are related to each other, so even if someone does’nt know the language, one can make out that the words ‘Hi’ and ‘Hello’ have roughly the same meaning. We will be using a technique similar to what a human uses while trying to find out related words. Yes! We’ll be guessing the meaning based on the words which are common between the sentences. Mathematically, learning a representation in word-2-vec means learning a mapping function from the word co-occurences, and that is exactly what we are heading for.</p>
<h4 id="But-How"><a href="#But-How" class="headerlink" title="But, How?"></a>But, How?</h4><p>First lets git rid of the punctuations and assign a random vector to each word. Now since these vectors are assigned randomly, it implies the current representation is useless. We’ll use our good old friend, <em>probability</em>, to convert these into meaningful representations. The idea is to maximize the probability of the appearence of a word, given the words that appear around it. Let’s assume the probability is given by $P(x|y)$ where $y$ is the set of words that appear in the same sentence in which $x$ occurs. Remember we are only taking one sentence at a time, so first we’ll maximize the probability of ‘Hi’ given {‘I’, ‘am’, ‘Bert’} , then we’ll maximize the probability of ‘I’ given {‘Hi’, ‘am’, ‘Bert’}. We will do it for each word in the first sentence, and then for the second sentence. Repeat this procedure for all the sentences over and over again until the feature vectors have converged. </p>
<p>One question that may arise now is, ‘How do these feature vectors relate with the probability?’. The answer is that in the probability function we’ll utilize the word vectors assinged to them. But, aren’t those vectors random? Ahh, they are at the start, but we promise you by the end of the blog they would have converged to the values which really gives some meaning to those seamingly random numbers.</p>
<h4 id="So-What-exactly-the-probability-function-helps-us-with"><a href="#So-What-exactly-the-probability-function-helps-us-with" class="headerlink" title="So, What exactly the probability function helps us with?"></a>So, What exactly the probability function helps us with?</h4><p>What does it mean to find the probability of a vector given other vectors? This actually is a simple question with a pretty simple answer, take it as a fill in the blank problem that you may have dealt with in the primary school,</p>
<p>Roses ____ red.</p>
<p>What is the most likely guess? Most people will fill it with an ‘are’. (Unless, you are pretending to be oversmart in an attempt to prove how cool you are). You were able to fill that, because, you’ve seen some examples of the word ‘are’ previously in life which help you with the context. The probability function is also trying to do the same, it is finding out the word which is most likely to occur given the words that are surrounding it.</p>
<h4 id="But-but-this-still-doesn’t-explain-how-it’s-gonna-do-that"><a href="#But-but-this-still-doesn’t-explain-how-it’s-gonna-do-that" class="headerlink" title="But but this still doesn’t explain how it’s gonna do that."></a>But but this still doesn’t explain how it’s gonna do that.</h4><p>In case you guessed ‘Neural Network’, you are correct. In this blog we’ll be using neural nets (feeling sleepy now, so let’s wrap this up)</p>
<p>It is not necesary to use neural nets to estimate the probability funciton but it works and looks cool :P, frankly, the authors used it, so we’ll follow them.</p>
<p>The input layer will have $|V|$ neurons, where $|V|$ is the number of words that are interesting to us. We will be using only one hidden layer for simplicity. It can have as many neurons as you want, but it is suggested to keep a number that is less than the number of words in the vocabulary. The output layer will also have the $|V|$ neurons.</p>
<p>Now let’s move on to the interpretation of input and output layers (don’t care about the hidden layer).<br>Lets suppose the words in the vocabulary are $V_1$, $V_2$, $…$ $V_i$, $….$ $V_n$. Assume that out of these V4,V7, V9 appears along with the word whose probability we are tying to maximise. so the input layers will have the 4th, 7th, and the 9th neuron with value 1 and all other will have the value 0. The hidden layer will then have some function of these values. The hidden layer have no non linear acitvation. The |V| neuron in the output layer will have a score, the higher it is ,the higher the chances of that word appearning along with the surrounding words. Apply Sigmoid, boom! we got the probabilities. </p>
<p>So a simple neural network will help us solve the fill in the blank problem.</p>
<h2 id="Deep-Walk-SkipGram-Analogy-Random-Walks"><a href="#Deep-Walk-SkipGram-Analogy-Random-Walks" class="headerlink" title="Deep Walk = SkipGram Analogy + Random Walks"></a>Deep Walk = SkipGram Analogy + Random Walks</h2><p>These random walks can be thought of as short sentences and phrases in a special language; the direct analog is to estimate the likelihood of observing vertex $v_i$ given all the previous vertices visited so far in the random walk, i.e. Our goal is to learn a latent representation, not only a probability distribution of node co-occurrences, and so we introduce a mapping function $ Φ: v ∈ V→R^{|V|×d} $. This mapping $Φ$ represents the latent social representation associated with each vertex $v$ in the graph. (In practice, we represent $Φ$ by a $|V|×d$ matrix of free parameters, which will serve later on as our $X_E$).</p>
<p>The problem then, is to estimate the likelihood: $ Pr ({v}_{i} | Φ(v1), Φ(v2), · · · , Φ(vi−1))) $</p>
<p>In simple words <em>DeepWalk</em> algorithm uses the notion of Random Walks to get the surrounding nodes(words) and ultimately calulate the probability given the context nodes. In simple words we use random walk to start at a node, finds out all the nodes which have and edge connecting with this start node and randomly select one out of them, then consider this new node as the start node and repeat the procedue after n iterations you will have traversed n nodes (some of them might repeat, but it does not matter as is the case of words in a sentence which may repeat as well). We will take n nodes as the surrounding nodes for the original node and will try to maximize probability with respect to those using the probability function estimate. </p>
<p><em>So, that is for you Ladies and Gentlemen , the <b>‘DeepWalk’</b> model.</em></p>
<p>Mathematically the Deep Walk algorithm is defined as follows,</p>
<img   src="/2022/09/09/DeepWalk%E8%A7%A3%E8%AF%BB/DeepWalk_Algo.jpg"  class="Deep Walk Algorithm">
<h2 id="PyTorch-Implementation-of-DeepWalk"><a href="#PyTorch-Implementation-of-DeepWalk" class="headerlink" title="PyTorch Implementation of DeepWalk"></a>PyTorch Implementation of DeepWalk</h2><p>Here we will use using the following graph as an example to implement Deep Walk on,<br><img   src="/2022/09/09/DeepWalk%E8%A7%A3%E8%AF%BB/graph.png"  class="Example Graph"></p>
<p>As you can see there are two connected components, so we can expect than when we create the vectors for each node, the vectors of [1 , 2, 3, 7] should be close and similarly that of [4, 5, 6] should be close. Also if  any two vectors are from different group then their vectors should also be far away.</p>
<p>Here we will represent the graph using the adjacency list representation. Make sure that you are able to understand that the given graph and this adjacency list are equivalent.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">adj_list = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">4</span>,<span class="number">6</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">3</span>]]</span><br><span class="line">size_vertex = <span class="built_in">len</span>(adj_list)  <span class="comment"># number of vertices</span></span><br></pre></td></tr></table></figure>
<h2 id="Imports"><a href="#Imports" class="headerlink" title="Imports"></a>Imports</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>
<h2 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w=<span class="number">3</span>            <span class="comment"># window size</span></span><br><span class="line">d=<span class="number">2</span>            <span class="comment"># embedding size</span></span><br><span class="line">y=<span class="number">200</span>          <span class="comment"># walks per vertex</span></span><br><span class="line">t=<span class="number">6</span>            <span class="comment"># walk length </span></span><br><span class="line">lr=<span class="number">0.025</span>       <span class="comment"># learning rate</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>] <span class="comment">#labels of available vertices</span></span><br></pre></td></tr></table></figure>
<h2 id="Random-Walk"><a href="#Random-Walk" class="headerlink" title="Random Walk"></a>Random Walk</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">RandomWalk</span>(<span class="params">node,t</span>):</span><br><span class="line">    walk = [node]        <span class="comment"># Walk starts from this node</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(t-<span class="number">1</span>):</span><br><span class="line">        node = adj_list[node][random.randint(<span class="number">0</span>,<span class="built_in">len</span>(adj_list[node])-<span class="number">1</span>)]</span><br><span class="line">        walk.append(node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> walk</span><br></pre></td></tr></table></figure>
<h2 id="Skipgram"><a href="#Skipgram" class="headerlink" title="Skipgram"></a>Skipgram</h2><p>The skipgram model is closely related to the CBOW model that we just covered. In the CBOW model we have to maximise the probability of the word given its surrounding word using a neural network. And when the probability is maximised, the weights learnt from the input to hidden layer are the word vectors of the given words. In the skipgram word we will be using a using single word to maximise the probability of the surrounding words. This can be done by using a neural network that looks like the mirror image of the network that we used for the CBOW. And in the end the weights of the input to hidden layer will be the corresponding word vectors.</p>
<p>Now let’s analyze the complexity.<br>There are $|V|$ words in the vocabulary so for each iteration we will be modifying a total of $|V|$ vectors. This is very complex, usually the vocabulary size is in million and since we usually need millions of iteration before convergence, this can take a long long time to run.</p>
<p>We will soon be discussing some methods like Hierarchical Softmax or negative sampling to reduce this complexity. But, first we’ll code for a simple skipgram model. The class defines the model, whereas the function ‘skip_gram’ takes care of the training loop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.phi  = nn.Parameter(torch.rand((size_vertex, d), requires_grad=<span class="literal">True</span>))    </span><br><span class="line">        self.phi2 = nn.Parameter(torch.rand((d, size_vertex), requires_grad=<span class="literal">True</span>))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, one_hot</span>):</span><br><span class="line">        hidden = torch.matmul(one_hot, self.phi)</span><br><span class="line">        out    = torch.matmul(hidden, self.phi2)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Model()</span><br></pre></td></tr></table></figure>
<p>when predicting the $k$-th vertex, the loss is as follows:</p>
<script type="math/tex; mode=display">
\begin{aligned}
loss_k &= - log \frac{e^{v_k}} {\sum_{i=1}^n e^{v_i}} \\
&= log {\sum_{i=1}^n e^{v_i}} - v_k
\end{aligned}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">skip_gram</span>(<span class="params">wvi,  w</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(wvi)):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(<span class="number">0</span>,j-w) , <span class="built_in">min</span>(j+w, <span class="built_in">len</span>(wvi))):</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#generate one hot vector</span></span><br><span class="line">            one_hot          = torch.zeros(size_vertex)</span><br><span class="line">            one_hot[wvi[j]]  = <span class="number">1</span>    <span class="comment"># 当前中心节点为1</span></span><br><span class="line">            </span><br><span class="line">            out              = model(one_hot)</span><br><span class="line">            loss             = torch.log(torch.<span class="built_in">sum</span>(torch.exp(out))) - out[wvi[k]]</span><br><span class="line">            loss.backward()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">                param.data.sub_(lr*param.grad)</span><br><span class="line">                param.grad.data.zero_()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(y)):</span><br><span class="line">    random.shuffle(v)</span><br><span class="line">    <span class="keyword">for</span> vi <span class="keyword">in</span> v:</span><br><span class="line">        path=RandomWalk(vi,t)    <span class="comment"># 生成一条随机游走的路径</span></span><br><span class="line">        skip_gram(path, w)</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 62.37it/s]
</code></pre><p>i’th row of the model.phi corresponds to vector of the i’th node. As you can see the vectors of [0, 1, 2,3 , 7] are very close, whereas their vector are much different from the vectors corresponding to [4, 5, 6].</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(model.phi)</span><br></pre></td></tr></table></figure>
<pre><code>Parameter containing:
tensor([[ 0.8624,  0.7754],
        [ 0.2209,  1.0904],
        [ 0.6065,  0.7289],
        [ 0.3771,  1.1625],
        [-0.0250, -1.2358],
        [ 0.0542, -1.2929],
        [-0.6126, -1.1073],
        [-0.9810,  0.9992]], requires_grad=True)
</code></pre><p>Now we will be discussing a variant of the above using Hierarchical softmax.</p>
<h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><p>As we have seen in the skip-gram model that the probability of any outcome depends on the total outcomes of our model. If you haven’t noticed this yet, let us explain you how!</p>
<p>When we calculate the probability of an outcome using softmax, this probability depends on the number of model parameters via the normalisation constant(denominator term) in the softmax.</p>
<p>$\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$</p>
<p>And the number of such parameters are linear in the total number of outcomes. It means if we are dealing with a very large graphical structure, it can be computationally very expensive and taking a lot of time.</p>
<h3 id="Can-we-somehow-overcome-this-challenge"><a href="#Can-we-somehow-overcome-this-challenge" class="headerlink" title="Can we somehow overcome this challenge?"></a>Can we somehow overcome this challenge?</h3><p>Obviously, Yes! (because we’re asking at this stage). </p>
<p>*Drum roll please*</p>
<p><b>Enter “Hierarchical Softmax(hs)”</b>.</p>
<p>Basically, hs is an alternative approximation to the softmax in which the probability of any one outcome depends on a number of model parameters that is only logarithmic in the total number of outcomes.</p>
<p>Hierarchical softmax uses a binary tree to represent all the words(nodes) in the vocabulary. Each leaf of the tree is a node of our graph, and there is a unique path from root to the leaf. Each intermediate node of tree explicitly represents the relative probabilities of its child nodes. So these nodes are associated to different vectors which our model is going to learn.</p>
<p>The idea behind decomposing the output layer into binary tree is to reduce the time complexity to obtain<br>probability distribution from $O(V)$ to $O(log(V))$</p>
<p>Let us understand the process with an example.</p>
<img   src="/2022/09/09/DeepWalk%E8%A7%A3%E8%AF%BB/tree.png"  class="binary tree">
<p>In this example, leaf nodes represent the original nodes of our graph. The highlighted nodes and edges make a path from root to an example leaf node $w_2$.</p>
<p>Here, length of the path $L(w_{2}) = 4$.</p>
<p>$n(w, j)$ means the $j^{th}$ node on the path from root to a leaf node $w$.</p>
<p>Now, view this tree as a decision process, or a random walk, that begins at the root of the tree and descents towards the leaf nodes at each step. It turns out that the probability of each outcome in the original distribution uniquely determines the transition probabilities of this random walk. If you want to go from root node to $w_2$(say), first you have to take a left turn, again left turn and then right turn. </p>
<p>Let’s denote the probability of going left at an intermediate node $n$ as $p(n,left)$ and probability of going right as $p(n,right)$. So we can define the probabilty of going to $w_2$ as follows.</p>
<p><b> $P(w2|wi) = p(n(w_{2}, 1), left) . p(n(w_{2}, 2),left) . p(n(w_{2}, 3), right)$ </b></p>
<p>Above process implies that the cost for computing the loss function and its gradient will be proportional to the number of nodes $(V)$ in the intermediate path between root node and the output node, which on average is no greater than $log(V)$. That’s nice! Isn’t it? In the case where we deal with a large number of outcomes, there will be a huge difference in the computational cost of ‘vanilla’ softmax and hierarchical softmax.</p>
<p>Implementation remains similar to the vanilla, except that we will only need to change the Model class by HierarchicalModel class, which is defined below.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func_L</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    w: Leaf node.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    count: The length of path from the root node to the given vertex.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    count=<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span>(w!=<span class="number">1</span>):</span><br><span class="line">        count+=<span class="number">1</span></span><br><span class="line">        w//=<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># func_n returns the nth node in the path from the root node to the given vertex</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func_n</span>(<span class="params">w, j</span>):</span><br><span class="line">    li=[w]</span><br><span class="line">    <span class="keyword">while</span>(w!=<span class="number">1</span>):</span><br><span class="line">        w = w//<span class="number">2</span></span><br><span class="line">        li.append(w)</span><br><span class="line"></span><br><span class="line">    li.reverse()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> li[j]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    out = <span class="number">1</span>/(<span class="number">1</span>+torch.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">HierarchicalModel</span>(torch.nn.Module):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(HierarchicalModel, self).__init__()</span><br><span class="line">        self.phi         = nn.Parameter(torch.rand((size_vertex, d), requires_grad=<span class="literal">True</span>))   </span><br><span class="line">        self.prob_tensor = nn.Parameter(torch.rand((<span class="number">2</span>*size_vertex, d), requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, wi, wo</span>):</span><br><span class="line">        one_hot     = torch.zeros(size_vertex)</span><br><span class="line">        one_hot[wi] = <span class="number">1</span></span><br><span class="line">        w = size_vertex + wo</span><br><span class="line">        h = torch.matmul(one_hot,self.phi)</span><br><span class="line">        p = torch.tensor([<span class="number">1.0</span>])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, func_L(w)-<span class="number">1</span>):</span><br><span class="line">            mult = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span>(func_n(w, j+<span class="number">1</span>)==<span class="number">2</span>*func_n(w, j)): <span class="comment"># Left child</span></span><br><span class="line">                mult = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">            p = p*sigmoid(mult*torch.matmul(self.prob_tensor[func_n(w,j)], h))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure>
<p>The input to hidden weight vector no longer represents the vector corresponding to each vector , so directly trying to read it will not provide any valuable insight, a better option is to predict the probability of different vectors against each other to figure out the likelihood of coexistance of the nodes.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hierarchicalModel = HierarchicalModel()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">HierarchicalSkipGram</span>(<span class="params">wvi,  w</span>):</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(wvi)):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(<span class="number">0</span>,j-w) , <span class="built_in">min</span>(j+w, <span class="built_in">len</span>(wvi))):</span><br><span class="line">            <span class="comment">#generate one hot vector</span></span><br><span class="line">       </span><br><span class="line">            prob = hierarchicalModel(wvi[j], wvi[k])</span><br><span class="line">            loss = - torch.log(prob)</span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> hierarchicalModel.parameters():</span><br><span class="line">                param.data.sub_(lr*param.grad)</span><br><span class="line">                param.grad.data.zero_()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y):</span><br><span class="line">    random.shuffle(v)</span><br><span class="line">    <span class="keyword">for</span> vi <span class="keyword">in</span> v:</span><br><span class="line">        wvi = RandomWalk(vi,t)</span><br><span class="line">        HierarchicalSkipGram(wvi, w)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">        <span class="built_in">print</span>((hierarchicalModel(i,j).item()*<span class="number">100</span>)//<span class="number">1</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(end = <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>30.0 23.0 18.0 27.0 23.0 19.0 4.0 52.0 
21.0 30.0 21.0 26.0 10.0 16.0 0.0 72.0 
20.0 29.0 25.0 24.0 14.0 21.0 4.0 59.0 
24.0 29.0 18.0 27.0 11.0 15.0 0.0 72.0 
27.0 20.0 28.0 23.0 42.0 29.0 28.0 0.0 
20.0 22.0 37.0 19.0 37.0 35.0 27.0 0.0 
23.0 23.0 31.0 22.0 33.0 31.0 34.0 0.0 
20.0 32.0 20.0 26.0 7.0 13.0 0.0 78.0 
</code></pre><h3>References</h3>

<ul>
<li><p><a class="link"   href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf" >DeepWalk: Online Learning of Social Representations<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://medium.com/@_init_/an-illustrated-explanation-of-using-skipgram-to-encode-the-structure-of-a-graph-deepwalk-6220e304d71b?source=---------13------------------" >An Illustrated Explanation of Using SkipGram To Encode The Structure of A Graph (DeepWalk)<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285" >Word Embedding<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://www.youtube.com/watch?v=qmCrtuS9vtU" >Centralized &amp; Scale Free Networks<i class="fas fa-external-link-alt"></i></a></p>
</li>
</ul>
<ul>
<li>Beautiful explanations by Chris McCormick:<ul>
<li><a class="link"   href="https://youtu.be/pzyIWCelt_E" >Hieararchical Softmax<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://mccormickml.com/2019/03/12/the-inner-workings-of-word2vec/" >word2vec<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" >Negative Sampling<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" >skip-gram<i class="fas fa-external-link-alt"></i></a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>DeepWalk</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>EOM公式推导</title>
    <url>/2025/10/10/EOM%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/</url>
    <content><![CDATA[<p>在uplift建模中，除了AUUC、QINI指标，还有EOM。它是基于离线RCT模拟评估在线业务收益的指标，EOM越高，业务收益越高。</p>
<span id="more"></span>
<p>在这里记录下EOM的公式推导。</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.1ovti3dugm.webp"  alt="EOM"></p>
<p>这个推导依赖于<strong>概率论中的两个核心概念</strong>：</p>
<ol>
<li><strong>全期望定律 (Law of Total Expectation)</strong></li>
<li><strong>随机实验中的独立性 (Independence in Randomized Experiments)</strong></li>
</ol>
<p><strong>目标：</strong> 证明 $\text{E}[Z] = \text{E}[Y | T = h(\mathbf{X})]$</p>
<p>其中，随机变量 $Z$ 的定义为：</p>
<script type="math/tex; mode=display">Z = \sum_{t=0}^{K} \frac{1}{p_t} Y \mathbb{I}\{h(\mathbf{X}) = t\} \mathbb{I}\{T = t\}</script><p>这里 $p_t = P(T=t)$ 是用户被分配到干预 $t$ 的概率。</p>
<h3 id="第一步：代入-Z-的定义并利用期望的线性性"><a href="#第一步：代入-Z-的定义并利用期望的线性性" class="headerlink" title="第一步：代入 $Z$ 的定义并利用期望的线性性"></a><strong>第一步：代入 $Z$ 的定义并利用期望的线性性</strong></h3><p>期望 $\text{E}[\cdot]$ 具有线性性，因此我们可以将期望 $\text{E}[Z]$ 拆分成和式的期望：</p>
<script type="math/tex; mode=display">\text{E}[Z] = \text{E}\left[\sum_{t=0}^{K} \frac{1}{p_t} Y \mathbb{I}\{h(\mathbf{X}) = t\} \mathbb{I}\{T = t\}\right]</script><p>将求和 $\sum$ 和常量 $\frac{1}{p_t}$ 移出期望：</p>
<script type="math/tex; mode=display">\text{E}[Z] = \sum_{t=0}^{K} \frac{1}{p_t} \text{E}\left[Y \mathbb{I}\{h(\mathbf{X}) = t\} \mathbb{I}\{T = t\}\right] \quad (*)</script><h3 id="第二步：利用全期望定律和指示函数"><a href="#第二步：利用全期望定律和指示函数" class="headerlink" title="第二步：利用全期望定律和指示函数"></a><strong>第二步：利用全期望定律和指示函数</strong></h3><p>根据概率论中对随机变量乘积期望的定义，两个指示函数 $\mathbb{I}\{A\}$ 和 $\mathbb{I}\{B\}$ 相乘，等价于条件 $A$ 和 $B$ <strong>同时成立</strong>。</p>
<script type="math/tex; mode=display">\text{E}[Y \mathbb{I}\{h(\mathbf{X}) = t\} \mathbb{I}\{T = t\}] = \text{E}\left[Y \cdot \mathbb{I}\left\{h(\mathbf{X}) = t, T = t\right\}\right]</script><p>利用全期望定律，将期望写成 <strong>联合概率的积分</strong> 形式：</p>
<script type="math/tex; mode=display">\text{E}\left[Y \cdot \mathbb{I}\left\{h(\mathbf{X}) = t, T = t\right\}\right] = \text{E}\left[Y \mid h(\mathbf{X}) = t, T = t\right] \cdot P\left(h(\mathbf{X}) = t, T = t\right)</script><p>将这个结果代回 $(*)$：</p>
<script type="math/tex; mode=display">\text{E}[Z] = \sum_{t=0}^{K} \frac{1}{p_t} \cdot \text{E}\left[Y \mid h(\mathbf{X}) = t, T = t\right] \cdot P\left(h(\mathbf{X}) = t, T = t\right)</script><h3 id="第三步：利用随机实验的独立性"><a href="#第三步：利用随机实验的独立性" class="headerlink" title="第三步：利用随机实验的独立性"></a><strong>第三步：利用随机实验的独立性</strong></h3><p>在RCT样本中，用户被分配到干预组 $T$ 的过程是独立于用户的特征 $\mathbf{X}$（以及模型基于 $\mathbf{X}$ 的预测 $h(\mathbf{X})$）的。</p>
<p>因此，<strong>事件 $\{h(\mathbf{X})=t\}$ 和 $\{T=t\}$ 是相互独立的。</strong></p>
<p>根据独立性，联合概率可以分解：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P\left(h(\mathbf{X}) = t, T = t\right) &= P\left(h(\mathbf{X}) = t\right) \cdot P\left(T = t\right)  \\
&= P\left(h(\mathbf{X}) = t\right) \cdot p_t
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\text{E}[Z] &= \sum_{t=0}^{K} \frac{1}{p_t} \cdot \text{E}\left[Y \mid h(\mathbf{X}) = t, T = t\right] \cdot \left(P\left(h(\mathbf{X}) = t\right) \cdot p_t\right) \\
&= \sum_{t=0}^{K} \text{E}\left[Y \mid h(\mathbf{X}) = t, T = t\right] \cdot P\left(h(\mathbf{X}) = t\right)
\end{aligned}</script><p>回顾全期望定律：$\text{E}[A] = \sum_{i} \text{E}[A | B=b_i] P(B=b_i)$。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{E}[Z] &=\sum_{t=0}^{K} \text{E}\left[Y \mid h(\mathbf{X}) = t, T = t\right] \cdot P\left(h(\mathbf{X}) = t\right) \\
&=\sum_{t=0}^{K} \text{E}\left[Y \mid T = t, h(\mathbf{X}) = t\right] \cdot P\left(h(\mathbf{X}) = t\right) \\
&=  \text{E}[Y | T = h(\mathbf{X})]
\end{aligned}</script><p>在营销场景的在线运筹中，$h(X) = t$ 表示运筹出一张券面额，如果实发面额$T$也等于$t$，那么$z=\frac{Y}{p_t}$，这样即可模拟出在线业务收益。$p_t$是该treatment的样本分布占比，$\frac{1}{p_t}$表示IPW，从而避免样本不均的影响。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611974973.66" >Uplift Modeling with Multiple Treatments and General Response Types<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Marketing</category>
      </categories>
      <tags>
        <tag>EOM</tag>
      </tags>
  </entry>
  <entry>
    <title>Enhancing CTR Prediction with De-correlated Expert Networks</title>
    <url>/2025/08/06/Enhancing-CTR-Prediction-with-De-correlated-Expert-Networks/</url>
    <content><![CDATA[<p>本文探索了专家网络的差异性对模型性能的影响，本质上是种bagging思想，从各个语义空间上提升模型的表达能力。</p>
<span id="more"></span>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><ul>
<li><strong>核心问题</strong>：MoE（混合专家）模型在CTR预估中，专家网络（Expert）的多样性对效果的影响</li>
<li><strong>关键发现</strong>：专家间差异性（不相关度）与模型AUC正相关  </li>
</ul>
<h2 id="方法论（Hetero-MoE）"><a href="#方法论（Hetero-MoE）" class="headerlink" title="方法论（Hetero-MoE）"></a>方法论（Hetero-MoE）</h2><h4 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.7zqqtylbd2.webp"  alt="model"></p>
<ul>
<li><strong>异构专家</strong>：每个Expert使用独立Embedding + 不同网络结构（CrossNet/CIN/DNN等）</li>
<li><strong>个性化Gate</strong>：基于对应Expert的Embedding生成权重  </li>
</ul>
<h4 id="差异性增强手段"><a href="#差异性增强手段" class="headerlink" title="差异性增强手段"></a>差异性增强手段</h4><div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>实现方式</th>
<th>技术细节</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Embedding</strong></td>
<td>每个Expert独立Embedding表</td>
<td>避免参数共享导致的表征同质化</td>
</tr>
<tr>
<td><strong>结构异构</strong></td>
<td>混合CrossNet/CIN/DNN等不同结构</td>
<td>不同结构捕获多样特征交互模式</td>
</tr>
<tr>
<td><strong>正则化</strong></td>
<td>皮尔逊相关系数损失</td>
<td>$L_corr = ∑(Pearson(E_i, E_j))$，$E_i$为Expert i的输出向量</td>
</tr>
</tbody>
</table>
</div>
<h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><ul>
<li><strong>基准对比</strong>：Hetero-MoE vs 传统MoE  </li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">AUC提升</th>
<th style="text-align:center">参数量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Shared-MoE</td>
<td style="text-align:center">+0.0%</td>
<td style="text-align:center">100%</td>
</tr>
<tr>
<td style="text-align:center">Hetero-MoE</td>
<td style="text-align:center">+1.8%</td>
<td style="text-align:center">105%</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>消融实验</strong>：  <ul>
<li>仅结构异构：+0.6% AUC  </li>
<li>仅Embedding独立：+0.9% AUC  </li>
<li>全方案：+1.8% AUC</li>
</ul>
</li>
</ul>
<h2 id="关键公式"><a href="#关键公式" class="headerlink" title="关键公式"></a>关键公式</h2><p>专家相关性损失（最小化皮尔逊系数）：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{corr} = \sum_{i \neq j} \left| \frac{\text{Cov}(E_i, E_j)}{\sigma_{E_i} \sigma_{E_j}} \right|</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://arxiv.org/pdf/2505.17925" >Enhancing CTR Prediction with De-correlated Expert Networks<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/JcvMQ5xJYLsCCWNqrv-ZiQ" >中科大&amp;腾讯：通过提升各个专家网络差异性提升基于MoE的CTR预估效果<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>CTR</tag>
        <tag>MoE</tag>
      </tags>
  </entry>
  <entry>
    <title>FLIP解读</title>
    <url>/2024/02/06/FLIP%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>FLIP由CLIP改进而来，其思想非常简单，通过在图片侧mask掉相当比例的patch（无须重构patch），实现速度和准确性的双重提升。</p>
<span id="more"></span>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.k290d3y8ylc.webp"  alt="model"></p>
<p>受MAE启发，FLIP对图像进行了mask来预训练。该方法有两方面收益：</p>
<ul>
<li>速度：ViT对图像编码的计算量大幅减少，训练速度更快</li>
<li>准确性：相同的显存可以存放更多的batch，从而构造更多的图文对进行对比学习，准确性得以提高</li>
</ul>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.2vrg1obvy7i0.png"  alt="speed"></p>
<p>值得注意的是，该预训练任务没有重构patch，个人理解：</p>
<ul>
<li>图片本身就包含了大量的冗余信息，mask掉部分patch不影响图片理解</li>
<li>mask部分patch，可以强制两边编码器去学习对方的上下文语义信息</li>
</ul>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>FLIP在下游实验的结果一片绿：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.2vmbpzmm3540.webp"  alt="experiment"></p>
<h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.751rg20p7vg0.webp"  alt="ablation"></p>
<ul>
<li>作者尝试在图像上的不同mask比例，50%最佳</li>
<li>作者也尝试了在文本上做mask，但性能略微有所下降</li>
<li>重构patch没有收益</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/e2OdCN6jMK-kWapaEaoO1A" >简单高效！何恺明大神之多模态预训练FLIP<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CLIP</tag>
        <tag>MAE</tag>
        <tag>Masked Autoencoders</tag>
        <tag>Contrastive Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>FLOPS、FLOPs、TOPS概念</title>
    <url>/2025/06/29/FLOPS%E3%80%81FLOPs%E3%80%81TOPS%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<p>在计算性能和硬件指标中，<strong>FLOPS、FLOP/s、TOPS</strong> 是常见的术语，但它们有明确的区别和应用场景。以下是详细解析：</p>
<span id="more"></span>
<h3 id="1-FLOPS（Floating-Point-Operations-per-Second）"><a href="#1-FLOPS（Floating-Point-Operations-per-Second）" class="headerlink" title="1. FLOPS（Floating Point Operations per Second）"></a><strong>1. FLOPS（Floating Point Operations per Second）</strong></h3><ul>
<li><strong>定义</strong>：<br>每秒浮点运算次数（Floating Point Operations Per Second），用于衡量计算设备的<strong>持续浮点计算能力</strong>。  </li>
<li><strong>特点</strong>：  <ul>
<li><strong>大写字母</strong>表示单位（如 <code>1 FLOPS = 1 次浮点运算/秒</code>）。  </li>
<li>通常用于描述 CPU、GPU 等通用计算硬件的理论峰值性能。  </li>
</ul>
</li>
<li><strong>示例</strong>：  <ul>
<li>NVIDIA A100 GPU 的峰值性能为 <strong>19.5 TFLOPS</strong>（19.5 × 10¹² 次浮点运算/秒）。  </li>
</ul>
</li>
</ul>
<h3 id="2-FLOP-s（Floating-Point-Operations）"><a href="#2-FLOP-s（Floating-Point-Operations）" class="headerlink" title="2. FLOP/s（Floating Point Operations）"></a><strong>2. FLOP/s（Floating Point Operations）</strong></h3><ul>
<li><strong>定义</strong>：<br>浮点运算总数（Floating Point Operations），<strong>不带时间单位</strong>，表示任务的总计算量。  </li>
<li><strong>特点</strong>：  <ul>
<li><strong>小写字母 <code>s</code></strong> 表示复数（Operations），而非时间（Second）。  </li>
<li>用于衡量算法或模型的复杂度。  </li>
</ul>
</li>
<li><strong>示例</strong>：  <ul>
<li>训练 ResNet-50 模型约需要 <strong>3.8 × 10⁹ FLOP</strong>（38亿次浮点运算）。  </li>
</ul>
</li>
</ul>
<h3 id="3-TOPS（Tera-Operations-per-Second）"><a href="#3-TOPS（Tera-Operations-per-Second）" class="headerlink" title="3. TOPS（Tera Operations per Second）"></a><strong>3. TOPS（Tera Operations per Second）</strong></h3><ul>
<li><strong>定义</strong>：<br>每秒万亿次操作次数（Tera Operations Per Second），通常用于衡量 <strong>整数运算或混合精度计算</strong> 的硬件性能。  </li>
<li><strong>特点</strong>：  <ul>
<li>1 TOPS = 10¹² 次操作/秒。  </li>
<li>主要用于 AI 加速器（如 NPU、TPU）或边缘计算设备。  </li>
<li><strong>不限定操作类型</strong>（可能是整数、矩阵乘加等）。  </li>
</ul>
</li>
<li><strong>示例</strong>：  <ul>
<li>华为 Ascend 910 AI 芯片的算力为 <strong>256 TOPS</strong>。  </li>
</ul>
</li>
</ul>
<h3 id="对比总结"><a href="#对比总结" class="headerlink" title="对比总结"></a><strong>对比总结</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><strong>术语</strong></th>
<th><strong>全称</strong></th>
<th><strong>单位</strong></th>
<th><strong>应用场景</strong></th>
<th><strong>关键区别</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>FLOPS</td>
<td>Floating Point Operations per Second</td>
<td>次浮点运算/秒</td>
<td>CPU/GPU 峰值算力</td>
<td>仅衡量浮点运算，带时间单位</td>
</tr>
<tr>
<td>FLOP/s</td>
<td>Floating Point Operations</td>
<td>次浮点运算（总量）</td>
<td>算法/模型计算量</td>
<td>无时间单位，仅表示总量</td>
</tr>
<tr>
<td>TOPS</td>
<td>Tera Operations per Second</td>
<td>万亿次操作/秒</td>
<td>AI 加速器（NPU/TPU）</td>
<td>包含整数/混合精度操作</td>
</tr>
</tbody>
</table>
</div>
<h3 id="常见误区"><a href="#常见误区" class="headerlink" title="常见误区"></a><strong>常见误区</strong></h3><ol>
<li><p><strong>FLOPS vs FLOP/s</strong>：  </p>
<ul>
<li>错误用法：<em>“这个模型需要 1 TFLOPS”</em> ❌（应使用 FLOP/s）。  </li>
<li>正确用法：<em>“这个模型需要 1 TFLOP/s 的计算量，GPU 的峰值性能是 10 TFLOPS”</em> ✅。  </li>
</ul>
</li>
<li><p><strong>TOPS 与 FLOPS 不可直接比较</strong>：  </p>
<ul>
<li>TOPS 可能包含整数运算（如 INT8），而 FLOPS 仅针对浮点（FP32/FP64）。  </li>
<li>例如：1 TOPS (INT8) ≠ 1 TFLOPS (FP32)，实际性能需结合硬件架构。</li>
</ul>
</li>
</ol>
<h3 id="实际应用场景"><a href="#实际应用场景" class="headerlink" title="实际应用场景"></a><strong>实际应用场景</strong></h3><ul>
<li><strong>训练深度学习模型</strong>：关注 <strong>FLOP/s</strong>（计算总量）和 <strong>TFLOPS</strong>（硬件算力）。  </li>
<li><strong>部署 AI 芯片</strong>：关注 <strong>TOPS</strong>（如自动驾驶芯片通常标称 TOPS）。  </li>
<li><strong>算法优化</strong>：通过降低 FLOP/s 来减少计算负担。  </li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>Floating Point Operations</tag>
      </tags>
  </entry>
  <entry>
    <title>FP16与BF16区别</title>
    <url>/2024/05/05/FP16%E4%B8%8EBF16%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.1hs0v083jb.png"  alt="16"></p>
<span id="more"></span>
<p>二者都是占用16bit空间。</p>
<ul>
<li>FP16由1个符号位、5个指数位和10个尾数位组成。FP16在表达小数时具有较高的精度，但表示的最大范围相对BF16比较小。相比BF16，在表达较大的数时更容易出现上溢的情况。</li>
<li>BF16由1个符号位、8个指数位和7个尾数位组成。相比于FP16，BF16牺牲了一些尾数位以增加指数位，扩大了表达的范围，但是精度降低了，因此对于对精度需求比较高的模型，模型可能效果不如FP16。</li>
</ul>
<p>模型训练时使用BF16和FP16都可以降低内存使用和传输量，提高训练效率。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/eTZHaDkxM0MjDl6tLX9lxw" >使用半精度训练时，BF16和FP16格式有什么异同？<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>FP16</tag>
        <tag>BF16</tag>
      </tags>
  </entry>
  <entry>
    <title>FM &amp; DeepFM</title>
    <url>/2022/01/19/FM%20&amp;%20DeepFM/</url>
    <content><![CDATA[<p><code>FM</code> 是搜广推里最经典的算法，这里记录一下原理与公式推导：</p>
<span id="more"></span>
<h1 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h1><h2 id="参数数量和时间复杂度优化"><a href="#参数数量和时间复杂度优化" class="headerlink" title="参数数量和时间复杂度优化"></a>参数数量和时间复杂度优化</h2><p>当我们使用一阶原始特征和二阶组合特征来刻画样本的时候，会得到如下式子：</p>
<script type="math/tex; mode=display">
\hat{y}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} w_{i j} x_{i} x_{j}</script><p>$x_i$ 和 $x_j$ 分别表示两个不同的特征取值，对于 $n$ 维的特征来说，这样的二阶组合特征一共有 $\frac{n(n-1)}{2}$ 种，也就意味着我们需要同样数量的权重参数。但是由于现实场景中的特征是高维稀疏的，导致 $n$ 非常大，比如上百万，这里<strong>两两特征组合的特征量级 $C_n^2$</strong> ，所带来的参数量就是一个天文数字。对于一个上百亿甚至更多参数空间的模型来说，我们需要海量训练样本才可以保证完全收敛。这是非常困难的。</p>
<p>FM解决这个问题的方法非常简单，它不再是简单地为交叉之后的特征对设置参数，而是设置了一种计算特征参数的方法。</p>
<p>FM模型引入了新的矩阵 $V$ ，它是一个 $n \times k$ 的二维矩阵。这里的 $k$ 是超参，一般不会很大，比如16、32之类。对于特征每一个维度 $x_i$ ，我们都可以找到一个表示向量 $v_i \in R^k$ 。从NLP的角度来说，就是为每个特征学习一个embedding。原先的参数量从 $O(n^2)$ 降低到了 $O(k \times n)$ 。ALBERT论文的因式分解思想跟这个非常相似：$O(V \times H) \ggg O(V \times E + E \times H)$</p>
<p>有了 $V$ 矩阵，上式就可以改写成如下形式：</p>
<script type="math/tex; mode=display">
\hat{y}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=1}^{n} v_{i}^{T} v_{j} x_{i} x_{j}</script><p>当 $k$ 足够大的时候，即 $k = n$ ，那么就有 $W = V$ 。在实际的应用场景当中，我们并不需要设置非常大的K，因为特征矩阵往往非常稀疏，我们可能没有足够多的样本来训练这么大量的参数，并且<strong>限制K也可以一定程度上提升FM模型的泛化能力</strong>。</p>
<p>此外这样做还有一个好处就是<strong>有利于模型训练</strong>，因为对于有些稀疏的特征组合来说，我们所有的样本当中可能都是空的。比如在刚才的例子当中用户A和电影B的组合，可能用户A在电影B上就没有过任何行为，那么这个数据就是空的，我们也不可能训练出任何参数来。但是引入了 $V$ 之后，虽然这两项缺失，但是我们针对用户A和电影B分别训练出了向量参数，我们用这两个向量参数点乘，就得到了这个交叉特征的系数。</p>
<p>虽然我们将模型的参数降低到了 $O(k \times n)$ ，但预测一条样本所需要的时间复杂度仍为 $O(k \times n^2)$ ，这仍然是不可接受的。所以对它进行优化也是必须的，并且这里的优化非常简单，可以<strong>直接通过数学公式的变形推导</strong>得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{i=1}^{n} \sum_{j=i+1}^{n} v_{i}^{T} v_{j} x_{i} x_{j} &=\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} v_{i}^{T} v_{j} x_{i} x_{j}-\frac{1}{2} \sum_{i=1}^{n} v_{i}^{T} v_{j} x_{i} x_{j} \\
&=\frac{1}{2}\left(\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{j, f} x_{i} x_{j}-\sum_{i=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{i, f} x_{i} x_{i}\right) \\
&=\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)\left(\sum_{j=1}^{n} v_{j, f} x_{j}\right)-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \\
&=\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)
\end{aligned}</script><p>FM模型预测的时间复杂度优化到了 $O(k \times n)$ .</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>优化过后的式子如下：</p>
<script type="math/tex; mode=display">
\hat{y}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)</script><p>针对FM模型我们一样可以使用梯度下降算法来进行优化。既然要使用梯度下降，那么我们就需要写出模型当中所有参数的偏导，主要分为三个部分：</p>
<ul>
<li>$w_0$ : $\frac{\partial \theta}{\partial w_{0}}=1$</li>
<li>$\sum_{i=1}^{n} w_{i} x_{i}$ : $\frac{\partial 0}{\partial w_{i}}=x_{i}$</li>
<li>$\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)$ : $\frac{\partial \hat{y}}{\partial v_{i, f}}  = \frac{1}{2} (2x_i (\sum_{j=1}^{n} v_{j, f} x_{j}) - 2v_{i,f} x_i^2) = x_{i} \sum_{j=1}^{n} v_{j, f} x_{j}-v_{i, f} x_{i}^{2}$</li>
</ul>
<p>综合如下：</p>
<script type="math/tex; mode=display">
\frac{\partial \hat{y}}{\partial \theta}= \begin{cases}1, & \text { if } \theta \text { is } w_{0} \\ x_{i}, & \text { if } \theta \text { is } w_{i} \\ x_{i} \sum_{j=1}^{n} v_{j, f} x_{j}-v_{i, f} x_{i}^{2} & \text { if } \theta \text { is } v_{i, f}\end{cases}</script><p>由于 $\sum_{j=1}^n v_{j,f} x_j$ 是可以提前计算好存储起来的，因此我们对所有参数的梯度计算也都能在 $O(1)$ 时间复杂度内完成。</p>
<h2 id="拓展到-d-维"><a href="#拓展到-d-维" class="headerlink" title="拓展到 $d$ 维"></a>拓展到 $d$ 维</h2><p>参照刚才的公式，可以写出FM模型推广到d维的方程：</p>
<script type="math/tex; mode=display">
\hat{y}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{l=2}^{d} \sum_{i_1=1}^{n-l+1} \cdots \sum_{i_{l}=i_{l-1}+1}^{n}\left(\Pi_{j-1}^{l} x_{i_{j}}\right)\left(\sum_{f=1}^{k} \Pi_{j=1}^{l} v_{i_{j}, f}^{l}\right)</script><p>以 $d=3$ 为例，上式为：</p>
<script type="math/tex; mode=display">
\hat{y}=w_{0}+\sum_{i=1}^{n} w_{i} x_{i} + \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} x_{i} x_{j}\left(\sum_{t=1}^{k} v_{i, t} v_{j, t}\right)+\sum_{i=1}^{n-2} \sum_{j=i+1}^{n-1} \sum_{l=j+1}^{n} x_{i} x_{j} x_{l}\left(\sum_{t=1}^{k} v_{i, t} v_{j, t} v_{l, t}\right)</script><p>它的复杂度是 $O(k \times n^d)$ 。当 $d=2$ 的时候，我们通过一系列变形将它的复杂度优化到了 $O(k \times n)$ 。而当 $d &gt; 2$ 的时候，没有很好的优化方法，而且三重特征的交叉往往没有意义，并且会过于稀疏，所以我们一般情况下只会使用 $d=2$ 的情况。</p>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">ndim = <span class="built_in">len</span>(feature_names)  <span class="comment"># 原始特征数量</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, k</span>):</span><br><span class="line">        <span class="built_in">super</span>(FM, self).__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.k = k</span><br><span class="line">        self.w = nn.Linear(self.dim, <span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 初始化V矩阵</span></span><br><span class="line">        self.v = nn.Parameter(torch.rand(self.dim, self.k) / <span class="number">100</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        linear = self.w(x)</span><br><span class="line">        <span class="comment"># 二次项</span></span><br><span class="line">        quadradic = <span class="number">0.5</span> * torch.<span class="built_in">sum</span>(torch.<span class="built_in">pow</span>(torch.mm(x, self.v), <span class="number">2</span>) - torch.mm(torch.<span class="built_in">pow</span>(x, <span class="number">2</span>), torch.<span class="built_in">pow</span>(self.v, <span class="number">2</span>)))</span><br><span class="line">        <span class="comment"># 套一层sigmoid转成分类模型，也可以不加，就是回归模型</span></span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(linear + quadradic)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">fm = FM(ndim, k)</span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line">optimizer = torch.optim.SGD(fm.parameters(), lr=<span class="number">0.005</span>, weight_decay=<span class="number">0.001</span>)</span><br><span class="line">iteration = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    fm.train()</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        output = fm(X)</span><br><span class="line">        l = loss_fn(output.squeeze(dim=<span class="number">1</span>), y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        iteration += <span class="number">1</span>        </span><br><span class="line">        <span class="keyword">if</span> iteration % <span class="number">200</span> == <span class="number">199</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fm.<span class="built_in">eval</span>()</span><br><span class="line">                output = fm(X_eva_tensor)</span><br><span class="line">                l = loss_fn(output.squeeze(dim=<span class="number">1</span>), y_eva_tensor)</span><br><span class="line">                acc = ((torch.<span class="built_in">round</span>(output).long() == y_eva_tensor.view(-<span class="number">1</span>, <span class="number">1</span>).long()).<span class="built_in">sum</span>().<span class="built_in">float</span>().item()) / <span class="number">1024</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125;, iteration: &#123;&#125;, loss: &#123;&#125;, acc: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, iteration, l.item(), acc))</span><br><span class="line">            fm.train()</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h1><img   src="/2022/01/19/FM%20&%20DeepFM/DeepFM.png"  class="">
<script type="math/tex; mode=display">
\hat{y}=\operatorname{sigmoid}\left(y_{F M}+y_{D N N}\right)</script><h2 id="FM-1"><a href="#FM-1" class="headerlink" title="FM"></a>FM</h2><img   src="/2022/01/19/FM%20&%20DeepFM/FM.png"  class="">
<p>该组件就是在计算FM：</p>
<script type="math/tex; mode=display">
y_{F M}=\langle w, x\rangle+\sum_{j_{1}=1}^{d} \sum_{j_{2}=j_{1}+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{j_{1}} \cdot x_{j_{2}}</script><p>注意不是：$w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)$</p>
<ul>
<li>每个 $Field$ 是one-hot形式，黄色的圆表示 $1$ ，蓝色的代表 $0$</li>
<li>连接黄色圆的黑线就是在做：$\langle w, x\rangle$</li>
<li>连接embedding的红色线就是在做：$\sum_{j_{1}=1}^{d} \sum_{j_{2}=j_{1}+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{j_{1}} \cdot x_{j_{2}}$</li>
</ul>
<h2 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h2><img   src="/2022/01/19/FM%20&%20DeepFM/Deep.png"  class="">
<p>DNN部分比较简单，但它是与FM部分共享Embedding的。</p>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s?__biz=Mzg5NTYyMDgyNg==&amp;mid=2247489278&amp;idx=1&amp;sn=f3652394955d719bf02a91ca3b179ed2&amp;source=41#wechat_redirect" >原创 | 想做推荐算法？先把FM模型搞懂再说<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://fancyerii.github.io/2019/12/19/deepfm/" >DeepFM模型CTR预估理论与实战<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/57873613" >深度推荐模型之DeepFM<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/techflow/p/14260630.html" >吃透论文——推荐算法不可不看的DeepFM模型<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Recommender Systems</tag>
      </tags>
  </entry>
  <entry>
    <title>Flash-Attention</title>
    <url>/2024/02/19/Flash-Attention/</url>
    <content><![CDATA[<p>这是一篇硬核的优化Transformer的工作。众所周知，Transformer模型的计算量和储存复杂度是 $O(N^2)$ 。尽管先前有了大量的优化工作，比如LongFormer、Sparse Transformer、Reformer等等，一定程度上减轻了Transformer的资源消耗，但对Transformer的性能有所折损，且扩展性不强，不能泛化到其它领域、以及复杂结构的叠加。</p>
<span id="more"></span>
<p>这篇工作从底层对Transformer的计算和读写进行了优化，主要有三个贡献：</p>
<ol>
<li>加速了模型计算：现在GPU的计算速度已经远远超过了内存读写速度(<strong><em>模型计算速度慢是因为IO慢，而不是$O(N^2)$的原因导致。也就是说transformer的瓶颈在IO，而不是运算</em></strong>)，当GPU完成计算后，内存却还在读取数据，造成GPU闲置而内存繁忙读（消费者早就消费完了，生产者还在缓慢生产）的现象，也就是内存墙问题。FlashAttention通过tiling和算子融合计算，将复杂操作放到SRAM中计算，并减少从HBM读取次数，加快了模型计算速度。而之前的工作虽然减少了Transformer的计算复杂度，却并没有减少模型计算时间。</li>
<li>节省了显存：FlashAttention通过引入全局统计量，避免实例化大注意力矩阵，减少了显存占用。</li>
<li>精确的注意力：FlashAttention从底层优化了Transformer的计算，但是任务指标上没有任何折损，与普通的Transformer结果是完全等价。</li>
</ol>
<h2 id="现代GPU内存分级"><a href="#现代GPU内存分级" class="headerlink" title="现代GPU内存分级"></a>现代GPU内存分级</h2><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.3j7dpa8fd1g0.webp"  alt="GPU"></p>
<p>flash attention的思路就是尽量地在SRAM中进行分块计算、算子融合，减少对HBM（即常说的显存）的读写，从加快模型计算，减轻内存墙问题。</p>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.4s3gxobimia0.webp"  alt="algorithm"></p>
<h2 id="tiling分块计算"><a href="#tiling分块计算" class="headerlink" title="tiling分块计算"></a>tiling分块计算</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ---------------------</span></span><br><span class="line"><span class="comment"># Tc: K和V的分块数</span></span><br><span class="line"><span class="comment"># Tr: Q的分块数量</span></span><br><span class="line"><span class="comment"># ---------------------</span></span><br><span class="line"><span class="keyword">for</span> <span class="number">1</span> &lt;= j &lt;= Tc:</span><br><span class="line">    <span class="keyword">for</span> <span class="number">1</span> &lt;= i &lt;= Tr:</span><br><span class="line">        do....</span><br></pre></td></tr></table></figure>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.3msrg3wcmqq0.webp"  alt="loop"></p>
<p>由于对$Q, K$矩阵进行了分块，就无法进行全局归一化。我们的最终目的是得到 $O$ ，作者这里根据公式推导，不断用当前最新的rowmax和rowsum去更新，直到遍历完最后一块，最终结果就和标准场景下的结果完全一致。</p>
<h2 id="计算量和显存分析"><a href="#计算量和显存分析" class="headerlink" title="计算量和显存分析"></a>计算量和显存分析</h2><ol>
<li>计算量：$O(N^2 d)$，跟标准attention计算一致</li>
</ol>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.6zirlknz7m80.webp"  alt="computation"></p>
<ol>
<li>显存：$m \in R^N, l \in R^N$</li>
</ol>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.2ysn4gh16f80.webp"  alt="gpu memory"></p>
<h2 id="IO复杂度分析"><a href="#IO复杂度分析" class="headerlink" title="IO复杂度分析"></a>IO复杂度分析</h2><ol>
<li>标准attention</li>
</ol>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.68jimy8oo9s0.webp"  alt="standard"></p>
<ol>
<li>flash-attention</li>
</ol>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.4qzzibsf9vy0.webp"  alt="flash"></p>
<p>可以看到，flash-attention通过算子融合、分块计算减少了IO，内存墙问题得以缓解。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/639228219" >FlashAttention:加速计算,节省显存, IO感知的精确注意力<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s?__biz=Mzg2NjcwNjcxNQ==&amp;mid=2247485453&amp;idx=1&amp;sn=beb642f06f3501bd235a8f42973e39fb&amp;chksm=ce47fc79f930756f991d93f69cad36409e3dcb58e517f41f583d29609b360e9b46f6777a42b4&amp;scene=21#wechat_redirect" >图解大模型计算加速系列：Flash Attention V1，从硬件到计算逻辑<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>LLM</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN,VAE,Diffusion对比</title>
    <url>/2023/06/24/GAN-VAE-Diffusion%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>对比下三种主流图片生成模型的优缺点：</p>
<span id="more"></span>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">模型</th>
<th style="text-align:left">优点</th>
<th style="text-align:left">缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">GAN</td>
<td style="text-align:left"><ul><li>生成的图片逼真</li></ul></td>
<td style="text-align:left"><ul><li>由于要同时训练判别器和生成器这两个网络，训练不稳定</li><li>GAN主要优化目标是使图片逼真，导致图片多样性不足</li><li>GAN的生成是隐式的，由网络完成，不遵循概率分布，可解释性不强</li></ul></td>
</tr>
<tr>
<td style="text-align:left">VAE</td>
<td style="text-align:left"><ul><li>学习的概率分布，可解释性强，图片多样性足</li></ul></td>
<td style="text-align:left"><ul><li>图片生成质量差</li></ul></td>
</tr>
<tr>
<td style="text-align:left">Diffusion</td>
<td style="text-align:left"><ul><li>生成的图片逼真</li><li>数学可解释性强</li></ul></td>
<td style="text-align:left"><ul><li>训练和推理成本高昂、速度慢，需要多步采样</li></ul></td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.bilibili.com/video/BV17r4y1u77B?t=1709.5" >DALL·E 2（内含扩散模型介绍）<i class="fas fa-external-link-alt"></i></a><ul>
<li>该视频讲到了GAN、VAE、DVAE、VQ-VAE、Diffusion、DDPM、Improved DDPM、classifier-(free) guidance</li>
</ul>
</li>
<li><a class="link"   href="https://www.51cto.com/article/709837.html" >从VAE到扩散模型：一文解读以文生图新范式<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT常见问答</title>
    <url>/2021/04/07/GBDT%E5%B8%B8%E8%A7%81%E9%97%AE%E7%AD%94/</url>
    <content><![CDATA[<p>关于GBDT的算法原理和实例讲解可见：</p>
<span id="more"></span>
<ul>
<li><a class="link"   href="https://blog.csdn.net/zpalyq110/article/details/79527653?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control" >GBDT算法原理以及实例讲解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/105497113" >GBDT总结<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<p>下面是涉及到的GBDT的面试问答：</p>
<ol>
<li><p>基本原理</p>
<p>通过多轮迭代，每轮迭代产生一个弱分类器（利用CART回归树构建），每个分类器在上一轮分类器的残差基础上进行训练。最后将这些弱分类器线性组合成一个强学习器。</p>
</li>
<li><p>GBDT如何做特征选择？</p>
<ol>
<li>遍历样本的特征，对于每个特征，遍历样本的切分点，选择最优的特征的最优切分点；</li>
<li>判断最优时使用平方误差。使用一个特征及其切分点可将样本分为两部分，每部分都计算一个标签的平均值，计算标签平均值与标签的平方误差之和，平方误差最小的特征–切分点组合即是最优的。</li>
</ol>
</li>
<li><p>GBDT如何构建特征？</p>
<p>gbdt 本身是不能产生特征的，但是我们可以利用gbdt去产生特征的组合。</p>
<img   src="/2021/04/07/GBDT%E5%B8%B8%E8%A7%81%E9%97%AE%E7%AD%94/combine_features.png"  class="">
<p>如上图所示，使用 GBDT 生成了两棵树，两颗树一共有五个叶子节点。我们将样本 X 输入到两颗树当中去，样本X 落在了第一棵树的第二个叶子节点，第二颗树的第一个叶子节点，于是我们便可以依次构建一个五纬的特征向量，每一个纬度代表了一个叶子节点，样本落在这个叶子节点上面的话那么值为1，没有落在该叶子节点的话，那么值为0。</p>
<p>​    于是对于该样本，我们可以得到一个向量 $[0,1,0,1,0]$ 作为该样本的组合特征，和原来的特征一起输入到逻辑回归当中进行训练。实验证明这样会得到比较显著的效果提升。</p>
</li>
<li><p>GBDT如何用于分类？</p>
<p>GBDT无论用于分类还是回归一直都是使用的CART 回归树。因为gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的。这里的残差就是当前模型的负梯度值 。这个要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的。残差相减是有意义的。如果选用的弱分类器是分类树，类别相减是没有意义的。上一轮输出的是样本 $x$ 属于A类，本一轮训练输出的是样本 $x$ 属于B类。 A 和 B 很多时候甚至都没有比较的意义，A类-B类是没有意义的。</p>
<p>​    具体到多分类这个任务上来，假设样本 $X$ 总共有 $K$ 类。来了一个样本  $x$ ，我们需要使用GBDT来判断 $x$ 属于样本的哪一类。</p>
<p>​    第一步 我们在训练的时候，是针对样本 $X$ 每个可能的类都训练一个分类回归树。举例说明，目前样本有三类，也就是 $K = 3$ 。样本 $x$ 属于第二类。那么针对该样本 $x$ 的分类结果，其实我们可以用一个 三维向量 $[0,1,0]$ 来表示。 $0$ 表示样本不属于该类， $1$ 表示样本属于该类。由于样本已经属于第二类了，所以第二类对应的向量维度为 $1$，其他位置为 $0$ 。</p>
<p>​    针对样本有三类的情况，我们实质上是在每轮的训练的时候是同时训练三颗树。第一颗树针对样本x的第一类，输入为 $(x,0)$ 。第二颗树输入针对样本x的第二类，输入为 $(x,1)$ 。第三颗树针对样本x的第三类，输入为 $(x,0)$</p>
<p>​    在这里每颗树的训练过程其实就是我们之前已经提到过的CATR TREE 的生成过程。在此处我们参照之前的生成树的程序 即可以就解出三颗树，以及三颗树对 $x$ 类别的预测值 $f_1(x),f_2(x),f_3(x)$ 。那么在此类训练中，我们仿照多分类的逻辑回归 ，使用softmax来产生概率，则属于类别 $1$ 的概率：</p>
<script type="math/tex; mode=display">
p_1 = \frac{e^{f_1(x)}}{\sum_{k=1}^3 e^{f_k(x)}}</script><p>​    并且我们我们可以针对类别1求出残差$y_1 = 0-p_1(x)$；类别2 求出残差$y_2 = 1-p_2(x)$；类别3 求出残差$y_3 = 0-p_3(x)$</p>
<p>​    然后开始第二轮训练 针对第一类 输入为$(x, y_1(x))$， 针对第二类输入为$(x, y_2(x))$，针对 第三类输入为$(x, y_3(x))$。继续训练出三颗树。一直迭代M轮，每轮构建3颗树。</p>
<p>​    所以当 $K=3$ ，我们其实应该有三个式子：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&F_{1 M}(x)=\sum_{m=1}^{M} \hat{C_{1 m}} I\left(x \epsilon R_{1 m}\right) \\
&F_{2 M}(x)=\sum_{m=1}^{M} C_{2 m} I\left(x \epsilon R_{2 m}\right) \\
&F_{3 M}(x)=\sum_{m=1}^{M} \hat{C_{3 m}} I\left(x \epsilon R_{3 m}\right)
\end{aligned}</script><p>​    当训练完毕以后，新来一个样本x，我们需要预测该样本的类别的时候，便可以有这三个式子产生三个值$F_{1M}(x),F_{2M}(x),F_{3M}(x)$。样本属于 某个类别c的概率为：</p>
<script type="math/tex; mode=display">
p_c = \frac{e^{F_{cM}(x)}}{\sum_{k=1}^3 e^{F_{kM}(x)}}</script></li>
</ol>
<ol>
<li><p>GBDT通过什么方式减少误差？</p>
<p>每棵树都是在拟合当前模型的预测值和真实值之间的误差，GBDT是通过不断迭代来使得误差减小的过程。</p>
</li>
<li><p>GBDT的效果相比于传统的LR，SVM效果为什么好一些 ？</p>
<p>GBDT基于树模型，继承了树模型的优点（对异常点鲁棒（使用了Huber损失函数和Quantile损失函数）、不相关的特征干扰性低（LR需要加正则）、可以很好地处理缺失值、受噪音的干扰小）</p>
<p>注：相对于RF，GBDT对异常值比较敏感，原因是当前的错误会延续给下一棵树。</p>
</li>
<li><p>RF和GBDT的异同</p>
<ul>
<li>相同点：都是由多棵树组成，最终结果由多棵树一起决定。</li>
<li>不同点：</li>
<li>集成学习：RF属于bagging思想，而GBDT是boosting思想</li>
<li>偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差</li>
<li>训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本</li>
<li>并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)</li>
<li>最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合</li>
<li>数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感</li>
<li>泛化能力：RF不易过拟合，而GBDT容易过拟合</li>
</ul>
</li>
<li><p>比较LR和GBDT，说说什么情景下GBDT不如LR？</p>
<ul>
<li>LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程。</li>
<li>GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合。</li>
</ul>
<p>当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。具体示例见：<a class="link"   href="https://zhuanlan.zhihu.com/p/156047718" >12. 比较LR和GBDT，说说什么情景下GBDT不如LR<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p>GBDT的参数有哪些，如何调参 ？</p>
<ul>
<li>学习率</li>
<li>最大弱学习器的个数（太小欠拟合，太大过拟合 ）</li>
<li>子采样（防止过拟合，太小欠拟合。GBDT中是不放回采样 ）</li>
<li>最大特征数</li>
<li>最大树深（太大过拟合）</li>
<li>内部节点再划分所需最小样本数（越大越防过拟合 ）</li>
<li>叶子节点最小的样本权重和（如果存在较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重。越大越防过拟合） </li>
<li>最大叶子节点数（太大过拟合）</li>
</ul>
</li>
<li><p>GBDT的优缺点？</p>
<p>优点：</p>
<ul>
<li><p>可以灵活处理各种类型的数据，包括连续值和离散值。 </p>
</li>
<li><p>在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。 </p>
</li>
<li><p>使用一些健壮的损失函数，对异常值的鲁棒性非常强</p>
<p>缺点：由于弱学习器之间存在依赖关系，难以并行训练数据。</p>
</li>
</ul>
</li>
<li><p>GBDT的“梯度提升”体现在哪个阶段？</p>
<p>在构建CART树时使用了损失函数的负梯度，而不是所谓的残差=真值-预测值；实际上是一种更宽广的概念，但是在平方损失的情况下，上面等式是成立的。另外使用损失函数的梯度可以保证损失函数最小值。所以GBDT的梯度提升体现在构建CART树的所需的负梯度阶段，其利用最速下降的近似方法。</p>
</li>
<li><p>怎样设置单棵树的停止生长条件？</p>
<ul>
<li>最大深度</li>
<li>最多叶子结点数</li>
<li>结点分裂时的最小样本数</li>
<li>loss满足约束条件</li>
</ul>
</li>
<li><p>GBDT哪些部分可以并行？</p>
<ul>
<li>计算并更新每个样本的负梯度；</li>
<li>分裂挑选最佳特征及其分割点时，对特征计算相应的误差及均值时；</li>
<li>最后预测过程中，每个样本将之前的所有树的结果累加的时候。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/132726342" >GBDT详解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/xwl198937/article/details/79749048" >GBDT几问<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/YangTinTin/article/details/104930839" >GBDT算法原理及常见面试问题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/25496196" >N问GBDT<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/ModifyRong/p/7744987.html" >机器学习算法GBDT的面试要点总结-上篇<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>GCN</title>
    <url>/2020/05/24/GCN/</url>
    <content><![CDATA[<p>最近两周断断续续学习了GCN有关的知识，在此主要记录一下GCN状态更新的公式推导。</p>
<span id="more"></span>
<h2 id="图卷积起缘"><a href="#图卷积起缘" class="headerlink" title="图卷积起缘"></a>图卷积起缘</h2><p>我们先探讨一个问题：<strong>为什么研究者们要设计图卷积操作，传统的卷积不能直接用在图上吗？</strong> 要理解这个问题，我们首先要理解能够应用传统卷积的<strong>图像(欧式空间)</strong>与<strong>图(非欧空间)</strong>的区别。如果把图像中的每个像素点视作一个结点，如下图左侧所示，一张图片就可以看作一个非常稠密的图；下图右侧则是一个普通的图。阴影部分代表<strong>卷积核</strong>，左侧是一个传统的卷积核，右侧则是一个图卷积核。</p>
<img   src="/2020/05/24/GCN/convolution.png"  class="">
<p>仔细观察上图，可以发现两点不同：</p>
<ol>
<li><p>在图像为代表的欧式空间中，结点的邻居数量都是固定的。比如说绿色结点的邻居始终是8个(边缘上的点可以做Padding填充)。但在图这种非欧空间中，结点有多少邻居并不固定。目前绿色结点的邻居结点有2个，但其他结点也会有5个邻居的情况。</p>
</li>
<li><p>欧式空间中的卷积操作实际上是用<strong>固定大小可学习的卷积核</strong>来抽取像素的特征，比如这里就是抽取绿色结点对应像素及其相邻像素点的特征。但是因为图里的邻居结点不固定，所以传统的卷积核不能直接用于抽取图上结点的特征。</p>
</li>
</ol>
<p>真正的难点聚焦于<strong>邻居结点数量不固定</strong>上。目前主流的研究从2条路来解决这件事：</p>
<ol>
<li><p>提出一种方式把非欧空间的图转换成欧式空间。</p>
</li>
<li><p>找出一种可处理变长邻居结点的卷积核在图上抽取特征。</p>
</li>
</ol>
<p>这两条实际上也是后续图卷积神经网络的设计原则，<strong>图卷积</strong>的本质是想找到<strong>适用于图的可学习卷积核</strong>。</p>
<h2 id="图卷积框架"><a href="#图卷积框架" class="headerlink" title="图卷积框架"></a>图卷积框架</h2><img   src="/2020/05/24/GCN/framework.png"  class="">
<p>如上图所示，输入的是整张图，前向传播过程如下：</p>
<ol>
<li><p>在<code>Convolution Layer 1</code>里，对每个结点的邻居都进行一次卷积操作，并用卷积的结果更新该结点；</p>
</li>
<li><p>经过激活函数如<code>ReLU</code>；</p>
</li>
<li><p>再过一层卷积层<code>Convolution Layer 2</code>与一层激活函数；</p>
</li>
<li><p>重复1~3步骤，直到层数达到预期深度。</p>
</li>
</ol>
<p>与GNN类似，图卷积神经网络也有一个局部输出函数，用于将结点的状态(包括隐藏状态与结点特征)转换成任务相关的标签，比如水军账号分类，这种任务称为<code>Node-Level</code>的任务；也有一些任务是对整张图进行分类的，比如化合物分类，这种任务称为<code>Graph-Level</code>的任务。<strong>卷积操作关心每个结点的隐藏状态如何更新</strong>，而对于<code>Graph-Level</code>的任务，它们会在卷积层后加入更多操作。</p>
<h3 id="卷积操作"><a href="#卷积操作" class="headerlink" title="卷积操作"></a>卷积操作</h3><h4 id="空域卷积-Spatial-Convolution"><a href="#空域卷积-Spatial-Convolution" class="headerlink" title="空域卷积(Spatial Convolution)"></a>空域卷积(Spatial Convolution)</h4><p>从设计理念上看，空域卷积与深度学习中的卷积的应用方式类似，其核心在于<strong>聚合邻居结点的信息</strong>。比如说，一种最简单的无参卷积方式可以是：将所有直连邻居结点的隐藏状态加和，来更新当前结点的隐藏状态。</p>
<img   src="/2020/05/24/GCN/spatial.png"  class="">
<p>常见的空域卷积网络有如下几种：</p>
<ul>
<li><p><a class="link"   href="https://arxiv.org/abs/1704.01212" >消息传递网络(Message Passing Neural Network)<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://arxiv.org/abs/1706.02216" >图采样与聚合(Graph Sample and Aggregate)<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://arxiv.org/pdf/1605.05273" >图结构序列化(PATCHY-SAN)<i class="fas fa-external-link-alt"></i></a></p>
</li>
</ul>
<h4 id="频域卷积-Spectral-Convolution"><a href="#频域卷积-Spectral-Convolution" class="headerlink" title="频域卷积(Spectral Convolution)"></a>频域卷积(Spectral Convolution)</h4><p>空域卷积非常直观地借鉴了图像里的卷积操作，但它缺乏一定的理论基础。而频域卷积则不同，它主要利用的是<strong>图傅里叶变换(Graph Fourier Transform)</strong>实现卷积。简单来讲，它利用图的<strong>拉普拉斯矩阵(Laplacian matrix)</strong>导出其频域上的的拉普拉斯算子，再类比频域上的欧式空间中的卷积，导出图卷积的公式。虽然公式的形式与空域卷积非常相似，但频域卷积的推导过程却有些艰深晦涩。因此本文主要来推导GCN的状态更新公式。</p>
<h5 id="傅里叶变换-Fourier-Transform"><a href="#傅里叶变换-Fourier-Transform" class="headerlink" title="傅里叶变换(Fourier Transform)"></a>傅里叶变换(Fourier Transform)</h5><p><strong>FT</strong>会将一个在空域(或时域)上定义的函数分解成频域上的若干频率成分。换句话说，傅里叶变换可以将一个函数从空域变到频域。用 $F$ 来表示傅里叶变换的话，这里有一个很重要的恒等式：</p>
<script type="math/tex; mode=display">
(f * g)(t) = F^{-1}[F[f(t)] \odot F[g(t)]]</script><p>$f$ 经过傅里叶变换后 $\hat{f}$ 如下所示：</p>
<script type="math/tex; mode=display">
\hat{f}(t)=\int f(x) e^{-2 \pi i x t} d x</script><p>其中$i = \sqrt{-1}$ 是虚数单位，$t$ 是任意实数。$e^{-2 \pi i x t}$ 是类比构造傅里叶变换的关键。它实际上是拉普拉斯算子$\Delta$的广义特征函数。</p>
<p>特征向量需要满足的定义式是：对于矩阵$A$，其特征向量满足的条件应是矩阵与特征向量$x$做乘法的结果，与特征向量乘标量λλ的结果一样，即满足如下等式：</p>
<script type="math/tex; mode=display">
Ax = \lambda x</script><p>$\Delta$ 作用在 $e^{-2 \pi i x t}$ 满足上述特征向量的定义：</p>
<script type="math/tex; mode=display">
\Delta e^{-2 \pi i x t}=\frac{\partial^{2}}{\partial t^{2}} e^{-2 \pi i x t}=-4 \pi^{2} x^{2} \exp ^{-2 \pi i x t}</script><p>$\Delta$ 即为 $-4 \pi^2 x^2$，注意这里 $t$ 是变量，$x$ 是常量。本质上，傅里叶变换是将$f(t)$映射到了以$\left\{e^{-2 \pi i x t}\right\}$为基向量的空间中。</p>
<h5 id="图上的傅里叶变换"><a href="#图上的傅里叶变换" class="headerlink" title="图上的傅里叶变换"></a>图上的傅里叶变换</h5><p>图上的拉普拉斯矩阵 $L$ 可以按照如下公式分解：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
    L= I_N - D^{- \frac {1} {2}}AD^{- \frac {1} {2}} = U \Lambda U^{T} \\
    U=\left(u_{1}, u_{2}, \cdots, u_{n}\right) \\
    \Lambda=\left[\begin{array}{ccc}
    \lambda_{1} & \dots & 0 \\
    \dots & \dots & \dots \\
    0 & \dots & \lambda_{n}
    \end{array}\right]
\end{array}</script><p>$I_N$ 为单位矩阵，$D$ 为度矩阵，$A$ 为领阶矩阵，$u$ 是特征向量，$\lambda$ 是特征值。</p>
<p>图上的卷积与传统的卷积非常相似，这里 $f$ 是特征函数，$g$ 是卷积核：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
(f * g)=F^{-1}[F[f] \odot F[g]] \\
\left(f *_{G} g\right)=U\left(U^{T} f \odot U^{T} g\right)=U\left(U^{T} g \odot U^{T} f\right)
\end{array}</script><p>如果把 $U^{T} g$ 整体看作可学习的卷积核，记作 $g_{\theta}$ ，最终图上的卷积公式即是：</p>
<script type="math/tex; mode=display">
o=\left(f *_{G} g\right)_{\theta}=U g_{\theta} U^{T} f</script><h5 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h5><p>频域卷积即是在 $g_\theta$ 上做文章。类比 $L$ 的分解公式，我们将 $g_\theta$ 看作是 $\Lambda$ 的函数 $g_\theta(\Lambda)$ 。由于特征分解计算量是非常巨大的，使用Chebyshev对 $g_\theta(\Lambda)$ 做近似估计：</p>
<script type="math/tex; mode=display">
g_{\theta^{\prime}}(\Lambda) \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{\Lambda})</script><p>$\tilde{\Lambda}=\frac{2}{\lambda_{\max }} \Lambda-I_{N}$ ，$\theta^{\prime} \in \mathbb{R}^{K}$ 是Chebyshev系数。切比雪夫多项式是递归定义的：$T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x), T_0(x) = 1, T_1(x) = x$ 。</p>
<p>基于上述假设，图上卷积公式近似为：</p>
<script type="math/tex; mode=display">
o \approx  \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{\Lambda}) f =  \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{L}) f</script><p>$\tilde{L}=\frac{2}{\lambda_{max}} L-I_{N}$，因为$\left(U \Lambda U^{\top}\right)^{k}=U \Lambda^{k} U^{\top}$ </p>
<p>取$K = 1$，将上述公式展开：</p>
<script type="math/tex; mode=display">
o \approx \theta_{0}^{\prime} f + \theta_{1}^{\prime}\left(L-I_{N}\right) f = \theta_{0}^{\prime} f - \theta_{1}^{\prime} D^{-\frac{1}{2}} A D^{-\frac{1}{2}} f</script><p>为了防止模型过拟合，我们还可以将参数进一步合并：</p>
<script type="math/tex; mode=display">
o \approx \theta\left(I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right) f</script><p>$\theta=\theta_{0}^{\prime}=-\theta_{1}^{\prime}$，$I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ 的特征值取值范围现在是 $[0, 2]$ 。为了防止误差在反向传播的过程中出现梯度弥散，将该式进行归一化：</p>
<script type="math/tex; mode=display">
I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} \\
\tilde{A}=A+I_{N} \\
\tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}</script><p>现在我们可以将 $l$ 层隐藏状态 $H^l \in \mathbb{R}^{N \times C}$ ，$C$ 是某个node的特征数量，那么最终的状态更新公式为：</p>
<script type="math/tex; mode=display">
H^{l+1} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^l \Theta</script><p>$\Theta \in \mathbb{R}^{C \times F}$ 是可训练的卷积核参数。</p>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><p><a class="link"   href="https://arxiv.org/abs/1809.05679" >Graph Convolutional Networks for Text Classification<i class="fas fa-external-link-alt"></i></a> 该篇论文使用GCN对圣经的章节进行分类。具体的实现思路见<a href="https://towardsdatascience.com/text-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f"><strong>Text-based Graph Convolutional Network — Bible Book Classification</strong></a>，代码见 <a class="link"   href="https://github.com/plkmo/Bible_Text_GCN" >https://github.com/plkmo/Bible_Text_GCN<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_2.html" >从图(Graph)到图卷积(Graph Convolution)：漫谈图神经网络模型 (二)
<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/54505069" >图卷积网络(GCN)新手村完全指南<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention Model</title>
    <url>/2019/07/25/Attention%20Model/</url>
    <content><![CDATA[<p>人脑的注意力模型，说到底是一种资源分配模型，在某个特定时刻，你的注意力总是集中在画面中的某个焦点部分，而对其它部分视而不见。Attention Model 被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中。</p>
<span id="more"></span>
<h2 id="RNN的局限"><a href="#RNN的局限" class="headerlink" title="RNN的局限"></a>RNN的局限</h2><p>机器翻译解决的是输入是一串在某种语言中的一句话，输出是目标语言相对应的话的问题，如将德语中的一段话翻译成合适的英语。之前的Neural Machine Translation(NMT)模型中，通常的配置是encoder-decoder结构，即encoder读取输入的句子将其转换为定长的一个向量，然后decoder再将这个向量翻译成对应的目标语言的文字。通常encoder及decoder均采用RNN结构如LSTM或GRU等。如下图所示，我们利用encoder RNN将输入语句信息总结到最后一个hidden vector中，并将其作为decoder初始的hidden vector，利用decoder解码成对应的其他语言中的文字。</p>
<img   src="/2019/07/25/Attention%20Model/1.jpg"  class="">
<p>但是这个结构有些问题，尤其是RNN机制实际中存在长程梯度消失或梯度爆炸的问题。对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息。所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。</p>
<p>为了解决这一由长序列到定长向量转化而造成的信息损失的瓶颈，Attention注意力机制被引入了。AM跟人类翻译文章时候的思路有些类似，即将注意力关注于我们翻译部分对应的上下文。在Attention模型中，当我们翻译当前词语时，我们会寻找源语句中相对应的几个词语，并结合之前的已经翻译的部分作出相应的 翻译。如下图所示，当我们翻译“knowledge”时，只需将注意力放在源句中“知识”的部分，当翻译“power”时，只需将注意力集中在”力量“。这样，当我们decoder预测目标翻译的时候就可以看到encoder的所有信息，而不仅局限于原来模型中定长的隐藏向量，并且不会丧失长程的信息。</p>
<img   src="/2019/07/25/Attention%20Model/2.gif"  class="">
<h2 id="Encoder-Decoder框架"><a href="#Encoder-Decoder框架" class="headerlink" title="Encoder-Decoder框架"></a>Encoder-Decoder框架</h2><p>目前绝大部分文献中出现的AM是附着在Encoder-Decoder框架下的，但AM可以看作一种通用的思想，本身并不依赖于Encoder-Decoder。Encoder-Decoder框架可以看作是一种文本处理领域的研究模式，下图是其抽象表示：</p>
<img   src="/2019/07/25/Attention%20Model/3.png"  class="">
<p>Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对 $ &lt; X, Y &gt; $（例如 $X$ 是一个问句， $Y$ 是答案； $X$ 是一个句子， $Y$ 是抽取的关系三元组； $X$ 是汉语句子， $Y$ 是汉语句子的英文翻译等等），我们的目标是给定输入句子 $X$ ，期待通过Encoder-Decoder框架来生成目标句子 $Y$ 。 $X$ 和 $Y$ 分别由各自的单词序列构成：</p>
<script type="math/tex; mode=display">
X = <x_1, x_2, \dots, x_m> \\
Y = <y_1, y_2, \dots, y_n></script><p>编码器Encoder对输入句子 $X$ 进行编码，将输入句子通过非线性变换转化为中间语义表示 $C$：</p>
<script type="math/tex; mode=display">
C = \mathcal{F}(x_1, x_2, \dots, x_m)</script><p>解码器Decoder的任务是根据句子 $X$ 的中间语义表示 $C$ 和之前已经生成的历史信息 $y_1, y_2, \dots. y_{i-1}$ 来生成i时刻要生成的单词 $y_i$ ：</p>
<script type="math/tex; mode=display">
y_i = \mathcal{G}(C, y_1, y_2, \dots, y_{i-1})</script><p>每个 $y_i$ 都依次产生，那么看起来就是整个系统根据输入句子 $X$ 生成了目标句子 $Y$。</p>
<h2 id="Attention-Model"><a href="#Attention-Model" class="headerlink" title="Attention Model"></a>Attention Model</h2><p>以上介绍的Encoder-Decoder模型可以看作是注意力不集中的分心模型。目标句子 $Y$ 中每个单词的生成过程如下：</p>
<script type="math/tex; mode=display">
\begin{align}
y_1 &= \mathcal{G}(C) \\
y_2 &= \mathcal{G}(C, y_1) \\
y_3 &= \mathcal{G}(C, y_1, y_2)
\end{align}</script><p>从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的语义编码 $C$ 都是一样的，没有任何区别。而语义编码 $C$ 是由句子 $X$ 的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词（$y_1, y_2, y_3$），其实句子$X$ 中任意单词对生成某个目标单词 $y_i$ 来说影响力都是相同的，没有任何区别（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。这就是为何说这个模型没有体现出注意力的缘由。</p>
<p>引入AM模型，以翻译一个英语句子举例：输入 $X$：Tom chase Jerry。 理想输出 $Y$：汤姆追逐杰瑞。</p>
<p>应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p>
<script type="math/tex; mode=display">
(Tom, 0.3) \quad (chase, 0.2) \quad (Jerry, 0.5)</script><p>每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词 $y_i$ 的时候，原先都是相同的中间语义表示 $C$ 会替换成根据当前生成单词而不断变化的 $C_i$ 。AM模型的关键就是这里，即由固定的中间语义表示 $C$ 换成了根据当前输出单词来调整成加入注意力模型的变化的 $C_i$ 。</p>
<img   src="/2019/07/25/Attention%20Model/4.png"  class="">
<p>即生成目标句子单词的过程成了下面的形式：</p>
<script type="math/tex; mode=display">
\begin{align}
y_1 &= \mathcal{G}(C_1) \\
y_2 &= \mathcal{G}(C_2, y_1) \\
y_3 &= \mathcal{G}(C_3, y_1, y_2)
\end{align}</script><p>而每个 $C_i$ 可能对应着不同的源语句子单词的注意力分配概率分布。比如对于上面的英汉翻译来说，其对应的信息可能如下：</p>
<script type="math/tex; mode=display">
\begin{align}
C_{汤姆} &= g(0.6*f2("Tom"), 0.2*f2(chase), 0.2*f2("Jerry")) \\
C_{追逐} &= g(0.2*f2("Tom"), 0.7*f2(chase), 0.1*f2("Jerry")) \\
C_{杰瑞} &= g(0.3*f2("Tom"), 0.2*f2(chase), 0.5*f2("Jerry")) \\
\end{align}</script><p>其中，$f2$ 函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个 $f2$ 函数的结果往往是某个时刻输入 $x_i$ 后隐层节点的状态值；$g$ 代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，$g$ 函数就是对构成元素加权求和，也就是常常在论文里看到的下列公式：</p>
<script type="math/tex; mode=display">
C_i = \sum_{j=1}^{T_x} \alpha_{ij}h_j</script><p>假设 $i$ 就是上面的“汤姆”，$T_x$ 为3，代表输入句子的长度，$h1=f2(“Tom”), h2=f2(“Chase”), h3=f2(“Jerry”)$，对应的注意力模型权值分别是$0.6,0.2,0.2$，所以 $g$ 函数就是个加权求和函数。$C_i$ 的形成过程如下图所示：</p>
<img   src="/2019/07/25/Attention%20Model/5.png"  class="">
<p>注意力分配概率分布 $(0.6, 0.2, 0.2)$ 怎么求出来的呢？为了便于说明，我们假设对图2的非AM模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，则图2的转换为下：</p>
<img   src="/2019/07/25/Attention%20Model/6.png"  class="">
<p>注意力分配概率分布值的通用计算过程：</p>
<img   src="/2019/07/25/Attention%20Model/7.png"  class="">
<p>对于采用RNN的Decoder来说，如果要生成 $Y_i$ 单词，在时刻 $i$ ，我们是可以知道在生成 $Y_i$ 之前的隐层节点i时刻的输出值 $H_i$ 的。而我们的目的是要计算生成 $Y_i$ 时的输入句子单词“Tom”、“Chase”、“Jerry”对 $Y_i$ 来说的注意力分配概率分布，那么可以用 $i$ 时刻的隐层节点状态 $H_i$ 去和输入句子中每个单词对应的RNN隐层节点状态 $h_j$ 进行对比，即通过函数 $F(h_j, H_i)$ 来获得目标单词 $Y_i$ 和每个输入单词对应的对齐可能性。这个 $F$ 函数在不同论文里可能会采取不同的方法，然后函数 $F$ 的输出经过Softmax进行归一化就得到了注意力分配概率分布。上图显示的是当输出单词为“汤姆”时刻对应的输入句子单词的对齐概率。绝大多数AM模型都是采取上述的计算框架来计算注意力分配概率分布信息<strong>，</strong>区别只是在 $F$ 的定义上可能有所不同。</p>
<h3 id="数学表达"><a href="#数学表达" class="headerlink" title="数学表达"></a>数学表达</h3><img   src="/2019/07/25/Attention%20Model/8.jpg"  class="">
<ol>
<li>我们首先利用RNN结构得到encoder的hidden state $(h_1, h_2, \dots, h_T)$</li>
<li>假设当前decoder的hidden state 是 $s_{t-1}$，我们可以计算每一个输入位置 $j$ 与当前输出位置的关联性，$e_{tj} = a(s_{t-1}, h_j)$ ，写成相应的向量形式即为 $\vec {e_t} = (a(s_{t-1}, h_1), a(s_{t-1}, h_2), \dots. a(s_{t-1}, h_T))$</li>
<li>对 $\vec {e_t}$ 进行softmax操作得到attention的分布。$\vec {\alpha_t} = softmax(\vec {e_t})$，展开形式为 $\alpha_{tj} = \frac {e^{e_{tj}}} {\sum_{k=1}^T e^{e_{tk}}}$</li>
<li>利用 $\vec {\alpha_t}$ 进行加权求和得到相应的context vector $\vec {c_t} = \sum_{j=1}^T \alpha_{tj}hj$</li>
<li>由此，我们可以计算decoder的下一个hidden state $s_t = f(s_{t-1}, y_{t-1}, c_t)$ 以及该位置的输出 $p(y_t | y_1, \dots, y_{t-1}, \vec x) = g(y_{i-1}, s_t, c_t)$</li>
</ol>
<h3 id="物理含义"><a href="#物理含义" class="headerlink" title="物理含义"></a>物理含义</h3><p>上述内容就是论文里面常常提到的Soft Attention Model（任何单词都会给一个权值，没有筛选条件）的基本思想。那么怎么理解AM模型的物理含义呢？一般文献里会把AM模型看作是单词对齐模型，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。在其他应用里面把AM模型理解成输入句子和目标句子单词之间的对齐概率也是很顺畅的想法。</p>
<h3 id="本质思想"><a href="#本质思想" class="headerlink" title="本质思想"></a>本质思想</h3><p>如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p>
<img   src="/2019/07/25/Attention%20Model/9.png"  class="">
<p>将Source中的构成元素想象成是由一系列的<Key($h_i$),Value($f2$)>数据对构成，此时给定Target中的某个元素Query($H_i$)，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p>
<script type="math/tex; mode=display">
Attention(Query, Source) = \sum_{i=1}^{L_x} Similarity(Query, Key_i) * Value_i</script><p>$Lx$ 表示 Source 的长度，如一句话中单词的个数。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西($h_i = f2$)，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p>
<p>从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p>
<p>从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p>
<p>AM的具体计算过程如下：</p>
<img   src="/2019/07/25/Attention%20Model/10.png"  class="">
<p>Query和Key的相似性计算有如下常用几种方法：</p>
<ul>
<li>点积：$Query \ast Key_i$</li>
<li>cosine：$\frac { Query \ast key_i } { | Query | \ast | Key_i | }$</li>
<li>多层感知器：$MLP(Query, Key_i)$</li>
<li>欧式距离：$\sum_{j=1}^n (Query_j - Key_{ij})^2$</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/47063917" >Attention机制详解（一）——Seq2Seq中的Attention<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/malefactor/article/details/78767781" >深度学习中的注意力机制(2017版)<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/guoyaohua/p/9429924.html?tdsourcetag=s_pcqq_aiomsg" >【NLP】Attention Model（注意力模型）学习总结<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Encoder-Decoder</tag>
      </tags>
  </entry>
  <entry>
    <title>GEMM优化</title>
    <url>/2024/04/22/GEMM%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<p>以矩阵相乘的优化为例：</p>
<span id="more"></span>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">sgemm_V1</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="type">float</span> * __restrict__ a, <span class="type">float</span> * __restrict__ b, <span class="type">float</span> * __restrict__ c,</span></span><br><span class="line"><span class="params">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    在我们的例子里，</span></span><br><span class="line"><span class="comment">    dim3 blockDim(BN/TN, BM/TM) = (16, 16)，即一个block中有256个thread</span></span><br><span class="line"><span class="comment">    dim3 gridDim((N + BN - 1) / BN, (M + BM - 1) / BM) = (4，4)，即一共16个block</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BM = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BN = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BK = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> TM = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> TN = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tx = threadIdx.x; <span class="comment">// thread在对应block内的行id</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ty = threadIdx.y; <span class="comment">// thread在对应block内的列id</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = ty * blockDim.x + tx; <span class="comment">// thread在对应block中的全局id（从左到右，从上到下，从0开始逐一标）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    在SMEM上对A和B，分别开辟大小为(BM, BK), (BK, BN)的空间</span></span><br><span class="line"><span class="comment">    对应到图例中，s_a为高亮红，s_b为高亮黄</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    __shared__ <span class="type">float</span> s_a[BM][BK];</span><br><span class="line">    __shared__ <span class="type">float</span> s_b[BK][BN];</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    初始化当前thread所维护的C矩阵（确定长度的数组，应该是定义在寄存器上的）</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="type">float</span> r_c[TM][TN] = &#123;<span class="number">0.0</span>&#125;;</span><br><span class="line">   </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    例：</span></span><br><span class="line"><span class="comment">    对于tid = 0的thread，以下四个值分别为((0, 0), (0, 0)),</span></span><br><span class="line"><span class="comment">    意味着它负责把s_a(0,0)开始的连续4个数，s_b(0,0)开始的连续4个数，从global memory加载到SMEM</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    对于tid = 1的thread，以下四个值分别为((0, 4), (0, 4)),</span></span><br><span class="line"><span class="comment">    意味着它负责把s_a(0,4)开始的连续4个数，s_b(0,4)开始的连续4个数，从global memory加载到SMEM</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    对于tid = 2的thread，以下四个值分别为((1, 0), (0, 8))</span></span><br><span class="line"><span class="comment">    此时s_a第一行的8个数已经被前面的thread取完了，所以现在从s_a第二行开始取，s_b第一行没取完，继续进行</span></span><br><span class="line"><span class="comment">   </span></span><br><span class="line"><span class="comment">    对于tid = 18的thread，以下四个值分别为((9, 0), (0, 72))，含义同上</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 当前thread负责把A中的相关数据从global memory加载到SMEM，</span></span><br><span class="line">    <span class="comment">// 这里在计算该thread负责加载的第一个数在s_a中的row</span></span><br><span class="line">    <span class="type">int</span> load_a_smem_m = tid &gt;&gt; <span class="number">1</span>;  <span class="comment">// tid/2, row of s_a</span></span><br><span class="line">    <span class="comment">// 当前thread负责加载的第一个数在s_a中的col</span></span><br><span class="line">    <span class="type">int</span> load_a_smem_k = (tid &amp; <span class="number">1</span>) &lt;&lt; <span class="number">2</span>;  <span class="comment">// (tid % 2 == 0) ? 0 : 4, col of s_a</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 当前thread负责把B中的相关数据从global memory加载到SMEM，</span></span><br><span class="line">    <span class="comment">// 这里在计算该thread负责加载的第一个数在s_b中的row</span></span><br><span class="line">    <span class="type">int</span> load_b_smem_k = tid &gt;&gt; <span class="number">5</span>;   <span class="comment">// tid/32, row of s_b</span></span><br><span class="line">    <span class="comment">// 当前thread负责加载的第一个数在s_b中的col</span></span><br><span class="line">    <span class="type">int</span> load_b_smem_n = (tid &amp; <span class="number">31</span>) &lt;&lt; <span class="number">2</span>;  <span class="comment">// (tid % 32) * 4, col of s_b</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    例：</span></span><br><span class="line"><span class="comment">    对于tid = 0的thread，以下两个值为(256, 128)，</span></span><br><span class="line"><span class="comment">    表示该thread从s_a上取的第一个数，其位置在A（位于global memory）上的row 256</span></span><br><span class="line"><span class="comment">    该thread从s_b上取的第一个数，其位置在B（位于global memory）上的col 128</span></span><br><span class="line"><span class="comment">   </span></span><br><span class="line"><span class="comment">    对于tid = 18的thread，以下两个值为(265, 200)，道理同上</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="type">int</span> load_a_gmem_m = by * BM + load_a_smem_m;  <span class="comment">// global row of a</span></span><br><span class="line">    <span class="type">int</span> load_b_gmem_n = bx * BN + load_b_smem_n;  <span class="comment">// global col of b</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    对每个block，它都要经历K/Bk = 128/8 = 16次循环，每次循环计算一块s_a * s_b的结果</span></span><br><span class="line"><span class="comment">    这也意味着，对每个block内的每个thread，它的外循环也是16次</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> bk = <span class="number">0</span>; bk &lt; (K + BK - <span class="number">1</span>) / BK; bk++) &#123;    </span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        1. 在block的单次循环中，需要把对应的s_a（高亮红）和s_b(高亮黄)从global memory</span></span><br><span class="line"><span class="comment">        加载到SMEM上，因此每个thread负责加载一部分s_a, s_b的数据，最后的__syncthreads()</span></span><br><span class="line"><span class="comment">        是保证thread们在正式计算前，都干完了自己加载的活，即完整的s_a, s_b已被加载到SMEM上</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="comment">// 在这次循环中，当前thread从s_a上取的第一个数，其位置在A（位于global memory）上的col，与load_a_gmem_m对应</span></span><br><span class="line">        <span class="type">int</span> load_a_gmem_k = bk * BK + load_a_smem_k;   <span class="comment">// global col of a</span></span><br><span class="line">        <span class="comment">// 在这次循环中，当前thread从s_a上取的第一个数，在A中的地址，即A[load_a_gmem_m][load_a_gmem_k]</span></span><br><span class="line">        <span class="type">int</span> load_a_gmem_addr = OFFSET(load_a_gmem_m, load_a_gmem_k, K);</span><br><span class="line">        <span class="comment">// 从这个地址开始，取出连续的4个数，将其从A所在的global memory上，加载到s_a上</span></span><br><span class="line">        <span class="comment">// 注：采用FLOAT4的好处是便于连续访存。如果存储的4个数在地址上不连续，你就发4条指令。float4的数据类型就只要发1条指令，一起取出</span></span><br><span class="line">        FLOAT4(s_a[load_a_smem_m][load_a_smem_k]) = FLOAT4(a[load_a_gmem_addr]);</span><br><span class="line">        <span class="comment">// 在这次循环中，当前thread从s_b上取的第一个数，其位置在B（位于global memory）上的row，与load_b_gmem_n对应</span></span><br><span class="line">        <span class="type">int</span> load_b_gmem_k = bk * BK + load_b_smem_k;   <span class="comment">// global row of b</span></span><br><span class="line">        <span class="comment">// 在这次循环中，当前thread从s_b上取的第一个数，在B中的地址，即B[load_b_gmem_k][load_b_gmem_n]</span></span><br><span class="line">        <span class="type">int</span> load_b_gmem_addr = OFFSET(load_b_gmem_k, load_b_gmem_n, N);</span><br><span class="line">        <span class="comment">// 同理将相关的数据从global memory加载到SMEM上</span></span><br><span class="line">        FLOAT4(s_b[load_b_smem_k][load_b_smem_n]) = FLOAT4(b[load_b_gmem_addr]);</span><br><span class="line">        <span class="comment">// 在所有thread间做一次同步，保证在下面的计算开始时，s_a, s_b相关的数据已经全部从global memory搬运到SMEM上了</span></span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        2. 在block的单次循环中，每个thread采用split-by-k的方式，</span></span><br><span class="line"><span class="comment">        逐步累加计算当前thread所维护的(TM, TN)块的结果</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="comment">// 遍历每一个(渐变红，渐变黄)对，可参见图例</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; BK; k++) &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> m = <span class="number">0</span>; m &lt; TM; m++) &#123;</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> n = <span class="number">0</span>; n &lt; TN; n++) &#123;</span><br><span class="line">                    <span class="type">int</span> comp_a_smem_m = ty * TM + m;</span><br><span class="line">                    <span class="type">int</span> comp_b_smem_n = tx * TN + n;</span><br><span class="line">                    <span class="comment">// 每次从SMEM上，各加载渐变红和渐变黄上的1个元素，到register，然后再计算</span></span><br><span class="line">                    r_c[m][n] += s_a[comp_a_smem_m][k] * s_b[k][comp_b_smem_n];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 做一次同步，保证所有的thread都计算完当前所维护的（TM, TN）块</span></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    3. </span></span><br><span class="line"><span class="comment">    此时，所有的block已做完循环，</span></span><br><span class="line"><span class="comment">    我们把当前thread计算出的结果（存放在r_c中，尺寸为(Tm, Tn)）写回</span></span><br><span class="line"><span class="comment">    global memory上的C矩阵对应位置中</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="comment">// 遍历当前thread结果矩阵的每一行</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM; i++) &#123;</span><br><span class="line">        <span class="comment">// 计算该thread结果矩阵的这一行，在C矩阵上对应的全局row</span></span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + ty * TM + i;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="comment">// 以4个数为1组，遍历该thread结果矩阵的每一列</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; TN; j += <span class="number">4</span>) &#123;</span><br><span class="line">            <span class="comment">// 计算这4个数中的第一个数在C矩阵上对应的全局col</span></span><br><span class="line">            <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN + j;</span><br><span class="line">            <span class="comment">// 将这4个数以FLOAT4写回global memory</span></span><br><span class="line">            <span class="type">int</span> store_c_gmem_addr = OFFSET(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">            FLOAT4(c[store_c_gmem_addr]) = FLOAT4(r_c[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/YLrsu1KAhzG8gFQ2L-TaMA" >从啥也不会到Cuda GEMM优化<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/584236348" >传统 CUDA GEMM 不完全指北<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/410278370" >CUDA 矩阵乘法终极优化指南<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>GLM</title>
    <url>/2019/05/29/GLM/</url>
    <content><![CDATA[<h2 id="为什么要引入GLM？"><a href="#为什么要引入GLM？" class="headerlink" title="为什么要引入GLM？"></a>为什么要引入GLM？</h2><p>我们知道了”回归“一般是用于预测样本的值，这个值通常是连续的。但是受限于其连续的特性，一般用它来进行分类的效果往往很不理想。为了保留线性回归”简单效果又不错“的特点，又想让它能够进行分类，因此需要对预测值再做一次处理。这个多出来的处理过程，就是GLM所做的最主要的事。而处理过程的这个函数，我们把它叫做连接函数。</p>
<span id="more"></span>
<p>如下图是一个广义模型的流程：</p>
<img   src="/2019/05/29/GLM/1.jpg"  class="">
<p>图中，当一个处理样本的回归模型是线性模型，且连接函数满足一定特性（特性下面说明）时，我们把模型叫做广义线性模型。因为广义模型的最后输出可以为离散，也可以为连续，因此，用广义模型进行分类、回归都是可以的。</p>
<p>但是为什么线性回归是广义线性模型的子类呢，因为连接函数是 $f(x)=x$ 本身的时候，也就是不做任何处理时，它其实就是一个线性回归。</p>
<p>所以模型的问题就转化成获得合适的连接函数？以及有了连接函数，怎么求其预测函数 $h_\theta(x)$ ？</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>刚才说了，只有连接函数满足一定特性才属于广义线性模型。特性是什么呢？先简单描述下背景。</p>
<p>在广义线性模型中，为了提高可操作性，因此限定了概率分布必须满足指数族分布：</p>
<script type="math/tex; mode=display">
p(y;\eta) = b(y)e^{ {\eta^T}{T(y)-a(\eta)} }</script><blockquote>
<ul>
<li>$\eta$ 称为这个分布的 <strong>自然参数(natural parameter)</strong> 或者 <strong>规范参数(canonical parameter)</strong>。$\eta=\theta^TX$ ，即自然参数=参数与自变量X的线性组合。</li>
<li>$T(y)$ 称为 <strong>充分统计量(sufficient statistic)</strong>。</li>
<li>$a(\eta)$ 称为 <strong>对数分割函数(log partition function)</strong>，$e^{-a(\eta)}$ 是分布 $p(y;\eta)$ 的归一化常数，用来确保该分布对 $y$ 的积分为 1。</li>
<li>当 $T,a,b$ 固定之后，也就确定了这样一个以 $\eta$ 为参数的分布族。</li>
</ul>
</blockquote>
<h3 id="广义线性模型的三个假设"><a href="#广义线性模型的三个假设" class="headerlink" title="广义线性模型的三个假设"></a>广义线性模型的三个假设</h3><ul>
<li>$(y|x;\theta)\sim Exponential Family(\eta)$：给定样本 $x$ 和 参数 $\theta$ ，样本分类 $y$ 服从指数分布。</li>
<li>给定一个 $x$ ，我们需要的目标函数为 $h_\theta(x) = E[T(y) | x]$ 。</li>
<li>$\eta = (\vec \theta)^T \vec X$ ，即自然参数 $\eta$ 和 输入 $\vec X$ 满足线性关系。</li>
</ul>
<h2 id="连接函数的获取"><a href="#连接函数的获取" class="headerlink" title="连接函数的获取"></a>连接函数的获取</h2><p>从上图可以看到 $\eta$ 为函数的输入，而 $h_\theta(x)$ 为函数的输出，所以有公式：</p>
<script type="math/tex; mode=display">
h_\theta(x) = f(\eta)</script><p>但是我们会把 $f$ 的逆 $f^{-1}$ 称为<strong>连接函数</strong> ， 也即以 $h_\theta(x)$ 为自变量，$\eta$ 为因变量的函数为连接函数：</p>
<script type="math/tex; mode=display">
\eta = f^{-1}(h_\theta(x))</script><p>所以求连接函数的步骤也就变成：</p>
<ol>
<li>将 $\vec Y$ 、$\vec X$ 所满足的分布转换成指数分布形式。</li>
<li>在指数分布形式中获得 $T(y)$ 的函数形式和 $\eta$  的值。</li>
<li>算出 $E[T(y)|x]$ 和 $\eta$ 的关系，并把 $(\vec \theta)^T$ 代入到$\eta$ 中，获得连接函数。</li>
</ol>
<h2 id="常见连接函数求解及对应回归"><a href="#常见连接函数求解及对应回归" class="headerlink" title="常见连接函数求解及对应回归"></a>常见连接函数求解及对应回归</h2><h3 id="伯努利分布-—-gt-Logistic-回归"><a href="#伯努利分布-—-gt-Logistic-回归" class="headerlink" title="伯努利分布 —-&gt; Logistic 回归"></a>伯努利分布 —-&gt; Logistic 回归</h3><p>伯努利分布只有0、1两种情况，因此它的概率分布可以写成：</p>
<script type="math/tex; mode=display">
p(y;\phi) = \phi^y(1-\phi)^{1-y} \qquad y=[0,1] \qquad \phi: 实验为1发生的概率</script><p>下面是逻辑回归的推导过程：</p>
<img   src="/2019/05/29/GLM/2.png"  class="">
<h3 id="多项分布-—-gt-softmax-回归"><a href="#多项分布-—-gt-softmax-回归" class="headerlink" title="多项分布 —-&gt; softmax 回归"></a>多项分布 —-&gt; softmax 回归</h3><p>前面说过的分类问题都是处理那些分两类的问题。比如区分猫或者狗的问题，就是一类是或者否的问题。但是现实生活中还有更加多的多类问题。比如猫分类，有田园猫，布偶猫，暹罗猫各种猫，这里就不能够用两分类来做了。 </p>
<p>这里先设问题需要区分 $k$ 类，即 $y \in \lbrace1, 2, 3, …, k\rbrace$ 。此处无疑使用多项式来建模。多项式分布是二项分布的一个扩展，其取值可以从$\lbrace1, 2, 3, …, k\rbrace$ 中取，可以简单建模如下：</p>
<script type="math/tex; mode=display">
\left[
\begin{matrix}
 1      & 0      & \cdots & 0      \\
 0      & 1      & \cdots & 0      \\
 \vdots & \vdots & \ddots & \vdots \\
 0      & 0      & \cdots & 1      \\
\end{matrix}
\right]</script><p>例如当$y=2$ 时，第二个数为1，其它数为0，因此它的概率分布如下：</p>
<img   src="/2019/05/29/GLM/3.png"  class="">
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/xierhacker/article/details/53364408" >机器学习笔记五：广义线性模型（GLM）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/BYRans/p/4905420.html" >Softmax回归（Softmax Regression）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://cloud.tencent.com/developer/article/1005793" >机器学习之回归（二）：广义线性模型（GLM）<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Neural Networks</tag>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT的实现细节</title>
    <url>/2024/03/18/GPT%E7%9A%84%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/</url>
    <content><![CDATA[<p>关于GPT的代码细节，这里梳理了一下：</p>
<span id="more"></span>
<h2 id="数据集构造"><a href="#数据集构造" class="headerlink" title="数据集构造"></a>数据集构造</h2><p>根据Transformer Decoder-Only特点，直接将多轮对话拼成一条session样本，过一次前向传播，得到多条回复的loss：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.8hg8njrad1.png"  alt="sample"></p>
<p>而以往的方法是将多轮对话拆成多条样本，存在大量重复计算问题，效率低下。且该方法对于靠前轮次对话影响权重更大，不符合对话常识，靠后轮次应该权重更大，证明见：<a class="link"   href="https://zhuanlan.zhihu.com/p/641562439" >大模型微调样本构造trick<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h2><p>在<a class="link"   href="https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L289" >karpathy/minGPT<i class="fas fa-external-link-alt"></i></a>项目中，是直接粗暴地生成固定长度的文本。这样做的问题就是生成的文本无法判断何处截断。</p>
<p>在构造模型输入的时候，我们就加入了 <code>&lt;EOS&gt;</code> token，来标记文本的结束。那么在推理阶段，<a class="link"   href="https://github.com/TransformersWsz/GPT2-NewsTitle/blob/1e04fc50429ac767aa81b62865d41c506191a478/generate_title.py#L142" >如果碰到该token，则结束生成<i class="fas fa-external-link-alt"></i></a>：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> token == <span class="string">&quot;&lt;EOS&gt;&quot;</span>:</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel" >GPT2LMHeadModel<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/karpathy/minGPT/blob/master/mingpt/model.py" >mingpt<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.dev/TransformersWsz/GPT2-NewsTitle" >GPT2-NewsTitle<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.bilibili.com/video/BV1Gt421L7dt/?spm_id_from=333.1007.tianma.23-1-87.click&amp;vd_source=3f2411263f367ccf993c28b58688c0e7" >全栈大模型微调框架LLaMA Factory：从预训练到RLHF的高效实现<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU利用率</title>
    <url>/2024/05/19/GPU%E5%88%A9%E7%94%A8%E7%8E%87/</url>
    <content><![CDATA[<p>英伟达官方的GPU利用率的定义如下：</p>
<script type="math/tex; mode=display">
GPU Util rate = \frac{number \  of \ active \ SM}{number \ of \ total \ SM} \times 100\%</script><span id="more"></span>
<h2 id="nvidia-smi-中的GPU利用率"><a href="#nvidia-smi-中的GPU利用率" class="headerlink" title="nvidia-smi 中的GPU利用率"></a><code>nvidia-smi</code> 中的GPU利用率</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">simple_kernel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    simple_kernel&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码片段将在单个流多处理器(SM)上启动指定的内核(线程)。根据常规理解，GPU的“利用率”应该计算为$\frac{1}{num_sm}$。但 <code>nvidia-smi</code> 却显示GPU利用率为100%：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.45hhpwcfe7.webp"  alt="nvidia-smi"></p>
<p>根据NVML的定义，“利用率”是指在过去的样本期间内发生某些活动的时间百分比。具体来说：</p>
<ul>
<li>GPU利用率：这表示一个或多个内核在GPU上执行的时间百分比</li>
</ul>
<p>NVML的定义完全不符合我们日常开发中的“利用率”理解。它仅测量给定采样周期内设备使用的时间部分，而不考虑该时间内使用的流式多处理器(SM)的数量。</p>
<p>通常，我们将“利用率”视为正在使用的GPU处理器的部分，用专业术语说就是“饱和度”：</p>
<blockquote>
<p>资源具有无法服务的额外工作的程度</p>
</blockquote>
<p>我们可以用 <code>dcgm-exporter</code> 来收集GPU的饱和度信息，这里引用<a class="link"   href="https://mp.weixin.qq.com/s/4_An51JuRGWTU0dLgZYHpQ" >Tim在路上<i class="fas fa-external-link-alt"></i></a>的图片：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.4g4bj2yxql.webp"  alt="gpu-util"></p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.7w6nb6993e.webp"  alt="sm"></p>
<p>上图可以看到当GPU利用率为100%时，SM占用率非常低(&lt;20%)，浮点运算(FP32/FP16/TensorCore)也保持在非常低的百分比，这表明GPU还没有饱和，而这才是真实的GPU利用现状。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/4_An51JuRGWTU0dLgZYHpQ" >理解NVIDIA GPU 性能：利用率与饱和度<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>GloVe</title>
    <url>/2021/07/21/GloVe/</url>
    <content><![CDATA[<p>GloVe的全称叫Global Vectors for Word Representation，它是一个基于全局词频统计（count-based &amp; overall statistics）的词表征工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性、类比性等。</p>
<span id="more"></span>
<h2 id="构建共现矩阵"><a href="#构建共现矩阵" class="headerlink" title="构建共现矩阵"></a>构建共现矩阵</h2><p>设共现矩阵为 $X$ ，其元素为 $X_{i,j}$ 。</p>
<p>$X_{i,j}$ 的意义为：在整个语料库中，单词 $i$ 和单词 $j$ 共同出现在一个窗口中的次数。</p>
<p>具体示例见：<a class="link"   href="https://blog.csdn.net/coderTC/article/details/73864097" >https://blog.csdn.net/coderTC/article/details/73864097<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="词向量与共现矩阵的近似关系"><a href="#词向量与共现矩阵的近似关系" class="headerlink" title="词向量与共现矩阵的近似关系"></a>词向量与共现矩阵的近似关系</h2><p>构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系：</p>
<script type="math/tex; mode=display">
\log X_{i k}=w_{i}^{T} w_{k}+b_{i}+b_{k}</script><p>具体公式推导见：<a class="link"   href="https://zhuanlan.zhihu.com/p/42073620" >https://zhuanlan.zhihu.com/p/42073620<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="构造损失函数"><a href="#构造损失函数" class="headerlink" title="构造损失函数"></a>构造损失函数</h2><script type="math/tex; mode=display">
J=\sum_{i k} f\left(X_{i k}\right)\left(w_{i}^{T} w_{k}+b_{i}+b_{k}-\log X_{i k}\right)^{2}</script><p>$f(x)$ 为权重函数，满足如下三个特点：</p>
<ul>
<li>$f(0)=0$ ，即两个单词没有在同一个滑动窗口中出现过，那么它们不应该参与到loss的计算中；</li>
<li>$f(x)$ 为非递减函数，即这些单词的权重要大于那些很少在一起出现的单词；</li>
<li>$f(x)$ 不能过大，达到一定程度后不再增加。如果汉语中“这”出现很多次，但重要程度很小；</li>
</ul>
<p>综上 $f(x)$ 定义如下：</p>
<script type="math/tex; mode=display">
f(x)=\left\{\begin{array}{c}
\left(\frac{x}{x_{\max }}\right)^{\alpha}, \text { if } x<x_{\max } \\
1, \text { otherwise }
\end{array}\right.</script><h2 id="GloVe与LSA、Word2Vec的区别"><a href="#GloVe与LSA、Word2Vec的区别" class="headerlink" title="GloVe与LSA、Word2Vec的区别"></a>GloVe与LSA、Word2Vec的区别</h2><ul>
<li>LSA是基于奇异值分解（SVD）的算法，该方法对term-document矩阵（矩阵的每个元素为tf-idf）进行奇异值分解，从而得到term的向量表示和document的向量表示。此处使用的tf-idf主要还是term的全局统计特征。而我们SVD的复杂度是很高的，所以它的计算代价比较大。还有一点是它对所有单词的统计权重都是一致的。</li>
<li>word2vec最大的缺点则是没有充分利用所有的语料，只利用了局部的上下文特征。</li>
<li>GloVe模型就是将这两种特征合并到一起的，即使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/coderTC/article/details/73864097" >理解GloVe模型（Global vectors for word representation）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/42073620" >（十五）通俗易懂理解——Glove算法原理<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/NLP-LOVE/ML-NLP" >ML-NLP<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>Word2Vec</tag>
      </tags>
  </entry>
  <entry>
    <title>Gumbel Softmax</title>
    <url>/2024/04/07/Gumbel-Softmax/</url>
    <content><![CDATA[<p>Argmax是不可求导的，Gumbel Softmax允许模型能从网络层的离散分布（比如类别分布categorical distribution）中稀疏采样的这个过程变得可微，从而允许反向传播时可以用梯度更新模型参数。</p>
<span id="more"></span>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><ol>
<li>对于某个网络层输出的 $\mathrm{n}$ 维向量 $v=\left[v_1, v_2, \ldots, v_n\right]$，生成 $\mathrm{n}$ 个服从均匀分布 $\mathrm{U}(0,1)$ 的独立样本 $\epsilon_1, \ldots, \epsilon_n$</li>
<li>通过 $G_i=-\log \left(-\log \left(\epsilon_i\right)\right)$ 计算得到 $G_i$</li>
<li>对应相加得到新的值向量 $v^{\prime}=\left[v_1+G_1, v_2+G_2, \ldots, v_n+G_n\right]$</li>
<li>通过softmax函数计算各个类别的概率大小，其中 $\tau$ 是温度参数：<script type="math/tex; mode=display">
p_\tau\left(v_i^{\prime}\right)=\frac{e^{v_i^{\prime} / r}}{\sum_{j=1}^n e^{v_j^{\prime} / \tau}}</script></li>
</ol>
<h2 id="Gumbel-Max-Trick"><a href="#Gumbel-Max-Trick" class="headerlink" title="Gumbel-Max Trick"></a>Gumbel-Max Trick</h2><p>Gumbel分布是专门用来建模从其他分布（比如高斯分布）采样出来的极值形成的分布，而我们这里“使用argmax挑出概率最大的那个类别索引”就属于取极值的操作，所以它属于Gumbel分布。</p>
<div class="keep-note danger"><p>注意，极值的分布也是有规律的。</p>
</div>
<p>Gumbel-Max Trick的采样思想：先用均匀分布采样出一个随机值，然后把这个值带入到gumbel分布的CDF函数的逆函数得到采样值，即我们最终想要的类别索引。公示如下：</p>
<script type="math/tex; mode=display">
z=\operatorname{argmax}_i\left(\log \left(p_i\right)+g_i\right) \\
g_i=-\log \left(-\log \left(u_i\right)\right), u_i \sim U(0,1)</script><p>上式使用了重参数技巧把采样过程分成了确定性的部分和随机性的部分，我们会计算所有类别的log分布概率（确定性的部分），然后加上一些噪音（随机性的部分），这里噪音是标准gumbel分布。在我们把采样过程的确定性部分和随机性部分结合起来之后，我们在此基础上再用一个argmax来找到具有最大概率的类别。</p>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>使用softmax替换不可导的argmax，用温度系数 $\tau$ 来近似argmax：</p>
<script type="math/tex; mode=display">
p_i^{\prime}=\frac{\exp \left(\frac{g_i+\log p_i}{\tau}\right)}{\sum_j \exp \left(\frac{g_j+\log p_j}{\tau}\right)}</script><p>$\tau$ 越大，越接近argmax。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://openreview.net/pdf?id=rkE3y85ee" >CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/633431594" >通俗易懂地理解Gumbel Softmax<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Argmax</tag>
        <tag>Softmax</tag>
        <tag>可导</tag>
      </tags>
  </entry>
  <entry>
    <title>IPW逆概率加权</title>
    <url>/2025/04/08/IPW%E9%80%86%E6%A6%82%E7%8E%87%E5%8A%A0%E6%9D%83/</url>
    <content><![CDATA[<p>IPW是个非常优雅的纠偏方法。下面介绍如何利用它来实现纠偏：</p>
<span id="more"></span>
<p>uplift定义如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tau(x) &= \mathbb{E}_{\mathbb{P}}(Y(1)-Y(0) \mid x) \\
\mu_1(x) &= \mathbb{E}_{\mathbb{P}}(Y \mid W=1, X=x) \\
 \mu_0(x) &= \mathbb{E}_{\mathbb{P}}(Y \mid W=0, X=x) \\

\tau(x) &= \mu_1(x) - \mu_0(x)
\end{aligned}</script><p>ESN纠偏：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\underbrace{P(Y, W=1 \mid X)}_{E S T R} & =\underbrace{P(Y \mid W=1, X)}_{T R} \cdot \underbrace{P(W=1 \mid X)}_\pi \\
& =\mu_1 \cdot \pi \\
\underbrace{P(Y, W=0 \mid X)}_{E S C R} & =\underbrace{P(Y \mid W=0, X)}_{C R} \cdot \underbrace{P(W=0 \mid X)}_{1-\pi} \\
& =\mu_0 \cdot(1-\pi) \\

\mu_1 &= \frac{ESTR}{\pi} \\
\mu_0 &= \frac{ESCR}{1-\pi} \\
\tau &= \mu_1 - \mu_0
\end{aligned}</script><blockquote>
<p>备注：纠偏效果严重依赖于IPW预估的准确性，需根据业务场景谨慎评估。</p>
</blockquote>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/629853695" >DESCN: Deep Entire Space Cross Networks | 多任务、端到端、IPW的共舞<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Marketing</category>
      </categories>
      <tags>
        <tag>IPW</tag>
      </tags>
  </entry>
  <entry>
    <title>InstructGPT</title>
    <url>/2023/07/09/InstructGPT/</url>
    <content><![CDATA[<p>ChatGPT背后的技术原理：</p>
<span id="more"></span>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.2mzdjib60zy0.webp"  alt="InstructGPT"></p>
<ul>
<li>第二步中已经完成了奖励模型的训练，在第三步中奖励模型用PPO来训练第一步中微调好的GPT3，使其能够生成符合指令的文本</li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>GPT</tag>
        <tag>Reinforcement Learning</tag>
        <tag>PPO</tag>
      </tags>
  </entry>
  <entry>
    <title>IoC &amp; DI</title>
    <url>/2018/05/10/IoC&amp;DI/</url>
    <content><![CDATA[<p>总算搞懂这两个 <code>spring</code> 的核心概念了。</p>
<span id="more"></span>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近在学习Spring框架，它的核心就是IoC容器。要掌握Spring框架，就必须要理解控制反转的思想以及依赖注入的实现方式。下面，我们将围绕下面几个问题来探讨控制反转与依赖注入的关系以及在Spring中如何应用。</p>
<ul>
<li>什么是控制反转？</li>
<li>什么是依赖注入？</li>
<li>它们之间有什么关系？</li>
<li>如何在Spring框架中应用依赖注入？</li>
</ul>
<h1 id="控制反转"><a href="#控制反转" class="headerlink" title="控制反转"></a>控制反转</h1><p>在讨论控制反转之前，我们先来看看软件系统中耦合的对象：</p>
<img   src="/2018/05/10/IoC&DI/1.jpg"  class="">
<p>从图中可以看到，软件中的对象就像齿轮一样，协同工作，但是互相耦合，一个零件不能正常工作，整个系统就崩溃了。这是一个强耦合的系统。齿轮组中齿轮之间的啮合关系,与软件系统中对象之间的耦合关系非常相似。对象之间的耦合关系是无法避免的，也是必要的，这是协同工作的基础。现在，伴随着工业级应用的规模越来越庞大，对象之间的依赖关系也越来越复杂，经常会出现对象之间的多重依赖性关系，因此，架构师和设计师对于系统的分析和设计，将面临更大的挑战。对象之间耦合度过高的系统，必然会出现牵一发而动全身的情形。</p>
<p>为了解决对象间耦合度过高的问题，软件专家Michael Mattson提出了IOC理论，用来实现对象之间的“解耦”。</p>
<p><strong>控制反转(Inversion of Control)</strong>是一种是面向对象编程中的一种设计原则，用来减低计算机代码之间的耦合度。其基本思想是：借助于“第三方”实现具有依赖关系的对象之间的解耦:</p>

<p>由于引进了中间位置的“第三方”，也就是IOC容器，使得A、B、C、D这4个对象没有了耦合关系，齿轮之间的传动全部依靠“第三方”了，全部对象的控制权全部上缴给“第三方”IOC容器，所以，IOC容器成了整个系统的关键核心，它起到了一种类似“粘合剂”的作用，把系统中的所有对象粘合在一起发挥作用，如果没有这个“粘合剂”，对象与对象之间会彼此失去联系，这就是有人把IOC容器比喻成“粘合剂”的由来。</p>
<p>我们再来看看，控制反转(IOC)到底为什么要起这么个名字？我们来对比一下：</p>
<ol>
<li>软件系统在没有引入IOC容器之前，如图1所示，对象A依赖于对象B，那么对象A在初始化或者运行到某一点的时候，自己必须主动去创建对象B或者使用已经创建的对象B。无论是创建还是使用对象B，控制权都在自己手上。</li>
<li>软件系统在引入IOC容器之后，这种情形就完全改变了，如图2所示，由于IOC容器的加入，对象A与对象B之间失去了直接联系，所以，当对象A运行到需要对象B的时候，IOC容器会主动创建一个对象B注入到对象A需要的地方。</li>
</ol>
<p>通过前后的对比，我们不难看出来：对象A获得依赖对象B的过程,由主动行为变为了被动行为，控制权颠倒过来了，这就是“控制反转”这个名称的由来。</p>
<p>控制反转不只是软件工程的理论，在生活中我们也有用到这种思想。再举一个现实生活的例子：<br>海尔公司作为一个电器制商需要把自己的商品分销到全国各地，但是发现，不同的分销渠道有不同的玩法，于是派出了各种销售代表玩不同的玩法，随着渠道越来越多，发现，每增加一个渠道就要新增一批人和一个新的流程，严重耦合并依赖各渠道商的玩法。实在受不了了，于是制定业务标准，开发分销信息化系统，只有符合这个标准的渠道商才能成为海尔的分销商。让各个渠道商反过来依赖自己标准。反转了控制，倒置了依赖。</p>
<p>我们把海尔和分销商当作软件对象，分销信息化系统当作IOC容器，可以发现，在没有IOC容器之前，分销商就像图1中的齿轮一样，增加一个齿轮就要增加多种依赖在其他齿轮上，势必导致系统越来越复杂。开发分销系统之后，所有分销商只依赖分销系统，就像图2显示那样，可以很方便的增加和删除齿轮上去。</p>
<h1 id="依赖注入"><a href="#依赖注入" class="headerlink" title="依赖注入"></a>依赖注入</h1><p><strong>依赖注入(Dependency Injection)</strong>就是将实例变量传入到一个对象中去(Dependency injection means giving an object its instance variables)。</p>
<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>如果在 <code>Class A</code> 中，有 <code>Class B</code> 的实例，则称 <code>Class A</code> 对 <code>Class B</code> 有一个依赖。例如下面类 <code>Human</code> 中用到一个 <code>Father</code> 对象，我们就说类 <code>Human</code> 对类 <code>Father</code> 有一个依赖：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Human</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    Father father;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Human</span><span class="params">()</span> &#123;</span><br><span class="line">        father = <span class="keyword">new</span> <span class="title class_">Father</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>仔细看这段代码我们会发现存在一些问题：</p>
<ol>
<li>如果现在要改变 <code>father</code> 生成方式，如需要用 <code>new Father(String name)</code> 初始化 <code>father</code>，需要修改 <code>Human</code> 代码。</li>
<li>如果想测试不同 <code>Father</code> 对象对 <code>Human</code>的影响很困难，因为 <code>father</code> 的初始化被写死在了 <code>Human</code> 的构造函数中。</li>
<li>如果 <code>new Father()</code> 过程非常缓慢，单测时我们希望用已经初始化好的 <code>father</code> 对象 <code>Mock</code> 掉这个过程也很困难。</li>
</ol>
<h2 id="注入"><a href="#注入" class="headerlink" title="注入"></a>注入</h2><p>上面将依赖在构造函数中直接初始化是一种 <code>Hard init</code> 方式，弊端在于两个类不够独立，不方便测试。我们还有另外一种 <code>Init</code> 方式，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Human</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    Father father;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Human</span><span class="params">(Father father)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.father = father;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面代码中，我们将 father 对象作为构造函数的一个参数传入。在调用 <code>Human</code> 的构造方法之前外部就已经初始化好了 <code>Father</code> 对象。像这种非自己主动初始化依赖，而通过外部来传入依赖的方式，我们就称为依赖注入。<br>现在我们发现上面 1 中存在的两个问题都很好解决了，简单的说依赖注入主要有两个好处：</p>
<ol>
<li>解耦，将依赖之间解耦。</li>
<li>因为已经解耦，所以方便做单元测试，尤其是 Mock 测试。</li>
</ol>
<h1 id="控制反转和依赖注入的关系"><a href="#控制反转和依赖注入的关系" class="headerlink" title="控制反转和依赖注入的关系"></a>控制反转和依赖注入的关系</h1><p>我们已经分别解释了控制反转和依赖注入的概念。有些人会把控制反转和依赖注入等同，但实际上它们有着本质上的不同：</p>
<ul>
<li><strong>控制反转</strong>是一种思想。</li>
<li><strong>依赖注入</strong>是一种设计模式。</li>
</ul>
<p>IoC框架使用依赖注入作为实现控制反转的方式，但是控制反转还有其他的实现方式，例如说<a class="link"   href="http://martinfowler.com/articles/injection.html#UsingAServiceLocator" >ServiceLocator<i class="fas fa-external-link-alt"></i></a>，所以不能将控制反转和依赖注入等同。</p>
<h2 id="Spring中的依赖注入"><a href="#Spring中的依赖注入" class="headerlink" title="Spring中的依赖注入"></a>Spring中的依赖注入</h2><p>上面我们提到，依赖注入是实现控制反转的一种方式。下面我们结合Spring的IoC容器，简单描述一下这个过程：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MovieLister</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> MovieFinder finder;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setFinder</span><span class="params">(MovieFinder finder)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.finder = finder;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ColonMovieFinder</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setFilename</span><span class="params">(String filename)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.filename = filename;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们先定义两个类，可以看到都使用了依赖注入的方式，通过外部传入依赖，而不是自己创建依赖。那么问题来了，谁把依赖传给他们，也就是说谁负责创建 <code>finder</code>，并且把 <code>finder</code> 传给 <code>MovieLister</code>。答案是Spring的IoC容器。</p>
<p>要使用IoC容器，首先要进行配置。这里我们使用xml的配置，也可以通过代码注解方式配置。下面是<code>spring.xml</code>的内容：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">beans</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;MovieLister&quot;</span> <span class="attr">class</span>=<span class="string">&quot;spring.MovieLister&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;finder&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">ref</span> <span class="attr">local</span>=<span class="string">&quot;MovieFinder&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;MovieFinder&quot;</span> <span class="attr">class</span>=<span class="string">&quot;spring.ColonMovieFinder&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;filename&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>movies1.txt<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在Spring中，每个bean代表一个对象的实例，默认是单例模式，即在程序的生命周期内，所有的对象都只有一个实例，进行重复使用。通过配置bean，IoC容器在启动的时候会根据配置生成bean实例。具体的配置语法参考Spring文档。这里只要知道IoC容器会根据配置创建 <code>MovieFinder</code>，在运行的时候把 <code>MovieFinder</code> 赋值给 <code>MovieLister</code> 的 <code>finder</code> 属性，完成依赖注入的过程。</p>
<p>下面给出测试代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testWithSpring</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">ApplicationContext</span> <span class="variable">ctx</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileSystemXmlApplicationContext</span>(<span class="string">&quot;spring.xml&quot;</span>);<span class="comment">//1</span></span><br><span class="line">    <span class="type">MovieLister</span> <span class="variable">lister</span> <span class="operator">=</span> (MovieLister) ctx.getBean(<span class="string">&quot;MovieLister&quot;</span>);<span class="comment">//2</span></span><br><span class="line">    Movie[] movies = lister.moviesDirectedBy(<span class="string">&quot;Sergio Leone&quot;</span>);</span><br><span class="line">    assertEquals(<span class="string">&quot;Once Upon a Time in the West&quot;</span>, movies[<span class="number">0</span>].getTitle());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>根据配置生成 <code>ApplicationContext</code>，即IoC容器。</li>
<li>从容器中获取 <code>MovieLister</code> 的实例。</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>控制反转是一种在软件工程中解耦合的思想，调用类只依赖接口，而不依赖具体的实现类，减少了耦合。控制权交给了容器，在运行的时候才由容器决定将具体的实现动态的“注入”到调用类的对象中。</li>
<li>依赖注入是一种设计模式，可以作为控制反转的一种实现方式。依赖注入就是将实例变量传入到一个对象中去(Dependency injection means giving an object its instance variables)。</li>
<li>通过IoC框架，类A依赖类B的强耦合关系可以在运行时通过容器建立，也就是说把创建B实例的工作移交给容器，类A只管使用就可以。</li>
</ol>
<hr>
<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><ul>
<li><a class="link"   href="http://blog.xiaohansong.com/2015/10/21/IoC-and-DI/" >http://blog.xiaohansong.com/2015/10/21/IoC-and-DI/<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>K-means</title>
    <url>/2021/07/28/K-means/</url>
    <content><![CDATA[<p>无监督学习中的典型代表，它将类别相同的样本汇聚在一起，但是聚类好之后，它并不知道每个样本的类别（看到之前有些博客说做分类的，当时一直困扰我，现在才知道这只是聚类，不是分类）。</p>
<span id="more"></span>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><ol>
<li>随机选择 $k$ 个样本作为初始化聚类中心：$a_1, a_2, \dots, a_k$ ；</li>
<li>针对数据集每个样本 $x_i$ ，计算其到每个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；</li>
<li>针对每个类别 $a_j$ ，重新计算它的聚类中心：$a_j = \frac{1}{|c_j|} \sum_{x \in c_j} x$ （即该类所有样本的质心）；</li>
<li>重复2-3步，直到达到某个终止条件（迭代次数、最小误差变化等）。</li>
</ol>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>容易理解，聚类效果不错，虽然是局部最优， 往往局部最优已经足够</li>
<li>处理大数据集的时候，该算法可以保证较好的伸缩性</li>
<li>算法复杂性低</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>$K$ 需要认为设定，不同的取值对最终聚类效果影响非常大</li>
<li>对初始簇类中心敏感，不同的初始取值对最终聚类效果影响非常大</li>
<li>对异常值敏感</li>
<li>不适合离散的分类、样本类别不平衡的分类、非凸形状的分类</li>
</ul>
<h2 id="如何选择-K-值？"><a href="#如何选择-K-值？" class="headerlink" title="如何选择 $K$ 值？"></a>如何选择 $K$ 值？</h2><p>基本思想还是最小化类内距离，最大化类间距离，使同一簇内样本尽可能相似，不同簇中样本尽可能不相似。</p>
<h4 id="手肘法"><a href="#手肘法" class="headerlink" title="手肘法"></a>手肘法</h4><p>随着 $K$ 值增大，误差值会越来越小（举一个极端的例子：当每一个样本被分为一个类时，类内间距最小，但这显然不是我们想要的）。因此可根据不同 $K$ 值下的误差曲线选择使误差平方和下降最快的 $K$ 值。当大于此 $K$ 值时，$K$ 值增大，但误差减少量很小。即选择曲线上的拐点最佳。在下面这个图中即选择 $k=2$ ，将样本分为两类。</p>
<img   src="/2021/07/28/K-means/1.png"  class="">
<h4 id="Gap-statistic"><a href="#Gap-statistic" class="headerlink" title="Gap statistic"></a>Gap statistic</h4><p>手肘法的缺点在于需要人工看不够自动化，这里提出Gap statistic：</p>
<script type="math/tex; mode=display">
Gap(K) = E(log D_k) - log(D_k)</script><p>其中 $D_k$ 为损失函数，这里 $E(log D_k)$ 指的是 $log(D_k)$ 的期望。这个数值通常通过蒙特卡洛模拟产生，我们在样本里所在的区域中按照均匀分布随机产生和原始样本数一样多的随机样本，并对这个随机样本做 K-Means，从而得到一个 $D_k$ 。如此往复多次，通常20次，我们可以得到20个 $log D_k$ 。对这20个数值求平均值，就得到了 $E(log D_k)$ ​的近似值。最终可以计算 Gap Statisitc。而 Gap statistic 取得最大值所对应的 $K$ 就是最佳的 $K$ 。</p>
<img   src="/2021/07/28/K-means/2.jpeg"  class="">
<p>由图可见，当 $K=3$ 时，$Gap(K)$ 取值最大，所以最佳的簇数是 $K=3$ 。</p>
<h2 id="如何初始化聚类中心？"><a href="#如何初始化聚类中心？" class="headerlink" title="如何初始化聚类中心？"></a>如何初始化聚类中心？</h2><p>这里主要介绍K-means++算法，主要思想是选择离已选中心点最远的点。这也比较符合常理，聚类中心当然是互相离得越远越好。</p>
<p>算法流程如下：</p>
<ol>
<li><p>随机选择一个中心点 $a_1$ ；</p>
</li>
<li><p>对于数据集中的每一个点 $x_i$ ，计算它与之前 $n$ 个聚类中心最远的距离 $D(x_i)$ ，并以 $\frac{D(x_i)^2}{\sum_{j=1}^n D(x_j)^2}$ 的概率选择作为新中心点 $a_i$ ；</p>
</li>
<li><p>重复2步骤，直到选出 $K$ 个聚类中心。</p>
</li>
</ol>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/78798251" >【机器学习】K-means（非常详细）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/m0_46568930/article/details/111991654" >【机器学习】——K_means如何选择k值？<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/weixin_42029738/article/details/81978038" >K-means原理、优化及应用<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/dpengwang/article/details/86574999" >K-means++ 中选择初始聚类中心<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>KV Cache</title>
    <url>/2023/09/13/KV-Cache/</url>
    <content><![CDATA[<p>大模型推理加速的一个常用技术是KV Cache，在不牺牲任何计算精度的前提下，通过空间换时间，提高推理性能。注意，这里的Cache概念非常简单，跟浏览器缓存、CPU缓存不是一个概念。</p>
<span id="more"></span>
<p>在生成式模型的推理过程中，假设给定一个输入文本，模型会输出一个长度为N的文本，但是该过程执行了N次推理。因为模型每次推理只输出一个token，然后将输出token与输入tokens拼接在一起，作为下一次推理的输入，这样不断反复直到遇到终止符。</p>
<p>由于生成式模型推理过程是单向的，即已经输出的token的embedding是不会再变化的，所以上述步骤可以优化。将Key和Value缓存起来，不用再经历前向传播算出embedding，只需要将上一轮输出的token前向传播算出embedding，然后与KV拼接，来预测出下一个token。这样模型的计算量大大减少，推理大幅加速。</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.42k0hefccxy0.webp"  alt="model"></p>
<p>伪代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">query = self._split_heads(query, self.num_heads, self.head_dim)</span><br><span class="line">key = self._split_heads(key, self.num_heads, self.head_dim)</span><br><span class="line">value = self._split_heads(value, self.num_heads, self.head_dim)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> layer_past <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># 当输出第一个token后，layer_past就是非None了</span></span><br><span class="line">    past_key, past_value = layer_past <span class="comment"># 取出之前计算好的 key, value</span></span><br><span class="line">    key = torch.cat((past_key, key), dim=-<span class="number">2</span>) <span class="comment"># past_key 与当前 token 对应的 key 拼接</span></span><br><span class="line">    value = torch.cat((past_value, value), dim=-<span class="number">2</span>) <span class="comment"># past_value 与当前 token 对应的 value 拼接</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> use_cache <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">    present = (key, value)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    present = <span class="literal">None</span></span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.zhihu.com/question/596900067/answer/3040011798" >KV Cache<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>LLM</tag>
        <tag>推理加速</tag>
        <tag>KV Cache</tag>
      </tags>
  </entry>
  <entry>
    <title>Knowledge Distillation</title>
    <url>/2023/06/26/Knowledge-Distillation/</url>
    <content><![CDATA[<p>知识蒸馏是将训练好的大模型包含的知识蒸馏到小模型中。在线上部署的时候，我们使用小模型即可。</p>
<span id="more"></span>
<h2 id="蒸馏过程示意图"><a href="#蒸馏过程示意图" class="headerlink" title="蒸馏过程示意图"></a>蒸馏过程示意图</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/v2-d01f5142d06aa27bc5e207831b5131d9_720w.3ji7m6t6c440.webp"  alt="procedure"></p>
<h2 id="总损失函数"><a href="#总损失函数" class="headerlink" title="总损失函数"></a>总损失函数</h2><script type="math/tex; mode=display">
\begin{align*}
    L = \alpha L_{\text {soft }}+\beta L_{\text {hard}}
\end{align*}</script><ul>
<li>$L_{\text {soft }}=-\sum_j^N p_j^T \log \left(q_j^T\right)$<ul>
<li>$p_i^T=\frac{\exp \left(v_i / T\right)}{\sum_k^N \exp \left(v_k / T\right)}, q_i^T=\frac{\exp \left(z_i / T\right)}{\sum_k^N \exp \left(z_k / T\right)}$</li>
<li>$v_i, z_i$ 分别为教师模型和学生模型产生的logits，$N$ 为类别数，$T$ 为温度</li>
</ul>
</li>
<li>$L_{h a r d}=-\sum_j^N c_j \log \left(q_j^1\right)$<ul>
<li>$q_i^1=\frac{\exp \left(z_i\right)}{\sum_k^N \exp \left(z_k\right)}$</li>
</ul>
</li>
</ul>
<h2 id="关于温度-T"><a href="#关于温度-T" class="headerlink" title="关于温度 $T$"></a>关于温度 $T$</h2><ul>
<li>$T$ 越大，softmax概率分布比原始更平缓，学生模型会更加关注到负标签的信息</li>
<li>$T$ 越小，softmax概率分布比原始更陡峭，学生模型会更加关注到正标签的信息</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/102038521" >知识蒸馏(Knowledge Distillation) 经典之作<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Softmax</tag>
        <tag>模型压缩</tag>
        <tag>温度</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM Inference Performance Engineering</title>
    <url>/2024/06/12/LLM-Inference-Performance-Engineering/</url>
    <content><![CDATA[<p><a class="link"   href="https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices" >https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Inference</tag>
        <tag>Throughput</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM微调过程中灾难性遗忘问题解决方法</title>
    <url>/2024/01/19/LLM%E5%BE%AE%E8%B0%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%81%BE%E9%9A%BE%E6%80%A7%E9%81%97%E5%BF%98%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>灾难性遗忘是LLM微调过程中最常见的问题，下面是一些解决办法：</p>
<span id="more"></span>
<ol>
<li>将重要的权重冻结：像Lora就是采用的这种方案，只学习部分网络权重。但这里Lora的配置其实是要注意一下，如果你是用Lora做预训练，lora训练模块可以配上 q_proj,v_proj,k_proj,o_proj  如果是微调则只需要训练q_proj,v_proj。lora_rank的设置也有讲究，初始设lora_ran为8，训练存在遗忘时，可以将 lora_rank改为64（原因是与原模型数据领域相差较大的话，需要更大的秩，原论文有说明）</li>
<li>复习：跟人一样，在预训练或微调时，回看之前训练的数据。还可以专门把特征图存起来，量化以后放在一个类似于记忆库的地方，之后在新任务上训练的时候从这个记忆库里重构出记忆和新数据一起训练。感兴趣可以看这篇论文：<a href="https://arxiv.org/pdf/1910.02509.pdf">REMIND Your Neural Network to Prevent<br>Catastrophic Forgetting</a></li>
<li>MoE：稀疏门控制的专家混合层，最近爆出GPT4是由 8个220B的模型组合。但个人体验，阉割版的GPT4变得智障了很多。</li>
<li>数据蒸馏：损失函数由teacher-student的KL loss和groud truth label构成：<a class="link"   href="https://github.com/beyondguo/LLM-Tuning/discussions/24" >https://github.com/beyondguo/LLM-Tuning/discussions/24<i class="fas fa-external-link-alt"></i></a></li>
<li>两阶段训练策略：第一阶段微调特定能力数据，在第二阶段微调通用数据+少量的特定能力数据</li>
</ol>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/Aa8jYs4xgcI4clwie-wO1g" >大语言模型Fine-tuning踩坑经验之谈<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/3RIBzuVlK0qHbO_Q04s-cw" >有被混合后的SFT数据伤到<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>灾难性遗忘</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM长上下文外推方法</title>
    <url>/2024/03/10/LLM%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%A4%96%E6%8E%A8%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>现在的LLM都集中在卷上下文长度了，最新的Claude3已经支持200K的上下文，见：<a class="link"   href="https://www.vellum.ai/llm-leaderboard#cost-context" >cost-context<i class="fas fa-external-link-alt"></i></a>。下面是一些提升LLM长度外推能力的方法总结：</p>
<span id="more"></span>
<h2 id="数据工程"><a href="#数据工程" class="headerlink" title="数据工程"></a>数据工程</h2><p>符尧大佬的最新工作：<a class="link"   href="https://arxiv.org/pdf/2402.10171.pdf" >Data Engineering for Scaling Language Models to 128K Context<i class="fas fa-external-link-alt"></i></a></p>
<p>作者假设LLM在预训练中已经获得了128k上下文内处理任意输入位置信息能力。现在这种能力只需通过轻量级的持续预训练与适当的数据混合轻松地激发出来：</p>
<ul>
<li>在更长的数据上继续预训练</li>
<li>混合各领域的数据</li>
<li>长序列上采样</li>
</ul>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>陈丹琦团队最新工作：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.9kfxf0ia9q.png"  alt="CEPE"></p>
<ul>
<li>小型编码器：并行对长上下文进行分块编码</li>
<li>交叉注意力模块：插入到解码器的每一层，用于关注编码器表示</li>
</ul>
<h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><h4 id="RoPE"><a href="#RoPE" class="headerlink" title="RoPE"></a>RoPE</h4><p>最经典的一版，具体讲解见：<a class="link"   href="https://transformerswsz.github.io/2023/09/04/%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" >旋转位置编码<i class="fas fa-external-link-alt"></i></a></p>
<h4 id="Position-Interpolation"><a href="#Position-Interpolation" class="headerlink" title="Position Interpolation"></a>Position Interpolation</h4><p>在两个token的距离相差超过2k时，RoPE远程衰减特性将不稳定。线性插值通过在两个位置之间再插入位置，进行区间细分，然后在少量样本上继续预训练，即可外推到32k等</p>
<h4 id="NTK-Aware-Interpolation"><a href="#NTK-Aware-Interpolation" class="headerlink" title="NTK-Aware Interpolation"></a>NTK-Aware Interpolation</h4><p>主要思路：高频外推，低频内插。</p>
<p>NTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。</p>
<h4 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h4><p>从attention score层面缓解：</p>
<ul>
<li>YaRN</li>
<li>logn</li>
</ul>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>较短的预训练模型（2k、4k）应用在长上下文会因为训练和推理的两个不一致导致效果下降：</p>
<ul>
<li>推理时用到了没训练过的位置编码</li>
<li>推理时注意力机制所处理的token数量远超训练时的数量，导致注意力机制的崩坏</li>
</ul>
<p>这两个问题分别可以从位置编码和attention score的放缩来缓解：</p>
<ul>
<li>线性插值PI、NTK插值、分部NTK插值都可以缓解第一个问题，</li>
<li>logn和YaRN则把第二个问题纳入的考虑。目前这些方法在实际应用中也有很多变体，包括超参的修改，函数的重定义等</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/q3Fen2U1GpwmPK3RSs2ZWQ" >符尧：仅靠数据工程我能将LLM的上下文检索能力拓展到128K<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/KTCltdMi3HRwO_LoZvQOGw" >陈丹琦团队新作：Llama-2上下文扩展至128k，10倍吞吐量仅需1/6内存<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/81NHGf5W8HEscW2dBK8MRg" >大模型处理长上下文方法一览<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>长度外推</tag>
        <tag>RoPE</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA2与LoRA结构详解</title>
    <url>/2023/11/06/LLaMA2%E4%B8%8ELoRA%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>llama模型结构如下：</p>
<span id="more"></span>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.2krpqrv609.png"  alt="llama"></p>
<p><a class="link"   href="https://blog.csdn.net/BIT_666/article/details/132161203" >https://blog.csdn.net/BIT_666/article/details/132161203<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>LLaMA</tag>
        <tag>LoRA</tag>
        <tag>q_proj</tag>
        <tag>k_proj</tag>
        <tag>v_proj</tag>
      </tags>
  </entry>
  <entry>
    <title>LLaMA2详解</title>
    <url>/2024/06/02/LLaMA2%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>LLaMA2的模型结构拆解：</p>
<span id="more"></span>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.2krpqrv609.png"  alt="llama"></p>
<h2 id="RoPE"><a href="#RoPE" class="headerlink" title="RoPE"></a>RoPE</h2><p>这个不多谈了，见：<a class="link"   href="https://transformerswsz.github.io/2023/09/04/%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" >旋转位置编码<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="RMSNorm"><a href="#RMSNorm" class="headerlink" title="RMSNorm"></a>RMSNorm</h2><p>标准Transformer的LayerNorm如下：</p>
<script type="math/tex; mode=display">
y=\frac{x-\operatorname{Mean}(x)}{\sqrt{\operatorname{Var}(x)+\varepsilon}} * W+B</script><p>llama2采用了RMSNorm：</p>
<script type="math/tex; mode=display">
y=\frac{x}{R M S(x)+\varepsilon}, RMS(x) = \sqrt{\frac{1}{n} \sum_{1}^n {x_i}^2}</script><p><a class="link"   href="https://github.com/meta-llama/llama/blob/b8348da38fde8644ef00a56596efb376f86838d1/llama/model.py#L52" >实现源码<i class="fas fa-external-link-alt"></i></a>：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_norm</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Apply the RMSNorm normalization to the input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x (torch.Tensor): The input tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        torch.Tensor: The normalized tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> x * torch.rsqrt(x.<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + self.eps)</span><br></pre></td></tr></table></figure></p>
<p>RMSNorm能保证激活效果变化，且计算效率能提升7%∼64%</p>
<h2 id="SwiGLU"><a href="#SwiGLU" class="headerlink" title="SwiGLU"></a>SwiGLU</h2><p>这个不多谈了，见：<a class="link"   href="https://transformerswsz.github.io/2024/05/09/SwiGLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" >SwiGLU激活函数<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.7lju1gc3hj.webp"  alt="MLP"></p>
<p><a class="link"   href="https://github.com/meta-llama/llama/blob/b8348da38fde8644ef00a56596efb376f86838d1/llama/model.py#L307" >up、gate、down都是三个linear层<i class="fas fa-external-link-alt"></i></a>：<code>down(up * silu(gate))</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="keyword">return</span> self.w2(F.silu(self.w1(x)) * self.w3(x))</span><br></pre></td></tr></table></figure>
<h2 id="GQA"><a href="#GQA" class="headerlink" title="GQA"></a>GQA</h2><p>这个不多谈了，见：<a class="link"   href="https://transformerswsz.github.io/2023/09/13/Multi-Query-Attention-Group-Query-Attention/" >Multi Query Attention &amp; Group Query Attention<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="padding-side"><a href="#padding-side" class="headerlink" title="padding_side"></a>padding_side</h2><p>llama系列训练和推理都是right padding：</p>
<ul>
<li>训练：其实只要设置padding mask，那么left/right pad是没有区别的。然而实际上huggingface中某些tokenizer在training的时候必须设成right padding，因为有些tokenizer使用的是absolute position id，导致非padding token的position id不是从0开始的。</li>
<li>推理：由于llama是decoder-only结构，每个token的生成依赖于上一个token。而上一个token如果是无实际意义的padding token，那么就打断了句子语义的连续性，导致生成文本质量较差。因此left padding比较make sense，但在llama推理的源码中，batch的时候采用<a class="link"   href="https://github.com/meta-llama/llama3/blob/14aab0428d3ec3a9596f1dea06d9c564f9c0e35f/llama/generation.py#L155" >right padding<i class="fas fa-external-link-alt"></i></a>。生成token的时候，<a class="link"   href="https://github.com/meta-llama/llama3/blob/14aab0428d3ec3a9596f1dea06d9c564f9c0e35f/llama/generation.py#L184" >从batch中长度最短的序列开始生成，其它较长序列的相同位置保持不变<i class="fas fa-external-link-alt"></i></a>，直到该序列开始生成。猜测llama这样做的原因是保持跟训练一致，但不如left padding优雅。</li>
</ul>
<h2 id="llama1-llama2-llama3区别"><a href="#llama1-llama2-llama3区别" class="headerlink" title="llama1,llama2,llama3区别"></a>llama1,llama2,llama3区别</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">差异</th>
<th style="text-align:center">llama1</th>
<th style="text-align:center">llama2</th>
<th style="text-align:center">llama3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">上下文长度</td>
<td style="text-align:center">2k</td>
<td style="text-align:center">4k</td>
<td style="text-align:center">8k</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/646852375" >大部分的大模型(LLM)采用左填充(left-padding)的原因<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/Yf_NU3pgedLHl8dWAaMRfQ" >从头预训练一只超迷你 LLaMA 3<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/675273498" >LLM padding 细节<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/BIT_666/article/details/132161203" >LLM - Transformer &amp;&amp; LLaMA2 结构分析与 LoRA 详解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>LLaMA</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM</title>
    <url>/2019/07/07/LSTM/</url>
    <content><![CDATA[<p>记录一下LSTM的模型结构与原理。</p>
<span id="more"></span>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><img   src="/2019/07/07/LSTM/1.jpeg"  class="">
<p>下面详细介绍LSTM的三个门：</p>
<h3 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h3><p>遗忘门决定了上一时刻的单元状态 $c_{t-1}$ 有多少保留到当前时刻 $c_t$ 。</p>
<img   src="/2019/07/07/LSTM/2.jpeg"  class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)</script><h3 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h3><p>输入门决定了当前时刻网络的输入 $x_t$ 有多少保存到单元状态 $c_t$ 。</p>
<img   src="/2019/07/07/LSTM/3.jpeg"  class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)</script><h3 id="当前输入的单元状态-tilde-c-t"><a href="#当前输入的单元状态-tilde-c-t" class="headerlink" title="当前输入的单元状态 $\tilde{c_{t}}$"></a>当前输入的单元状态 $\tilde{c_{t}}$</h3><p>可以把它想象成不包含上一时刻的长期状态 $c_{t-1}$ 时，我们生成的当前此刻的长期状态 $\tilde{c_{t}}$ 。</p>
<img   src="/2019/07/07/LSTM/4.jpeg"  class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
\tilde{c}_{t}=\tanh \left(W_{c} \cdot\left[h_{t-1}, x_{t}\right]+b_{c}\right)</script><h3 id="计算当前时刻的单元状态-c-t"><a href="#计算当前时刻的单元状态-c-t" class="headerlink" title="计算当前时刻的单元状态 $c_t$"></a>计算当前时刻的单元状态 $c_t$</h3><img   src="/2019/07/07/LSTM/5.jpeg"  class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
c_{t}=f_{t} \circ c_{t-1}+i_{t} \circ \tilde{c}_{t}</script><p>这样，我们就能够将当前的记忆 $\tilde{c_{t}}$ 和长期的记忆 $c_{t-1}$ 组合在一起，形成新的单元状态 $c_t$ 。由于遗忘门的控制，它可以保存很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。</p>
<h3 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h3><p>用来控制单元状态 $c_t$ 有多少输入到 LSTM 的当前输出值 $h_t$ 。</p>
<img   src="/2019/07/07/LSTM/6.jpeg"  class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
o_{t}=\sigma\left(W_{o} \cdot\left[h_{t-1}, x_{t}\right]+b_{o}\right)</script><h3 id="计算输出值-h-t"><a href="#计算输出值-h-t" class="headerlink" title="计算输出值 $h_t$"></a>计算输出值 $h_t$</h3><img   src="/2019/07/07/LSTM/7.jpeg"  class="">
<p>公式如下：</p>
<script type="math/tex; mode=display">
h_{t}=o_{t} \circ \tanh \left(c_{t}\right)</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a class="link"   href="https://zhuanlan.zhihu.com/p/44124492" >LSTM：RNN最常用的变体<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Learn To Rank</title>
    <url>/2024/07/07/Learn-To-Rank/</url>
    <content><![CDATA[<p>在信息检索中，给定一个query，搜索引擎召回一系列相关的Documents，然后对这些Documents进行排序，最后将Top N的Documents输出。</p>
<span id="more"></span>
<div class="keep-note danger"><p>排序问题最关注的是各Documents之间的相对顺序关系，而不是各个Documents的预测分最准确。</p>
</div>
<h2 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:center">人工标注</th>
<th style="text-align:center">行为日志</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">简介</td>
<td style="text-align:center">人工对抽样出来作为training data的query-doc pair进行相关程度的判断和标注</td>
<td style="text-align:center">根据用户的实际搜索和点击行为，来判断query-doc的相关性。比如同一个query下，不同doc的点击数来作为它们相关程度的大小</td>
</tr>
<tr>
<td style="text-align:center">优点</td>
<td style="text-align:center">准确性高</td>
<td style="text-align:center">无须人工干预，成本低</td>
</tr>
<tr>
<td style="text-align:center">缺点</td>
<td style="text-align:center">代价高且耗时</td>
<td style="text-align:center">用户行为日志存在大量偏差，比如：<li>位置偏差：用户倾向于点击列表靠前的item</li><li>样本选择偏差：有用户点击的query知识总体query的一个子集，无法获取全部的query下doc的label</li></td>
</tr>
</tbody>
</table>
</div>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p>这里主要介绍<a class="link"   href="https://chatgpt.com/share/613f6af0-fdc1-4435-81e0-8c3a3b763779" >NDCG<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="三大rank算法"><a href="#三大rank算法" class="headerlink" title="三大rank算法"></a>三大rank算法</h2><h3 id="pointwise"><a href="#pointwise" class="headerlink" title="pointwise"></a>pointwise</h3><p>pointwise方法损失函数计算只与单个document有关，本质上是训练一个分类模型或者回归模型，判断这个document与当前的这个query相关程度，最后的排序结果就是从模型对这些document的预测分值进行一个排序。</p>
<ul>
<li>优点：实现简单</li>
<li>缺点：<ul>
<li>精确打分，而不是相对打分，无法实现排序</li>
<li>损失函数也没有建模到预测排序中的位置信息</li>
</ul>
</li>
</ul>
<h3 id="pairwise"><a href="#pairwise" class="headerlink" title="pairwise"></a>pairwise</h3><p>pairwise方法在计算目标损失函数的时候，每一次需要基于一个pair的document的预测结果进行损失函数的计算。其中模型输入和对应的标签label形式如下：</p>
<ul>
<li>输入：一个文档对(docA, docB)</li>
<li><p>输出：相对序(1 or 0.5 or 0)</p>
</li>
<li><p>优点：实现简单；建模了两个文档相对序关系</p>
</li>
<li>缺点<ul>
<li>样本对量级高，$O(n^2)$</li>
<li>对错误标注数据敏感，会造成多个pair对错误</li>
<li>仅考虑了文档对pair的相对位置，仍然没有建模到预测排序中的位置信息</li>
</ul>
</li>
</ul>
<h4 id="经典模型RankNet"><a href="#经典模型RankNet" class="headerlink" title="经典模型RankNet"></a>经典模型RankNet</h4><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.8ojkqhz95u.png"  alt="RankNet"></p>
<h3 id="listwise"><a href="#listwise" class="headerlink" title="listwise"></a>listwise</h3><p>Listwise方法是直接对整个list的document的排序进行优化，目标损失函数中优化整个list的document的排序结果。其中模型输入和对应的标签label形式如下：</p>
<ul>
<li>输入: 整个list document</li>
<li><p>输出: 排序好的document list</p>
</li>
<li><p>优点：直接建模list内的所有文档序关系，与评估目标一致</p>
</li>
<li>缺点<ul>
<li>计算复杂度高</li>
</ul>
</li>
</ul>
<h4 id="经典模型ListMLE"><a href="#经典模型ListMLE" class="headerlink" title="经典模型ListMLE"></a>经典模型ListMLE</h4><p>直接以真实标签顺序为目标，最大化预测结果排序与目标一致的概率即可。<br><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.7egnk999fg.png"  alt="ListMLE"></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/bentuwuying/p/6681943.html" >Learning to Rank简介<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://xdren69.github.io/2021/04/26/learning-to-rank/" >learning to rank中的Listwise，Pairwise和Pointwise<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_36478718/article/details/122598406" >Learning to Rank : ListNet与ListMLE<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>LightGCL解读</title>
    <url>/2023/03/22/LightGCL%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>传统的基于图的对比学习范式容易丢失结构信息，LightGCL通过SVD分解邻接矩阵构造了正样本图，并最大程度保留了原图全局结构信息：</p>
<span id="more"></span>
<img   src="/2023/03/22/LightGCL%E8%A7%A3%E8%AF%BB/model.png"  class="model">
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/576480565" >ICLR’23 UnderReview | LightGCL: 简单而有效的图对比学习推荐系统<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/6251584.html" >奇异值分解(SVD)原理与在降维中的应用<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Contrastive Learning</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Bash及Vim常用命令</title>
    <url>/2018/02/07/Linux%20Bash%E5%8F%8AVim%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>本人算是Linux菜鸟一个，只用到一些很基础的命令，在此记录一下。</p>
<span id="more"></span>
<h2 id="Bash-快捷键"><a href="#Bash-快捷键" class="headerlink" title="Bash 快捷键"></a>Bash 快捷键</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>ctrl + a</code></td>
<td style="text-align:center">移到命令行首</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + e</code></td>
<td style="text-align:center">移到命令行尾</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + f</code></td>
<td style="text-align:center">按字符右移</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + b</code></td>
<td style="text-align:center">按字符左移</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + u</code></td>
<td style="text-align:center">从光标处(不包含)删除至命令行首(包含)</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + k</code></td>
<td style="text-align:center">从光标处(不包含)删除至命令行尾(包含)</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + w</code></td>
<td style="text-align:center">从光标处(不包含)删除至单词字首(包含)</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + d</code></td>
<td style="text-align:center">删除光标处的字符</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + h</code></td>
<td style="text-align:center">删除光标前的字符</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + l</code></td>
<td style="text-align:center">清屏</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl + c</code></td>
<td style="text-align:center">终止命令</td>
</tr>
<tr>
<td style="text-align:center"><code>o</code></td>
<td style="text-align:center">在光标所在位置的下一行打开新行插入</td>
</tr>
<tr>
<td style="text-align:center"><code>O</code></td>
<td style="text-align:center">在光标所在位置的上一行打开新行插入</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Vim-快捷键"><a href="#Vim-快捷键" class="headerlink" title="Vim 快捷键"></a>Vim 快捷键</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">命令</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>x</code></td>
<td style="text-align:center">删除光标所在处字符</td>
</tr>
<tr>
<td style="text-align:center"><code>X</code></td>
<td style="text-align:center">删除光标所在处前一个字符</td>
</tr>
<tr>
<td style="text-align:center"><code>u</code></td>
<td style="text-align:center">撤销</td>
</tr>
<tr>
<td style="text-align:center"><code>i</code></td>
<td style="text-align:center">在当前光标之前插入文本</td>
</tr>
<tr>
<td style="text-align:center"><code>a</code></td>
<td style="text-align:center">在当前光标之后插入文本</td>
</tr>
<tr>
<td style="text-align:center"><code>gg</code></td>
<td style="text-align:center">跳转到文件头</td>
</tr>
<tr>
<td style="text-align:center"><code>shift + g</code></td>
<td style="text-align:center">跳转到文件末尾行首</td>
</tr>
<tr>
<td style="text-align:center"><code>dd</code></td>
<td style="text-align:center">删除一行</td>
</tr>
<tr>
<td style="text-align:center"><code>dw</code></td>
<td style="text-align:center">从光标处(包含)删除到下一个单词开头</td>
</tr>
<tr>
<td style="text-align:center"><code>de</code></td>
<td style="text-align:center">从光标处(包含)删除到本单词末尾</td>
</tr>
<tr>
<td style="text-align:center"><code>db</code></td>
<td style="text-align:center">从光标处(不包含)删除到前一个单词</td>
</tr>
<tr>
<td style="text-align:center"><code>0</code></td>
<td style="text-align:center">移动到行首</td>
</tr>
<tr>
<td style="text-align:center"><code>shift + $</code></td>
<td style="text-align:center">移动到行尾</td>
</tr>
<tr>
<td style="text-align:center"><code>yy + p</code></td>
<td style="text-align:center">复制当前行到下一行</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Bash</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux sort命令</title>
    <url>/2022/08/22/Linux%20sort%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p><code>sort</code> 命令用于对字符串排序，在日常的脚本处理中非常有用，用法也很简单。</p>
<span id="more"></span>
<p>有数据文件如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> fruit.txt</span><br></pre></td></tr></table></figure>
<pre><code>banana;30;5.5
apple;10;2.5
pear;90;2.3
orange;20;3.4
</code></pre><p>三列信息为水果名称、销售数量、单价。现要求以单价来降序输出这些水果信息：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sort</span> -t <span class="string">&quot;;&quot;</span> -k 3 -n -r fruit.txt</span><br></pre></td></tr></table></figure>
<pre><code>banana;30;5.5
orange;20;3.4
apple;10;2.5
pear;90;2.3
</code></pre><p>具体的参数说明：</p>
<ul>
<li><code>-t</code>：列分隔符</li>
<li><code>-k</code>：取第3列作为排序键</li>
<li><code>-n</code>：根据数值大小排序，默认是字符ASCII大小排序</li>
<li><code>-r</code>：逆序输出</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/51linux/archive/2012/05/23/2515299.html" >linux sort 命令详解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.runoob.com/linux/linux-comm-sort.html" >Linux sort 命令<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux中[]和[[]]区别</title>
    <url>/2022/08/27/Linux%E4%B8%AD-%E5%92%8C-%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h2 id=""><a href="#" class="headerlink" title="[]"></a>[]</h2><ul>
<li>比较运算符只有==和!=，两者都是用于字符串比较的，不可用于整数比较，整数比较只能使用-eq，-gt这种形式。无论是字符串比较还是整数比较都不支持&gt;&lt;。如果实在想用，对于字符串比较可以使用转义形式，如果比较”ab”和”bc”：[ ab \&lt; bc ]。使用-a 和-o 表示逻辑与和逻辑或。</li>
</ul>
<h2 id="-1"><a href="#-1" class="headerlink" title="[[]]"></a>[[]]</h2><ul>
<li>使用[[ … ]]条件判断结构，而不是[ … ]，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt;和&gt; 操作符能够正常存在于[[ ]]条件判断结构中，但是如果出现在[ ]结构中的话，会报错。比如可以直接使用<code>if [[ $a != 1 &amp;&amp; $a != 2 ]]</code>, 如果不适用双括号, 则为<code>if [ $a -ne 1] &amp;&amp; [ $a != 2 ]</code>或者<code>if [ $a -ne 1 -a $a != 2 ]</code>。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>条件判断中推荐使用[[]]，功能强大，且与编程语言规则保持一致。另外，不管在[]还是[[]]下，=和==等价，推荐使用后者。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/zeweiwu/p/5485711.html" >Shell test 单中括号[] 双中括号[[]] 的区别<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.runoob.com/w3cnote/linux-shell-brackets-features.html" >shell 中各种括号的作用()、(())、[]、[[]]、{}<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux定时任务 - crontab</title>
    <url>/2019/02/19/Linux%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%20-%20crontab/</url>
    <content><![CDATA[<p>Linux系统是由 <code>cron</code> 这个系统服务来控制的。Linux系统上面原本就有非常多的计划性任务，因此这个系统服务是默认启动的。但是使用者也可以设置计划任务，Linux系统提供了控制计划任务的命令：<code>crontab</code></p>
<span id="more"></span>
<h1 id="crond-进程"><a href="#crond-进程" class="headerlink" title="crond 进程"></a><code>crond</code> 进程</h1><p><code>crond</code> 是Linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动 <code>crond</code> 进程，<font color="green"><code>crond</code> 进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。</font></p>
<h1 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h1><p>Linux下的任务调度分为两类：</p>
<ul>
<li><p>系统任务调度：系统周期性所要执行的工作。</p>
<ul>
<li><p>常见的系统工作有：</p>
<ul>
<li>写缓存数据到硬盘</li>
<li>日志清理等</li>
</ul>
</li>
<li><p>全局配置文件( <code>/etc</code> 目录 )</p>
<ul>
<li><code>cron.d</code> : 系统自动定期执行的任务。</li>
<li><code>crontab</code> : 设定定时任务执行文件。</li>
<li><code>cron.deny</code> : 用于控制不让哪些用户使用 <code>crontab</code> 的功能。</li>
<li><code>cron.hourly</code> : 每小时执行一次的任务。</li>
<li><code>cron.daily</code> : 每天执行一次的任务。</li>
<li><code>cron.weekly</code> : 每周执行一次的任务。</li>
<li><code>cron.monthly</code> : 每个月执行一次的任务。</li>
</ul>
</li>
<li><p><code>/etc/crontab</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SHELL=/bin/bash</span><br><span class="line">PATH=/sbin:/bin:/usr/sbin:/usr/bin</span><br><span class="line">MAILTO=root</span><br><span class="line"></span><br><span class="line"><span class="comment"># For details see man 4 crontabs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Example of job definition:</span></span><br><span class="line"><span class="comment"># .---------------- minute (0 - 59)</span></span><br><span class="line"><span class="comment"># |  .------------- hour (0 - 23)</span></span><br><span class="line"><span class="comment"># |  |  .---------- day of month (1 - 31)</span></span><br><span class="line"><span class="comment"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</span></span><br><span class="line"><span class="comment"># |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</span></span><br><span class="line"><span class="comment"># |  |  |  |  |</span></span><br><span class="line"><span class="comment"># *  *  *  *  * user-name  command to be executed</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>SHELL</code> 指定来系统要使用哪个shell，这里是bash。</li>
<li><code>PATH</code> 指定系统执行命令的路径。</li>
<li><code>MAILTO</code> 指定crond的任务执行信息将通过电子邮件发送给root用户。</li>
</ul>
</li>
</ul>
</li>
<li><p>用户任务调度：用户定期要执行的工作。</p>
<ul>
<li>常见的用户工作有：<ul>
<li>数据备份</li>
<li>定时邮件提醒等</li>
</ul>
</li>
<li>所有用户定义的 <code>crontab</code> 文件都被保存在 <code>/var/spool/cron</code> 目录中。其文件名与用户名一致。每个用户都有自己的 <code>cron</code> 配置文件,通过 <code>crontab -e</code> 就可以编辑,一般情况下我们编辑好用户的 <code>cron</code> 配置文件保存退出后,系统会自动就存放于 <code>/var/spool/cron/</code> 目录中,文件以用户名命名。Linux的 <code>crond</code> 进程是每隔一分钟去读取一次 <code>/var/spool/cron</code> , <code>/etc/crontab</code> , <code>/etc/cron.d</code> 下面所有的内容。</li>
</ul>
</li>
</ul>
<h1 id="crontab-格式说明"><a href="#crontab-格式说明" class="headerlink" title="crontab 格式说明"></a><code>crontab</code> 格式说明</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># .---------------- minute (0 - 59)</span></span><br><span class="line"><span class="comment"># |  .------------- hour (0 - 23)</span></span><br><span class="line"><span class="comment"># |  |  .---------- day of month (1 - 31)</span></span><br><span class="line"><span class="comment"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</span></span><br><span class="line"><span class="comment"># |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</span></span><br><span class="line"><span class="comment"># |  |  |  |  |</span></span><br><span class="line"><span class="comment"># *  *  *  *  * user-name  command to be executed</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>minute</code> : 表示分钟，可以是从0到59之间的任何整数。</p>
</li>
<li><p><code>hour</code> : 表示小时，可以是从0到23之间的任何整数。</p>
</li>
<li><p><code>day</code> : 表示日期，可以是从1到31之间的任何整数。</p>
</li>
<li><p><code>month</code> : 表示月份，可以是从1到12之间的任何整数。</p>
</li>
<li><p><code>week</code> : 表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。</p>
</li>
<li><p><code>command</code> : 要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。</p>
</li>
</ul>
<p>在以上各个字段中，还可以使用以下特殊字符：</p>
<ul>
<li><p><code>*</code> : 代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。</p>
</li>
<li><p><code>,</code> : 可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”</p>
</li>
<li><p><code>-</code> : 可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”</p>
</li>
<li><p><code>/</code> : 可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。</p>
</li>
</ul>
<h1 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h1><ol>
<li>在 <code>/root</code> 编写一个 <code>hello.py</code> 脚本，内容如下：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello World&quot;</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>在 <code>/root</code> 编写一个 <code>test.sh</code> 脚本，内容如下：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">python /root/hello.py &gt;&gt; test.txt</span><br></pre></td></tr></table></figure>
<ol>
<li>输入 <code>crontab -e</code> 命令进入编辑模式，编辑内容如下：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">*/5 * * * * /root/test.sh</span><br></pre></td></tr></table></figure>
<ol>
<li><code>crontab -l</code> 和 <code>crontab -r</code> 分别可以列出当前用户定时任务和删除当前用户定时任务</li>
</ol>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><ol>
<li><code>* * * * * commad</code> -&gt; 每一分钟执行一次command</li>
<li><code>3,15 8-11 */2 * * /etc/init.d/network restart</code> -&gt; 每隔两天的上午8点到11店的第3和第15分钟执行 <code>/etc/init.d/network restart</code></li>
</ol>
<h1 id="踩点"><a href="#踩点" class="headerlink" title="踩点"></a>踩点</h1><ol>
<li>有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。在crontab文件中定义多个调度任务时，需要特别注意的一个问题就是环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这 样，系统执行任务调度时就没有问题了。不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shell脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。例如上文的：<font color="color">#!/bin/bash</font></li>
<li>新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。</li>
</ol>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/intval/p/5763929.html" >Linux定时任务Crontab命令详解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/zoulongbin/p/6187238.html" >Linux 定时任务crontab_014<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>定时任务</tag>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux常用命令示例</title>
    <url>/2022/08/07/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E7%A4%BA%E4%BE%8B/</url>
    <content><![CDATA[<p>记录一下Linux常用命令的使用示例：</p>
<span id="more"></span>
<h2 id="du"><a href="#du" class="headerlink" title="du"></a>du</h2><ul>
<li>显示某目录下各文件夹大小<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">du</span> -h –-max-depth=1 ./</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><p><code>awk</code> 高阶使用较为复杂，这里记录一些例子：</p>
<h3 id="提取出文件路径中以特定字符串开头的字段"><a href="#提取出文件路径中以特定字符串开头的字段" class="headerlink" title="提取出文件路径中以特定字符串开头的字段"></a>提取出文件路径中以特定字符串开头的字段</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;/user/wl_0/sdf/wl_1/gf&quot;</span> | awk -v FS=<span class="string">&quot;/&quot;</span> <span class="string">&#x27;&#123; for (i = 1; i &lt;= NF; ++i) &#123;if ( $i ~ /^wl/ ) print $i&#125; &#125;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>结果：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wl_0</span><br><span class="line">wl_1</span><br></pre></td></tr></table></figure></p>
<h3 id="显示一行有多少列"><a href="#显示一行有多少列" class="headerlink" title="显示一行有多少列"></a>显示一行有多少列</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">head</span> -n 1 filename | awk -F <span class="string">&#x27;\t&#x27;</span> <span class="string">&#x27;&#123;print NF&#125;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="对某列求和"><a href="#对某列求和" class="headerlink" title="对某列求和"></a>对某列求和</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第一列</span></span><br><span class="line">awk <span class="string">&#x27;&#123;sum += $1&#125;;END &#123;print sum&#125;&#x27;</span> filename</span><br></pre></td></tr></table></figure>
<h3 id="关于-BEGIN-和-END-使用"><a href="#关于-BEGIN-和-END-使用" class="headerlink" title="关于 BEGIN 和 END 使用"></a>关于 <code>BEGIN</code> 和 <code>END</code> 使用</h3><p><a class="link"   href="https://www.linuxprobe.com/awk-begin-end.html" >如何使用 awk 的特殊模式 BEGIN 与 END<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="find"><a href="#find" class="headerlink" title="find"></a>find</h2><ul>
<li>查找指定目录及子目录下特定文件名的位置：<code>find ./ -name db.json</code><ul>
<li>模糊匹配：<code>find ./ -name &quot;*.json&quot;</code></li>
</ul>
</li>
</ul>
<h2 id="shuf"><a href="#shuf" class="headerlink" title="shuf"></a>shuf</h2><ul>
<li>随机抽取文件的10行<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">shuf</span> -n10 filename</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Bash</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux开机自启动</title>
    <url>/2019/07/01/Linux%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8/</url>
    <content><![CDATA[<p>记录一下Linux开机自启动的原理与实践。</p>
<span id="more"></span>
<blockquote>
<p>实验环境：CentOS7</p>
</blockquote>
<h1 id="etc-init-d"><a href="#etc-init-d" class="headerlink" title="/etc/init.d"></a><code>/etc/init.d</code></h1><p>这是指向 <code>/etc/rc.d/init.d</code> 的软链接。这个目录存放的是一些脚本，一般是linux以rpm包安装时设定的一些服务的启动脚本。系统在安装时装了很多rpm包，这里面就有很多对应的脚本。执行这些脚本可以用来<code>start || stop || reload || status || restart</code> 这些服务。举个例子来说，如果你要重新启动 sendmail 的话，而且你的 sendmail 是以 rpm 来安装的，那么使用 <code>/etc/rc.d/init.d/sendmail restart</code> 就可以直接启动 sendmail 了。</p>
<h2 id="runlevel"><a href="#runlevel" class="headerlink" title="runlevel"></a><code>runlevel</code></h2><p><code>/etc/rc.d/init.d/</code> 这个目录下的脚本就类似与windows中的注册表，在系统启动的时候执行。程序运行到这里(init进程读取了运行级别)， 是该运行init.d里的脚本了，但是并不是直接运行，而是有选择的。因为系统并不需要启动所有的服务。系统是如何选择哪些需要启动哪些不要呢？这时 <code>runlevel</code> 就起作用了。在RH9和FC7的源码中它都是一开始就 <code>check_runlevel()</code> ，知道了运行级别之后，对于每一个运行级别，在 <code>/etc/rc.d/</code> 下都有一个子目录分别是 <code>rc0.d, rc1.d ... rc6.d</code> 。每个目录下都是到 <code>/etc/rc.d/init.d/</code> 目录的一部分脚本一些软链接。每个级别要执行哪些服务就在相对应的目录下，比如级别6要启动的服务就都放在rc6.d下，但是放在这个rc6.d下的都是一些软链接文件，链接到 <code>/etc/rc.d/init.d/</code> 中相对应的文件，真正干活的是 <code>/etc/rc.d/init.d/</code> 里的脚本。<br><img   src="/2019/07/01/Linux%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8/1.png"  class="1"></p>
<p>(<code>/etc/</code> 下的 <code>rc0.d, rc1.d ... rc6.d</code> 是指向 <code>/etc/rc.d/</code> 下的 <code>rc0.d, rc1.d ... rc6.d</code> 的软链接。)</p>
<h2 id="KS"><a href="#KS" class="headerlink" title="KS"></a><code>KS</code></h2><p>如上图所示，你会发现许多 <code>rc#.d</code> 形式存在的目录，这里 <code>#</code> 代表一个指定的初始化级别，范围是0~6）:</p>
<ul>
<li><strong><em>0</em></strong>：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动</li>
<li><strong><em>1</em></strong>：单用户工作状态，root权限，用于系统维护，禁止远程登陆</li>
<li><strong><em>2</em></strong>：多用户状态(没有NFS)</li>
<li><strong><em>3</em></strong>：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式</li>
<li><strong><em>4</em></strong>：系统未使用，保留</li>
<li><strong><em>5</em></strong>：X11控制台，登陆后进入图形GUI模式</li>
<li><strong><em>6</em></strong>：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动</li>
</ul>
<p>在这些目录之下，包含了许多对进程进行控制的脚本。这些脚本要么以 <code>K##</code> 开头，要么以 <code>S##</code> 开头：</p>
<ul>
<li><strong><em>K</em></strong>：kill，系统将终止对应的服务</li>
<li><strong>S</strong>：start，系统将启动对应的服务</li>
<li><strong>##</strong>：同一运行级别下脚本执行的顺序，数值小的先执行，数值大的后执行。很多时候这些执行顺序是很重要的，比如要启动Apache服务，就必须先配置网络接口。</li>
</ul>
<h1 id="etc-rc-local"><a href="#etc-rc-local" class="headerlink" title="/etc/rc.local"></a><code>/etc/rc.local</code></h1><p>这是指向 <code>/etc/rc.d/rc.local</code> 的软链接。这是使用者自订开机启动程序,把需要开机自动运行的程序写在这个脚本里。也就是说，我有任何想要在开机时就进行的工作时，直接将他写入 <code>/etc/rc.d/rc.local</code> ， 那么该工作就会在开机的时候自动被载入。这一点和windows里面的“启动”菜单有点像。该脚本是在系统初始化级别脚本运行之后再执行的，因此可以安全地在里面添加你想在系统启动之后执行的脚本。常见的情况是开机自启动 <code>mongod</code> 服务：<br><img   src="/2019/07/01/Linux%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8/2.png"  class="2"></p>
<h1 id="开机自启动"><a href="#开机自启动" class="headerlink" title="开机自启动"></a>开机自启动</h1><p>常见的开机自启动有两种方法：</p>
<ul>
<li>通过将 shell script写入到 <code>/etc/rc.d/rc.local</code></li>
<li>通过 <code>/etc/rc.d/init.d/</code>。假设 <code>/etc/rc.d/init.d/radisd</code> 已存在，文件内容有两行注释如下：</li>
</ul>
<img   src="/2019/07/01/Linux%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8/3.png"  class="3">
<p>表示 <code>redisd</code> 的运行级别是 <code>2 3 4 5</code>，各级别 <strong><em>S(2 3 4 5)</em></strong> 分数为90，<strong><em>K(0 6)</em></strong> 分数为10。<code>chkconfig redisd on</code> 即可将 <code>redisd</code> 添加到系统服务中。这样我们以后就可以通过 <code>systemctl start redisd || systemctl stop redisd</code> 来启动和关闭 <code>redisd</code> 服务。同样的，在 <code>/etc/rc.d/rc(2\3\4\5).d</code> 文件夹下就会有 <code>S90redisd</code> 软链接到 <code>/etc/rc.d/init.d/redisd</code> ，在 <code>/etc/rc.d/rc(0\6).d</code> 文件夹下就会有 <code>K10redisd</code> 软链接到 <code>/etc/rc.d/init.d/redisd</code> 。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/songpeiying/article/details/79933181" >查看Linux服务器的运行级别<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/fatt/p/4790561.html" >/etc/rc.local 与 /etc/init.d Linux 开机自动运行程序<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux引号</title>
    <url>/2022/08/08/Linux%E5%BC%95%E5%8F%B7/</url>
    <content><![CDATA[<p>Linux的引号分为单引号、双引号、反引号三种。</p>
<span id="more"></span>
<h2 id="单引号"><a href="#单引号" class="headerlink" title="单引号"></a>单引号</h2><p>被单引号包括的字符串被看作是普通字符串，不会对特殊字符进行转义：<br><img   src="/2022/08/08/Linux%E5%BC%95%E5%8F%B7/single.jpg"  class="single"></p>
<h2 id="双引号"><a href="#双引号" class="headerlink" title="双引号"></a>双引号</h2><p>被双引号包括的字符会进行转义：<br><img   src="/2022/08/08/Linux%E5%BC%95%E5%8F%B7/double.jpg"  class="double"></p>
<h2 id="反引号"><a href="#反引号" class="headerlink" title="反引号"></a>反引号</h2><p>如果需要执行是shell命令的字符串，则使用反引号：<br><img   src="/2022/08/08/Linux%E5%BC%95%E5%8F%B7/reverse.jpg"  class="reverse"></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="http://c.biancheng.net/view/951.html" >Shell（Bash）单引号、双引号和反引号用法详解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux条件判断X的作用</title>
    <url>/2022/08/04/Linux%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%ADX%E7%9A%84%E4%BD%9C%E7%94%A8/</url>
    <content><![CDATA[<p>在shell脚本中经常遇到这样的条件判断：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [[ <span class="string">&quot;X&quot;</span><span class="variable">$&#123;var&#125;</span> == <span class="string">&quot;X&quot;</span> ]]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;null&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<p>主要是用来判断变量 <code>var</code> 是否为空。</p>
<p>如果不加 <code>&quot;X&quot;</code>，判断 <code>var</code> 是否等于某一个值，比如 <code>&quot;0&quot;</code> ，一旦出现 <code>var</code> 为空或者未设置，那么条件表达式就为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [[  == <span class="string">&quot;0&quot;</span> ]]</span><br></pre></td></tr></table></figure><br>语法错误。加上 <code>&quot;X&quot;</code>后就可以避免此错误。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/readnap/article/details/105047518" >shell if [ “x${var}” == “x” ]中x的作用<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux魔法变量</title>
    <url>/2022/08/07/Linux%E9%AD%94%E6%B3%95%E5%8F%98%E9%87%8F/</url>
    <content><![CDATA[<p>在shell编程中经常遇到一些魔法变量，这里列举一下：</p>
<span id="more"></span>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">变量</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>$$</code></td>
<td style="text-align:center">当前进程id</td>
</tr>
<tr>
<td style="text-align:center"><code>$0</code></td>
<td style="text-align:center">当前脚本的名称</td>
</tr>
<tr>
<td style="text-align:center"><code>$1</code></td>
<td style="text-align:center">当前脚本的第一个参数</td>
</tr>
<tr>
<td style="text-align:center"><code>NF</code></td>
<td style="text-align:center">与<code>awk</code>配合使用，当前行的参数个数</td>
</tr>
<tr>
<td style="text-align:center"><code>$NF</code></td>
<td style="text-align:center">与<code>awk</code>配合使用，当前行的最后参数</td>
</tr>
<tr>
<td style="text-align:center"><code>$?</code></td>
<td style="text-align:center">上个命令的退出状态，或函数的返回值</td>
</tr>
<tr>
<td style="text-align:center"><code>$#</code></td>
<td style="text-align:center">当前脚本的参数个数（不包含脚本名称）</td>
</tr>
<tr>
<td style="text-align:center"><code>$@</code></td>
<td style="text-align:center">当前脚本的所有参数（一个列表）</td>
</tr>
<tr>
<td style="text-align:center"><code>$*</code></td>
<td style="text-align:center">当前脚本的所有参数（一个值）</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>M1 Mac安装Homebrew</title>
    <url>/2024/09/10/M1-Mac%E5%AE%89%E8%A3%85Homebrew/</url>
    <content><![CDATA[<p>Homebrew对ARM芯片的Mac支持不友好，这里切换到国内镜像网站安装，速度快且稳定，没有乱七八糟的报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/bin/bash -c <span class="string">&quot;<span class="subst">$(curl -fsSL https://gitee.com/ineo6/homebrew-install/raw/master/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://brew.idayer.com/" >Homebrew 中文网<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>Homebrew</tag>
        <tag>ARM</tag>
      </tags>
  </entry>
  <entry>
    <title>MFU</title>
    <url>/2026/01/25/MFU/</url>
    <content><![CDATA[<p><strong>MFU（Model FLOPs Utilization，模型浮点运算利用率）</strong> 是衡量深度神经网络（DNN）在训练或推理过程中<strong>硬件计算效率</strong>的关键指标。它回答了一个核心问题：</p>
<blockquote>
<p><strong>“我们的 GPU/TPU 算力，到底有多少真正用在了模型计算上？”</strong></p>
</blockquote>
<span id="more"></span>
<hr>
<h2 id="1-MFU-的定义"><a href="#1-MFU-的定义" class="headerlink" title="1. MFU 的定义"></a>1. MFU 的定义</h2><p>MFU 表示模型在实际运行中<strong>有效利用的浮点运算比例</strong>，相对于硬件理论上能够提供的最大浮点运算能力（FLOPS）：</p>
<script type="math/tex; mode=display">
\text{MFU} = \frac{\text{实际执行的有效 FLOPs/s}}{\text{硬件理论峰值 FLOPs/s}}</script><ul>
<li><p><strong>分子（有效 FLOPs/s）</strong>：<br>模型一次前向（或前向+反向）传播所需的浮点运算量（由模型结构精确计算），除以实际耗时。</p>
</li>
<li><p><strong>分母（峰值 FLOPS）</strong>：<br>所用硬件在理想情况下的最大计算吞吐（例如 NVIDIA A100 在 FP16 下约为 $3.12 \times 10^{14}$ FLOPS）。</p>
</li>
</ul>
<p>✅ <strong>MFU 越高，说明硬件资源越高效地用于“有意义的计算”</strong>，而非浪费在通信、内存瓶颈或调度开销上。</p>
<blockquote>
<p>💡 <strong>注意</strong>：本文以<strong>前向传播</strong>为例计算 MFU，这是评估推理或训练吞吐时的常见简化场景。</p>
</blockquote>
<hr>
<h2 id="2-实例演示：三层-MLP-的-MFU-计算"><a href="#2-实例演示：三层-MLP-的-MFU-计算" class="headerlink" title="2. 实例演示：三层 MLP 的 MFU 计算"></a>2. 实例演示：三层 MLP 的 MFU 计算</h2><p>考虑一个简单的 3 层全连接网络（MLP）：</p>
<ul>
<li><strong>层 1</strong>：$1024 \rightarrow 4096$  </li>
<li><strong>层 2</strong>：$4096 \rightarrow 4096$  </li>
<li><strong>层 3</strong>：$4096 \rightarrow 1024$  </li>
<li><strong>批量大小（Batch Size）</strong>：$B = 64$  </li>
<li><strong>硬件</strong>：NVIDIA A100（FP16 峰值算力 = $3.12 \times 10^{14}$ FLOPS）</li>
</ul>
<h4 id="2-1-计算前向传播-FLOPs"><a href="#2-1-计算前向传播-FLOPs" class="headerlink" title="2.1 计算前向传播 FLOPs"></a>2.1 计算前向传播 FLOPs</h4><p>单层 MLP 前向传播的 FLOPs 公式为：</p>
<script type="math/tex; mode=display">
\text{FLOPs}_{\text{layer}} = 2 \times B \times d_{\text{in}} \times d_{\text{out}}</script><blockquote>
<p>注：乘加操作（multiply-add）计为 2 次浮点运算。</p>
</blockquote>
<p>逐层计算：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>层</th>
<th>计算式</th>
<th>FLOPs</th>
</tr>
</thead>
<tbody>
<tr>
<td>层 1</td>
<td>$2 \times 64 \times 1024 \times 4096$</td>
<td>536,870,912</td>
</tr>
<tr>
<td>层 2</td>
<td>$2 \times 64 \times 4096 \times 4096$</td>
<td>2,147,483,648</td>
</tr>
<tr>
<td>层 3</td>
<td>$2 \times 64 \times 4096 \times 1024$</td>
<td>536,870,912</td>
</tr>
</tbody>
</table>
</div>
<p><strong>总 FLOPs</strong>：</p>
<script type="math/tex; mode=display">
536.87\,\text{M} + 2,147.48\,\text{M} + 536.87\,\text{M} = 3,221.22\,\text{M} = 3.221 \times 10^9\ \text{FLOPs}</script><h4 id="2-2-计算-MFU"><a href="#2-2-计算-MFU" class="headerlink" title="2.2 计算 MFU"></a>2.2 计算 MFU</h4><p>假设该前向传播在 A100 上耗时极短（理想情况下仅受算力限制），则理论最大 FLOPs/s 为 $3.12 \times 10^{14}$。</p>
<p>但即使我们<strong>瞬时完成</strong>这次前向（即耗时趋近于 0），MFU 仍由单位时间内的有效算力决定。若以“单次前向”为单位，则：</p>
<script type="math/tex; mode=display">
\text{MFU} = \frac{3.221 \times 10^9}{3.12 \times 10^{14}} \approx 0.001\%</script><blockquote>
<p>🔥 <strong>结论</strong>：这个小模型的 MFU 极低！<br>原因：A100 的算力极其强大（312 TFLOPS），而小型 MLP 的计算量太小，无法“喂饱”GPU。</p>
</blockquote>
<hr>
<h2 id="3-如何提高-MFU？"><a href="#3-如何提高-MFU？" class="headerlink" title="3. 如何提高 MFU？"></a>3. 如何提高 MFU？</h2><p>以下策略可显著提升 MFU，尤其在大模型训练中至关重要：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>优化方向</th>
<th>具体方法</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>增加模型参数</strong></td>
<td>提高模型单位时间内的计算量</td>
</tr>
<tr>
<td><strong>增大 Batch Size</strong></td>
<td>提高计算密度，摊薄通信与 kernel 启动开销</td>
</tr>
<tr>
<td><strong>高效并行策略</strong></td>
<td>结合 Tensor Parallelism、Pipeline Parallelism、ZeRO 等</td>
</tr>
<tr>
<td><strong>Kernel 优化</strong></td>
<td>使用 fused kernels、FlashAttention、cuBLASLt 等</td>
</tr>
<tr>
<td><strong>减少通信瓶颈</strong></td>
<td>利用 NVLink/InfiniBand，重叠计算与通信（如梯度同步）</td>
</tr>
<tr>
<td><strong>混合精度训练</strong></td>
<td>使用 FP16/BF16，既节省显存又提升 FLOPS 利用率</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>📌 <strong>行业参考值</strong>：  </p>
<ul>
<li>LLaMA 训练 MFU：30%–40%  </li>
<li>PaLM 训练 MFU：约 46%  </li>
<li>高度优化系统：可达 50%+（但极少超过 60%）</li>
</ul>
</blockquote>
<hr>
<h2 id="4-MFU-vs-GPU-利用率：本质区别"><a href="#4-MFU-vs-GPU-利用率：本质区别" class="headerlink" title="4. MFU vs GPU 利用率：本质区别"></a>4. MFU vs GPU 利用率：本质区别</h2><p>很多人误将 <code>nvidia-smi</code> 中的 <strong>GPU-Util%</strong> 当作性能指标，但它与 MFU <strong>完全不同</strong>，甚至可能产生误导。</p>
<h4 id="4-1-核心对比"><a href="#4-1-核心对比" class="headerlink" title="4.1 核心对比"></a>4.1 核心对比</h4><div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th><strong>MFU</strong></th>
<th><strong>GPU 利用率（GPU-Util%）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>衡量对象</strong></td>
<td>模型计算效率（有效 FLOPs / 峰值 FLOPs）</td>
<td>GPU 是否“有事做”（任意引擎活跃时间占比）</td>
</tr>
<tr>
<td><strong>关注点</strong></td>
<td><strong>有意义的浮点运算</strong>是否被充分利用</td>
<td>GPU 的任一单元（SM、Copy Engine、解码器等）是否忙碌</td>
</tr>
<tr>
<td><strong>是否反映模型效率</strong></td>
<td>✅ 是</td>
<td>❌ 否（可能“忙但无效”）</td>
</tr>
<tr>
<td><strong>受通信/IO 影响</strong></td>
<td>✅ 是（通信不产生 FLOPs）</td>
<td>❌ 否（拷贝数据也会拉高利用率）</td>
</tr>
</tbody>
</table>
</div>
<h4 id="4-2-为什么-GPU-Util-很“虚”？"><a href="#4-2-为什么-GPU-Util-很“虚”？" class="headerlink" title="4.2 为什么 GPU-Util% 很“虚”？"></a>4.2 为什么 GPU-Util% 很“虚”？</h4><p><code>nvidia-smi</code> 的 GPU-Util% 定义为：  </p>
<blockquote>
<p>“在过去采样周期内，GPU 上<strong>至少有一个引擎处于活跃状态</strong>的时间比例。”</p>
</blockquote>
<p>这意味着：</p>
<ul>
<li>即使 GPU 只在做 <strong>CPU→GPU 数据拷贝</strong>（无任何矩阵运算），Util% 也可能显示 <strong>90%+</strong>。</li>
<li>小 batch 导致频繁 kernel 启动和空闲，Util% 仍可能“看起来不错”。</li>
</ul>
<h4 id="📌-场景举例"><a href="#📌-场景举例" class="headerlink" title="📌 场景举例"></a>📌 场景举例</h4><blockquote>
<p><strong>情况</strong>：训练大模型，batch size 太小，每次前向仅 1ms，随后等待 9ms 进行 All-Reduce 同步。  </p>
<ul>
<li><strong>GPU-Util%</strong>：≈60%（采样捕捉到计算片段）  </li>
<li><strong>MFU</strong>：&lt;10%（90% 时间未执行有效 FLOPs）</li>
</ul>
<p><strong>情况</strong>：内存带宽受限的 wide MLP  </p>
<ul>
<li><strong>GPU-Util%</strong>：80%（memory controller 忙）  </li>
<li><strong>MFU</strong>：15%（SM 单元“饿死”，无法满负荷计算）</li>
</ul>
</blockquote>
<h4 id="4-3-一个形象比喻"><a href="#4-3-一个形象比喻" class="headerlink" title="4.3 一个形象比喻"></a>4.3 一个形象比喻</h4><ul>
<li><p><strong>GPU 利用率</strong> ≈ “工厂的灯是不是亮着”<br>→ 灯亮 ≠ 在生产，可能只是有人在擦地板。</p>
</li>
<li><p><strong>MFU</strong> ≈ “生产线每小时产出 / 理论最大产能”<br>→ 直接衡量<strong>有效产出效率</strong>。</p>
</li>
</ul>
<hr>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><blockquote>
<p>✅ <strong>MFU 是衡量“算力是否被有效用于模型计算”的黄金指标</strong>。<br>✅ 高 MFU = 高训练效率 = 低成本。<br>✅ 在千亿参数模型时代，<strong>提升几个百分点的 MFU，可能节省数百万美元训练费用</strong>。</p>
<p>❌ 不要被 <code>nvidia-smi</code> 的 GPU-Util% 迷惑——它告诉你 GPU “在忙”，但没说“忙得值不值”。</p>
</blockquote>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>MFU</tag>
      </tags>
  </entry>
  <entry>
    <title>MIND解读</title>
    <url>/2024/02/28/MIND%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>这篇paper的核心是胶囊网络，该网络采用了动态路由算法自动对用户历史行为序列进行聚类，提取出多个兴趣向量，代表用户的不同兴趣。当用户再有新的交互时，通过胶囊网络，还能实时的改变用户的兴趣表示向量，做到在召回阶段的实时个性化。</p>
<span id="more"></span>
<h2 id="前置知识-胶囊网络"><a href="#前置知识-胶囊网络" class="headerlink" title="前置知识-胶囊网络"></a>前置知识-胶囊网络</h2><p>本质上就是个聚类网络，将输入的多个向量聚类输出多个向量：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/20240228/image.4666l530vwo0.png"  alt="capsule"></p>
<p>算法迭代流程：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.7w6k34geqa.webp"  alt="algo"></p>
<h2 id="MIND模型"><a href="#MIND模型" class="headerlink" title="MIND模型"></a>MIND模型</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.syonic1kc.webp"  alt="MIND"></p>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>用户向量（item向量对用户多个兴趣向量的weighted sum）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\overrightarrow{\boldsymbol{v}}_u & =\text { Attention }\left(\overrightarrow{\boldsymbol{e}}_i, \mathrm{~V}_u, \mathrm{~V}_u\right) \\
& =\mathrm{V}_u \operatorname{softmax}\left(\operatorname{pow}\left(\mathrm{V}_u^{\mathrm{T}} \overrightarrow{\boldsymbol{e}}_i, p\right)\right)
\end{aligned}</script><p>loss：</p>
<script type="math/tex; mode=display">
\operatorname{Pr}(i \mid u)=\operatorname{Pr}\left(\overrightarrow{\boldsymbol{e}}_i \mid \overrightarrow{\boldsymbol{v}}_u\right)=\frac{\exp \left(\overrightarrow{\boldsymbol{v}}_u^{\mathrm{T}} \overrightarrow{\boldsymbol{e}}_i\right)}{\sum_{j \in I} \exp \left(\overrightarrow{\boldsymbol{v}}_u^{\mathrm{T}} \overrightarrow{\boldsymbol{e}}_j\right)}</script><p>Then, the overall objective function for training MIND is</p>
<script type="math/tex; mode=display">
L=\sum_{(u, i) \in \mathcal{D}} \log \operatorname{Pr}(i \mid u)</script><p>由于$\mathcal{D}$数量太大，团队采用了负采样技术</p>
<h4 id="在线服务"><a href="#在线服务" class="headerlink" title="在线服务"></a>在线服务</h4><p>同时利用MIND模型产出的多个兴趣向量进行ann检索召回，最终排序得到topK个商品</p>
<h2 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h2><ol>
<li>如何确定兴趣胶囊的数量？</li>
</ol>
<p>团队用了一种启发式方式自适应调整聚类中心的数量：</p>
<script type="math/tex; mode=display">
K_u^{\prime}=\max \left(1, \min \left(K, \log _2\left(\left|\mathcal{I}_u\right|\right)\right)\right)</script><p>该超参对实验影响多大，团队并没有在这上面进行深入实验</p>
<ol>
<li>既然胶囊网络是为了聚类，为什么不直接使用k-means方法？</li>
<li>论文说当用户有新的交互时，通过胶囊网络，还能实时的改变用户的兴趣表示向量，做到在召回阶段的实时个性化。但如果是用户兴趣发生变化了呢？比如之前有两个兴趣胶囊（体育、旅游），现在用户多了个兴趣（数码），那就要新增一个兴趣胶囊，等于模型要重新训练？</li>
</ol>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://github.com/datawhalechina/fun-rec/blob/master/docs/ch02/ch2.1/ch2.1.4/MIND.md" >fun-rec/MIND<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>胶囊网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Shared-Bottom &amp; MoE &amp; MMoE</title>
    <url>/2025/05/18/MOE-MMOE/</url>
    <content><![CDATA[<p>Shared-Bottom、MoE、MMoE的结构对比如下：</p>
<span id="more"></span>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.6bhar8rvjy.webp"  alt="model"></p>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>Multi-Task</tag>
      </tags>
  </entry>
  <entry>
    <title>MR编程注意事项</title>
    <url>/2022/09/06/MR%E7%BC%96%E7%A8%8B%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</url>
    <content><![CDATA[<p>在公司集群上跑MapReduce的时候会遇到一些异常报错，主要还是我们编程时没注意极端情况，想当然的认为没有bug就能顺利运行。以下列举几种例子：</p>
<span id="more"></span>
<h2 id="Reduce卡在某个进度"><a href="#Reduce卡在某个进度" class="headerlink" title="Reduce卡在某个进度"></a>Reduce卡在某个进度</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">	System.out.prinltn(<span class="string">&quot;Hello World&quot;</span>);</span><br><span class="line">    <span class="comment">// String[] arr = iterator.next().toString().split(&quot;\t&quot;);</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这是因为没有进行 <code>iterator.next()</code> 操作，导致程序陷入死循环。如果其中还有写数据的逻辑，那么可能导致磁盘空间紧张。</p>
<h2 id="Inner-error-IOException"><a href="#Inner-error-IOException" class="headerlink" title="Inner error, IOException"></a>Inner error, IOException</h2><p>如果单独拉一个part下来能测试通过，但在集群上老是报上述错误，那么有两种情况：</p>
<ul>
<li>相同key下的value内的元素过多，有千万个</li>
<li>不同的key太多，有千万个</li>
</ul>
<p>上述两种情况不一定会触发异常报错，但如果出现了，请从这两个方面排查。</p>
<h2 id="程序没报异常，但是failed-with-code-137"><a href="#程序没报异常，但是failed-with-code-137" class="headerlink" title="程序没报异常，但是failed with code 137"></a>程序没报异常，但是failed with code 137</h2><p>这是因为reduce阶段iterator内的元素个数太多，导致内存溢出，解决方法是设置更大的内存或者设置多个key来均匀分布value。</p>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
        <tag>Big Data</tag>
      </tags>
  </entry>
  <entry>
    <title>Mixtral MoE代码解读</title>
    <url>/2024/08/06/Mixtral-Moe%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>一直对稀疏专家网络好奇，有些专家没被选中，那么梯度是否为0，这一轮被选中有梯度，下一轮没被选中无梯度，模型可以训练收敛吗？</p>
<span id="more"></span>
<ul>
<li>由于每个token都会选择topk个专家，所以在每一轮epoch中，所有专家都参与了前向传播，所以梯度都能得到更新</li>
<li>即使真有专家一直没被选中，那么其梯度保持不变，没有参与更新而已</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取到每个token的mlp层输入特征 </span></span><br><span class="line">batch_size, sequence_length, hidden_dim = hidden_states.shape</span><br><span class="line">hidden_states = hidden_states.view(-<span class="number">1</span>, hidden_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到每个专家的打分，维度是batch * sequence, num_experts，取topk个专家</span></span><br><span class="line">router_logits = self.gate(hidden_states)</span><br><span class="line">routing_weights = F.softmax(router_logits, dim=<span class="number">1</span>, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取到topk个专家的打分，需要计算在归一化一下，用于对后面的expert计算出来的结果进行加权</span></span><br><span class="line">routing_weights /= routing_weights.<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># routing_weights、selected_experts 维度是一致的，取了topk   (bs * sl, topk)</span></span><br><span class="line">routing_weights = routing_weights.to(hidden_states.dtype)</span><br><span class="line"></span><br><span class="line">final_hidden_states = torch.zeros(</span><br><span class="line">            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果不做后面的维度切换，那expert_mask的维度是 (bs*sl, topk, n_experts)，但是后面要遍历n_experts来计算，所以颠倒一下，得到(n_experts, topk, bs * sl); </span></span><br><span class="line">expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> expert_idx <span class="keyword">in</span> <span class="built_in">range</span>(self.num_experts):</span><br><span class="line">    expert_layer = self.experts[expert_idx]</span><br><span class="line">    idx, top_x = torch.where(expert_mask[expert_idx])</span><br><span class="line">    </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    这样取到expert_mask[expert_idx]，从上面的注释可以知道维度是[topk, bs * sl]</span></span><br><span class="line"><span class="string">    torch.where的结果，第一个结果代表选到了哪一行，第二个代表选择了哪一列。对应到实际意义，top_x表示取的列，也就是取哪些token</span></span><br><span class="line"><span class="string">    而行表示，取到的这些token，根据路由gate计算，当前expert是排行第几</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 即当前expert没有被任何一个token选中，运气是真差</span></span><br><span class="line">    <span class="keyword">if</span> top_x.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># tensor index使用list比tensor快</span></span><br><span class="line">    top_x_list = top_x.tolist()</span><br><span class="line">    idx_list = idx.tolist()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前面hidden states已经转成了 [bs * sl, hs]，根据top_x可以找到需要计算的token，这些token依旧是有序的</span></span><br><span class="line">    <span class="comment"># hidden_states[None, top_x_list]的shape是(1, top_x_list.size(), hidden_dim)</span></span><br><span class="line">    current_state = hidden_states[<span class="literal">None</span>, top_x_list].reshape(-<span class="number">1</span>, hidden_dim)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 找到这个expert对应的权重乘进去</span></span><br><span class="line">    <span class="comment"># 上面计算的权重是routing_weights，维度是bs * sl, topk</span></span><br><span class="line">    <span class="comment"># 根据top_x_list对应的token，idx_list表示topk中第几个</span></span><br><span class="line">    <span class="comment"># 可以直接取到相应的权重</span></span><br><span class="line">    current_hidden_states = expert_layer(current_state) * routing_weights[top_x_list, idx_list, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 合到最终的特征里边去</span></span><br><span class="line">    final_hidden_states.index_add_(<span class="number">0</span>, top_x, current_hidden_states.to(hidden_states.dtype))</span><br><span class="line">    </span><br><span class="line">final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)</span><br></pre></td></tr></table></figure>
<h2 id="官方实现"><a href="#官方实现" class="headerlink" title="官方实现"></a>官方实现</h2><p>只能说官方的实现太简洁、优雅了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MoeLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, experts: <span class="type">List</span>[nn.Module], gate: nn.Module, moe_args: MoeArgs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(experts) &gt; <span class="number">0</span></span><br><span class="line">        self.experts = nn.ModuleList(experts)</span><br><span class="line">        self.gate = gate</span><br><span class="line">        self.args = moe_args</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        gate_logits = self.gate(inputs)</span><br><span class="line">        weights, selected_experts = torch.topk(gate_logits, self.args.num_experts_per_tok)</span><br><span class="line">        weights = F.softmax(weights, dim=<span class="number">1</span>, dtype=torch.<span class="built_in">float</span>).to(inputs.dtype)</span><br><span class="line">        results = torch.zeros_like(inputs)</span><br><span class="line">        <span class="keyword">for</span> i, expert <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.experts):</span><br><span class="line">            batch_idx, nth_expert = torch.where(selected_experts == i)</span><br><span class="line">            results[batch_idx] += weights[batch_idx, nth_expert, <span class="literal">None</span>] * expert(inputs[batch_idx])</span><br><span class="line">        <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/NNyyA7zJb5-Su5H353lovw" >理解Mixtral Moe模型原理与代码实现<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Sparse MOE</tag>
      </tags>
  </entry>
  <entry>
    <title>MoCo解读</title>
    <url>/2023/02/01/MoCo%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>MoCo方法由何凯明团队提出，是无监督对比学习的代表作。经过MoCo预训练的视觉表征迁移到各种下游任务时，其效果超过了有监督预训练模型。</p>
<span id="more"></span>
<h2 id="两点创新"><a href="#两点创新" class="headerlink" title="两点创新"></a>两点创新</h2><p>对比学习的思想是将相似的样本距离拉近，不相似的样本距离拉远。对比学习主要在两方面进行设计：</p>
<ul>
<li>代理任务</li>
<li>损失函数</li>
</ul>
<p>MoCo将对比学习当作字典查询任务，在字典中与query匹配的key视为正样本，否则为负样本：</p>
<img   src="/2023/02/01/MoCo%E8%A7%A3%E8%AF%BB/1.png"  class="MoCo">
<p>损失函数InfoNCE为：</p>
<script type="math/tex; mode=display">
\mathcal{L}_q=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\sum_{i=0}^K \exp \left(q \cdot k_i / \tau\right)}</script><p>其中，$\tau$ 是温度系数，该超参设置需要注意。太大会导致query与所有样本的相似度都很接近，太小会导致模型偏向学习区分度高的样本。</p>
<p>上式与多分类交叉熵损失函数非常相似，只不过前者 $K$ 表示样本类别，而后者表示正样本与负样本的总个数。</p>
<h2 id="与传统自监督学习对比"><a href="#与传统自监督学习对比" class="headerlink" title="与传统自监督学习对比"></a>与传统自监督学习对比</h2><img   src="/2023/02/01/MoCo%E8%A7%A3%E8%AF%BB/2.png"  class="2">
<ul>
<li>图(a)中两个编码器同步更新，保证了样本特征的一致性，但负样本个数受限，即使能达到8000多，还是无法放下所有的负样本</li>
<li>图(b)放下了所有的负样本，但bank中不同样本的特征是在不同时刻的编码器下获得的，牺牲了特征的一致性</li>
</ul>
<img   src="/2023/02/01/MoCo%E8%A7%A3%E8%AF%BB/3.png"  class="3">
<ul>
<li>图(c)则是采样了动量更新key编码器的方式，解决了字典大小受限和特征不一致性问题：</li>
</ul>
<script type="math/tex; mode=display">
\theta_{\mathrm{k}} \leftarrow m \theta_{\mathrm{k}}+(1-m) \theta_{\mathrm{q}}</script><h2 id="伪代码解读"><a href="#伪代码解读" class="headerlink" title="伪代码解读"></a>伪代码解读</h2><img   src="/2023/02/01/MoCo%E8%A7%A3%E8%AF%BB/4.png"  class="4">
<ol>
<li>新的batch进行一轮前向传播</li>
<li>更新query编码器参数</li>
<li>动量更新key编码器参数</li>
<li>将该batch放入队列<ul>
<li>虽然同一队列的batch样本表征仍然是不同时刻的key编码器获得，但由于key编码器更新非常缓慢，样本表征的差异可以忽略不计：</li>
<li><img   src="/2023/02/01/MoCo%E8%A7%A3%E8%AF%BB/5.png"  class="5"></li>
</ul>
</li>
<li>将老batch移出队列：这样MoCo就能无限扩展，预训练海量样本</li>
</ol>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h4 id="原始数据集ImageNet"><a href="#原始数据集ImageNet" class="headerlink" title="原始数据集ImageNet"></a>原始数据集ImageNet</h4><img   src="/2023/02/01/MoCo%E8%A7%A3%E8%AF%BB/6.png"  class="6">
<h4 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a>下游任务</h4><img   src="/2023/02/01/MoCo%E8%A7%A3%E8%AF%BB/7.png"  class="7">
<h4 id="与传统自监督学习对比-1"><a href="#与传统自监督学习对比-1" class="headerlink" title="与传统自监督学习对比"></a>与传统自监督学习对比</h4><img   src="/2023/02/01/MoCo%E8%A7%A3%E8%AF%BB/8.png"  class="8">
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ol>
<li>为什么用队列而不是mini-batch？以及为什么使用动量更新的方式？</li>
</ol>
<ul>
<li>MoCo将对比学习看作是一种字典查询的方式，字典的特点就是大，所以得用队列，这样就可以从大量的负样本中学习到好的语义特征。</li>
<li>由于队列非常大，每次step只会补充进一部分样本，且每个step完成之后key编码器都会更新参数，这就导致队列中老样本（对应老的key编码器得到的表征）与新样本的表征丧失了一致性。所以得动量更新，每次key编码器的参数更新幅度都非常小。</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.bilibili.com/video/BV1C3411s7t9/?spm_id_from=333.999.0.0&amp;vd_source=3f2411263f367ccf993c28b58688c0e7" >MoCo 论文逐段精读【论文精读】<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cvmart.net/community/detail/5179" >大概是全网最详细的何恺明团队顶作 MoCo 系列解读！（上）<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>PyTorch</tag>
        <tag>Contrastive Learning</tag>
        <tag>Unsupervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Multi Query Attention &amp; Group Query Attention</title>
    <url>/2023/09/13/Multi-Query-Attention-Group-Query-Attention/</url>
    <content><![CDATA[<p>Multi Query Attention(MQA)在2019年就被提出来了，用于推理加速，但在当时并没有受到很多关注，毕竟一张2080就能跑Bert-base了。随着LLM的大火，MQA所带来的收益得以放大。</p>
<span id="more"></span>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>Multi Query Attention(MQA)跟Multi Head Attention(MHA)只有一词之差，但其思路非常简单，几乎跟MHA一致：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.670koriphig0.webp"  alt="model"></p>
<p>MHA的Query、Key、Value分拆成8个头，每个头进行self-attention运算，而MQA是Query分成8个头，每个头共享一组Key和Value</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MHA: Q, K, V = (512, 768), # seq_len, hidden_dim</span><br><span class="line">			拆成8个头：</span><br><span class="line">			Q : (8, 512, 96) </span><br><span class="line">			k, v: (8, 512, 96)</span><br><span class="line">MQA: </span><br><span class="line"> Q -&gt; (512, 768) </span><br><span class="line"> K -&gt; (512, 96)</span><br><span class="line"> v -&gt; (512, 96)</span><br><span class="line">把Q拆成8个头：</span><br><span class="line">Q： (8, 512, 96)</span><br><span class="line">K, V：(512, 96)</span><br></pre></td></tr></table></figure>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><ul>
<li><p>MHA</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">self.Wqkv = nn.Linear( </span><br><span class="line">            d_model,</span><br><span class="line">            d_model * <span class="number">3</span>,</span><br><span class="line">            device=device,</span><br><span class="line">        )</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>将 <code>d_model * 3</code> 拆成3个768维</p>
</li>
<li><p>MQA</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">self.Wqkv = nn.Linear( </span><br><span class="line">            d_model,</span><br><span class="line">            d_model + <span class="number">2</span> * self.head_dim,</span><br><span class="line">            device=device,</span><br><span class="line">        )</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>将 <code>d_model + 2 * self.head_dim</code> 拆成1个768维 + 2个96维</p>
</li>
</ul>
<p>可以看到参数数量大幅减少。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>实验指标略微降低，但推理加速非常明显。</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/image.194dl27xykcg.webp"  alt="result"></p>
<h2 id="Group-Query-Attention"><a href="#Group-Query-Attention" class="headerlink" title="Group Query Attention"></a>Group Query Attention</h2><p>Q拆分成8个头，K和V分别拆成4个头，然后对应进行attention运算。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://arxiv.org/pdf/1911.02150.pdf">Fast Transformer Decoding: One Write-Head is All<br>You Need</a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/645808819" >[LLM] multi query attention加速推理解码<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>LLM</tag>
        <tag>推理加速</tag>
      </tags>
  </entry>
  <entry>
    <title>NTK-Aware Interpolation</title>
    <url>/2024/04/30/NTK-Aware-Interpolation/</url>
    <content><![CDATA[<p>主要思路：高频外推，低频内插。</p>
<script type="math/tex; mode=display">
m \theta_i=m *(\text { base } * \alpha)^{-2 i / d}=m *(10000 * \alpha)^{-2 i / d}</script><span id="more"></span>
<p>NTK的优点是不用微调的情况下，能比线性插值做得好。但是由于低频部分还是会有部分被外推到超出范围的值，因此在设定系数的时候，要比需要的设得更大才行。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/81NHGf5W8HEscW2dBK8MRg" >大模型处理长上下文方法一览<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/670280576" >详解基于调整RoPE旋转角度的大模型长度外推方法<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>长度外推</tag>
        <tag>RoPE</tag>
      </tags>
  </entry>
  <entry>
    <title>On-policy VS Off-policy</title>
    <url>/2026/03/01/On-policy-VS-Off-policy/</url>
    <content><![CDATA[<p>此前对于rl的这两个概念一直很模糊，在此整理一下。</p>
<span id="more"></span>
<p>首先介绍下行为策略和目标策略：</p>
<ul>
<li>行为策略(Behavior Policy)：用来收集数据的策略，也就是实际与环境交互、生成样本的策略。</li>
<li>目标策略(Target Policy)：我们要学习和优化的策略，也就是最终想要得到的策略。</li>
</ul>
<blockquote>
<p>如果行为策略与目标策略一致，则是on-policy，否则为off-policy。</p>
</blockquote>
<p>下面将以两个经典算法为例介绍on-policy、off-policy区别。</p>
<h2 id="SARSA-On-policy"><a href="#SARSA-On-policy" class="headerlink" title="SARSA(On-policy)"></a>SARSA(On-policy)</h2><p>算法流程如下：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.9rjxxkigy3.webp"  alt="sarsa"></p>
<p>python伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># SARSA 算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sarsa</span>():</span><br><span class="line">    <span class="comment"># 初始化 Q 表</span></span><br><span class="line">    Q = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        state = env.reset()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ⚠️ 关键点1：用 ε-greedy 选第一个动作</span></span><br><span class="line">        action = epsilon_greedy(Q, state)  <span class="comment"># 这就是当前的行为策略</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            <span class="comment"># 执行动作，得到下一状态和奖励</span></span><br><span class="line">            next_state, reward, done = env.step(action)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># ⚠️ 关键点2：还是用 ε-greedy 选下一个动作</span></span><br><span class="line">            next_action = epsilon_greedy(Q, next_state)  <span class="comment"># 还是当前的行为策略</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># ⚠️ 关键点3：更新时用的是下一个动作的 Q 值</span></span><br><span class="line">            <span class="comment"># SARSA 更新公式</span></span><br><span class="line">            Q[state][action] += lr * (reward + gamma * Q[next_state][next_action] - Q[state][action])</span><br><span class="line">            </span><br><span class="line">            state, action = next_state, next_action</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>策略类型</th>
<th>是什么</th>
<th>在代码中的位置</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>行为策略</strong></td>
<td>ε-greedy</td>
<td><code>epsilon_greedy(Q, state)</code></td>
</tr>
<tr>
<td><strong>目标策略</strong></td>
<td>ε-greedy</td>
<td>同上，就是同一个策略</td>
</tr>
</tbody>
</table>
</div>
<p>收集数据时用的是 ε-greedy，更新时用的还是 ε-greedy 选的 next_action，两者策略一致，是on-policy。</p>
<h2 id="DQN-Off-policy"><a href="#DQN-Off-policy" class="headerlink" title="DQN(Off-policy)"></a>DQN(Off-policy)</h2><p>算法流程如下：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.491thfbbvt.webp"  alt="dqn"></p>
<p>python伪代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># DQN 算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dqn</span>():</span><br><span class="line">    Q = Network()  <span class="comment"># Q网络</span></span><br><span class="line">    replay_buffer = []  <span class="comment"># 经验池</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        state = env.reset()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            <span class="comment"># ⚠️ 关键点1：行为策略是 ε-greedy</span></span><br><span class="line">            action = epsilon_greedy(Q, state)  <span class="comment"># 这就是当前的行为策略</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 执行动作，存经验</span></span><br><span class="line">            next_state, reward, done = env.step(action)</span><br><span class="line">            replay_buffer.append((state, action, reward, next_state, done))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># ⚠️ 关键点2：从经验池采样（可能是旧策略的数据）</span></span><br><span class="line">            batch = random.sample(replay_buffer, <span class="number">32</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> s, a, r, s_next, d <span class="keyword">in</span> batch:</span><br><span class="line">                <span class="comment"># ⚠️ 关键点3：目标策略是 greedy！</span></span><br><span class="line">                target = r + gamma * <span class="built_in">max</span>(Q_target(s_next))  <span class="comment"># 用 max，不是 ε-greedy！</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 更新 Q 网络</span></span><br><span class="line">                loss = MSE(Q(s)[a], target)</span><br><span class="line">                loss.backward()</span><br><span class="line">            </span><br><span class="line">            state = next_state</span><br></pre></td></tr></table></figure></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>策略类型</th>
<th>是什么</th>
<th>在代码中的位置</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>行为策略</strong></td>
<td>ε-greedy</td>
<td>收集数据时的策略</td>
</tr>
<tr>
<td><strong>目标策略</strong></td>
<td><strong>greedy</strong></td>
<td>更新时的 <code>max(Q_target(s_next))</code></td>
</tr>
</tbody>
</table>
</div>
<p>收集数据用的ε-greedy，但更新永远选最好的动作greedy，两者策略不一致，是off-policy。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/26603719923" >强化学习基础3：一文彻底讲清On-policy与Off-policy<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>On-policy</tag>
        <tag>Off-policy</tag>
      </tags>
  </entry>
  <entry>
    <title>Oracle中call和exec区别</title>
    <url>/2017/10/09/Oracle%E4%B8%ADcall%E5%92%8Cexec%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>在<font color="red">SQL Plus</font>中这两种方法都可以使用：</p>
<ul>
<li><code>exec pro_name(参数1..)</code></li>
<li><code>call pro_name(参数1..)</code></li>
</ul>
<span id="more"></span>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><ol>
<li>exec是SQL Plus命令，只能在SQL Plus中使用；call为SQL命令，没有限制.</li>
<li><font color="red">存储过程或函数没有参数时,exec可以直接跟过程名（可以省略()），但call则必须带上().</font>

</li>
</ol>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><h3 id="exec"><a href="#exec" class="headerlink" title="exec"></a>exec</h3><ul>
<li>调用存储过程<ul>
<li>有参数：<code>exec mypro(12,&#39;fsdf&#39;);</code></li>
<li>没有参数：<code>exec mypro;</code>，也可以写成<code>exec mypro();</code></li>
</ul>
</li>
<li>调用函数<ul>
<li>有参数：<code>var counts number;exec :counts:=myfunc(&#39;fsde&#39;);</code></li>
<li>没有参数：<code>var counts number;exec :counts:=myfunc;</code>，也可以写成<code>var counts number;exec :counts:=myfunc();</code></li>
</ul>
</li>
</ul>
<h3 id="call"><a href="#call" class="headerlink" title="call"></a>call</h3><ul>
<li>调用存储过程<ul>
<li>有参数：<code>call mypro(23,&#39;fth&#39;);</code></li>
<li>无参数：<code>call mypro();</code></li>
</ul>
</li>
<li>调用函数<ul>
<li>有参数：<code>var counts number;call myfunc(&#39;asd&#39;) into :counts;</code></li>
<li>无参数：<code>var counts number;call myfunc() into :counts;</code></li>
</ul>
</li>
</ul>
<h2 id="其他注意事项"><a href="#其他注意事项" class="headerlink" title="其他注意事项"></a>其他注意事项</h2><ul>
<li>oracle 中字符串应该是单引号而不是双引号</li>
<li>每写完一条sql语句应加上 <font color="red">;</font></li>
<li>为了防止call 和 exec 无参数的存储过程或函数的错误，建议全部加上<font color="red">()</font></li>
</ul>
]]></content>
      <categories>
        <category>DataBase</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA</title>
    <url>/2021/07/30/PCA/</url>
    <content><![CDATA[<p><code>PCA</code> 是最经典的降维算法。</p>
<h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><p>在统计学中，<strong>方差</strong>是用来度量单个随机变量的离散程度，而<strong>协方差</strong>则一般用来衡量两个随机变量的联合变化程度。</p>
<span id="more"></span>
<h4 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h4><script type="math/tex; mode=display">
\sigma_{x}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}</script><p>$n$ 表示样本数量，$\bar{x}$ 表示观测样本的均值。</p>
<h4 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h4><script type="math/tex; mode=display">
\sigma(x, y) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})</script><p>$\bar{x}, \bar{y}$ 分别表示两个随机变量所对应的观测样本均值。方差 $\sigma_x^2$ 可看作随机变量 $x$ 关于自身的协方差 $\sigma(x, x)$ 。</p>
<h4 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h4><p>给定 $d$ 个随机变量 $x_k$ ，$k=1, 2, \dots, d$ 。我们用 $x_{ki}$ 表示随机变量 $x_k$ 中的第 $i$ 个观测样本，每个随机变量所对应的观测样本数量均为 $n$ 。</p>
<p>对于这些随机变量，我们可以根据协方差的定义，求出两两之间的协方差，即：</p>
<script type="math/tex; mode=display">
\sigma\left(x_{a}, x_{b}\right)=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{a i}-\bar{x}_{a}\right)\left(x_{b i}-\bar{x}_{b}\right)</script><p>因此协方差矩阵为：</p>
<script type="math/tex; mode=display">
\Sigma=\left[\begin{array}{ccc}
\sigma\left(x_{1}, x_{1}\right) & \cdots & \sigma\left(x_{1}, x_{d}\right) \\
\vdots & \ddots & \vdots \\
\sigma\left(x_{d}, x_{1}\right) & \cdots & \sigma\left(x_{d}, x_{d}\right)
\end{array}\right] \in \mathbb{R}^{d \times d}</script><p>其中，对角线上的元素为各个随机变量的方差，非对角线上的元素为两两随机变量之间的协方差。</p>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>PCA(主成分分析)是比较常见的线性降维方法，通过线性投影将高维数据映射到低维数据中，所期望的是在投影的维度上，新特征自身的方差尽量大，方差越大特征越有效，尽量使产生的新特征间的相关性越小。</p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p>假设有 $m$ 条数据，每条数据有 $n$ 个特征。 $x_j^i$ 表示第 $i$ 个样本的第 $j$ 个特征。</p>
<ol>
<li><p>均值归一化：</p>
<script type="math/tex; mode=display">
 \begin{aligned}
 \mu_{j} &=\frac{1}{m} \sum_{i=1}^{m} x_{j}^{i} \\
 x_{j}^{i} &:= \frac{x_{j}^{i}-\mu_{j}}{s_{j}}
 \end{aligned}</script><p> 其中 $s_j = max(x_j) - min(x_j)$</p>
</li>
<li><p>计算协方差矩阵：</p>
<script type="math/tex; mode=display">
 \Sigma = \frac{1}{m} X X^T \in \mathbb{R}^{n \times n}</script></li>
<li><p>计算特征向量：</p>
<script type="math/tex; mode=display">
 [U, S, V] = svd(\Sigma)</script><p> 其中左奇异向量、奇异值矩阵、右奇异向量：$U \in \mathbb{R}^{n \times n}, S \in \mathbb{R}^{n \times m}, V \in \mathbb{R}^{m \times m}$</p>
</li>
<li><p>从 $U$ 中取前 $k$ 列：$U_{reduce} \in \mathbb{R}^{n \times k}$</p>
</li>
<li>计算得到降维后的数据：$Z = U_{reduce}^T * X, Z \in \mathbb{R}^{k \times m}$</li>
</ol>
<h3 id="如何选择-k-？"><a href="#如何选择-k-？" class="headerlink" title="如何选择 $k$ ？"></a>如何选择 $k$ ？</h3><script type="math/tex; mode=display">
\frac{\sum_{i=1}^{k} s_{i i}}{\sum_{i=1}^{n} s_{i i}} \geqslant 0.99</script><p>选择满足上述条件的最小 $k$</p>
<h3 id="降维的应用"><a href="#降维的应用" class="headerlink" title="降维的应用"></a>降维的应用</h3><ul>
<li>数据压缩，减少占用的存储空间</li>
<li>加快算法计算速度</li>
<li>低维平面可以可视化数据</li>
</ul>
<h3 id="PCA为什么要用协方差矩阵的特征向量矩阵来做投影矩阵呢？"><a href="#PCA为什么要用协方差矩阵的特征向量矩阵来做投影矩阵呢？" class="headerlink" title="PCA为什么要用协方差矩阵的特征向量矩阵来做投影矩阵呢？"></a>PCA为什么要用协方差矩阵的特征向量矩阵来做投影矩阵呢？</h3><p>降维的目的就是“降噪”和“去冗余”。</p>
<p>“降噪”的目的就是使保留下来的维度间的相关性尽可能小，而“去冗余”的目的就是使保留下来的维度含有的“能量”即方差尽可能大。</p>
<p>我们要最大化方差来保留更多的信息。去噪。<br>有趣的是，协方差矩阵能同时表现不同维度间的相关性以及各个维度上的方差。</p>
<p>协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。协方差矩阵的主对角线上的元素是各个维度上的方差(即能量)，其他元素是两两维度间的协方差(即相关性)。</p>
<p>先看“降噪”，让保留下的不同维度间的相关性尽可能小，也就是说让协方差矩阵中非对角线元素都基本为零。达到这个目的的方式——矩阵对角化。</p>
<p>再看“去冗余”，对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。我们只取那些含有较大能量(特征值)的维度，其余的就舍掉即可。</p>
<h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><p>LDA(线性判别分析)是一种经典的降维方法。和PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。  </p>
<p>LDA分类思想简单总结如下：  </p>
<ul>
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。  </li>
<li>对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。  </li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。  </li>
</ul>
<p>如果用一句话概括LDA思想：<strong>投影后类内方差最小，类间方差最大。</strong></p>
<h2 id="LDA和PCA异同"><a href="#LDA和PCA异同" class="headerlink" title="LDA和PCA异同"></a>LDA和PCA异同</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">异同点</th>
<th style="text-align:left">LDA</th>
<th style="text-align:left">PCA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">相同点</td>
<td style="text-align:left">1. 两者均可以对数据进行降维；<br />2. 两者在降维时均使用了矩阵特征分解的思想；<br />3. 两者都假设数据符合高斯分布；</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">不同点</td>
<td style="text-align:left">有监督</td>
<td style="text-align:left">无监督</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">降维最多降到 $k-1$ 维</td>
<td style="text-align:left">降维多少没有限制</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">可以用于降维，还可以用于分类</td>
<td style="text-align:left">只用于降维</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">选择分类性能最好的投影方向</td>
<td style="text-align:left">选择样本点投影具有最大方差的方向</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">更明确，更能反映样本间差异</td>
<td style="text-align:left">目的较为模糊</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/37609917" >方差、协方差<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/6251584.html" >奇异值分解(SVD)原理与在降维中的应用<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/zhq9695/article/details/83009196" >吴恩达机器学习（十二）主成分分析（降维、PCA）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_34069667/article/details/107996892" >数据分析面试【机器学习】总结之-PCA主成成分分析 常见面试题整理<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title>P &amp; NP &amp; NPC</title>
    <url>/2019/01/24/P%20&amp;%20NP%20&amp;%20NPC/</url>
    <content><![CDATA[<p>这是计算机界的经典问题了。</p>
<span id="more"></span>
<h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><p>常见的时间复杂度分为两种级别：</p>
<ul>
<li>多项式级：$O(1), O(log(n)), O(n^k) $</li>
<li>非多项式级：$O(a^n), O(n!)$</li>
</ul>
<p>当我们在解决一个问题时，我们选择的算法通常都需要是多项式级的复杂度，非多项式级的复杂度需要的时间太多，往往会超时，除非是数据规模非常小。</p>
<p>自然地，人们会想到一个问题：会不会所有的问题都可以找到复杂度为多项式级的算法呢？很遗憾，答案是否定的。有些问题甚至根本不可能找到一个正确的算法来，这称之为“不可解问题”(Undecidable Decision Problem)。<a class="link"   href="http://www.matrix67.com/blog/article.asp?id=62" >The Halting Problem<i class="fas fa-external-link-alt"></i></a>就是一个著名的不可解问题。比如，输出从1到n这n个数的全排列。不管你用什么方法，你的复杂度都是阶乘级，因为你总得用阶乘级的时间打印出结果来。有人说，这样的“问题”不是一个“正规”的问题，正规的问题是让程序解决一个问题，输出一个“YES”或“NO”（这被称为判定性问题），或者一个什么什么的最优值（这被称为最优化问题）。那么，根据这个定义，我也能举出一个不大可能会有多项式级算法的问题来：Hamilton回路。问题是这样的：给你一个图，问你能否找到一条经过每个顶点一次且恰好一次（不遗漏也不重复）最后又走回来的路（满足这个条件的路径叫做Hamilton回路）。这个问题现在还没有找到多项式级的算法。事实上，这个问题就是我们后面要说的NPC问题。</p>
<h1 id="P问题"><a href="#P问题" class="headerlink" title="P问题"></a>P问题</h1><blockquote>
<p>如果一个问题可以找到一个能在多项式的时间里解决它的算法，那么这个问题就属于P问题。</p>
</blockquote>
<p>我们常见到的一些信息奥赛的题目都是P问题。道理很简单，一个用穷举换来的非多项式级时间的超时程序不会涵盖任何有价值的算法。</p>
<h1 id="NP问题"><a href="#NP问题" class="headerlink" title="NP问题"></a>NP问题</h1><blockquote>
<p>可以在多项式的时间里验证一个解的问题。</p>
</blockquote>
<p>举个例子：给你一个边带权值的图，让你找出一条路径使得该路径边权小于100。这个图是非常复杂的，你只能用非多项式的时间去穷举每一条路径，这是非常耗时的。但如果我给你一条路径（即一个解），你只需要在多项式的时间内遍历这条路径，将这条路径的权值相加，如果权值和小于100，那么这个解就是正确的，即：我在多项式时间内验证了一个解。</p>
<font color="green">之所以要定义NP问题，是因为通常只有NP问题才可能找到多项式的算法。我们不会指望一个连多项式地验证一个解都不行的问题存在一个解决它的多项式级的算法。</font>


<h1 id="P问题属于NP问题"><a href="#P问题属于NP问题" class="headerlink" title="P问题属于NP问题"></a>P问题属于NP问题</h1><p>所有的P类问题都是NP问题。也就是说，能多项式地解决一个问题，必然能多项式地验证一个问题的解——既然正解都出来了，验证任意给定的解也只需要比较一下就可以了。关键是，人们想知道，是否所有的NP问题都是P类问题。现在，所有对NP问题的研究都集中在一个问题上，即究竟是否有P=NP？通常所谓的“NP问题”，其实就一句话：证明或推翻P=NP。</p>
<h1 id="约化（Reducibility-归约）"><a href="#约化（Reducibility-归约）" class="headerlink" title="约化（Reducibility , 归约）"></a>约化（Reducibility , 归约）</h1><p>简单地说，一个问题A可以约化为问题B的含义即是，可以用问题B的解法解决问题A，或者说，问题A可以“变成”问题B。《算法导论》上举了这么一个例子。比如说，现在有两个问题：求解一个一元一次方程和求解一个一元二次方程。那么我们说，前者可以约化为后者，意即知道如何解一个一元二次方程那么一定能解出一元一次方程。我们可以写出两个程序分别对应两个问题，那么我们能找到一个“规则”，按照这个规则把解一元一次方程程序的输入数据变一下，用在解一元二次方程的程序上，两个程序总能得到一样的结果。这个规则即是：两个方程的对应项系数不变，一元二次方程的二次项系数为0。按照这个规则把前一个问题转换成后一个问题，两个问题就等价了。同样地，我们可以说，Hamilton回路可以约化为TSP问题(Travelling Salesman Problem，旅行商问题)：在Hamilton回路问题中，两点相连即这两点距离为0，两点不直接相连则令其距离为1，于是问题转化为在TSP问题中，是否存在一条长为0的路径。Hamilton回路存在当且仅当TSP问题中存在长为0的回路。</p>
<p>“问题A可约化为问题B”有一个重要的直观意义：B的时间复杂度高于或者等于A的时间复杂度。也就是说，问题A不比问题B难。这很容易理解。正如解一元二次方程比解一元一次方程难，因为解决前者的方法可以用来解决后者。</p>
<p>很显然，约化具有一项重要的性质：约化具有传递性。如果问题A可约化为问题B，问题B可约化为问题C，则问题A一定可约化为问题C。这个道理非常简单，就不必阐述了。</p>
<p>现在再来说一下约化的标准概念就不难理解了：如果能找到这样一个变化法则，对任意一个程序A的输入，都能按这个法则变换成程序B的输入，使两程序的输出相同，那么我们说，问题A可约化为问题B。</p>
<p>当然，我们所说的“可约化”是指的可“多项式地”约化(Polynomial-time Reducible)，即变换输入的方法是能在多项式的时间里完成的。约化的过程只有用多项式的时间完成才有意义。</p>
<p>从约化的定义中我们看到，一个问题约化为另一个问题，时间复杂度增加了，问题的应用范围也增大了。通过对某些问题的不断约化，我们能够不断寻找复杂度更高，但应用范围更广的算法来代替复杂度虽然低，但只能用于很小的一类问题的算法。再回想前面讲的P和NP问题，联想起约化的传递性，自然地，我们会想问，如果不断地约化上去，不断找到能“通吃”若干小NP问题的一个稍复杂的大NP问题，那么最后是否有可能找到一个时间复杂度最高，并且能“通吃”所有的 NP问题的这样一个超级NP问题？答案居然是肯定的。也就是说，存在这样一个NP问题，所有的NP问题都可以约化成它。换句话说，只要解决了这个问题，那么所有的NP问题都解决了。这种问题的存在难以置信，并且更加不可思议的是，这种问题不只一个，它有很多个，它是一类问题。这一类问题就是传说中的NPC 问题，也就是NP-完全问题。NPC问题的出现使整个NP问题的研究得到了飞跃式的发展。我们有理由相信，NPC问题是最复杂的问题。再次回到全文开头，我们可以看到，人们想表达一个问题不存在多项式的高效算法时应该说它“属于NPC问题”。此时，我的目的终于达到了，我已经把NP问题和NPC问题区别开了。</p>
<h1 id="NPC问题"><a href="#NPC问题" class="headerlink" title="NPC问题"></a>NPC问题</h1><p>NPC问题的定义非常简单。同时满足下面两个条件的问题就是NPC问题：</p>
<ul>
<li><p>它得是一个NP问题；</p>
</li>
<li><p>所有的NP问题都可以约化到它。</p>
<p>证明一个问题是 NPC问题也很简单。先证明它至少是一个NP问题，再证明其中一个已知的NPC问题能约化到它（由约化的传递性，则NPC问题定义的第二条也得以满足；至于第一个NPC问题是怎么来的，下文将介绍），这样就可以说它是NPC问题了。</p>
<font color="green">既然所有的NP问题都能约化成NPC问题，那么只要任意一个NPC问题找到了一个多项式的算法，那么所有的NP问题都能用这个算法解决了，NP也就等于P 了。因此，给NPC找一个多项式算法太不可思议了。因此，前文才说，“正是NPC问题的存在，使人们相信P≠NP”。我们可以就此直观地理解，NPC问题目前没有多项式的有效算法，只能用指数级甚至阶乘级复杂度的搜索。</font>

</li>
</ul>
<h1 id="逻辑电路"><a href="#逻辑电路" class="headerlink" title="逻辑电路"></a>逻辑电路</h1><p>不要以为NPC问题是一纸空谈。NPC问题是存在的。确实有这么一个非常具体的问题属于NPC问题。下文即将介绍它。</p>

<h2 id="电路可满足性"><a href="#电路可满足性" class="headerlink" title="电路可满足性"></a>电路可满足性</h2><blockquote>
<p>具有一个输出的布尔组合电路是可满足的，指的是它有一组可满足的赋值，使得组合电路的输出为1。</p>
<p>电路可满足性(Circuit Satisfiablity，简记Circuit SAT)问题指的是，给定一个由逻辑门AND、<br>OR和NOT构成的组合电路<em>C</em>，它可满足吗?</p>
</blockquote>
<p>上述电路当输入0,1,0,1时，输出为1.</p>
<p>当然肯定也存在输出永远为true的逻辑电路，即无论输入是什么，输出都是true。我们就说这个逻辑电路不存在使输出为false的一组输入。</p>
<p>逻辑电路问题属于NPC问题。这是有严格证明的。它显然属于NP问题，并且可以直接证明所有的NP问题都可以约化到它（不要以为NP问题有无穷多个将给证明造成不可逾越的困难）。证明过程相当复杂，其大概意思是说任意一个NP问题的输入和输出都可以转换成逻辑电路的输入和输出（想想计算机内部也不过是一些 0和1的运算），因此对于一个NP问题来说，问题转化为了求出满足结果为True的一个输入（即一个可行解）。</p>
<p>有了第一个NPC问题后，一大堆NPC问题就出现了，因为再证明一个新的NPC问题只需要将一个已知的NPC问题约化到它就行了。后来，Hamilton 回路成了NPC问题，TSP问题也成了NPC问题。<font color="green">现在被证明是NPC问题的有很多，任何一个找到了多项式算法的话所有的NP问题都可以完美解决了。</font>因此说，正是因为NPC问题的存在，P=NP变得难以置信。P=NP问题还有许多有趣的东西，有待大家自己进一步的挖掘。攀登这个信息学的巅峰是我们这一代的终极目标。现在我们需要做的，至少是不要把概念弄混淆了。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="http://www.matrix67.com/blog/archives/105" >什么是P问题、NP问题和NPC问题<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>时间复杂度</tag>
        <tag>归约</tag>
      </tags>
  </entry>
  <entry>
    <title>PEFT-LISA</title>
    <url>/2024/04/03/PEFT-LISA/</url>
    <content><![CDATA[<p>LISA是LoRA的简化版，但其抓住了LoRA微调的核心，即LoRA侧重更新LLM的底层embedding和顶层head。</p>
<span id="more"></span>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.4uaperxni9.png"  alt="phe"></p>
<p>根据上述现象，LISA提出两点改进：</p>
<ul>
<li>始终更新LLM的底层embedding和顶层head</li>
<li>随机更新中间层的hidden state</li>
</ul>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.6f0ge908x1.webp"  alt="phe"></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h4 id="显存占用"><a href="#显存占用" class="headerlink" title="显存占用"></a>显存占用</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.4cknq74f33.webp"  alt="gpu"></p>
<p>毕竟模型参数大头还是在底层embedding，所以显存占用并没有减少太多。</p>
<h4 id="训练时间"><a href="#训练时间" class="headerlink" title="训练时间"></a>训练时间</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.8ad16vg8o3.webp"  alt="time"></p>
<h4 id="下游任务微调"><a href="#下游任务微调" class="headerlink" title="下游任务微调"></a>下游任务微调</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.101xvtqs27.webp"  alt="time"></p>
<p>在MT-BENCH上，LISA超过了LoRA，甚至全量参数微调。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/7s8NNGYlq4JWeln0TkOKmQ" >比LoRA还快50%的微调方法来了！一张3090性能超越全参调优，UIUC联合LMFlow团队提出LISA<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/OptimalScale/LMFlow" >LMFlow<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>PEFT</title>
    <url>/2023/07/18/PEFT/</url>
    <content><![CDATA[<p>下面是一些参数高效的微调大模型方法：</p>
<span id="more"></span>
<h2 id="Adapter"><a href="#Adapter" class="headerlink" title="Adapter"></a>Adapter</h2><h4 id="模型总览"><a href="#模型总览" class="headerlink" title="模型总览"></a>模型总览</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.1ocq1j9sds1s.webp"  alt="Adapter"></p>
<p>Adapter作为一个插件加入到大模型内，微调下游任务时，固定大模型参数，只训练Adapter参数。</p>
<h2 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h2><p>LoRA名为大语言模型的低阶适应，最初设计用于微调LLM，但却在文生图领域大放异彩，并逐渐被人数知。其思想跟ResNet非常相似，通过在大模型旁侧添加一路分支，冻结大模型参数，学习分支参数（也即残差），达到微调效果。</p>
<h4 id="模型总览-1"><a href="#模型总览-1" class="headerlink" title="模型总览"></a>模型总览</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.2nti7ywk7se0.webp"  alt="lora"></p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.6rua2cocjfs0.webp"  alt="formula"></p>
<p>如果 $\Delta W$ 跟 $W_0$ 一样，也是 $\mathbb{R}^{d \times d}$，那么残差学习同样需要训练大量的参数，并没有达到参数高效的目标。而在我们学习中，常用的减少矩阵参数大小方法就是矩阵分解，因此作者对输入先降采样，再上采样，实现输入与输出维度一致。</p>
<h2 id="Prefix-Tuning"><a href="#Prefix-Tuning" class="headerlink" title="Prefix-Tuning"></a>Prefix-Tuning</h2><p>该方法主要用来做NLG任务（Table-to-text Generation、 Summarization），在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而大模型参数冻结。</p>
<h4 id="模型总览-2"><a href="#模型总览-2" class="headerlink" title="模型总览"></a>模型总览</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.1lj0d2s95wsg.webp"  alt="prefix tuning"></p>
<p>Prefix tokens初始化如下：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.lq7qaw0w668.webp"  alt="init"></p>
<p>需要注意的是，在低资源场景下，用任务相关的单词来初始化prefix tokens，效果更好：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.36f27jncegw0.webp"  alt="words"></p>
<h2 id="Prompt-tuning"><a href="#Prompt-tuning" class="headerlink" title="Prompt-tuning"></a>Prompt-tuning</h2><p>Prompt-Tunning算是prefix-Tunning的简化版本，面向NLU任务，进行了更全面的效果对比，并且在大模型上成功打平了LM微调的效果。</p>
<h4 id="模型总览-3"><a href="#模型总览-3" class="headerlink" title="模型总览"></a>模型总览</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.3w6seu3s3tm0.webp"  alt="prompt tuning"></p>
<ul>
<li>初始化：Prompt-tuning在输入层前置多个可训练的tokens，固定住大模型参数。实验结果表明用类标签来初始化prompts效果最好。</li>
<li>prompt ensembling：针对同一个任务，构造多个不同的prompts，就相当于训练了多个模型。</li>
</ul>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.5md9psbb9i40.webp"  alt="ensemble"></p>
<h4 id="Prompt-tuning-与-Prefix-Tuning-不同"><a href="#Prompt-tuning-与-Prefix-Tuning-不同" class="headerlink" title="Prompt-tuning 与 Prefix-Tuning 不同"></a>Prompt-tuning 与 Prefix-Tuning 不同</h4><ul>
<li>两者的基座模型不同，一个是T5，一个是BART和GPT2</li>
<li>前者关注NLU，后者关注NLG</li>
<li>前者参数更少，只需微调embeding层；后者需要微调所有层embedding，以及需要在输入层之后接一个MLP来稳定训练</li>
</ul>
<h2 id="P-tuning-V1-amp-P-tuning-V2"><a href="#P-tuning-V1-amp-P-tuning-V2" class="headerlink" title="P-tuning V1 &amp; P-tuning V2"></a>P-tuning V1 &amp; P-tuning V2</h2><p>P-tuning主要用GPT来做NLU任务，达到甚至超过BERT同等水平。</p>
<h4 id="模型总览-4"><a href="#模型总览-4" class="headerlink" title="模型总览"></a>模型总览</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.6krujsjxqj40.webp"  alt="v1"></p>
<p>v1做了如下两点优化：</p>
<ul>
<li>考虑到预训练模型本身的embedding就比较离散了（随机初始化+梯度传回来小，最后只是小范围优化），同时prompt本身也是互相关联的，所以作者先用LSTM对prompt进行编码。</li>
<li>在prompt模板中，加入一些anchor tokens效果会更好。</li>
</ul>
<p>v2主要是在大模型的每一层加入可训练prompts：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.77uorheyphc0.webp"  alt="v2"></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/gogoSandy/p/17202169.html" >解密Prompt系列3. 冻结LM微调Prompt: Prefix-Tuning &amp; Prompt-Tuning &amp; P-Tuning<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/400790006" >Prompt范式第二阶段｜Prefix-tuning、P-tuning、Prompt-tuning<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://juejin.cn/post/7242677017057755191" >大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>PEPNet</title>
    <url>/2025/02/28/PEPNet/</url>
    <content><![CDATA[<p>鉴于PEPNet已经是多场景、多任务建模的baseline，这里有必要详细讲解一下。</p>
<span id="more"></span>
<h2 id="背景动机"><a href="#背景动机" class="headerlink" title="背景动机"></a>背景动机</h2><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.5xarunoecl.webp"  alt="example"></p>
<p>用户在多场景、多任务下的行为存在共性和差异性，如何用联合建模来捕捉这些特性又避免跷跷板效应成为一大难点。</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.7egwweyjxj.webp"  alt="model"></p>
<p>模型分为三部分：</p>
<h4 id="Gate-Neural-Unit-Gate-NU"><a href="#Gate-Neural-Unit-Gate-NU" class="headerlink" title="Gate Neural Unit(Gate NU)"></a>Gate Neural Unit(Gate NU)</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.3gojfqshav.webp"  alt="gate"></p>
<p>PEPNet的核心组件，由两层MLP构成，利用门控机制动态激活模型参数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{x}^{\prime} &= Relu(xW+b) \\
\boldsymbol{\delta} &= \gamma * \operatorname{Sigmoid}\left(\mathbf{x}^{\prime} \mathbf{W}^{\prime}+\boldsymbol{b}^{\prime}\right), \boldsymbol{\delta} \in[0, \gamma]
\end{aligned}</script><h4 id="Embedding-Personalized-Network-EPNet"><a href="#Embedding-Personalized-Network-EPNet" class="headerlink" title="Embedding Personalized Network(EPNet)"></a>Embedding Personalized Network(EPNet)</h4><p>在特征embedding层，显示注入场景的先验信息，从而强化场景个性化特征（可以理解为与该场景相关的特征筛选）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
E &= E\left(\mathcal{F}_S\right) \oplus E\left(\mathcal{F}_D\right) \\
\delta_{\text {domain }} &= \mho_{e p}\left(E\left(\mathcal{F}_d\right) \oplus(\oslash(\mathbf{E}))\right) \\
\mathrm{O}_{e p} &= \delta_{\text {domain }} \otimes \mathrm{E}
\end{aligned}</script><p>场景侧特征包括场景id、user和item在该场景下的统计量信息，比如曝光、点击等。</p>
<h4 id="Parameter-Personalized-Network-PPNet"><a href="#Parameter-Personalized-Network-PPNet" class="headerlink" title="Parameter Personalized Network(PPNet)"></a>Parameter Personalized Network(PPNet)</h4><p>对多任务DNN参数施加样本粒度的个性化影响，来选择和增强用户对于该任务的个性化参数信号，也就是在网络中注入用户在某个场景某类交互行为的倾向性先验：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{O}_{\text {prior }} &= E\left(\mathcal{F}_u\right) \oplus E\left(\mathcal{F}_i\right) \oplus E\left(\mathcal{F}_a\right) \\
\delta_{\text {task }} &= \mho_{\text {pp }}\left(\mathbf{O}_{\text {prior }} \oplus\left(\oslash\left(\mathbf{O}_{e p}\right)\right)\right) \\
\mathbf{O}_{p p} &= \boldsymbol{\delta}_{\text {task }} \otimes \mathbf{H} \\
\mathbf{O}_{p p}^{(l)} &= \boldsymbol{\delta}_{\text {task }}^{(l)} \otimes \mathbf{H}^{(l)} \\
\mathbf{H}^{(l+1)} & =f\left(\mathbf{O}_{p p}^{(l)} \mathbf{W}^{(l)}+\boldsymbol{b}^{(l)}\right), l \in\{1, \ldots, L\}
\end{aligned}</script><ul>
<li>$\mathcal{F}_u, \mathcal{F}_i, \mathcal{F}_a$分别表示user、item、author侧特征</li>
<li>$\mathbf{O}_{e p}$需要做梯度冻结，这样PPNet网络的更新不会影响EPNet，避免跷跷板问题</li>
</ul>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.3rbd8x840o.webp"  alt="exp"></p>
<p>在各场景、各任务中表现优异，均为sota，PEPNet建模的效果确实显著。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/611532716" >千人千模 | PEPNet: 2023快手多任务多场景建模<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://dl.acm.org/doi/pdf/10.1145/3580305.3599884" >PEPNet<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>Multi-Task</tag>
        <tag>Multi-Domain</tag>
      </tags>
  </entry>
  <entry>
    <title>PLE讲解</title>
    <url>/2023/11/14/PLE%E8%AE%B2%E8%A7%A3/</url>
    <content><![CDATA[<p><a class="link"   href="https://github.com/datawhalechina/fun-rec/blob/master/docs/ch02/ch2.2/ch2.2.5/PLE.md" >https://github.com/datawhalechina/fun-rec/blob/master/docs/ch02/ch2.2/ch2.2.5/PLE.md<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>MTL</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas入门</title>
    <url>/2022/08/19/Pandas%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<p>日常工作中经常需要数据分析，以前都是python脚本读取文件然后统计，十分麻烦。尝试了下Pandas，真香！</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!cat data</span><br></pre></td></tr></table></figure>
<pre><code>123    600
23    67
563    456
345    345
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<h2 id="读取txt文件"><a href="#读取txt文件" class="headerlink" title="读取txt文件"></a>读取txt文件</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_table(<span class="string">&quot;./data&quot;</span>, sep=<span class="string">&quot;\t&quot;</span>, header=<span class="literal">None</span>, names=[<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>])    <span class="comment"># 添加自定义列名：A, B</span></span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure>
<pre><code>     A    B
0  123  600
1   23   67
2  563  456
3  345  345
</code></pre><h2 id="添加新的列"><a href="#添加新的列" class="headerlink" title="添加新的列"></a>添加新的列</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&quot;C&quot;</span>] = df[<span class="string">&quot;B&quot;</span>] / df[<span class="string">&quot;A&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="built_in">print</span>(df[[<span class="string">&quot;A&quot;</span>, <span class="string">&quot;C&quot;</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>     A    B         C
0  123  600  4.878049
1   23   67  2.913043
2  563  456  0.809947
3  345  345  1.000000
     A         C
0  123  4.878049
1   23  2.913043
2  563  0.809947
3  345  1.000000
</code></pre><h2 id="条件过滤"><a href="#条件过滤" class="headerlink" title="条件过滤"></a>条件过滤</h2><ul>
<li>单条件</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = df[df[<span class="string">&quot;C&quot;</span>]&gt;<span class="number">2.5</span>]</span><br><span class="line"><span class="built_in">print</span>(t)</span><br></pre></td></tr></table></figure>
<pre><code>     A    B         C
0  123  600  4.878049
1   23   67  2.913043
</code></pre><ul>
<li>多条件</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = df[(df[<span class="string">&quot;A&quot;</span>]&gt;<span class="number">100</span>) &amp; (df[<span class="string">&quot;C&quot;</span>]&lt;<span class="number">3</span>)]</span><br><span class="line"><span class="built_in">print</span>(t)</span><br></pre></td></tr></table></figure>
<pre><code>     A    B         C
2  563  456  0.809947
3  345  345  1.000000
</code></pre><h2 id="保存到文件"><a href="#保存到文件" class="headerlink" title="保存到文件"></a>保存到文件</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_csv(<span class="string">&quot;./final.txt&quot;</span>, sep=<span class="string">&quot;\t&quot;</span>, index=<span class="literal">False</span>, header=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!cat final.txt</span><br></pre></td></tr></table></figure>
<pre><code>123    600    4.878048780487805
23    67    2.9130434782608696
563    456    0.8099467140319716
345    345    1.0
</code></pre><h2 id="列过滤并求和"><a href="#列过滤并求和" class="headerlink" title="列过滤并求和"></a>列过滤并求和</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!cat stu.csv</span><br></pre></td></tr></table></figure>
<pre><code>name,age
Tome,12
Jack,NULL
Node,13
Node,NULL
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&quot;./stu.csv&quot;</span>)</span><br><span class="line">a= df[df[<span class="string">&quot;age&quot;</span>].notna()][<span class="string">&quot;age&quot;</span>].<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<pre><code>25.0
</code></pre><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/87334662" >pandas里面按条件筛选<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>PowerDesigner连接MySQL逆向生成物理模型</title>
    <url>/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>根据MySQL现有的表格结构来反向生成含有依赖关系表格模型。</p>
<span id="more"></span>
<p>系统环境：Win10 64位系统</p>
<h2 id="下载安装ODBC"><a href="#下载安装ODBC" class="headerlink" title="下载安装ODBC"></a>下载安装ODBC</h2><p>到<a class="link"   href="https://dev.mysql.com/downloads/connector/odbc/" >MySQL官网<i class="fas fa-external-link-alt"></i></a>上下载ODBC，选择<font color=red>mysql-connector-odbc-5.3.9-win32.msi</font> 这一点非常重要，下面会说明理由。安装就很简单了，一路next下去</p>
<img   src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/1.png"  class="">
<h2 id="配置ODBC数据源"><a href="#配置ODBC数据源" class="headerlink" title="配置ODBC数据源"></a>配置ODBC数据源</h2><p>1.打开管理工具（不知道在哪儿的话，可以问cortana），双击<font color=red>ODBC数据源(32位)</font>，如下图所示：</p>
<img   src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/2.png"  class="">
<p>2.点击添加，选择<font color=red>MySQL ODBC 5.3 Unicode Driver</font></p>
<img   src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/3.png"  class="">
<p>3.点击完成，会弹出配置界面，前面两个随便填写，<font color=red>User和Password就填写你连接数据库的用户名和密码，Database选择你所要连接的数据库</font>，点击Test会弹出连接成功的提示框</p>
<img   src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/4.png"  class="">，点击OK就配置完成了

## 使用PowerDesigner逆向生成物理模型
1.打开PowerDesigner新建模型，DBMS选择MySQL5.0

<img   src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/5.png"  class="">
<p>2.菜单栏 Database -&gt; Connect，点击弹出连接界面。从下拉菜单中选择刚刚配置的ODBC数据源，点击Connect即可连接成功。</p>
<img   src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/6.png"  class="">
<font color=red>注意：现在来解释一下为什么选择32位的安装包，如果选择了64位的，此时点击Connect会弹出报错框：在指定的DSN中，驱动程序和应用程序的体系结构不匹配，SQLSTATE=IM014.具体原因我也不知道。</font>

<p>3.菜单栏 Database -&gt; Update Model From Database…，弹出如下界面：</p>
<img   src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/7.png"  class="">
<p>点击确定，PowerDesigner默认选中所有数据库的所有表，要想生成我们想要的数据库的物理模型，先反选一下Deselect All，</p>
<img   src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/8.png"  class="">
<p>再选中partysystem数据库，Select All即选中该数据库中的所有表</p>
<img   src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/9.png"  class="">
<p>最后点击OK，即生成我们想要的物理模型。</p>
<img   src="/2017/09/17/PowerDesigner%E8%BF%9E%E6%8E%A5MySQL%E9%80%86%E5%90%91%E7%94%9F%E6%88%90%E7%89%A9%E7%90%86%E6%A8%A1%E5%9E%8B/10.png"  class="">
<p>我这个数据库里面的表结构比较单一，所以生成的物理模型很简单。</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>PowerDesigner</tag>
      </tags>
  </entry>
  <entry>
    <title>Q-Learning中的Q值和奖励R</title>
    <url>/2025/09/04/Q-Learning%E4%B8%AD%E7%9A%84Q%E5%80%BC%E5%92%8C%E5%A5%96%E5%8A%B1R/</url>
    <content><![CDATA[<p>为什么不用奖励R作为当前Q值，这里解释一下。</p>
<span id="more"></span>
<h2 id="核心比喻：下象棋"><a href="#核心比喻：下象棋" class="headerlink" title="核心比喻：下象棋"></a>核心比喻：下象棋</h2><ul>
<li>奖励R：就像是<strong>吃掉对方一个棋子</strong>。<ul>
<li>你吃掉一个“兵”，获得一点小奖励。</li>
<li>你吃掉一个“车”，获得一个很大的奖励。</li>
<li>你“将死”了对方，获得一个巨大的、终结比赛的奖励。</li>
</ul>
</li>
</ul>
<p>奖励R是环境给你的、立即可见的、直接的反馈。</p>
<ul>
<li>Q值：就像是<strong>顶尖棋手大脑里对当前棋局（状态）和下一步（动作）的“棋形判断”</strong>。<ul>
<li>它不是一个简单的、眼前的得失。它会综合考虑：“如果我走这步‘马’，虽然可能会丢一个‘兵’（短期负奖励），但我会获得极大的攻势，在十步之后有可能‘将死’对方（巨大的长期收益）。”</li>
<li>或者：“如果我吃这个‘车’（巨大的短期奖励），但我的‘将’会暴露在对方的火力下，导致我五步之后被将死（灾难性的长期后果），那这步棋的整体价值其实非常低。”</li>
</ul>
</li>
</ul>
<p>Q值是智能体自己对未来总收益的一个预测和评估。</p>
<p><strong>结论：奖励是“眼前小利”，而Q值是“深谋远虑”。</strong></p>
<h2 id="正式区别：奖励R-vs-Q值"><a href="#正式区别：奖励R-vs-Q值" class="headerlink" title="正式区别：奖励R vs Q值"></a>正式区别：奖励R vs Q值</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">特性</th>
<th style="text-align:left">奖励R</th>
<th style="text-align:left">Q值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>来源</strong></td>
<td style="text-align:left">环境给的。是游戏规则设定好的。</td>
<td style="text-align:left">智能体自己学习和计算出来的。</td>
</tr>
<tr>
<td style="text-align:left"><strong>时间尺度</strong></td>
<td style="text-align:left"><strong>即时</strong>的、<strong>短期</strong>的。只关心<strong>下一步</strong>的收益。</td>
<td style="text-align:left"><strong>累积</strong>的、<strong>长期</strong>的。关心<strong>从现在到游戏结束</strong>的所有收益总和。</td>
</tr>
<tr>
<td style="text-align:left"><strong>视角</strong></td>
<td style="text-align:left"><strong>局部</strong>的、<strong>单一事件</strong>的反馈。</td>
<td style="text-align:left"><strong>全局</strong>的、<strong>战略级</strong>的评估。</td>
</tr>
<tr>
<td style="text-align:left"><strong>类比</strong></td>
<td style="text-align:left"><strong>工资/奖金</strong>：做完一项工作，立刻拿到钱。</td>
<td style="text-align:left"><strong>职业规划</strong>：选择一份工作，不仅看起薪，更看未来发展、股票期权、技能成长等所有未来收益的总和。</td>
</tr>
<tr>
<td style="text-align:left"><strong>依赖性</strong></td>
<td style="text-align:left"><strong>只依赖于当前的动作和状态</strong>。</td>
<td style="text-align:left"><strong>依赖于当前的动作、状态、以及后续所有的决策</strong>。</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Q-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>RAG</title>
    <url>/2024/05/12/RAG/</url>
    <content><![CDATA[<p>现有的LLM已经具备了理解、生成、逻辑和记忆能力，RAG(Retrieval Augmented Generation)则是为其套上外挂，使LLM能够访问训练数据来源之外的权威知识库，并生成领域特定的内容，而无须重新训练模型。</p>
<span id="more"></span>
<h2 id="RAG的优势"><a href="#RAG的优势" class="headerlink" title="RAG的优势"></a>RAG的优势</h2><ul>
<li>经济高效：LLM无须重新训练，即可访问和生成领域内容。</li>
<li>减轻幻觉：LLM根据用户输入，并根据它的训练语料生成内容。RAG引入了信息检索组件，该组件利用用户输入首先从新数据源提取信息。用户查询和相关信息都提供给LLM。LLM使用新知识及其训练数据来创建更好的响应。</li>
</ul>
<h2 id="RAG的缺点"><a href="#RAG的缺点" class="headerlink" title="RAG的缺点"></a>RAG的缺点</h2><ul>
<li>维护成本高：RAG需要实时维护其数据库，是个系统工程</li>
<li>平响增加：RAG增加了检索流程，使得响应耗时增加，影响用户体验</li>
</ul>
<h2 id="RAG的工作流程"><a href="#RAG的工作流程" class="headerlink" title="RAG的工作流程"></a>RAG的工作流程</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.4913czxecz.png"  alt="RAG"></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://aws.amazon.com/cn/what-is/retrieval-augmented-generation/" >什么是 RAG？<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>RLHF讲解</title>
    <url>/2023/11/13/RLHF%E8%AE%B2%E8%A7%A3/</url>
    <content><![CDATA[<p>RLHF包含了两个至关重要的步骤：</p>
<ol>
<li>训练Reward Model</li>
<li>用Reward Model和SFT Model构造Reward Function，基于PPO算法来训练LLM<ol>
<li>frozen RM</li>
<li>frozen SFT Model</li>
<li>Actor $\pi_{\Phi}^{R L}$ initialized from SFT Model</li>
<li>Critic $V_\eta$ initialized from RM</li>
</ol>
</li>
</ol>
<p>最大化目标函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { objective }(\phi)= & E_{(x, y) \sim D_{\pi_\phi \mathrm{RL}}}\left[r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]+ \\
& \gamma E_{x \sim D_{\text {pectrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]
\end{aligned}</script><span id="more"></span>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.6qiivvmcc5c0.png"  alt="rlhf"></p>
<p>训练流程：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">policy_model = load_model()</span><br><span class="line">ref_policy_model = policy_model.copy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">    <span class="comment"># 采样（上一个epoch的actor模型和critic模型）</span></span><br><span class="line">    prompts = sample_prompt()</span><br><span class="line">    <span class="comment"># old_log_probs是上一个epoch的actor模型的对数概率</span></span><br><span class="line">    <span class="comment"># old_values是上一个epoch的critic模型的预估期望收益</span></span><br><span class="line">    responses, old_log_probs, old_values = respond(policy_model, prompts)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反馈</span></span><br><span class="line">    <span class="comment"># 固定的reward模型</span></span><br><span class="line">    scores = reward_model(prompts, responses)</span><br><span class="line">    <span class="comment"># 固定的sft模型</span></span><br><span class="line">    ref_log_probs, _ = analyze_responses(ref_policy_model, prompts, responses)</span><br><span class="line">    rewards = reward_func(reward_model, scores, old_log_probs, ref_log_probs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 学习，为了更新actor和critic模型</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 这里的values用于更新critic模型</span></span><br><span class="line">        log_probs, values = analyze_responses(policy_model, prompts, responses)</span><br><span class="line">        advantages = advantage_func(rewards, old_values)</span><br><span class="line">        actor_loss = actor_loss_func(advantages, old_log_probs, log_probs)</span><br><span class="line">        critic_loss = critic_loss_func(rewards, values)</span><br><span class="line">        loss = actor_loss + <span class="number">0.1</span> * critic_loss</span><br><span class="line">        train(loss, policy_model.parameters())</span><br></pre></td></tr></table></figure></p>
<ul>
<li>frozen RM 和 frozen SFT是用来计算rewards的</li>
<li>actor和critict会在epoch训练中同步更新</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/657490625" >RLHF理论篇<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/645225982" >拆解大语言模型RLHF中的PPO<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>RM</tag>
        <tag>PPO</tag>
        <tag>Actor-Critic</tag>
      </tags>
  </entry>
  <entry>
    <title>Relation Classification with Entity Type Restriction</title>
    <url>/2022/05/10/Relation-Classification-with-Entity-Type-Restriction/</url>
    <content><![CDATA[<p>这是一篇ACL Findings的论文，idea很简单，但却非常奏效。</p>
<span id="more"></span>
<p>关系分类旨在预测句子中两个实体间的关系，这篇论文通过实体类型来限制关系的搜索范围。例如两个实体类型都是<code>person</code>，那么他们的关系就可以排除<code>出生地</code>，这样就能减少候选关系的数量：</p>
<img   src="/2022/05/10/Relation-Classification-with-Entity-Type-Restriction/example.jpg"  class="">
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><img   src="/2022/05/10/Relation-Classification-with-Entity-Type-Restriction/model.jpg"  class="">
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><img   src="/2022/05/10/Relation-Classification-with-Entity-Type-Restriction/algorithm.jpg"  class="">
<script type="math/tex; mode=display">
\begin{aligned}
R_{(t s, t o)} &=\left\{r \in R \mid(s, o) \in D_{r}\right\} \\
&=\{r \in R \mid t s \in S(r) \text { and } \text { to } \in O(r)\}
\end{aligned}</script><ol>
<li>首先根据句子中实体的类型将句子归好组</li>
<li>对于每一组，收集所有关系组成一个集合 $R_{(t s, t o)}$</li>
<li>针对该关系集合学习一个分类器 $f_g$</li>
</ol>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>由于该方法是模型无关的，所以作者在两个代表性模型上做了实验：</p>
<img   src="/2022/05/10/Relation-Classification-with-Entity-Type-Restriction/result.png"  class="">
<p><code>RECENT</code> 分别比 <code>GCN</code> 和 <code>SpanBERT</code> 高了6.9和4.4，提升还是非常明显的。</p>
<hr>
<h2 id="Cite"><a href="#Cite" class="headerlink" title="Cite"></a>Cite</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;lyu-chen-2021-relation,</span><br><span class="line">    title = &quot;Relation Classification with Entity Type Restriction&quot;,</span><br><span class="line">    author = &quot;Lyu, Shengfei and Chen, Huanhuan&quot;,</span><br><span class="line">    booktitle = &quot;Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021&quot;,</span><br><span class="line">    month = aug,</span><br><span class="line">    year = &quot;2021&quot;,</span><br><span class="line">    address = &quot;Online&quot;,</span><br><span class="line">    publisher = &quot;Association for Computational Linguistics&quot;,</span><br><span class="line">    url = &quot;https://aclanthology.org/2021.findings-acl.34&quot;,</span><br><span class="line">    doi = &quot;10.18653/v1/2021.findings-acl.34&quot;,</span><br><span class="line">    pages = &quot;390--395&quot;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Relation Classification</tag>
      </tags>
  </entry>
  <entry>
    <title>SENet在双塔中的应用</title>
    <url>/2024/02/05/SENet%E5%9C%A8%E5%8F%8C%E5%A1%94%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<p>SENet思想非常简单，模型结构如下：</p>
<span id="more"></span>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/96cf35a3f9d6db7affd5bca632a35891f42f9f91/image.4a55hw1xznq0.webp"  alt="SENet"></p>
<p>对比推荐系统，将CV图像的通道数 $channel_{num}$ 看作是user侧或ad侧的 $feature_{num}$ 。以user侧特征 $v_i$ 为例：</p>
<h2 id="Squeeze"><a href="#Squeeze" class="headerlink" title="Squeeze"></a>Squeeze</h2><script type="math/tex; mode=display">
z_i = \frac{\sum_{t=1}^k v_i^t}{k}</script><p>将特征 $v_i$ 的embeeding平均池化成 $z_i$，embedding size为 $k$</p>
<h2 id="Excitation"><a href="#Excitation" class="headerlink" title="Excitation"></a>Excitation</h2><script type="math/tex; mode=display">
S = \phi(W_2 \phi(W_1 Z)), Z \in R^{feature_{num}}</script><ul>
<li>$W_1$ 用于降维，从而实现将池化后的特征进行特征交互，生成中间向量</li>
<li>$\phi$ 是激活函数</li>
<li>$W_2$ 用于升维，将中间向量恢复成 $feature_{num}$ 个权重值，得到每个特征的重要性</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SENet并不能将user侧和item侧的特征交互提前或者使其获得更深层次的交互，user侧和item侧的特征交互仍然只发生在最后的内积那一步，这是由其双塔结构导致的。SENet的作用是提前将各侧的重要特征升权，不重要特征降权。</p>
<h2 id="个人疑问"><a href="#个人疑问" class="headerlink" title="个人疑问"></a>个人疑问</h2><p>既然SENet目的是为了对特征进行升降权，那么为什么不直接在特征侧引入一个可学习的参数矩阵呢？</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/358779957" >SENet双塔模型：在推荐领域召回粗排的应用及其它<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/32702350" >解读Squeeze-and-Excitation Networks（SENet）<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>Dual Tower</tag>
        <tag>SENet</tag>
        <tag>召回负例</tag>
        <tag>粗排负例</tag>
      </tags>
  </entry>
  <entry>
    <title>SGM: Sequence Generation Model for Multi-Label Classification</title>
    <url>/2025/03/09/SGM-Sequence-Generation-Model-for-Multi-Label-Classification/</url>
    <content><![CDATA[<p>为了建模多标签之间的依赖关系，本篇工作用序列生成的方式来解决该问题。</p>
<span id="more"></span>
<p>当前label的预测不仅依赖于输入上下文，也依赖于已输出的所有label。用seq2seq建模标签依赖是一种非常自然的思路，但存在如下两大问题：</p>
<ol>
<li>序列建模强调标签的先后顺序，即位置关系，而多标签是一个集合，不存在位置约束，哪个标签在前在后没有关系，只要输出正确就行。这种情况下，ground truth该如何构造？</li>
<li>序列生成是自回归形式，当前label的生成依赖于上一个label，如果上一个label是错误的，那么将会严重影响后续所有label的预测。这种情况下，该减轻预测错误的label所导致的连锁反应？</li>
</ol>
<p>SGM针对上述问题提出了如下建模思路：</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>经典的序列生成范式：</p>
<script type="math/tex; mode=display">
p(\boldsymbol{y} \mid \boldsymbol{x})=\prod_{i=1}^n p\left(y_i \mid y_1, y_2, \cdots, y_{i-1}, \boldsymbol{x}\right)</script><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.1e8r4ljlun.webp"  alt="model"></p>
<h4 id="问题1的解法"><a href="#问题1的解法" class="headerlink" title="问题1的解法"></a>问题1的解法</h4><p>作者根据训练集中标签出现的频次来构造标签序列：高频标签置前，低频标签置后。同时在序列头尾插入 <code>bos</code> 和 <code>eos</code> 表示序列的开始与结束。</p>
<h4 id="问题2的解法"><a href="#问题2的解法" class="headerlink" title="问题2的解法"></a>问题2的解法</h4><p>引入Global Embedding考虑所有可能label的信息，避免贪心依赖上一个label：</p>
<script type="math/tex; mode=display">
\overline{\boldsymbol{e}}=\sum_{i=1}^L y_{t-1}^{(i)} \boldsymbol{e}_i \\
g\left(\boldsymbol{y}_{t-1}\right)=(\mathbf{1}-\boldsymbol{H}) \odot \boldsymbol{e}+\boldsymbol{H} \odot \overline{\boldsymbol{e}} \\
\boldsymbol{H}=\boldsymbol{W}_1 \boldsymbol{e}+\boldsymbol{W}_2 \overline{\boldsymbol{e}}</script><p>$y_{t-1}$是在$t-1$时间步预测的标签概率分布，$e_i$是$l_i$的embedding。本质上就是根据概率分布对所有可能标签做加权求和。$H$则是门控机制，控制加权embedding的比例。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.2yyi43ptta.webp"  alt="exp"></p>
<p>加上GE效果更加明显！</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://aclanthology.org/C18-1330.pdf" >SGM<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/377543518?utm_psn=1879874767093494719" >多标签文本分类-如何有效的利用标签之间的关系<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://transformerswsz.github.io/2024/03/18/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E6%96%B0%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95/" >多标签分类新建模方法<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Multi-label Classification</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>SRU解读</title>
    <url>/2023/06/15/SRU%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>该篇论文实现了隐藏层维度的并行计算，但并没有解除时间步上的依赖。不过这样的改进，在模型训练和推理加速上的收益已经非常大了。</p>
<span id="more"></span>
<p>笔记见：<a class="link"   href="https://kdocs.cn/l/cbNfimpPLCvc" >https://kdocs.cn/l/cbNfimpPLCvc<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/353500337" >Simple Recurrent Units了解一下<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.zhihu.com/question/65244705" >如何评价新提出的RNN变种SRU?<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>RNN</tag>
        <tag>Parallelizing</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM</title>
    <url>/2019/06/22/SVM/</url>
    <content><![CDATA[<p>支持向量机(Support Vector Machine)是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM的学习算法是求解凸二次规划的最优化算法。</p>
<span id="more"></span>
<p>假设在一个二维线性可分的数据集中，图一A所示，我们要找到一条把两组数据分开，这条直线可以是图一B中的直线，也可以是图一C中的直线，或者图一D中的直线，但是哪条直线能够达到最好的泛化能力呢？那就是一个能使两类之间的空间大小最大的一个超平面。</p>
<img   src="/2019/06/22/SVM/1.png"  class="">
<p>这个超平面在二维平面上看到的就是一条直线，在三维空间中就是一个平面。因此，我们把这个划分数据的决策边界统称为超平面。<font color="green">离这个超平面最近的点就叫做支持向量，点到超平面的距离叫间隔。</font>支持向量机就是要使超平面和支持向量之间的间隔尽可能的大，这样超平面才可以将两类样本准确的分开，而保证间隔尽可能的大就是保证我们的分类器误差尽可能的小，尽可能的健壮。</p>
<h1 id="线性可分SVM"><a href="#线性可分SVM" class="headerlink" title="线性可分SVM"></a>线性可分SVM</h1><p>要使得支持向量到超平面的间隔最大化，我们首先定义超平面 $h(x)$ ：</p>
<script type="math/tex; mode=display">
h(x) = w^Tx + b \qquad w为权重向量，b为偏置向量</script><p>样本点 $x$ 到最优超平面的几何间隔为：</p>
<script type="math/tex; mode=display">
r = \frac {h(x)} {||w||} = \frac {w^T + b} {||w||}</script><p>$||w||$ 是向量 $w$ 的内积，即 $||w|| = \sqrt{w_0^2 + w_1^2 + \dots +  w_n^2 }$ ，而 $h(x)$ 表示函数间隔：</p>
<script type="math/tex; mode=display">
\hat{r} = h(x)</script><p>函数间隔 $h(x)$ 不是一个标准的间隔度量，它不适合用来做最大化的间隔值。因为，一旦超平面固定以后，如果我们人为的放大或缩小 $w$ 和 $b$ 值，那这个超平面也会无限的放大或缩小，这将对分类造成严重影响。而几何间隔是函数间隔除以 $w$ ，当 $w$ 的值无限放大或缩小时，$||w||$ 也会等倍地放大或缩小，而整个 $r$ 保持不变，它只随着超平面的移动而变化，不受两个参数的影响。因而，我们用几何间隔来做最大化间隔度量。</p>
<h2 id="最大化间隔"><a href="#最大化间隔" class="headerlink" title="最大化间隔"></a>最大化间隔</h2><p>在SVM中，我们把几何间隔 $r$ 作为最大化间隔，并且采用 $-1$ 和 $+1$ 作为类别标签。</p>
<p>如下图所示，在这个 $\mathbb{R}^2$ 空间中，假设我们已经确定了一个超平面（图中虚线），这个超平面的函数关系式为 $h(x) = w^Tx + b = 0$ 。我们想要所有的样本点都尽可能的原理这个超平面，只需保证支持向量的点 $x^*$ 尽可能地远离它。</p>
<img   src="/2019/06/22/SVM/2.png"  class="">
<p>我们把其中一个支持向量 $x^*$ 到最优超平面的距离定义为：</p>
<script type="math/tex; mode=display">
r^* = \frac {h(x^*)} {||w||} = 
\begin{cases}
\frac {1} {||w||}&  \quad {if : y* = h(x^*) = +1}\\
\frac {-1} {||w||}& \quad {if : y* = h(x^*) = -1}
\end{cases}</script><p>这是我们通过把函数间隔 $h(x)$ 固定为 $1$ 而得来的。我们可以把这个式子想象成还存在两个平面，这两个平面分别是 $w^Tx_s+b=1$ 和 $w^Tx_s+b=-1$ ，对应上图中的两根实线。这些支持向量 $x_s$ 就在这两个平面上，这两个平面离最优超平面的距离越大，我们的间隔也就越大。对于其他的点 $x_i$ 如果满足 $w^Tx_i+b&gt;1$ ，则被分为 $1$ 类，如果满足满足 $w^Tx_i+b&lt;-1$ ，则被分为 $-1$ 类。即有约束条件：</p>
<script type="math/tex; mode=display">
\begin{cases}
w^Tx_i+b \geqslant 1 &  \quad y_i = +1 \\
w^Tx_i+b \leqslant -1 & \quad y_i = -1
\end{cases}</script><p>支持向量到超平面的距离知道后，那么分割的间隔 $\gamma$ 为：</p>
<script type="math/tex; mode=display">
\gamma = 2r^* = \frac {2} {||w||}</script><p>注意到最大化 $\frac {2} {||w||}$ 和最小化 $\frac {1} {2} ||w||^2$ 是等价的，于是就得到下面的线性可分支持向量机学习的最优化问题：</p>
<script type="math/tex; mode=display">
\begin{cases}
\min_{w, b} \; \frac {1} {2} \|w\|^2 \\
y_i(w^Tx_i+b) \geqslant 1, \quad (i = 1, \dots , n) 
\end{cases}</script><p>这种式子采用拉格朗日乘数法来求解，即：</p>
<script type="math/tex; mode=display">
L(x) = f(x) + \sum \alpha g(x)</script><p>$f(x)$ 是我们需要最小化的目标函数， $g(x)$ 是不等式约束条件， $\alpha$ 是对应的约束系数，也称拉格朗日乘子。为了使得拉格朗日函数得到最优解，我们需要加入能使该函数有最优化解法的KKT条件，或者叫最优化条件。即假设存在一点 $x^*$：</p>
<ul>
<li>$L(x^{\star})$ 对 $x^{\star}$ 求导为 $0$</li>
<li>$\alpha_ig_i(x^*)=0$ 对于所有的 $i = 1,\dots,n$</li>
</ul>
<p>这样构造的拉格朗日函数为：</p>
<script type="math/tex; mode=display">
L(w, b, a) = \frac {1} {2} w^Tw - \sum_{i=1}^n a_i[y_i(w^T x_i + b) - 1]</script><p>以上的KKT条件 $\alpha_i[y_i(w^Tx_i+b)-1] = 0$ 表示，只有距离最优超平面的支持向量 $(x_i, y_i)$ 对应的 $\alpha$ 非零，其他所有点集的 $\alpha$ 等于零。综上所述，引入拉格朗日乘子后，我们的目标变为：</p>
<script type="math/tex; mode=display">
\min_{w,b}\max_{a \geqslant 0} L(w, b, a)</script><p>根据拉格朗日对偶性，目标问题的对偶问题是极大极小问题，即先求 $L(w, b, \alpha)$ 对 $w, b$ 的极小，再求对 $\alpha$ 的极大：</p>
<script type="math/tex; mode=display">
\max_{a \geqslant 0} \min_{w, b} L(w, b, \alpha)</script><p>用 $L(w, b, \alpha)$ 对 $w$ 和 $b$ 分别求导，并令其等于 $0$ ：</p>
<script type="math/tex; mode=display">
\begin{cases}
\frac {\partial L(w, b, \alpha)} {\partial w} = 0\\
\frac {\partial L(w, b, \alpha)} {\partial b} = 0
\end{cases}</script><p>得到：</p>
<script type="math/tex; mode=display">
\begin{cases}
w = \sum_{i=1}^n \alpha_i y_i x_i \\
\sum_{i=1}^n \alpha_i y_i = 0
\end{cases}</script><p>把该式代入原来的拉格朗日式子得（推导见《统计学习方法》P103~P105）：</p>
<script type="math/tex; mode=display">
L(\alpha) = \sum_{i=1}^n \alpha_i - \frac {1} {2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \geqslant 0 (i = 1, \dots, n)</script><p>该 $L(\alpha)$ 函数消去了向量 $w$ 和向量 $b$ ，仅剩 $\alpha$ 这个未知参数，只要我们能够最大化 $L(\alpha)$，就能求出对应的 $\alpha$ ，进而求得  $w$ 和 $b$ 。对于如何求解 $\alpha$，SMO算法给出了完美的解决方案，下一节我们详细讲述。这里我们假设通过SMO算法确定了最优 $\alpha^*$，则：</p>
<script type="math/tex; mode=display">
w^* = \sum_{i=1}^n \alpha_i^* y_i x_i</script><p>最后使用一个正的支持向量 $x^*$ ，就可以计算出 $b$ ：</p>
<script type="math/tex; mode=display">
b^* = 1 - w^{*T} x^*</script><h2 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a><a id="softgap">软间隔</a></h2><p>以上的推导都是在<font color="red">完全线性可分</font>的条件下进行的，但是现实世界的许多问题并不都是线性可分的，尤其存在许多复杂的非线性可分的情形。要解决这些线性不可分问题，有如下两种方法：</p>
<ul>
<li>放宽严格的间隔，构造<a href="#softgap">软间隔</a>。</li>
<li>运用<a href="#kernel">核函数</a>将数据映射到高维空间，然后在高维空间中寻找超平面进行线性划分。</li>
</ul>
<p>我们首先讨论软间隔。假设两个类有几个数据点混在一起，这些点对最优超平面形成了噪声干扰，软间隔就是要扩展一下我们的目标函数和KKT条件，允许少量这样的噪声存在。具体地说，就要引入松驰变量 $\xi$ 来量化分类器的违规行为：</p>
<script type="math/tex; mode=display">
\begin{cases}
\min_{w, b} \; \frac {1} {2} \|w\|^2 + C\sum_{i=1}^n \xi_i, \quad C为惩罚因子 \\
y_i (w^T x_i + b) \geqslant 1 - \xi_i , \quad i = 1,\dots, n \\
\xi_i \geqslant 0 ,  \quad i = 1, \dots , n
\end{cases}</script><h3 id="C-和-xi"><a href="#C-和-xi" class="headerlink" title="$C$ 和 $\xi$"></a>$C$ 和 $\xi$</h3><img   src="/2019/06/22/SVM/3.jpg"  class="">
<p>如上图所示，$\xi$ 表示噪声样本点到本类样本点边界的偏移距离。$C$ 可被视为一个由用户依据经验或分析选定的“正则化”参数。噪声点在现实世界是天然存在的，如果对于他们不进行容错，那么我们是无论如何也不能把样本分开的。而引入惩罚因子，目的就是，对这类误分的样本进行容错，相当于把点拉到正确一侧：</p>
<ul>
<li>当 $C$ 很大时，$\xi$ 趋近于0，表示惩罚很大，容忍度很低。这样错分较少，对样本的拟合性较好，但容易过拟合。</li>
<li>当 $C$ 很小时，$\xi$ 变大，表示惩罚很小，容忍度高。这样错分较多，对样本的拟合性下降。</li>
</ul>
<p>对上述不等式同样运用拉格朗日乘子法和KKT条件得（推导见《统计学习方法》P109~P111）：</p>
<script type="math/tex; mode=display">
L(\alpha) = \sum_{i=1}^n \alpha_i - \frac {1} {2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
\sum_{i=1}^n \alpha_i y_i = 0, \quad 0 \leqslant \alpha_i \leqslant C (i = 1, \dots , n)</script><p>可以看到，松驰变量 $\xi$ 没有出现在 $L(\alpha)$ 中，线性可分与不可分的差异体现在约束 $\alpha_i \geqslant 0$ 被替换成了约束 $0 \leqslant \alpha_i \leqslant C$。但是，这两种情况下求解 $w$ 和 $b$ 是非常相似的，对于支持向量的定义也都是一致的。在不可分情况下，对应的KKT条件为：</p>
<script type="math/tex; mode=display">
\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i] = 0, \quad (i = 1, \dots, n)</script><h2 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h2><p>1996年， John Platt发布了一个称为SMO的强大算法，用于训练SVM。 SMO表示序列最小优化（Sequential Minimal Optimization）。 Platt的SMO算法是将大优化问题分解为多个小优化问题来求解，这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果是完全一致的。</p>
<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>求出一系列 $\alpha$，一旦求出了这些  $\alpha$，就很容易计算出权重向量 $w$ 和 $b$，并得到分隔超平面。</p>
<h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><p>每次循环中选择两个 $\alpha$ 进行优化处理。一旦找到一对合适的 $\alpha$ ，那么就增大其中一个同时减小另一个。这里所谓的“合适”就是指两个 $\alpha$ 必须要符合一定的条件，条件之一就是这两个 $\alpha$ 必须要在间隔边界之外，而其第二个条件则是这两个  $\alpha$ 还没有进行过区间化处理或者不在边界上。<br>对SMO具体的分析如下，在上一节我们已经得出了：</p>
<script type="math/tex; mode=display">
L(\alpha) = \sum_{i=1}^n \alpha_i - \frac {1} {2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
\sum_{i=1}^n \alpha_i y_i = 0, \quad 0 \leqslant \alpha_i \leqslant C (i = 1, \dots , n)</script><p>其中 $(x_i, y_i)$ 已知，$C$ 可以人为设定。现在就是要最大化 $L(\alpha)$ ，求得参数 $\alpha = [\alpha_1, \alpha_2, \dots, \alpha_n]$。SMO算法是一次选择两个 $\alpha$ 进行优化，那我们就选择 $\alpha_1$ 和 $\alpha_2$ ，然后把其它参数 $[\alpha_3, \alpha_4, \dots, \alpha_n]$ 固定，这样 $\alpha_1, \alpha_2$ 表示为下面的式子，其中 $\zeta$ 为实数值：</p>
<script type="math/tex; mode=display">
\alpha_1 y_1 + \alpha_2 y_2 = - \sum_{i=3}^n \alpha_i y_i = \zeta</script><p>然后用 $\alpha_2$ 来表示 $\alpha_1$ ：</p>
<script type="math/tex; mode=display">
\alpha_1 = \frac {\zeta - \alpha_2 y_2} {y_1}</script><p>把上式代入 $L(\alpha)$ 中：</p>
<script type="math/tex; mode=display">
L(\alpha) = L(\frac {\zeta - \alpha_2 y_2} {y_1}, \alpha_2, \dots, \alpha_n)</script><p>省略一系列化解过程后，最后会化解成我们熟悉的一元二次方程，$a, b, c$ 均是实数值：</p>
<script type="math/tex; mode=display">
L(\alpha_2) = a \alpha_2^2 + b \alpha_2 + c</script><p>最后对 $\alpha_2$ 求导，解得 $\alpha_2$ 的具体值，我们暂时把这个实数值叫 $\alpha_2^{\star}$ ，而这个 $\alpha_2^{\star}$ 需要满足一个条件 $L \leqslant \alpha_2^{\star} \leqslant H$ ，如下图所示：</p>
<img   src="/2019/06/22/SVM/4.png"  class="">
<p>根据之前的条件 $0 \leqslant \alpha_i \leqslant C$ 和等式 $\alpha_1 y_1 + \alpha_2 y_2 = \zeta$ ，当 $y_1$ 和 $y_2$ 异号时：</p>
<script type="math/tex; mode=display">
\begin{cases}
L &= \max(0, \alpha_2 - \alpha_1) \\
H &= \min(C, C + \alpha_2 - \alpha_1)
\end{cases}</script><p>当 $y_1$ 和 $y_2$ 同号时：</p>
<script type="math/tex; mode=display">
\begin{cases}
L &= \max(0, \alpha_2 + \alpha_1 - C) \\
H &= \min(C, \alpha_2 + \alpha_1)
\end{cases}</script><p>最后，满足条件的 $\alpha_2$ 应该由下面的式子得到， $\alpha_2^{\star\star}$ 才为最终的值：</p>
<script type="math/tex; mode=display">
\alpha_2^{**} = \begin{cases}
H, \quad \alpha_2^* > H \\
\alpha_2^*, \quad L \leqslant \alpha_2^* \leqslant H \\
L, \quad \alpha_2^* < L
\end{cases}</script><p>求得 $\alpha_2^{\star\star}$ 后就能求得 $\alpha_1^{\star\star}$ 了，然后我们重复地按照最优化 $ (\alpha_1, \alpha_2) $ 的方式继续选择 $ (\alpha_3, \alpha_4), (\alpha_5, \alpha_6), \dots, (\alpha_{n-1}, \alpha_n) $ 进行优化求解，这样 $ \alpha = [\alpha_1, \alpha_2, \dots, \alpha_n] $ 求解出来后，整个线性划分问题就迎刃而解。</p>
<h1 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a><a id="kernel">核函数</a></h1><p>对于以上几节讲的SVC算法，我们都在线性可分或存在一些噪声点的情况下进行的二分类，但是如果我们存在两组数据，它们的散点图如下图所示，你可以看出这完全是一个非线性不可分的问题，我们无法使用之前讲的SVC算法在这个二维空间中找到一个超平面把这些数据点准确的分开。</p>
<img   src="/2019/06/22/SVM/5.png"  class="">
<p>解决这个划分问题我们需要引入一个核函数，核函数能够恰当地计算给定数据的内积，将数据从输入空间的非线性转变到特征空间，特征空间具有更高甚至无限的维度，从而使得数据在该空间中被转换成线性可分的。如下图所示，我们把二维平面的一组数据，通过核函数映射到了一个三维空间中，这样，我们的超平面就面成了一个平面（在二维空间中是一条直线），这个平面就可以准确的把数据划分开了。</p>
<img   src="/2019/06/22/SVM/6.png"  class="">
<p>核函数有Sigmoid核、线性核、多项式核和高斯核等，其中高斯核和多项式核比较常用，两种核函数均可以把低维数据映射到高维数据。高斯核的公式如下，$\sigma$ 是达到率，即函数值跌落到 $0$ 的速度参数：</p>
<script type="math/tex; mode=display">
K(x_1, x_2) = e^{\frac {- \|x_1 - x_2\|^2} {2 \sigma^2}}</script><p>多项式核函数的公式如下，$R$ 为实数，$d$ 为低维空间的维数：</p>
<script type="math/tex; mode=display">
K(x_1, x_2) = (\langle x_1, x_2 \rangle + R)^d</script><p>应用于我们的上个例子，我们先定义用 $\phi : x \to H$ 表示从输入空间 $x \subset \mathbb{R}^n$ 到特征空间 $H$ 的一个非线性变换。假设在特征空间中的问题是线性可分的，那么对应的最优超平面为：</p>
<script type="math/tex; mode=display">
w^{\phi T} \phi(x) + b = 0</script><p>通过拉格朗日函数我们推导出：</p>
<script type="math/tex; mode=display">
w^{\phi *} = \sum_{i=1}^n \alpha_i^* y_i \phi(x_i)</script><p>代入上式的特征空间的最优超平面为：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n \alpha_i^* y_i \phi^T(x_i) \phi(x) + b = 0</script><p>这里的 $\phi^T(x_i) \phi(x)$ 表示内积，用核函数代替内积则为：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^n \alpha_i^* y_i K(x_i, x) + b = 0</script><p>我们的核函数均是内积函数，通过在低维空间对输入向量求内积来映射到高维空间，从而解决在高维空间中数据线性可分的问题。</p>
<p>我们以多项式核来解释一下为什么核函数可以把低维数据映射成高维数据。</p>
<p>假设有两个输入样本，它们均为二维行向量 $a = [x_1, x_2], b = [x_3, x_4]$ ，他们的内积为：</p>
<script type="math/tex; mode=display">
\langle a, b \rangle = a b^T = \begin{bmatrix} x_1 & x_2 \end{bmatrix} \begin{bmatrix} x_3 \\ x_4 \end{bmatrix} = x_1 x_3 + x_2 x_4</script><p>用多项式核函数进行映射，令 $R=0, d=2$：</p>
<script type="math/tex; mode=display">
K(a, b) = (\langle x_1, x_2 \rangle)^2 = (x_1 x_3 + x_2 x_4)^2 = x_1^2 x_3^2 + 2x_1 x_2 x_3 x_4 + x_2^2 x_4^2 = \phi(a) \phi(b)</script><p>按照线性代数中的标准定义， $\phi(a)$ 和 $\phi(b)$ 为映射后的三维行向量和三维列向量，即：</p>
<script type="math/tex; mode=display">
\phi(a) = \begin{bmatrix} x_1^2 & \sqrt 2 x_1 x_2 & x_2^2 \end{bmatrix} \\
\phi(b) = \begin{bmatrix} x_3^2 \\ \sqrt2 x_3 x_4 \\ x_4^2  \end{bmatrix}</script><p>它们的内积用向量的方式表示则更直观：</p>
<script type="math/tex; mode=display">
\phi(a) \phi(b) = \begin{bmatrix} x_1^2 & \sqrt 2 x_1 x_2 & x_2^2 \end{bmatrix} \begin{bmatrix} x_3^2 \\ \sqrt2 x_3 x_4 \\ x_4^2  \end{bmatrix} = x_1^2 x_3^2 + 2x_1 x_2 x_3 x_4 + x_2^2 x_4^2</script><p>这样我们就把二维数据映射成了三维数据。对于高斯核的映射，会用到泰勒展开式，这个后面再学习了。</p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>上面说了那么多，全是从数学角度进行分析推导的。你可能明白了SVM的数学原理，当你进行编程的时候，还是一脸懵逼。因为如果按照上面的求解过程来的话，实在是太复杂了。但是在计算机里，求解SVM却是非常简单的事。我们只需给出SVC的损失函数，然后使用GD算法，就能很好地求出 $\theta$ ，也就是上述的 $w$ ：</p>
<script type="math/tex; mode=display">
J(\theta) = \min_{\theta} C [ \sum_{i=1}^m y^{(i)} cost_1(\theta^T x^{(i)}) + (1 - y^{(i)})cost_0(\theta^T x^{(i)}) ] + \frac {1} {2} \sum_{j=1}^n \theta_j^2 \quad C 为惩罚因子</script><p>上述函数分析具体可见：<a class="link"   href="https://github.com/TransformersWsz/Halfrost-Field/blob/master/contents/Machine_Learning/Support_Vector_Machines.ipynb" >Support_Vector_Machines.ipynb<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/vipyoumay/p/7560061.html?tdsourcetag=s_pcqq_aiomsg" >SVM原理以及Tensorflow 实现SVM分类(附代码) <i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://baijiahao.baidu.com/s?id=1621964725382082396&amp;wfr=spider&amp;for=pc" >深度讲解支持向量机背后的数学思想<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/TransformersWsz/Halfrost-Field/blob/master/contents/Machine_Learning/Support_Vector_Machines.ipynb" >Support_Vector_Machines.ipynb<i class="fas fa-external-link-alt"></i></a></li>
<li>《统计学习方法》</li>
<li><a class="link"   href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1052089362&amp;courseId=1004570029" >吴恩达机器学习<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Classification</tag>
      </tags>
  </entry>
  <entry>
    <title>STL学习</title>
    <url>/2021/08/12/STL%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>记录一下常见的STL的概念和用法。</p>
<span id="more"></span>
<h2 id="vector"><a href="#vector" class="headerlink" title="vector"></a><code>vector</code></h2><p><code>vector</code> 是一个自动扩容的容器，支持随机访问，底层通过动态数组实现。</p>
<h4 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h4><p>当 <code>vector</code> 执行 <code>insert</code> 或者 <code>push_back</code> 时，如果当容器的存储数量已经达到容量，就会触发扩容。具体流程如下：</p>
<ol>
<li>申请新的内存空间（原内存空间的1.5~2倍）</li>
<li>把原空间的元素拷贝到新的空间里</li>
<li>释放原空间</li>
<li>数组指针指向新空间</li>
</ol>
<h4 id="成员函数"><a href="#成员函数" class="headerlink" title="成员函数"></a>成员函数</h4><ul>
<li><code>size()</code> : 容器中元素的个数</li>
<li><code>capacity()</code> : 器在分配那块内存上可以容纳的元素的个数</li>
<li><code>resize(n)</code> : 强制将容器改为容纳为 <code>n</code> 个数据，分三种情况讨论：<ol>
<li><code>n &lt; size()</code> : 容器尾部元素被销毁</li>
<li><code>n &gt; size()</code> : 新构造的元素会添加到末尾</li>
<li><code>n &gt; capacity()</code> : 在元素加入前会进行重新分配</li>
</ol>
</li>
<li><code>reserve(n)</code> : 强制容器把它的容量改为不小于 <code>n</code> 。如果 <code>n</code> 小于当前容量，则 <code>vector</code> 会忽略它</li>
</ul>
<p><code>resize</code> 和 <code>reserve</code> 都保证了 <code>vector</code> 空间的大小，至少达到它们参数所指定的大小。</p>
<h4 id="push-back-与-emplace-back-区别"><a href="#push-back-与-emplace-back-区别" class="headerlink" title="push_back 与 emplace_back 区别"></a><code>push_back</code> 与 <code>emplace_back</code> 区别</h4><p><code>emplace_back</code> 和 <code>push_back</code> 的区别，就在于底层实现的机制不同。</p>
<ul>
<li><code>push_back</code> 向容器尾部添加元素时，首先会创建这个元素，然后再将这个元素拷贝或者移动到容器中（如果是拷贝的话，事后会自行销毁先前创建的这个元素）；</li>
<li><code>emplace_back</code> 是C++ 11标准新增加的成员函数。在实现时，则是直接在容器尾部创建这个元素，省去了拷贝或移动元素的过程。</li>
</ul>
<h2 id="map-与-unordered-map"><a href="#map-与-unordered-map" class="headerlink" title="map 与 unordered_map"></a><code>map</code> 与 <code>unordered_map</code></h2><ul>
<li><code>map</code> 内部结构是由<a class="link"   href="https://transformerswsz.github.io/2021/08/23/BST%20&amp;%20AVL%20&amp;%20RBT/" >RBT<i class="fas fa-external-link-alt"></i></a>来实现的。查找、插入、删除都很稳定，时间复杂度为 $O(log_2 n)$</li>
<li><code>unordered_map</code> 内部是一个hash_table，一般是由一个大vector，vector元素节点可挂接链表来解决冲突。<ul>
<li><code>hash_map</code>：由于在C++标准库中没有定义散列表hash_map，标准库的不同实现者将提供一个通常名为hash_map的非标准散列表。因为这些实现不是遵循标准编写的，所以它们在功能和性能保证上都有微妙的差别。从C++11开始，哈希表实现已添加到C++标准库标准。决定对类使用备用名称，以防止与这些非标准实现的冲突，并防止在其代码中有hash_table的开发人员无意中使用新类。所选择的备用名称是unordered_map，它更具描述性，因为它暗示了类的映射接口和其元素的无序性质。可见 <code>hash_map</code> 跟 <code>unordered_map</code> 本质是一样的，只不过 <code>unordered_map</code> 被纳入了C++标准库标准。</li>
</ul>
</li>
</ul>
<img   src="/2021/08/12/STL%E5%AD%A6%E4%B9%A0/unordered_map.jpeg"  class="">
<h4 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h4><ul>
<li>若考虑有序，查询速度稳定，非频繁查询那么考虑使用 <code>map</code></li>
<li>若非常高频查询(100个元素以上，unordered_map都会比map快)，内部元素可非有序，数据大超过1k甚至几十万上百万时候就要考虑使用 <code>unordered_map</code></li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/Payshent/article/details/73835795?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.base&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.base" >STL中vector的实现及面试问题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/xx18030637774/article/details/82780878" >C/C++之vector的内存管理和效率<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://c.biancheng.net/view/vip_7721.html" >快速入门c++ stl<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq_30392565/article/details/51835770" >hash_map/unordered_map原理和使用整理<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>C++</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM常见面试问答</title>
    <url>/2021/06/17/SVM%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E7%AD%94/</url>
    <content><![CDATA[<p>记录一下SVM的常见面试问答：</p>
<span id="more"></span>
<h2 id="1-SVM原理"><a href="#1-SVM原理" class="headerlink" title="1. SVM原理"></a>1. SVM原理</h2><p>SVM 是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。</p>
<ul>
<li>当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即硬间隔支持向量机。</li>
<li>当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即软间隔支持向量机。</li>
<li>当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</li>
</ul>
<h2 id="2-SVM为什么采用间隔最大化？"><a href="#2-SVM为什么采用间隔最大化？" class="headerlink" title="2. SVM为什么采用间隔最大化？"></a>2. SVM为什么采用间隔最大化？</h2><ul>
<li>唯一解：当训练数据线性可分时，存在无穷多个超平面可以将两类数据正确分开。线性可分支持向量利用间隔最大化求得最优超平面，此时，解是唯一的。</li>
<li>强泛化性：划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强。</li>
</ul>
<h2 id="3-SVM为什么要从原始问题变为对偶问题来求解？"><a href="#3-SVM为什么要从原始问题变为对偶问题来求解？" class="headerlink" title="3. SVM为什么要从原始问题变为对偶问题来求解？"></a>3. SVM为什么要从原始问题变为对偶问题来求解？</h2><p>原问题：</p>
<script type="math/tex; mode=display">
\min _{w, b} \max _{\lambda} L(w, b, \lambda)=\frac{1}{2}\|w\|^{2}+\sum_{i=1}^{n} \lambda_{i}\left[1-y_{i}\left(w^{T} x_{i}+b\right)\right]</script><p>对偶问题：</p>
<script type="math/tex; mode=display">
\max _{\lambda} \min _{w, b} L(w, b, \lambda)=\frac{1}{2}\|w\|^{2}+\sum_{i=1}^{n} \lambda_{i}\left[1-y_{i}\left(w^{T} x_{i}+b\right)\right]</script><ul>
<li>对偶问题往往更易求解：<ul>
<li>问题复杂度降低：由求特征向量 $w$ 转化为求比例系数 $\lambda$，在原始问题下，求解的复杂度与 $w$ 的维度有关。在对偶问题下，只与样本数量 $n$ 有关；</li>
<li>求解更高效：因为只用求解比例系数 $\lambda$ ，而 $\lambda$ 只有支持向量才为非0，其他全为0；</li>
</ul>
</li>
<li>可以自然引入核函数，进而推广到非线性分类问题。</li>
</ul>
<h2 id="4-KKT条件的作用"><a href="#4-KKT条件的作用" class="headerlink" title="4. KKT条件的作用"></a>4. KKT条件的作用</h2><script type="math/tex; mode=display">
d^{*}=\max _{\alpha, \beta: \alpha_{i} \geq 0} \min _{w} \mathcal{L}(w, \alpha, \beta) \leq \min _{w} \max _{\alpha, \beta: \alpha_{i} \geq 0} \mathcal{L}(w, \alpha, \beta)=p^{*}</script><h2 id="5-SMO基本原理"><a href="#5-SMO基本原理" class="headerlink" title="5. SMO基本原理"></a>5. SMO基本原理</h2><p>SMO每次选择两个变量 $\alpha_i$ 和 $\alpha_j$ ，并固定其它参数。在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p>
<ol>
<li>选取一对需更新的变量 $\alpha_i$ 和 $\alpha_j$ ；</li>
<li>固定 $\alpha_i$ 和 $\alpha_j$ 以外的参数，求解对偶问题 $max_{\alpha} (\sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j})$ 以获得更新后的  $\alpha_i$ 和 $\alpha_j$  .</li>
</ol>
<p>至于如何选取，SMO采用了启发式方法：使选取的两变量所对应的样本之间的间隔最大。直观解释：这样的两变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。</p>
<h2 id="6-什么是软间隔支持向量机？"><a href="#6-什么是软间隔支持向量机？" class="headerlink" title="6. 什么是软间隔支持向量机？"></a>6. 什么是软间隔支持向量机？</h2><blockquote>
<p>当训练数据近似线性可分时，软间隔最大化允许出现分类错误，此时超平面不能将所有训练数据点都分类正确。</p>
</blockquote>
<p>引入松弛变量 $\xi$ 来量化分类器的违规行为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\min _{W, b, \xi} \frac{1}{2}\|W\|^{2}+C \sum_{i=1}^{n} \xi_{i} \\
&\text { s.t. } y_{i}\left(X_{i}^{T} W+b\right) \geq 1-\xi_{i} \\
&\xi_{i} \geq 0, i=1,2, \ldots n
\end{aligned}</script><p>$C$ 是惩罚参数，越小对误分类惩罚越小，容易欠拟合；越大对误分类惩罚越大，容易过拟合。</p>
<h2 id="7-核函数作用以及如何选取核函数？"><a href="#7-核函数作用以及如何选取核函数？" class="headerlink" title="7. 核函数作用以及如何选取核函数？"></a>7. 核函数作用以及如何选取核函数？</h2><blockquote>
<p>当样本在原始空间线性不可分时，核函数可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。</p>
</blockquote>
<p>核函数有如下几种：</p>
<ul>
<li>线性核</li>
<li>多项式核</li>
<li>高斯核（RBF核）</li>
<li>Sigmoid核</li>
</ul>
<p>核函数选取技巧：</p>
<ul>
<li>如果特征的数量大到和样本数量差不多，则选用LR或者线性核函数；</li>
<li>如果特征的数量小，样本的数量正常，则选用高斯核函数；</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/43827793" >SVM 高频面试题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/35755150" >推导 | SVM详解（1）SVM基本型<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/33940537" >支持向量机SVM总结之拉格朗日对偶问题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/93715996" >【机器学习面试总结】—— SVM<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/breezezz/p/11303722.html" >拉格朗日对偶性<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>Self-Instruct</title>
    <url>/2023/10/11/Self-Instruct/</url>
    <content><![CDATA[<p>本篇工作利用LLM的生成能力，来产生大量指令数据集（指令、输入、输出），无需人工标注数据。</p>
<span id="more"></span>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.4zfugmx2oqc0.webp"  alt="flow"></p>
<p>其中，在对任务判别的时候，需要区分是输出优先还是输入优先：</p>
<ul>
<li>输入优先没问题，符合人类直觉，给定指令和输入，然后产生输出</li>
<li>当任务是分类任务的时候，采用输出优先，即先生成一个标签，然后根据标签生成相应的输入文本。这是因为分类任务，如果输入优先，模型倾向于生成正确的文本，比如语法正确的语句，不会产生错误的语句。因此先给出标签“错误”，强制模型根据错误标签生成错误的语句</li>
</ul>
<p>根据LLM生成的指令来微调LLM，更多是为了提升LLM在零样本任务上的泛化能力：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.6x9fo3xko5s0.webp"  alt="ret"></p>
<p>千万不要误解成了模型自己生成输入和标签，然后自己学习，自娱自乐。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Instruction</tag>
      </tags>
  </entry>
  <entry>
    <title>SimCSE论文及源码解读</title>
    <url>/2022/05/01/SimCSE%E8%AE%BA%E6%96%87%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>对比学习的思想是拉近同类样本的距离，增大不同类样本的距离，目标是要从样本中学习到一个好的语义表示空间。SimCSE是一种简单的无监督对比学习框架，它通过对同一句子两次Dropout得到一对正样例，将该句子与同一个batch内的其它句子作为一对负样例。模型结构如下所示：</p>
<span id="more"></span>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/simcse.ldig50thwww.jpg"  alt="simcse"></p>
<p>损失函数为：</p>
<script type="math/tex; mode=display">
\ell_{i}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i}}, \mathbf{h}_{i}^{z_{i}^{\prime}}\right) / \tau}}{\sum_{j=1}^{N} e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i}}, \mathbf{h}_{j}^{z_{j}^{\prime}}\right) / \tau}}</script><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>在作者的代码中，并不是将一个句子输入到模型中两次，而是复制一份放到同一个batch里。模型的核心是 <a href="https://github.com/princeton-nlp/SimCSE/blob/e3aa97b6d04c3d84f6bc46abb06c1bd056cab6d7/simcse/models.py#L97"><code>cl_forward</code></a> 函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cl_forward</span>(<span class="params">cls,</span></span><br><span class="line"><span class="params">    encoder,</span></span><br><span class="line"><span class="params">    input_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    attention_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    token_type_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    position_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    head_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    inputs_embeds=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    labels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    output_attentions=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    output_hidden_states=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    return_dict=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    mlm_input_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    mlm_labels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    return_dict = return_dict <span class="keyword">if</span> return_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> cls.config.use_return_dict</span><br><span class="line">    ori_input_ids = input_ids    <span class="comment"># 形状为[bs, num_sent, sent_len], bs=32</span></span><br><span class="line">    batch_size = input_ids.size(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># Number of sentences in one instance</span></span><br><span class="line">    <span class="comment"># 2: pair instance，[自己，自己]; 3: pair instance with a hard negative，[自己，自己，难例]</span></span><br><span class="line">    num_sent = input_ids.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    mlm_outputs = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># Flatten input for encoding</span></span><br><span class="line">    input_ids = input_ids.view((-<span class="number">1</span>, input_ids.size(-<span class="number">1</span>))) <span class="comment"># [bs * num_sent, sent_len]</span></span><br><span class="line">    attention_mask = attention_mask.view((-<span class="number">1</span>, attention_mask.size(-<span class="number">1</span>))) <span class="comment"># [bs * num_sent, sent_len]</span></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        token_type_ids = token_type_ids.view((-<span class="number">1</span>, token_type_ids.size(-<span class="number">1</span>))) <span class="comment"># [bs * num_sent, sent_len]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get raw embeddings, [bs, num_sent, sent_len, hidden_size]</span></span><br><span class="line">    outputs = encoder(</span><br><span class="line">        input_ids,</span><br><span class="line">        attention_mask=attention_mask,</span><br><span class="line">        token_type_ids=token_type_ids,</span><br><span class="line">        position_ids=position_ids,</span><br><span class="line">        head_mask=head_mask,</span><br><span class="line">        inputs_embeds=inputs_embeds,</span><br><span class="line">        output_attentions=output_attentions,</span><br><span class="line">        output_hidden_states=<span class="literal">True</span> <span class="keyword">if</span> cls.model_args.pooler_type <span class="keyword">in</span> [<span class="string">&#x27;avg_top2&#x27;</span>, <span class="string">&#x27;avg_first_last&#x27;</span>] <span class="keyword">else</span> <span class="literal">False</span>,</span><br><span class="line">        return_dict=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MLM auxiliary objective</span></span><br><span class="line">    <span class="keyword">if</span> mlm_input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mlm_input_ids = mlm_input_ids.view((-<span class="number">1</span>, mlm_input_ids.size(-<span class="number">1</span>)))</span><br><span class="line">        mlm_outputs = encoder(</span><br><span class="line">            mlm_input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            position_ids=position_ids,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            inputs_embeds=inputs_embeds,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            output_hidden_states=<span class="literal">True</span> <span class="keyword">if</span> cls.model_args.pooler_type <span class="keyword">in</span> [<span class="string">&#x27;avg_top2&#x27;</span>, <span class="string">&#x27;avg_first_last&#x27;</span>] <span class="keyword">else</span> <span class="literal">False</span>,</span><br><span class="line">            return_dict=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pooling</span></span><br><span class="line">    pooler_output = cls.pooler(attention_mask, outputs)</span><br><span class="line">    pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-<span class="number">1</span>))) <span class="comment"># (bs, num_sent, hidden_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># If using &quot;cls&quot;, we add an extra MLP layer</span></span><br><span class="line">    <span class="comment"># (same as BERT&#x27;s original implementation) over the representation.</span></span><br><span class="line">    <span class="keyword">if</span> cls.pooler_type == <span class="string">&quot;cls&quot;</span>:</span><br><span class="line">        pooler_output = cls.mlp(pooler_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Separate representation, [bs, hidden_size], 同一样本经过“两次Dropout”得到的两个句向量</span></span><br><span class="line">    z1, z2 = pooler_output[:,<span class="number">0</span>], pooler_output[:,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Hard negative</span></span><br><span class="line">    <span class="keyword">if</span> num_sent == <span class="number">3</span>:</span><br><span class="line">        z3 = pooler_output[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gather all embeddings if using distributed training</span></span><br><span class="line">    <span class="keyword">if</span> dist.is_initialized() <span class="keyword">and</span> cls.training:</span><br><span class="line">        <span class="comment"># Gather hard negative</span></span><br><span class="line">        <span class="keyword">if</span> num_sent &gt;= <span class="number">3</span>:</span><br><span class="line">            z3_list = [torch.zeros_like(z3) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dist.get_world_size())]</span><br><span class="line">            dist.all_gather(tensor_list=z3_list, tensor=z3.contiguous())</span><br><span class="line">            z3_list[dist.get_rank()] = z3</span><br><span class="line">            z3 = torch.cat(z3_list, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Dummy vectors for allgather</span></span><br><span class="line">        z1_list = [torch.zeros_like(z1) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dist.get_world_size())]</span><br><span class="line">        z2_list = [torch.zeros_like(z2) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dist.get_world_size())]</span><br><span class="line">        <span class="comment"># Allgather</span></span><br><span class="line">        dist.all_gather(tensor_list=z1_list, tensor=z1.contiguous())</span><br><span class="line">        dist.all_gather(tensor_list=z2_list, tensor=z2.contiguous())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Since allgather results do not have gradients, we replace the</span></span><br><span class="line">        <span class="comment"># current process&#x27;s corresponding embeddings with original tensors</span></span><br><span class="line">        z1_list[dist.get_rank()] = z1</span><br><span class="line">        z2_list[dist.get_rank()] = z2</span><br><span class="line">        <span class="comment"># Get full batch embeddings: (bs x N, hidden)</span></span><br><span class="line">        z1 = torch.cat(z1_list, <span class="number">0</span>)</span><br><span class="line">        z2 = torch.cat(z2_list, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [bs, bs]，计算该样本与其它样本的相似度</span></span><br><span class="line">    cos_sim = cls.sim(z1.unsqueeze(<span class="number">1</span>), z2.unsqueeze(<span class="number">0</span>))</span><br><span class="line">    <span class="comment"># Hard negative</span></span><br><span class="line">    <span class="keyword">if</span> num_sent &gt;= <span class="number">3</span>:</span><br><span class="line">        z1_z3_cos = cls.sim(z1.unsqueeze(<span class="number">1</span>), z3.unsqueeze(<span class="number">0</span>))</span><br><span class="line">        cos_sim = torch.cat([cos_sim, z1_z3_cos], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [bs, ], 内容为[0,1,...,bs-1]，表示每个样本最相似的样本下标</span></span><br><span class="line">    labels = torch.arange(cos_sim.size(<span class="number">0</span>)).long().to(cls.device)</span><br><span class="line">    <span class="comment"># 此处显示出对比学习loss和常规交叉熵loss的区别，</span></span><br><span class="line">    <span class="comment"># 对比学习的label数是[bs,bs]，而交叉熵的label数是[bs, label_nums]</span></span><br><span class="line">    loss_fct = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate loss with hard negatives</span></span><br><span class="line">    <span class="keyword">if</span> num_sent == <span class="number">3</span>:</span><br><span class="line">        <span class="comment"># Note that weights are actually logits of weights</span></span><br><span class="line">        z3_weight = cls.model_args.hard_negative_weight</span><br><span class="line">        weights = torch.tensor(</span><br><span class="line">            [[<span class="number">0.0</span>] * (cos_sim.size(-<span class="number">1</span>) - z1_z3_cos.size(-<span class="number">1</span>)) + [<span class="number">0.0</span>] * i + [z3_weight] + [<span class="number">0.0</span>] * (z1_z3_cos.size(-<span class="number">1</span>) - i - <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(z1_z3_cos.size(-<span class="number">1</span>))]</span><br><span class="line">        ).to(cls.device)</span><br><span class="line">        cos_sim = cos_sim + weights</span><br><span class="line"></span><br><span class="line">    loss = loss_fct(cos_sim, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate loss for MLM</span></span><br><span class="line">    <span class="keyword">if</span> mlm_outputs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> mlm_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mlm_labels = mlm_labels.view(-<span class="number">1</span>, mlm_labels.size(-<span class="number">1</span>))</span><br><span class="line">        prediction_scores = cls.lm_head(mlm_outputs.last_hidden_state)</span><br><span class="line">        masked_lm_loss = loss_fct(prediction_scores.view(-<span class="number">1</span>, cls.config.vocab_size), mlm_labels.view(-<span class="number">1</span>))</span><br><span class="line">        loss = loss + cls.model_args.mlm_weight * masked_lm_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">        output = (cos_sim,) + outputs[<span class="number">2</span>:]</span><br><span class="line">        <span class="keyword">return</span> ((loss,) + output) <span class="keyword">if</span> loss <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> output</span><br><span class="line">    <span class="keyword">return</span> SequenceClassifierOutput(</span><br><span class="line">        loss=loss,</span><br><span class="line">        logits=cos_sim,</span><br><span class="line">        hidden_states=outputs.hidden_states,</span><br><span class="line">        attentions=outputs.attentions,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>上述代码考虑诸多场景，比如分布式训练、难例三元组、mlm mask，写的较为复杂。</p>
<p>以下是简化版，更加符合论文的表述：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss_func = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simcse_loss</span>(<span class="params">batch_emb</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于无监督SimCSE训练的loss</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batch_size = batch_emb.size(<span class="number">0</span>)    <span class="comment"># [bs, hidden_size]</span></span><br><span class="line">    <span class="comment"># 构造标签, [bs, 2], bs=64</span></span><br><span class="line">    y_true = torch.cat([torch.arange(<span class="number">1</span>, batch_size, step=<span class="number">2</span>, dtype=torch.long).unsqueeze(<span class="number">1</span>),</span><br><span class="line">                        torch.arange(<span class="number">0</span>, batch_size, step=<span class="number">2</span>, dtype=torch.long).unsqueeze(<span class="number">1</span>)],</span><br><span class="line">                       dim=<span class="number">1</span>).reshape([batch_size,])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算score和loss</span></span><br><span class="line">    norm_emb = F.normalize(batch_emb, dim=<span class="number">1</span>, p=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># [bs, bs]，计算该样本与其它样本的相似度</span></span><br><span class="line">    sim_score = torch.matmul(norm_emb, norm_emb.transpose(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 对角线的位置，也就是自身的余弦相似度，肯定为1，不产生loss，需要mask掉</span></span><br><span class="line">    sim_score = sim_score - torch.eye(batch_size) * <span class="number">1e12</span></span><br><span class="line">    sim_score = sim_score * <span class="number">20</span>    <span class="comment"># 温度系数</span></span><br><span class="line">    loss = loss_func(sim_score, y_true)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></p>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ul>
<li>如果同一个batch里有其它语义相似的正样本，但在这里被当作了负样例处理，不是也拉远了同类样本的距离吗？</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://github.com/princeton-nlp/SimCSE" >princeton-nlp/SimCSE<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/IDWih5h2rLNqr3g0s8Y9zQ" >“被玩坏了”的Dropout<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/12UvfXnaB4NTy54wWIFZdQ" >细节满满！理解对比学习和SimCSE，就看这6个知识点<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/483453992" >SIMCSE算法源码分析<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Contrastive Learning</tag>
        <tag>Dropout</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring学习笔记一</title>
    <url>/2022/07/18/Spring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80/</url>
    <content><![CDATA[<p>公司项目中用到了Spring框架，虽然本科的时候接触过，但对其原理一知半解，现在重新学习一下。</p>
<span id="more"></span>
<h2 id="Spring的整体框架"><a href="#Spring的整体框架" class="headerlink" title="Spring的整体框架"></a>Spring的整体框架</h2><img   src="/2022/07/18/Spring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80/spring.png"  class="spring">
<p>其中包括四个模块：</p>
<ul>
<li><code>Beans</code>：实现Bean的工厂模式，Bean可以理解为组件，是JEE中基本的代码组织单位，Spring中Bean形式是普通Java类</li>
<li><code>Core</code>：Spring框架的核心，提供控制反转/依赖注入功能</li>
<li><code>Context</code>：表示Spring应用的环境，通过此模块可访问任意Bean，ApplicationContext接口是该模块的关键组成</li>
<li><code>SpEL</code>：提供对表达式语言(SpEL)支持</li>
</ul>
<p>如果学习的话，在 <code>pom.xml</code> 中只需引入前三个模块。</p>
<h2 id="applicationContext-xml"><a href="#applicationContext-xml" class="headerlink" title="applicationContext.xml"></a><code>applicationContext.xml</code></h2><p>这里介绍一下各配置项的说明：</p>
<img   src="/2022/07/18/Spring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80/xml.png"  class="xml">
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><p>本人在 <a class="link"   href="https://github.com/TransformersWsz/spl" >spl<i class="fas fa-external-link-alt"></i></a> 建立了一个最简单的Spring项目，可以直接使用，仅供学习。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="http://c.biancheng.net/spring/bean-definition.html" >Spring Bean定义<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.qikegu.com/docs/1468" >Spring 框架模块<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>IoC</tag>
      </tags>
  </entry>
  <entry>
    <title>SwiGLU激活函数</title>
    <url>/2024/05/09/SwiGLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>SwiGLU激活函数已经成为LLM的标配了。它是GLU的变体，公式如下：</p>
<script type="math/tex; mode=display">
\operatorname{SwiGLU}(x, W, V, b, c, \beta)=\operatorname{Swish}_\beta(x W+b) \otimes(x V+c)</script><span id="more"></span>
<h2 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h2><script type="math/tex; mode=display">
\operatorname{Swish_\beta}(x)=x \otimes \sigma(\beta x)</script><p>在nlp和cv任务上，Swish性能都和GELU接近，稍微略高点。但Swish公式更简洁优雅。</p>
<p>GELU早期被BERT、RoBERTa、ALBERT采用。</p>
<h2 id="GLU"><a href="#GLU" class="headerlink" title="GLU"></a>GLU</h2><script type="math/tex; mode=display">
\operatorname{GLU}(x, W, V, b, c)=\sigma(x W+b) \otimes(x V+c)</script><p>单纯从公式看，GLU是一个神经网络层。左右两个线性变换层，左边再接一个门控机制来控制信息流通多少。</p>
<h2 id="SwiGLU"><a href="#SwiGLU" class="headerlink" title="SwiGLU"></a>SwiGLU</h2><p>将Swish作为左侧激活函数就得到了SwiGLU。代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">F.silu(self.w1(x)) * self.w2(x)</span><br></pre></td></tr></table></figure></p>
<p>在 <a class="link"   href="https://arxiv.org/pdf/2002.05202" >GLU Variants Improve Transformer<i class="fas fa-external-link-alt"></i></a> 论文中，作者比较了各种GLU变体的激活函数，SwiGLU在各项任务上表现出众。但作者并未给出解释原因，只能说后验是这样，那就选它呗，所以成了LLM的标配。</p>
<h2 id="各激活函数示意图"><a href="#各激活函数示意图" class="headerlink" title="各激活函数示意图"></a>各激活函数示意图</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.8kzwfgcqya.png"  alt="act"></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://arxiv.org/pdf/2002.05202" >GLU Variants Improve Transformer<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://arxiv.org/pdf/1710.05941v1" >SWISH: A SELF-GATED ACTIVATION FUNCTION<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.jiqizhixin.com/articles/2019-12-30-4" >超越ReLU却鲜为人知，3年后被挖掘：BERT、GPT-2等都在用的激活函数<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/650237644" >大模型基础｜激活函数｜从ReLU 到SwiGLU<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>激活函数</tag>
        <tag>GELU</tag>
        <tag>Swish</tag>
        <tag>GLU</tag>
      </tags>
  </entry>
  <entry>
    <title>Swift可选类型总结</title>
    <url>/2017/12/28/Swift%E5%8F%AF%E9%80%89%E7%B1%BB%E5%9E%8B%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>Swift的可选(Optional)类型，用于处理值缺失的情况。可选表示<font color="red">“那儿有一个值，并且它等于x”或者“那儿没有值，为nil”</font>。它的定义通过在类型声明后加一个 <code>?</code> 操作符来完成的:<br><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> str <span class="operator">=</span> <span class="type">String</span>?</span><br></pre></td></tr></table></figure><br><span id="more"></span></p>
<p><code>Optional</code> 其实是个 <code>enum</code> ，里面有 <code>None</code> 和 <code>Some</code> 两种类型。其实所谓的 <code>nil</code> 就是 <code>Optional.None</code> ，当你声明一个可选变量的时候没有提供初始值，它的值默认为 <code>nil</code> 。 <code>非nil</code> 就是 <code>Optional.Some</code> ，然后会通过 <code>Some(T)</code> 包装(<code>wrap</code>)原始值，这也是为什么在使用 <code>Optional</code> 的时候要拆包(<code>unwrap</code> : 从 <code>enum</code> 中取出来原始值)的原因。下面是 <code>enum Optional</code>  的定义:</p>
<figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> <span class="title class_">Optional</span>&lt;<span class="title class_">Wrapped</span>&gt; : <span class="title class_">ExpressibleByNilLiteral</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> none</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="keyword">some</span>(<span class="type">Wrapped</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">init</span>(<span class="keyword">_</span> <span class="params">some</span>: <span class="type">Wrapped</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">func</span> <span class="title function_">map</span>&lt;<span class="type">U</span>&gt;(<span class="keyword">_</span> <span class="params">transform</span>: (<span class="type">Wrapped</span>) <span class="keyword">throws</span> -&gt; <span class="type">U</span>) <span class="keyword">rethrows</span> -&gt; <span class="type">U</span>?</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">func</span> <span class="title function_">flatMap</span>&lt;<span class="type">U</span>&gt;(<span class="keyword">_</span> <span class="params">transform</span>: (<span class="type">Wrapped</span>) <span class="keyword">throws</span> -&gt; <span class="type">U</span>?) <span class="keyword">rethrows</span> -&gt; <span class="type">U</span>?</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">init</span>(<span class="params">nilLiteral</span>: ())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">var</span> unsafelyUnwrapped: <span class="type">Wrapped</span> &#123; <span class="keyword">get</span> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>既然这样， 那我们如何理解上述变量的声明呢？<br><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> str <span class="operator">=</span> <span class="type">String</span>?</span><br><span class="line"><span class="comment">//我声明了一个Optional类型的变量，它可能包含一个String值，也可能什么都不包含，即nil</span></span><br></pre></td></tr></table></figure><br>也就是说我们实际上声明的是一个 <code>Optional</code> 类型，而不是 <code>String</code> 类型。</p>
<h2 id="和-的比较"><a href="#和-的比较" class="headerlink" title="? 和 ! 的比较"></a>? 和 ! 的比较</h2><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Cocoa</span><br><span class="line"><span class="keyword">var</span> str : <span class="type">String</span>?</span><br><span class="line">str <span class="operator">=</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line"><span class="keyword">if</span> str <span class="operator">!=</span> <span class="literal">nil</span>&#123;</span><br><span class="line">    <span class="comment">//print(str)</span></span><br><span class="line">    <span class="built_in">print</span>(str<span class="operator">!</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;字符串为nil&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出结果为: <font color="green">Hello World</font></p>
<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ul>
<li>如果是执行 <code>print(str)</code> 这句话，那么输出为 <code>Optional(&quot;Hello World&quot;)</code>。</li>
<li>使用 <code>!</code> 来获取一个不存在的可选值会导致运行时错误。使用 <code>!</code> 来强制解析值之前，一定要确定可选包含一个 <code>非nil</code> 的值。</li>
</ul>
<p>怎么使用 <code>Optional</code> 值呢？在苹果文档中也有提到说，在使用 <code>Optional</code> 值的时候需要在具体的操作，比如调用方法、属性、下标索引等前面需要加上一个?，如果是 <code>nil</code> 值，也就是 <code>Optional.None</code> ，会跳过后面的操作不执行，如果有值，就是 <code>Optional.Some</code> ，可能就会拆包(<code>unwrap</code>)，然后对拆包后的值执行后面的操作，来保证执行这个操作的安全性。</p>
<p>举例如下：<br><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span> length <span class="operator">=</span> str<span class="operator">?</span>.count</span><br><span class="line"><span class="comment">//如果你确定有值的话，也可以这样写</span></span><br><span class="line"><span class="comment">//let length = str!.count</span></span><br></pre></td></tr></table></figure></p>
<h3 id="拆包-unwrap"><a href="#拆包-unwrap" class="headerlink" title="拆包(unwrap)"></a>拆包(unwrap)</h3><p>上文提到 <code>Optional</code> 值需要拆包才能得到原来值，并判断其值是否为空才能对其操作。下面介绍两种拆包方法：</p>
<ul>
<li>可选绑定(optional binding)</li>
</ul>
<p>使用可选绑定（optional binding）来判断可选类型是否包含值，如果包含就把值赋给一个临时常量或者变量。可选绑定可以用在if和while语句中来对可选类型的值进行判断并把值赋给一个常量或者变量。举例如下：<br><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Cocoa</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> str : <span class="type">String</span>? <span class="operator">=</span> <span class="string">&quot;Hello&quot;</span></span><br><span class="line"><span class="keyword">let</span> world <span class="operator">=</span> <span class="string">&quot;World&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">let</span> const <span class="operator">=</span> str&#123;</span><br><span class="line">    <span class="built_in">print</span>(const <span class="operator">+</span> <span class="string">&quot; &quot;</span> <span class="operator">+</span> world)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;str is nil&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>硬解包<br>即直接在可选类型后面加一个 <code>!</code> 来表示它肯定有值。举例如下：<figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Cocoa</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> str : <span class="type">String</span>? <span class="operator">=</span> <span class="string">&quot;Hello&quot;</span></span><br><span class="line"><span class="keyword">let</span> world <span class="operator">=</span> <span class="string">&quot;World&quot;</span></span><br><span class="line"><span class="keyword">if</span> str <span class="operator">!=</span> <span class="literal">nil</span>&#123;</span><br><span class="line">    <span class="built_in">print</span>(str<span class="operator">!</span> <span class="operator">+</span> <span class="string">&quot; &quot;</span> <span class="operator">+</span> world)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;str is nil&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="错误示范"><a href="#错误示范" class="headerlink" title="错误示范"></a><font color="red">错误示范</font></h4><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Cocoa</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> str:<span class="type">String</span>?</span><br><span class="line"><span class="keyword">let</span> world <span class="operator">=</span> <span class="string">&quot;Hi&quot;</span></span><br><span class="line"><span class="built_in">print</span>(str<span class="operator">!</span> <span class="operator">+</span> world)</span><br></pre></td></tr></table></figure>
<p>以上代码在编译阶段不会报错.因为使用了硬解包, 编译器认为可选类型是有值的, 所以编译是通过的。当代码运行起来时，会报错：</p>
<font color="red">Fatal error: Unexpectedly found nil while unwrapping an Optional value.</font>

<h3 id="隐式拆包-自动解析"><a href="#隐式拆包-自动解析" class="headerlink" title="隐式拆包(自动解析)"></a>隐式拆包(自动解析)</h3><p>你可以在声明可选变量时使用感叹号 <code>!</code> 替换问号<code>?</code>。这样可选变量在使用时就不需要再加一个感叹号 <code>!</code> 来获取值，它会自动解析。<br>举例如下:<br><figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Cocoa</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> str:<span class="type">String</span>!</span><br><span class="line">str <span class="operator">=</span> <span class="string">&quot;Hello World!&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> str <span class="operator">!=</span> <span class="literal">nil</span> &#123;</span><br><span class="line">   <span class="built_in">print</span>(str)</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&quot;str is nil&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>等于说你每次对这种类型的值操作时，都会自动在操作前补上一个 <code>!</code> 进行拆包，然后在执行后面的操作，当然如果该值是 <code>nil</code> ，会报错crash掉。</p>
<p>总而言之，<code>?</code> 和 <code>!</code> 坑还是很多的，需要不断在实践中检验和体会。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.jianshu.com/p/89a2afb82488" >Swift中 ！和 ？的区别及使用<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://www.runoob.com/swift/swift-optionals.html" >Swift 可选(Optionals)类型<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Swift</tag>
        <tag>可选类型</tag>
      </tags>
  </entry>
  <entry>
    <title>TDM检索技术讲解</title>
    <url>/2024/02/27/TDM%E6%A3%80%E7%B4%A2%E6%8A%80%E6%9C%AF%E8%AE%B2%E8%A7%A3/</url>
    <content><![CDATA[<p>召回的任务是从海量商品库中挑选出与用户最相关的topK个商品。传统的召回检索时间复杂度是 $O(N)$ ，而阿里的TDM通过对全库商品构建一个树索引，将时间复杂度降低到 $O(logN)$ 。</p>
<span id="more"></span>
<h2 id="模型概览"><a href="#模型概览" class="headerlink" title="模型概览"></a>模型概览</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.1zhztkufxh.webp"  alt="model"></p>
<p>树的每个节点输入到左侧复杂模型的时候，都是一个embedding，这样user向量和item向量可以提早交互，提升模型表达能力。</p>
<p>在树的每一层采用beam search的方式采样出topK个结点，到了最后一层叶子节点的时候，即可得到topK个商品。</p>
<h2 id="联合训练"><a href="#联合训练" class="headerlink" title="联合训练"></a>联合训练</h2><p><img   src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20220420220831318.png"  alt="model"></p>
<ol>
<li>初始化二叉树：首先借助商品的类别信息进行排序，将相同类别的商品放到一起，然后递归的将同类别中的商品等量的分到两个子类中，直到集合中只包含一项，即最终的具体商品</li>
<li>基于树模型生成样本：如果用户点击了某个item，那么从根节点到该叶子节点上的所有节点（不包含根节点）都是正样本，而在每一层的节点中进行随机负采样</li>
<li>训练上图的复杂模型直至收敛</li>
<li>基于复杂模型得到所有叶子节点的embedding，迭代地进行k-means聚类。每迭代一次，生成一层中间树节点。需要注意如下如下两点：<ul>
<li><strong>训练得到的中间节点embedding在聚类过程中是不使用的，只用到叶子节点embedding</strong></li>
<li><strong>在线上serving的时候，中间节点embedding会输入到模型中得到topK个结点，每一层下去得到topK个商品</strong></li>
</ul>
</li>
<li>重复2~4过程</li>
</ol>
<div class="keep-note danger"><p>上述的步骤，至始至终都是在训练一个模型，而不是每一层一个模型（或者每棵树一个模型）</p>
</div>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/sw16_sUsyYuzpqqy39RsdQ" >阿里妈妈深度树检索技术(TDM)及应用框架的探索实践<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/78941783" >阿里TDM：Tree-based Deep Model<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/93201318" >阿里妈妈TDM模型详解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/PaddlePaddle/PaddleRec/blob/master/models/treebased/README.md" >Paddle TDM 模型实现<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>Tree-based Model</tag>
      </tags>
  </entry>
  <entry>
    <title>ThinkPHP多表回滚无效</title>
    <url>/2018/04/15/ThinkPHP%E5%A4%9A%E8%A1%A8%E5%9B%9E%E6%BB%9A%E6%97%A0%E6%95%88/</url>
    <content><![CDATA[<p>今天首次用到了多表回滚，遇到了一个坑，记录一下。</p>
<span id="more"></span>
<p><font color="red">错误代码如下：</font><br><figure class="highlight php"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="variable">$Member</span> = <span class="title function_ invoke__">D</span>(<span class="string">&quot;Member&quot;</span>);</span><br><span class="line">    <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">startTrans</span>();</span><br><span class="line">    <span class="variable">$member_condition</span>[<span class="string">&#x27;id&#x27;</span>] = <span class="number">11641</span>;</span><br><span class="line">    <span class="variable">$member_data</span>[<span class="string">&#x27;id&#x27;</span>] = <span class="number">10000</span>;</span><br><span class="line">    <span class="variable">$member_res</span> = <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">where</span>(<span class="variable">$member_condition</span>)-&gt;<span class="title function_ invoke__">save</span>(<span class="variable">$member_data</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="variable">$member_res</span> === <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            <span class="variable">$User</span> = <span class="title function_ invoke__">D</span>(<span class="string">&quot;User&quot;</span>);</span><br><span class="line">            <span class="variable">$User</span>-&gt;<span class="title function_ invoke__">startTrans</span>();</span><br><span class="line">            <span class="variable">$user_condition</span>[<span class="string">&#x27;account&#x27;</span>] = <span class="string">&#x27;111111&#x27;</span>;</span><br><span class="line">            <span class="variable">$user_data</span>[<span class="string">&#x27;username&#x27;</span>] = <span class="string">&quot;4324&quot;</span>;</span><br><span class="line">            <span class="variable">$user_res</span> = <span class="variable">$User</span>-&gt;<span class="title function_ invoke__">where</span>(<span class="variable">$user_condition</span>)-&gt;<span class="title function_ invoke__">save</span>(<span class="variable">$user_data</span>);</span><br><span class="line">            <span class="keyword">if</span> (<span class="variable">$user_res</span> === <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="variable">$User</span>-&gt;<span class="title function_ invoke__">commit</span>();</span><br><span class="line">                <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">commit</span>();</span><br><span class="line">                <span class="keyword">echo</span> <span class="string">&quot;全部修改成功&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="variable">$User</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">                <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">                <span class="keyword">echo</span> <span class="string">&quot;User表未受影响，全部回滚！&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">catch</span> (<span class="built_in">Exception</span> <span class="variable">$e</span>)&#123;</span><br><span class="line">            <span class="variable">$User</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">            <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">            <span class="keyword">echo</span> <span class="string">&quot;User修改异常，全部回滚！&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">&quot;Member表未受影响，回滚！&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="keyword">catch</span> (<span class="built_in">Exception</span> <span class="variable">$e</span>)&#123;</span><br><span class="line">    <span class="variable">$Member</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">&quot;Member修改异常！&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>我的思路是对 <code>Member</code> 和 <code>User</code> 分别开启事务，只要有一个表修改失败，那么就全部回滚。但事实确是开启了两个事务后，这两个事务都无法回滚。如果只开启一个事务，那么该事务是可以回滚的。在tp官方文档里面也没找到什么解释。解决方法如下所示：<br><figure class="highlight php"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">    <span class="variable">$Model</span> = <span class="title function_ invoke__">M</span>();</span><br><span class="line">	<span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">startTrans</span>();</span><br><span class="line">	<span class="variable">$member_condition</span>[<span class="string">&#x27;id&#x27;</span>] = <span class="number">11641</span>;</span><br><span class="line">    <span class="variable">$member_data</span>[<span class="string">&#x27;id&#x27;</span>] = <span class="number">10000</span>;</span><br><span class="line">    <span class="variable">$member_res</span> = <span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">table</span>(<span class="string">&#x27;party_member&#x27;</span>)-&gt;<span class="title function_ invoke__">where</span>(<span class="variable">$member_condition</span>)-&gt;<span class="title function_ invoke__">save</span>(<span class="variable">$member_data</span>);</span><br><span class="line"></span><br><span class="line">    <span class="variable">$user_condition</span>[<span class="string">&#x27;account&#x27;</span>] = <span class="string">&#x27;111111&#x27;</span>;</span><br><span class="line">    <span class="variable">$user_data</span>[<span class="string">&#x27;username&#x27;</span>] = <span class="string">&quot;4324&quot;</span>;</span><br><span class="line">    <span class="variable">$user_res</span> = <span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">table</span>(<span class="string">&#x27;party_user&#x27;</span>)-&gt;<span class="title function_ invoke__">where</span>(<span class="variable">$user_condition</span>)-&gt;<span class="title function_ invoke__">save</span>(<span class="variable">$user_data</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="variable">$member_res</span> === <span class="number">1</span> &amp;&amp; <span class="variable">$user_res</span> === <span class="number">1</span>) &#123;</span><br><span class="line">	    <span class="keyword">echo</span> <span class="string">&quot;commit&quot;</span>;</span><br><span class="line">        <span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">commit</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">&quot;rollback&quot;</span>;</span><br><span class="line">        <span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="keyword">catch</span> (<span class="built_in">Exception</span> <span class="variable">$e</span>)&#123;</span><br><span class="line">    <span class="keyword">echo</span> <span class="string">&quot;发生异常&quot;</span>;</span><br><span class="line">    <span class="variable">$Model</span>-&gt;<span class="title function_ invoke__">rollback</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>对于多表的事务处理，先用 M 函数实例化一个空对象，使用 table 方法进行多个表的操作，如果操作成功则提交，失败则回滚。</p>
<p>另外一点需要说明的是，在有些集成环境中MySQL默认的引擎是 <code>MyISAM</code>，若想提供事务支持，需将数据库引擎改为 <code>InnoDB</code> 。</p>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>ThinkPHP</tag>
        <tag>多表回滚</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2019/07/25/Transformer/</url>
    <content><![CDATA[<p>要点如下：</p>
<span id="more"></span>
<img   src="/2019/07/25/Transformer/1.png"  class="">
<img   src="/2019/07/25/Transformer/2.jpg"  class="">
<h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><ul>
<li><code>Self-Attention</code>：表示自注意。在机器翻译中，attention分配通常是目标单词对源语句各单词的概率分布。而self-attention表示source —&gt; source的attention分配，这样每个单词便能捕获与其他所有单词的关系特征，解决了RNN无法学习长程特征的问题。</li>
<li><code>Multi-Head</code>：表示 $X$ 同时做多次映射得到多个query、key、value。</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://arxiv.org/abs/1706.03762" >Attention Is All You Need<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://jalammar.github.io/illustrated-transformer/" >The Illustrated Transformer<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Attention</tag>
        <tag>Encoder-Decoder</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer输入长度受限的改进方案</title>
    <url>/2023/07/07/Transformer%E8%BE%93%E5%85%A5%E9%95%BF%E5%BA%A6%E5%8F%97%E9%99%90%E7%9A%84%E6%94%B9%E8%BF%9B%E6%96%B9%E6%A1%88/</url>
    <content><![CDATA[<p>汇总一下解决Transformer输入长度受限这一问题的相关工作：</p>
<span id="more"></span>
<ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/zJE-1xtD49vUiUOqlv3Hpw" >太牛了！微软最新研究：LONGNET，Transformer序列长度可支持 10亿+ Token<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>Paper Reading</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer面试要点</title>
    <url>/2021/03/18/Transformer%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/</url>
    <content><![CDATA[<p>记录一下常见的Transformer面试要点：</p>
<span id="more"></span>
<p>Transformer的核心在如下两张图上：</p>
<img   src="/2021/03/18/Transformer%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/1.png"  class="">
<h2 id="1-为什么Transformer-需要进行-Multi-head-Attention？"><a href="#1-为什么Transformer-需要进行-Multi-head-Attention？" class="headerlink" title="1. 为什么Transformer 需要进行 Multi-head Attention？"></a>1. 为什么Transformer 需要进行 Multi-head Attention？</h2><img   src="/2021/03/18/Transformer%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/2.png"  class="">
<ul>
<li>将模型分为多个头，形成多个子空间，让模型去关注不同方面的信息；</li>
<li>把多头注意力看作一个ensemble，模型内部的集成，类似于CNN中使用的多个卷积核，所以很多时候可以认为多头注意力可以帮助我们捕捉到更为丰富的特征信息。</li>
</ul>
<h2 id="2-Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？"><a href="#2-Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？" class="headerlink" title="2. Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？"></a>2. Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</h2><p>如果Q,K,V都是一个值,那么就变为了Self-Attention的形式：</p>
<img   src="/2021/03/18/Transformer%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/3.png"  class="">
<p>在实践中，Q和K的乘法是为了得到二者的相似度，一般我们的K和V是相同的，Q和K进行操作是为了得到一个attention score矩阵，这样可以得到Q关于V的表示，但一般我们再计算Q,K,V的时候会先都分别乘上一个不同的矩阵W，这么做可以增加模型的表达能力，实践中经常也会带来一定的帮助。</p>
<h2 id="3-Transformer中的attention为什么要进行scaled？"><a href="#3-Transformer中的attention为什么要进行scaled？" class="headerlink" title="3. Transformer中的attention为什么要进行scaled？"></a>3. Transformer中的attention为什么要进行scaled？</h2><p>softmax的计算公式如下：</p>
<img   src="/2021/03/18/Transformer%E9%9D%A2%E8%AF%95%E8%A6%81%E7%82%B9/4.png"  class="">
<ul>
<li>非常大的$d_k$值会将softmax推向梯度非常小的区域，梯度消失为0，造成参数更新困难</li>
<li>$\frac{1}{\sqrt{d_k}}$ 使得$D(\frac{qk}{\sqrt{d_k}})=1$，有效地控制了梯度消失的问题</li>
</ul>
<h2 id="4-Attention相对于CNN、RNN的优势？"><a href="#4-Attention相对于CNN、RNN的优势？" class="headerlink" title="4. Attention相对于CNN、RNN的优势？"></a>4. Attention相对于CNN、RNN的优势？</h2><ul>
<li>参数少，算力要求低</li>
<li>并行化，速度快</li>
<li>可解释性强，不会遗忘长文本的信息</li>
</ul>
<h2 id="5-Attention的计算方式"><a href="#5-Attention的计算方式" class="headerlink" title="5. Attention的计算方式"></a>5. Attention的计算方式</h2><ul>
<li>多层MLP：$a(q, k)=w_{2}^{T} \tanh \left(W_{1}[q ; k]\right)$</li>
<li>BiLinear: $a(q, k)=q^{T} W k$</li>
<li>Scaled-Dot Product: $a(q, k)=\frac{q^{T} k}{\sqrt{d_{k}}}$</li>
<li>欧式距离</li>
<li>cosine</li>
</ul>
<h2 id="6-残差网络的作用"><a href="#6-残差网络的作用" class="headerlink" title="6. 残差网络的作用"></a>6. 残差网络的作用</h2><p>ResNet的目标是在网络加深的情况下解决网络退化的问题。</p>
<h2 id="7-LayerNorm的作用，为什么不用BN？"><a href="#7-LayerNorm的作用，为什么不用BN？" class="headerlink" title="7. LayerNorm的作用，为什么不用BN？"></a>7. LayerNorm的作用，为什么不用BN？</h2><p>归一化的作用：</p>
<ul>
<li>保持每一层特征分布的稳定性，将梯度从饱和区拉回非饱和区，从而加快模型训练速度，缓解过拟合</li>
</ul>
<p>LN not BN：</p>
<ul>
<li><p>BN对batch_size很敏感，LN不存在这个问题</p>
</li>
<li><p>CV使用BN是认为不同卷积核feature map（channel维）之间的差异性很重要，LN会损失channel的差异性，对于batch内的不同样本，同一卷积核提取特征的目的性是一致的，所以使用BN仅是为了进一步保证同一个卷积核在不同样本上提取特征的稳定性。</p>
<p>而NLP使用LN是认为batch内不同样本同一位置token之间的差异性更重要，而embedding维，网络对于不同token提取的特征目的性是一致的，使用LN是为了进一步保证在不同token上提取的稳定性。NLP每个序列的长度是不一致的，BN不适用。</p>
</li>
</ul>
<h2 id="8-Position-Encoding的设计思路"><a href="#8-Position-Encoding的设计思路" class="headerlink" title="8. Position Encoding的设计思路"></a>8. Position Encoding的设计思路</h2><script type="math/tex; mode=display">
PE(pos, 2i) = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\
PE(pos, 2i+1) = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})</script><h3 id="原因："><a href="#原因：" class="headerlink" title="原因："></a>原因：</h3><ul>
<li>单词在句子中的位置和排列顺序非常重要，它们不仅是一个句子的语法结构的组成部分，更是表达语义的重要概念；</li>
<li>Transformer使用纯attention结构，丢失了词序信息，有必要把词序信号加到词向量上帮助模型学习这些信息。</li>
</ul>
<h3 id="线性分配一个数值给每个时间步的缺点？"><a href="#线性分配一个数值给每个时间步的缺点？" class="headerlink" title="线性分配一个数值给每个时间步的缺点？"></a>线性分配一个数值给每个时间步的缺点？</h3><ul>
<li>数值巨大，且模型可能遇到比训练集所有句子都要长的句子；</li>
<li>数据集中不一定在所有数值上都会包含相对应长度的句子，也就是模型很有可能没有看到过任何一个这样的长度的样本句子，这会严重影响模型的泛化能力；</li>
</ul>
<h4 id="良好的PE方案需满足以下要求："><a href="#良好的PE方案需满足以下要求：" class="headerlink" title="良好的PE方案需满足以下要求："></a>良好的PE方案需满足以下要求：</h4><ul>
<li>它能为每个时间步输出一个独一无二的编码；</li>
<li>不同长度的句子之间，任何两个时间步之间的距离应该保持一致；</li>
<li>模型应该能毫不费力地泛化到更长的句子。它的值应该是有界的；</li>
<li>它必须是确定性的。</li>
</ul>
<h3 id="相对位置的线性关系"><a href="#相对位置的线性关系" class="headerlink" title="相对位置的线性关系"></a>相对位置的线性关系</h3><p>正弦曲线函数的位置编码的另一个特点是，它能让模型毫不费力地关注相对位置信息。具体公式推导见<a class="link"   href="https://zhuanlan.zhihu.com/p/106644634" >More: 相对位置的线性关系<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>面试</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>U-Net原理及代码实现</title>
    <url>/2024/08/08/U-Net%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>U-Net是医疗领域进行语义分割的利器，随着AIGC的爆火，U-Net已成为Diffusion Model的backbone，有必要详细记录下。</p>
<span id="more"></span>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.7zqcfa5u8y.png"  alt="unet"></p>
<p>U-Net包含了编码器和解码器部分：</p>
<ul>
<li>编码器：通过下采样，实现了特征的层次提取。该过程类似于人类视觉系统，先关注局部细节，然后逐步构建出整体的语义信息</li>
<li>解码器：通过反卷积和跳跃连接，将编码器中相应尺寸的特征图与解码器中的特征图拼接，从而实现特征的层次恢复。该过程有助于网络在解码过程中更好地利用上下文信息，提高分割的准确性</li>
</ul>
<h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>原论文的输入维度对于上下采样时的大小变换不友好，因此出现了维度裁剪的情况。这里以实现下述结构的U-Net为例：</p>
<p><img   src="https://pytorch.org/assets/images/unet_brain_mri.png"  alt="code"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(UNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下采样</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">down_conv_block</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 上采样</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">up_conv_block</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">            <span class="keyword">return</span> nn.ConvTranspose2d(in_channels, out_channels, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        self.encoder1 = down_conv_block(in_channels, <span class="number">32</span>)</span><br><span class="line">        self.encoder2 = down_conv_block(<span class="number">32</span>, <span class="number">64</span>)</span><br><span class="line">        self.encoder3 = down_conv_block(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.encoder4 = down_conv_block(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">        self.bottleneck = down_conv_block(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.upconv4 = up_conv_block(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.decoder4 = down_conv_block(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.upconv3 = up_conv_block(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.decoder3 = down_conv_block(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.upconv2 = up_conv_block(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.decoder2 = down_conv_block(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.upconv1 = up_conv_block(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.decoder1 = down_conv_block(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">        self.output = nn.Conv2d(<span class="number">32</span>, out_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Encoding path</span></span><br><span class="line">        <span class="comment"># 1, 32, 256, 256</span></span><br><span class="line">        enc1 = self.encoder1(x)</span><br><span class="line">        <span class="comment"># 1, 64, 128, 128</span></span><br><span class="line">        enc2 = self.encoder2(self.pool(enc1))</span><br><span class="line">        <span class="comment"># 1, 128, 64, 64</span></span><br><span class="line">        enc3 = self.encoder3(self.pool(enc2))</span><br><span class="line">        <span class="comment"># 1, 256, 32, 32</span></span><br><span class="line">        enc4 = self.encoder4(self.pool(enc3))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Bottleneck</span></span><br><span class="line">        <span class="comment"># 1, 512, 16, 16</span></span><br><span class="line">        bottleneck = self.bottleneck(self.pool(enc4))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Decoding path</span></span><br><span class="line">        <span class="comment"># 1, 256, 32, 32</span></span><br><span class="line">        dec4 = self.upconv4(bottleneck)</span><br><span class="line">        <span class="comment"># 1, 512, 32, 32</span></span><br><span class="line">        dec4 = torch.cat((dec4, enc4), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1, 256, 32, 32</span></span><br><span class="line">        dec4 = self.decoder4(dec4)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1, 128, 64, 64</span></span><br><span class="line">        dec3 = self.upconv3(dec4)</span><br><span class="line">        <span class="comment"># 1, 256, 64, 64</span></span><br><span class="line">        dec3 = torch.cat((dec3, enc3), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1, 128, 64, 64</span></span><br><span class="line">        dec3 = self.decoder3(dec3)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1, 64, 128, 128</span></span><br><span class="line">        dec2 = self.upconv2(dec3)</span><br><span class="line">        <span class="comment"># 1, 128, 128, 128</span></span><br><span class="line">        dec2 = torch.cat((dec2, enc2), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1, 64, 128, 128</span></span><br><span class="line">        dec2 = self.decoder2(dec2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1, 32, 256, 256</span></span><br><span class="line">        dec1 = self.upconv1(dec2)</span><br><span class="line">        <span class="comment"># 1, 64, 256, 256</span></span><br><span class="line">        dec1 = torch.cat((dec1, enc1), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1, 32, 256, 256</span></span><br><span class="line">        dec1 = self.decoder1(dec1)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.output(dec1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = UNet(in_channels=<span class="number">3</span>, out_channels=<span class="number">1</span>)</span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>)  <span class="comment"># Batch size 1, 1 channel, 572x572 image size</span></span><br><span class="line">output_tensor = model(input_tensor)</span><br><span class="line"><span class="built_in">print</span>(output_tensor.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" >U-Net: Convolutional Networks for Biomedical Image Segmentation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/" >mateuszbuda_brain-segmentation-pytorch_unet<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>U-Net</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>UI设计常识</title>
    <url>/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/</url>
    <content><![CDATA[<p>前端的一个基础知识：</p>
<span id="more"></span>
<h1 id="手机屏幕尺寸"><a href="#手机屏幕尺寸" class="headerlink" title="手机屏幕尺寸"></a>手机屏幕尺寸</h1><blockquote>
<p>屏幕对角线长度，单位为英寸。</p>
</blockquote>
<img   src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/1.jpg"  class="">
<hr>

<h1 id="显示分辨率"><a href="#显示分辨率" class="headerlink" title="显示分辨率"></a>显示分辨率</h1><blockquote>
<p>屏幕拥有的像素总数，单位为像素（pixel，简写px）</p>
</blockquote>
<img   src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/2.jpg"  class="">
<hr>

<h1 id="屏幕像素密度"><a href="#屏幕像素密度" class="headerlink" title="屏幕像素密度"></a>屏幕像素密度</h1><blockquote>
<p>Pixels Per Inch，简写PPI或ppi，指的是每英寸所拥有的像素数。</p>
</blockquote>
<img   src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/3.jpg"  class="">
<img   src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/4.jpg"  class="">
<p>同样是尺寸大小为5英寸的屏幕，显示分辨率为 4 <em> 4px 的屏幕显示质量大于显示分辨率为 3 </em> 3px 的屏幕。</p>
<hr>

<h1 id="逻辑分辨率与虚拟尺寸单位"><a href="#逻辑分辨率与虚拟尺寸单位" class="headerlink" title="逻辑分辨率与虚拟尺寸单位"></a>逻辑分辨率与虚拟尺寸单位</h1><p>由于市面上手机种类繁多，不同的屏幕尺寸与不同的显示分辨率，为开发提供了极大的不便。</p>
<img   src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/5.jpg"  class="">
<p>为了尽可能减少开发人员的工作成本，开发人员需要一套统一的分辨率和尺寸单位，由此衍生出一个新的分辨率——逻辑分辨率（单位是虚拟尺寸单位）：</p>
<ul>
<li>Android 的虚拟尺寸单位是dp（用于元素）和sp（用于字体）。</li>
<li>iOS 的虚拟尺寸单位是pt。</li>
</ul>
<p>正常来说，设计师设计时采用的是显示分辨率（单位：px），程序员开发时采用的逻辑分辨率（单位：虚拟尺寸单位）。</p>
<hr>

<h1 id="逻辑分辨率与转换率的制定"><a href="#逻辑分辨率与转换率的制定" class="headerlink" title="逻辑分辨率与转换率的制定"></a>逻辑分辨率与转换率的制定</h1><p>设置逻辑分辨率的原因是为了通过将显示分辨率通过一定的倍数（转换率）缩放至一个新的分辨率大小，使得原本不同手机的显示分辨率差异缩小。开发中采用的分辨率时，方便于适配更多的机型。</p>
<img   src="/2018/12/18/UI%E8%AE%BE%E8%AE%A1%E5%B8%B8%E8%AF%86/6.jpg"  class="">
<p>在制定转换率与逻辑分辨率的时候，我们需要注意如下三点：</p>
<ol>
<li>不同iphone间的逻辑分辨率尽量接近。</li>
<li>转换率最好是整数。</li>
<li>不同iphone转换成逻辑分辨率后的ppi尽量接近。<hr>

</li>
</ol>
<h1 id="切图"><a href="#切图" class="headerlink" title="切图"></a>切图</h1><p>对于@2x的设计来说，1 <em> 1 pt = 2 </em> 2px；对于@3x的设计来说，1 <em> 1 pt = 3 </em> 3 px.</p>
<p>所以设计好的@1x的图，要把它切开，由 1 <em> 1 pt 的大小切成 2 </em> 2 px 和 3 * 3 px 的大小。故谓之切图。 </p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.zhihu.com/question/26195746" >切图常说的@1X@2X@3X是什么意思？<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>UI设计</tag>
        <tag>前端</tag>
        <tag>分辨率</tag>
      </tags>
  </entry>
  <entry>
    <title>Uplift Model离线评估指标</title>
    <url>/2024/09/16/Uplift-Model%E7%A6%BB%E7%BA%BF%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<p>uplift建模难点在于无法获得个体的ground truth，因为它是反事实的。只能通过构造treatment和control两组镜像人群，对比两组人群的转化增量，来实现模型性能的评估。</p>
<span id="more"></span>
<h2 id="Uplift-Curve"><a href="#Uplift-Curve" class="headerlink" title="Uplift Curve"></a>Uplift Curve</h2><h4 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h4><script type="math/tex; mode=display">
f(k)=\left(\frac{Y_k^T}{N_k^T}-\frac{Y_k^C}{N_k^C}\right)\left(N_k^T+N_k^C\right)</script><p>具体计算步骤如下：</p>
<ol>
<li>模型对样本集预测，然后将样本按照预测得到的uplift value进行降序排序</li>
<li>取topK个样本，计算得到 $f(k)$ 。以 $k$ 为横轴，$f(k)$ 为纵轴，画出Uplift Curve<ul>
<li>$Y_k^T$ 表示topK个样本中， treatment组有转化的样本数，$Y_k^C$同理</li>
<li>$N_k^T$ 表示topK个样本中， treatment组的总样本数，$N_k^C$同理</li>
</ul>
</li>
<li>Uplift Curve下的面积即是AUUC，AUUC越大，表示模型性能越好</li>
</ol>
<h4 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">uplift_curve</span>(<span class="params">y_true, uplift, treatment</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute Uplift curve.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For computing the area under the Uplift Curve, see :func:`.uplift_auc_score`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_true (1d array-like): Correct (true) binary target values.</span></span><br><span class="line"><span class="string">        uplift (1d array-like): Predicted uplift, as returned by a model.</span></span><br><span class="line"><span class="string">        treatment (1d array-like): Treatment labels.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        array (shape = [&gt;2]), array (shape = [&gt;2]): Points on a curve.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    See also:</span></span><br><span class="line"><span class="string">        :func:`.uplift_auc_score`: Compute normalized Area Under the Uplift curve from prediction scores.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :func:`.perfect_uplift_curve`: Compute the perfect Uplift curve.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :func:`.plot_uplift_curve`: Plot Uplift curves from predictions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :func:`.qini_curve`: Compute Qini curve.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    References:</span></span><br><span class="line"><span class="string">        Devriendt, F., Guns, T., &amp; Verbeke, W. (2020). Learning to rank for uplift modeling. ArXiv, abs/2002.05897.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    check_consistent_length(y_true, uplift, treatment)</span><br><span class="line">    check_is_binary(treatment)</span><br><span class="line">    check_is_binary(y_true)</span><br><span class="line"></span><br><span class="line">    y_true, uplift, treatment = np.array(y_true), np.array(uplift), np.array(treatment)</span><br><span class="line"></span><br><span class="line">    desc_score_indices = np.argsort(uplift, kind=<span class="string">&quot;mergesort&quot;</span>)[::-<span class="number">1</span>]</span><br><span class="line">    y_true, uplift, treatment = y_true[desc_score_indices], uplift[desc_score_indices], treatment[desc_score_indices]</span><br><span class="line"></span><br><span class="line">    y_true_ctrl, y_true_trmnt = y_true.copy(), y_true.copy()</span><br><span class="line"></span><br><span class="line">    y_true_ctrl[treatment == <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    y_true_trmnt[treatment == <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    distinct_value_indices = np.where(np.diff(uplift))[<span class="number">0</span>]</span><br><span class="line">    threshold_indices = np.r_[distinct_value_indices, uplift.size - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    num_trmnt = stable_cumsum(treatment)[threshold_indices]</span><br><span class="line">    y_trmnt = stable_cumsum(y_true_trmnt)[threshold_indices]</span><br><span class="line"></span><br><span class="line">    num_all = threshold_indices + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    num_ctrl = num_all - num_trmnt</span><br><span class="line">    y_ctrl = stable_cumsum(y_true_ctrl)[threshold_indices]</span><br><span class="line"></span><br><span class="line">    curve_values = (np.divide(y_trmnt, num_trmnt, out=np.zeros_like(y_trmnt), where=num_trmnt != <span class="number">0</span>) -</span><br><span class="line">                    np.divide(y_ctrl, num_ctrl, out=np.zeros_like(y_ctrl), where=num_ctrl != <span class="number">0</span>)) * num_all</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> num_all.size == <span class="number">0</span> <span class="keyword">or</span> curve_values[<span class="number">0</span>] != <span class="number">0</span> <span class="keyword">or</span> num_all[<span class="number">0</span>] != <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Add an extra threshold position if necessary</span></span><br><span class="line">        <span class="comment"># to make sure that the curve starts at (0, 0)</span></span><br><span class="line">        num_all = np.r_[<span class="number">0</span>, num_all]</span><br><span class="line">        curve_values = np.r_[<span class="number">0</span>, curve_values]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> num_all, curve_values</span><br></pre></td></tr></table></figure>
<h2 id="Qini-Curve"><a href="#Qini-Curve" class="headerlink" title="Qini Curve"></a>Qini Curve</h2><p>当treatment组和control组的样本数量（在topK样本里）相差比较大的时候，Uplift Curve的计算会存在问题。因此Qini引入缩放因子来减少样本量级差异所带来的影响。</p>
<h4 id="计算公式-1"><a href="#计算公式-1" class="headerlink" title="计算公式"></a>计算公式</h4><script type="math/tex; mode=display">
g(k)=Y_k^T- Y_k^C \times \frac{N_k^T}{N_k^C}</script><p>具体计算步骤如下：</p>
<ol>
<li>模型对样本集预测，然后将样本按照预测得到的uplift value进行降序排序</li>
<li>取topK个样本，计算得到 $g(k)$ 。以 $k$ 为横轴，$g(k)$ 为纵轴，画出Qini Curve<ul>
<li>$Y_k^T$ 表示topK个样本中， treatment组有转化的样本数，$Y_k^C$同理</li>
<li>$N_k^T$ 表示topK个样本中， treatment组的总样本数，$N_k^C$同理</li>
</ul>
</li>
<li>Qini Curve下的面积即是Qini coefficient，Qini coefficient越大，表示模型性能越好</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">qini_curve</span>(<span class="params">y_true, uplift, treatment</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute Qini curve.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For computing the area under the Qini Curve, see :func:`.qini_auc_score`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_true (1d array-like): Correct (true) binary target values.</span></span><br><span class="line"><span class="string">        uplift (1d array-like): Predicted uplift, as returned by a model.</span></span><br><span class="line"><span class="string">        treatment (1d array-like): Treatment labels.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        array (shape = [&gt;2]), array (shape = [&gt;2]): Points on a curve.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    See also:</span></span><br><span class="line"><span class="string">        :func:`.uplift_curve`: Compute the area under the Qini curve.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :func:`.perfect_qini_curve`: Compute the perfect Qini curve.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :func:`.plot_qini_curves`: Plot Qini curves from predictions..</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :func:`.uplift_curve`: Compute Uplift curve.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    References:</span></span><br><span class="line"><span class="string">        Nicholas J Radcliffe. (2007). Using control groups to target on predicted lift:</span></span><br><span class="line"><span class="string">        Building and assessing uplift model. Direct Marketing Analytics Journal, (3):14–21, 2007.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Devriendt, F., Guns, T., &amp; Verbeke, W. (2020). Learning to rank for uplift modeling. ArXiv, abs/2002.05897.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    check_consistent_length(y_true, uplift, treatment)</span><br><span class="line">    check_is_binary(treatment)</span><br><span class="line">    check_is_binary(y_true)</span><br><span class="line">    y_true, uplift, treatment = np.array(y_true), np.array(uplift), np.array(treatment)</span><br><span class="line"></span><br><span class="line">    desc_score_indices = np.argsort(uplift, kind=<span class="string">&quot;mergesort&quot;</span>)[::-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    y_true = y_true[desc_score_indices]</span><br><span class="line">    treatment = treatment[desc_score_indices]</span><br><span class="line">    uplift = uplift[desc_score_indices]</span><br><span class="line"></span><br><span class="line">    y_true_ctrl, y_true_trmnt = y_true.copy(), y_true.copy()</span><br><span class="line"></span><br><span class="line">    y_true_ctrl[treatment == <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    y_true_trmnt[treatment == <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    distinct_value_indices = np.where(np.diff(uplift))[<span class="number">0</span>]</span><br><span class="line">    threshold_indices = np.r_[distinct_value_indices, uplift.size - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    num_trmnt = stable_cumsum(treatment)[threshold_indices]</span><br><span class="line">    y_trmnt = stable_cumsum(y_true_trmnt)[threshold_indices]</span><br><span class="line"></span><br><span class="line">    num_all = threshold_indices + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    num_ctrl = num_all - num_trmnt</span><br><span class="line">    y_ctrl = stable_cumsum(y_true_ctrl)[threshold_indices]</span><br><span class="line"></span><br><span class="line">    curve_values = y_trmnt - y_ctrl * np.divide(num_trmnt, num_ctrl, out=np.zeros_like(num_trmnt), where=num_ctrl != <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> num_all.size == <span class="number">0</span> <span class="keyword">or</span> curve_values[<span class="number">0</span>] != <span class="number">0</span> <span class="keyword">or</span> num_all[<span class="number">0</span>] != <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Add an extra threshold position if necessary</span></span><br><span class="line">        <span class="comment"># to make sure that the curve starts at (0, 0)</span></span><br><span class="line">        num_all = np.r_[<span class="number">0</span>, num_all]</span><br><span class="line">        curve_values = np.r_[<span class="number">0</span>, curve_values]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> num_all, curve_values</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/627342229" >排序评估：AUUC、Qini coefficient<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.uplift-modeling.com/en/latest/_modules/sklift/metrics/metrics.html#qini_auc_score" >Source code for sklift.metrics.metrics<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Marketing</category>
      </categories>
      <tags>
        <tag>AUUC</tag>
        <tag>Uplift Curve</tag>
        <tag>Qini coefficient</tag>
        <tag>Qini Curve</tag>
      </tags>
  </entry>
  <entry>
    <title>VAE</title>
    <url>/2022/01/12/VAE/</url>
    <content><![CDATA[<p>这段时间看了VAE的有关知识，但网上关于VAE的讲解较为理论复杂，我这里就记录一下自己的想法了。</p>
<span id="more"></span>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>VAE从概率的角度描述隐空间与输入样本，它将样本的隐变量建模为<strong>概率分布</strong>, 而非像AE一样把隐变量看做是离散的值。</p>
<h2 id="AE-VS-VAE"><a href="#AE-VS-VAE" class="headerlink" title="AE VS VAE"></a>AE VS VAE</h2><img   src="/2022/01/12/VAE/1.png"  class="">
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><img   src="/2022/01/12/VAE/2.png"  class="">
<p>我们假设隐变量的概率分布为标准正态分布$N(0, 1)$（这种分布不是必须的，也可以是其它分布）。而描述正态分布需要有两个参数$\mu_x, \sigma_x$，在encoder端使用神经网络来拟合这两个参数。在decoder端，使用神经网络来还原出原始图像。因此，VAE的损失函数分为两部分：</p>
<ul>
<li><p>正则化项，也就是KL Loss</p>
</li>
<li><p>重构损失</p>
</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
L &= L_{Recon} + L_{KL} \\
&= \|x-\hat{x}\|^{2}+\mathrm{KL}[N(\mu_{x}, \sigma_{x}), N(0, 1)] \\
&= \|x-d(z)\|^{2}+KL[N(\mu_{x}, \sigma_{x}), N(0, 1)]
\end{aligned}</script><p>关于$KL\left[N\left(\mu_{x}, \sigma_{x}\right), N(0,1)\right]$的推导如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& KL\left(N\left(\mu, \sigma^{2}\right) \| N(0,1)\right) \\
&= \int \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{\frac{-(x-\mu)^{2}}{2 \sigma^{2}} }\left(\log \frac{\frac{e^{ \frac{-(x-\mu)^{2}}{2 \sigma^{2}} }}{\sqrt{2 \pi \sigma^{2}}} }{\frac{e^{\frac{-x^{2}}{2}}}{\sqrt{2 \pi}} }\right) d x \\
&= \int \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{\frac{-(x-\mu)^{2}}{2 \sigma^{2}} } \log \left\{\frac{1}{\sqrt{\sigma^{2}}} \exp \left\{\frac{1}{2}\left[x^{2}- \frac{(x-\mu)^{2}}{\sigma^{2}} \right]\right\}\right\} d x \\
&= \frac{1}{2} \int \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{\frac{-(x-\mu)^{2}}{2 \sigma^{2}} }\left[-\log \sigma^{2}+x^{2}- \frac{(x-\mu)^{2}}{\sigma^{2}} \right] d x \\
&= \frac{1}{2}\left(-\log \sigma^{2}+\mu^{2}+\sigma^{2}-1\right)
\end{aligned}</script><h2 id="重参数技巧"><a href="#重参数技巧" class="headerlink" title="重参数技巧"></a>重参数技巧</h2><p>我们从概率分布中采样出 $z$ ，但是该过程是不可导的。VAE通过重参数化使得梯度不因采样而断裂。</p>
<img   src="/2022/01/12/VAE/3.png"  class="">
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实VAE可以看成一个做降维的model，我们希望把一个高维的特征投影到一个低维的流型上。而在VAE中，这个低维流型就是一个多元标准正态分布。为了使投影准确，于是通过希望每一个样本$X_i$的计算出来的期望与方差都接近与我们希望投影的分布，所以这里就有了KL Loss。至于重构损失，是可以使采样的时候更加准确，能够采样到我们在encode的时候投影到的点。</p>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><ul>
<li><p><a class="link"   href="https://adaning.github.io/posts/9047.html" >Pytorch实现: VAE<i class="fas fa-external-link-alt"></i></a> 这篇博客实现了VAE，整体上代码简单易懂。在generation阶段，我们只需从学习到的概率分布中采样，然后送入decoder中解码，即可获得生成的图片。</p>
</li>
<li><p>小小将的VAE实现，可以直接运行：<a class="link"   href="https://github.com/xiaohu2015/nngen/blob/main/models/vae.ipynb" >https://github.com/xiaohu2015/nngen/blob/main/models/vae.ipynb<i class="fas fa-external-link-alt"></i></a></p>
</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><p><a class="link"   href="https://zhuanlan.zhihu.com/p/34998569" >变分自编码器VAE：原来是这么一回事<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" >Understanding Variational Autoencoders (VAEs)<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://adaning.github.io/posts/9047.html" >Pytorch实现: VAE<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://adaning.github.io/posts/53598.html" >变分自编码器入门<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://colab.research.google.com/drive/1ZhmA2XxJ3oZC7A-U2mpUdB2eZZLz5NfW?usp=sharing#scrollTo=E7R4BFye1eAW" >VAE.ipynb - Colaboratory<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://www.bilibili.com/video/BV1Wv411h7kN?p=45" >李宏毅2021春机器学习课程<i class="fas fa-external-link-alt"></i></a></p>
</li>
<li><p><a class="link"   href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/GAN%20(v3" >VAE.pdf(ntu.edu.tw)<i class="fas fa-external-link-alt"></i></a>)</p>
</li>
<li><p><a class="link"   href="https://blog.csdn.net/StreamRock/article/details/81258543" >VAE的推导<i class="fas fa-external-link-alt"></i></a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>AutoEncoder</tag>
      </tags>
  </entry>
  <entry>
    <title>Uplift Tree建模</title>
    <url>/2024/11/25/Uplift-Tree%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>决策树构建流程：</p>
<span id="more"></span>
<h3 id="1-节点划分"><a href="#1-节点划分" class="headerlink" title="1. 节点划分"></a>1. 节点划分</h3><p>在每个树节点上，遍历所有特征以及该特征下的所有取值，以最大化实验组和对照组之间的转化率差异。</p>
<h3 id="2-增益计算"><a href="#2-增益计算" class="headerlink" title="2. 增益计算"></a>2. 增益计算</h3><p>每个节点根据实验组和对照组在该特征下的转化率差异来计算增益。例如，某一群体可能在干预下表现出更高的购买率，而另一群体则没有显著变化。增益计算公式如下：</p>
<script type="math/tex; mode=display">
D_{\text {gain }}=D_{\text {after }\_{\text {split }}}\left(P^T(Y), P^C(Y)\right)-D_{\text {before}\_{\text {split }}}\left(P^T(Y), P^C(Y)\right)</script><ul>
<li>$P^T(Y)$表示实验组样本中类别$Y$的概率</li>
<li>$P^C(Y)$表示对照组样本中类别$Y$的概率</li>
<li>$D$表示度量距离，可以欧氏距离、KL散度等</li>
<li>$D_{\text {after }_{\text {split }}}\left(P^T(Y), P^C(Y)\right)$ 即为该叶子节点的uplift值，一般都是$P^T(Y) - P^C(T)$</li>
</ul>
<h3 id="3-增益预测"><a href="#3-增益预测" class="headerlink" title="3. 增益预测"></a>3. 增益预测</h3><p>给定一用户，根据该用户的所有特征以及该特征下的所有取值，将其分配到某一个叶子节点，得到该用户的uplift值。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://cloud.tencent.com/developer/article/1913905" >因果推断笔记——uplift建模、meta元学习、Class Transformation Method（八）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/636342238" >闲聊因果效应（5）：树模型（Tree-Based）、分类模型（The class transformation）<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Marketing</category>
      </categories>
      <tags>
        <tag>Tree</tag>
        <tag>Uplift</tag>
      </tags>
  </entry>
  <entry>
    <title>ViT的若干细节</title>
    <url>/2024/03/03/ViT%E7%9A%84%E8%8B%A5%E5%B9%B2%E7%BB%86%E8%8A%82/</url>
    <content><![CDATA[<p>之前只看了ViT的大概结构，具体的模型细节和代码实现知之甚少。随着ViT逐渐成为CV领域的backbone，有必要重新审视下。</p>
<span id="more"></span>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.ltb9u1l3.webp"  alt="ViT"></p>
<h2 id="patch-gt-token"><a href="#patch-gt-token" class="headerlink" title="patch -&gt; token"></a>patch -&gt; token</h2><p>为了将图片处理成序列格式，很自然地想到将图片分割成一个个patch，再把patch处理成token。</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.54xi0lkn9a.webp"  alt="patch"></p>
<p>假设图片大小为 $224 \times 224 \times 3$ (即 $H \times W \times C$ )，每个patch大小为 $16 \times 16 \times 3$，那么序列长度就是 $196$，序列的形状是 $196 \times 768$。</p>
<p>如何将大小为 $16 \times 16 \times 3$ 的patch，映射为 $768$ 维的token？源码是直接将其<a class="link"   href="https://github.com/lucidrains/vit-pytorch/blob/5578ac472faf3903d4739ba783f3875b77177e57/vit_pytorch/vit.py#L96" >reshape<i class="fas fa-external-link-alt"></i></a></p>
<div class="keep-note danger"><p>在reshape之后，还需要过一层$768 \times 768$的embedding层。因为reshape后的$768$维向量是参数无关的，不参与梯度更新，过完embedding层，即拥有了token embedding的语义信息。</p>
</div>
<h4 id="处理成patch的好处"><a href="#处理成patch的好处" class="headerlink" title="处理成patch的好处"></a>处理成patch的好处</h4><ul>
<li>减少计算量：如果按照pixel维度计算self-attention，那复杂度大大增加。patch size越大，复杂度越低。stable diffusion也是这个思路，在latent space进行扩散，而不是pixel</li>
<li>减少图像冗余信息：图像是有大量冗余信息的，处理成patch不影响图片语义信息</li>
</ul>
<h2 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h2><p>论文采用的是可学习式位置编码，跟bert类似，<a class="link"   href="https://github.com/lucidrains/vit-pytorch/blob/5578ac472faf3903d4739ba783f3875b77177e57/vit_pytorch/vit.py#L102" >初始化一个可学习的1-d参数向量<i class="fas fa-external-link-alt"></i></a></p>
<p>其它的位置编码方案结果对比：<br><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.7smyazbtz0.webp"  alt="pos"></p>
<div class="keep-note danger"><p>个人感觉2-d位置编码更make sense，它保留了patch之间的空间位置关系，跟CNN类似。直接粗暴地拉平成一维序列，则丢弃了这种空间信息。</p>
</div>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.8z69jlfhxt.webp"  alt="exp"></p>
<p>在相同的数据集JFT-300M上预训练后，ViT在所有的下游任务上，都超过了BiT。值得注意的是，准确率上提升不大，但训练时间大为缩短。</p>
<blockquote>
<p>可能是基于Transformer架构的VIT，和卷积神经网络相比，更适合做切分均匀的矩阵计算，这样我们就能把参数均匀切到不同卡上做分布式训练，更好利用GPU算力，提升训练效率。</p>
</blockquote>
<p>但transformer架构有个独门绝技，那就是大力出奇迹。数据量越大，模型参数越多，任务效果就越好。下图就是证明：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.6f0f6yxsxl.webp"  alt="exp"></p>
<h2 id="ViT学习到空间局部性了吗？"><a href="#ViT学习到空间局部性了吗？" class="headerlink" title="ViT学习到空间局部性了吗？"></a>ViT学习到空间局部性了吗？</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.45f93hws8.webp"  alt="local"></p>
<p>可以看到，每个patch除了跟自己最相似外，其与周围的patch相关性高于距离较远的patch。这就说明ViT通过位置编码，已经学到了一定的空间局部性。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>ViT证明了Transformer架构在CV领域的可行性，以后Transformer将大一统各领域。NLP的成功经验非常有潜力迁移到CV领域，比如scaling law，大数据+大模型的范式将开拓出CV的新一片天地。<ul>
<li>大数据+大模型真的是既无脑又有效，通过这种方式让Transformer自己去学习到特定领域的归纳偏置。可以说Transformer下限比CNN低，但上限又是CNN无法企及的。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/kcqYF-Z3AwbPLQUOozyI0Q" >再读VIT，还有多少细节是你不知道的<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>VIT</tag>
      </tags>
  </entry>
  <entry>
    <title>Win11+Docker搭建CUDA开发环境</title>
    <url>/2023/12/24/Win11-Docker%E6%90%AD%E5%BB%BACUDA%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<p>最近入门了CUDA编程，先记录下搭建环境过程。</p>
<span id="more"></span>
<p>由于在windows和wsl上折腾了好久，装cuda、cudnn、cmake、gcc等软件，还经常遇到依赖、版本许多问题，最终污染了系统环境。在朋友的安利下，采用docker容器开发方案，试一下真香。</p>
<h2 id="本人软硬件条件"><a href="#本人软硬件条件" class="headerlink" title="本人软硬件条件"></a>本人软硬件条件</h2><ul>
<li>OS: win11</li>
<li>GPU: RTX 3060</li>
<li>Driver Version: 537.42</li>
<li>CUDA Version: 12.2</li>
<li>Docker: Dokcer Desktop 4.12.0</li>
</ul>
<p>目前想在docker容器里调用windows gpu，已经不再需要安装镜像nvidia-docker了。新版docker已经支持透传gpu，直接在参数里添加 <code>--gpus all</code> 即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -it --gpus all --name gpu_test -e NVIDIA_DRIVER_CAPABILITIES=compute,utility -e NVIDIA_VISIBLE_DEVICES=all mortals/codeenv:conda-cuda11.8</span><br></pre></td></tr></table></figure>
<p>Dockerfile见：<a class="link"   href="https://github.com/mortals-debuging/pytorch-docker/blob/master/cuda11_8/base-cuda11.8.Dockerfile" >base-cuda11.8<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/646152162" >2023完整版：深度学习环境在Docker上搭建（基于Linux和WSL）<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
        <tag>Docker</tag>
        <tag>Win11</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows Terminal主题配置</title>
    <url>/2022/03/01/Windows%20Terminal%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>个人在Windows Terminal上配置了一款ubuntu的主题，图片和设置文件见：<a class="link"   href="https://github.com/TransformersWsz/theme_for_windows_terminal" >A theme for windows terminal<i class="fas fa-external-link-alt"></i></a></p>
<span id="more"></span>
<img   src="/2022/03/01/Windows%20Terminal%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/example.jpg"  class="">]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>terminal</tag>
      </tags>
  </entry>
  <entry>
    <title>XGBoost</title>
    <url>/2021/05/31/XGBoost/</url>
    <content><![CDATA[<p>记录一下XGBoost的的学习过程。</p>
<span id="more"></span>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>我们要预测一家人对电子游戏的喜好程度，有年龄、性别、职业这些特征。根据之前训练出来的多棵树来对这些样本打分，如下图所示：</p>
<img   src="/2021/05/31/XGBoost/1.png"  class="">
<p>注意，<font color="red">上述分数是由训练所得</font>。与GBDT类似，两棵树的结论累加起来便是最终结论。如果不考虑工程实现、解决问题上的一些差异，XGBoost与GBDT比较大的不同就是目标函数的定义：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Obj^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t)}\right)+\sum_{i=1}^{t} \Omega\left(f_{i}\right) \\
&=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text { constant }
\end{aligned}</script><p>前 $t-1$ 棵树的复杂度之和可以用一个常量 $constant$ 表示。上述公式由两部分组成：</p>
<ul>
<li>损失函数：揭示训练误差</li>
<li>正则化项：惩罚复杂模型</li>
</ul>
<h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>前沿知识：泰勒展开式：</p>
<script type="math/tex; mode=display">
f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}</script><p>定义如下符号：</p>
<script type="math/tex; mode=display">
g_{i}=\frac{\partial \; l\left(y_{i}, \hat{y_i}^{(t-1)}\right)}{\partial \;  {\hat{y_i}^{(t-1)}} } \\ 

h_{i}=\frac{\partial^2 \; l\left(y_{i}, \hat{y_i}^{(t-1)}\right)}{\partial^2 \;  {\hat{y_i}^{(t-1)}} }</script><p>因此：</p>
<script type="math/tex; mode=display">
O b j^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)+\text { constant }</script><p>对应关系如下：</p>
<ul>
<li>(1)式 $x$ $\Leftrightarrow$ (2)式 $\hat{y_i}^{(t-1)}$</li>
<li>(1)式 $\Delta x$ $\Leftrightarrow$ (2)式 $f_{t}\left(x_{i}\right)$</li>
</ul>
<p>由于 $\hat{y_i}^{(t-1)}$ 是已知的，因此 $l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)$ 也是个常数项，可以合并到 $constant$ 去。将 $constant$ 去掉，上述公式可以简化为：</p>
<script type="math/tex; mode=display">
O b j^{(t)} \simeq \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)</script><h3 id="正则化项"><a href="#正则化项" class="headerlink" title="正则化项"></a>正则化项</h3><img   src="/2021/05/31/XGBoost/2.png"  class="">
<ul>
<li>$q(x)$ 表示将样本 $x$ 映射到某个叶子节点的编号上</li>
<li>$w$ 表示叶子节点的得分</li>
</ul>
<p>注意，<font color="red">多个样本可以落到同一个叶子节点上，这时它们的得分是一样的</font>。</p>
<p>XGBoost定义树的复杂度如下：</p>
<img   src="/2021/05/31/XGBoost/3.png"  class="">
<h3 id="重新组织损失函数"><a href="#重新组织损失函数" class="headerlink" title="重新组织损失函数"></a>重新组织损失函数</h3><p>由于 $w$ 是我们要求的参数，因此将上述公式组织成关于 $w$ 的函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
O b j^{(t)} & \simeq \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\
&=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\lambda \frac{1}{2} \sum_{j=1}^{T} w_{j}^{2} \\
&=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T
\end{aligned}</script><ul>
<li>$I_{j}=\left\{i \mid q\left(x_{i}\right)=j\right\}$ 表示样本下标集合：这些样本可以落到下标为 $j$ 的叶子节点</li>
</ul>
<p>这样上式可以看作关于 $w$ 的一元二次函数。</p>
<p>定义 $G_{j}=\sum_{i \in I_{j}} g_{i} \quad H_{j}=\sum_{i \in I_{j}} h_{i}$ ，上式继续简化为：</p>
<script type="math/tex; mode=display">
Obj^{(t)} = \sum_{j=1}^T \left[ \frac{1}{2} (H_j + \lambda) w_j^2 + G_j w_j \right] + \gamma T</script><p>当 $w_j = - \frac{G_j}{H_j + \lambda}$ 时，$Obj^{(t)}$ 取得最小：$-\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T$</p>
<p>下图给个示例：</p>
<img   src="/2021/05/31/XGBoost/4.png"  class="">
<hr>
<h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h3 id="1-二阶泰勒展开的优势在哪儿？"><a href="#1-二阶泰勒展开的优势在哪儿？" class="headerlink" title="1. 二阶泰勒展开的优势在哪儿？"></a>1. 二阶泰勒展开的优势在哪儿？</h3><p>PPT上是这样说的：</p>
<img   src="/2021/05/31/XGBoost/5.png"  class="">
<p>主要有如下两点理由：</p>
<ul>
<li>XGBoost是以mse为基础推导出来的，在mse的情况下，xgboost的目标函数展开就是一阶项+二阶项的形式，而其他类似logloss这样的目标函数不能表示成这种形式。为了后续推导的统一，所以将目标函数进行二阶泰勒展开，就可以直接自定义损失函数了，只要二阶可导即可，增强了模型的扩展性。</li>
<li>二阶信息能够让梯度收敛的更快，类似牛顿法比SGD收敛更快。一阶信息描述梯度变化方向，二阶信息可以描述梯度变化方向是如何变化的。</li>
</ul>
<h3 id="2-XGBoost与GBDT的区别"><a href="#2-XGBoost与GBDT的区别" class="headerlink" title="2. XGBoost与GBDT的区别"></a>2. XGBoost与GBDT的区别</h3><ul>
<li>基分类器：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的逻辑回归或者线性回归。</li>
<li>导数信息：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。</li>
<li>正则项：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。</li>
<li>列抽样：XGBoost支持列采样，与随机森林类似，用于防止过拟合。</li>
<li>缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。</li>
<li>并行化：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。</li>
</ul>
<h3 id="3-XGBoost为什么可以并行训练？"><a href="#3-XGBoost为什么可以并行训练？" class="headerlink" title="3. XGBoost为什么可以并行训练？"></a>3. XGBoost为什么可以并行训练？</h3><ul>
<li>XGBoost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。</li>
<li>XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。</li>
</ul>
<h3 id="4-XGBoost如何防止过拟合？"><a href="#4-XGBoost如何防止过拟合？" class="headerlink" title="4. XGBoost如何防止过拟合？"></a>4. XGBoost如何防止过拟合？</h3><ul>
<li>目标函数添加正则项：叶子节点个数+叶子节点权重的L2正则化</li>
<li>列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可）</li>
<li>子采样：每轮计算可以不使用全部样本，使算法更加保守</li>
<li>shrinkage: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间</li>
</ul>
<h3 id="5-XGBoost如何处理缺失值？"><a href="#5-XGBoost如何处理缺失值？" class="headerlink" title="5. XGBoost如何处理缺失值？"></a>5. XGBoost如何处理缺失值？</h3><ul>
<li>在特征 $k$ 上寻找最佳分割点时，不会对该列特征缺失的样本进行遍历，而只对该列特征值为非缺失的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找分割点的时间开销。</li>
<li>在逻辑实现上，为了保证完备性，会将该特征值缺失的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。</li>
<li>如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。</li>
</ul>
<h3 id="6-XGBoost中的一棵树的停止生长条件"><a href="#6-XGBoost中的一棵树的停止生长条件" class="headerlink" title="6.  XGBoost中的一棵树的停止生长条件"></a>6.  XGBoost中的一棵树的停止生长条件</h3><ul>
<li>当新引入的一次分裂所带来的增益 $Gain &lt; \gamma$ 时，放弃当前的分裂。</li>
<li>当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合。</li>
<li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。</li>
</ul>
<h3 id="7-XGBoost中如何对树进行剪枝？"><a href="#7-XGBoost中如何对树进行剪枝？" class="headerlink" title="7. XGBoost中如何对树进行剪枝？"></a>7. XGBoost中如何对树进行剪枝？</h3><ul>
<li>在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。</li>
<li>在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。</li>
<li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。</li>
<li>XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。</li>
</ul>
<h3 id="8-XGBoost如何分裂节点？"><a href="#8-XGBoost如何分裂节点？" class="headerlink" title="8. XGBoost如何分裂节点？"></a>8. XGBoost如何分裂节点？</h3><p>从树深度0开始，每一节点都遍历所有的特征，比如年龄、性别等等，然后对于某个特征，<strong>先按照该特征里的值进行排序，然后线性扫描该特征进而确定最好的分割点</strong>，最后对所有特征进行分割后，我们选择所谓的增益Gain最高的那个特征。Gain的计算公式如下：</p>
<img   src="/2021/05/31/XGBoost/6.png"  class="">
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/156047718" >XGBoost原理及常见面试题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/v_JULY_v/article/details/81410574" >通俗理解kaggle比赛大杀器xgboost<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLO v1讲解</title>
    <url>/2024/03/08/YOLO-v1%E8%AE%B2%E8%A7%A3/</url>
    <content><![CDATA[<p>YOLO是最经典的一阶目标检测框架，记录一下v1思路。</p>
<span id="more"></span>
<h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.es98zgnbw.webp"  alt="model"></p>
<ol>
<li>输入数据一张 $448 \times 448 \times 3$ 的图片，切分成 $7 \times 7$ 的网格</li>
<li>将图片经过多层CNN，下采样得到 $7 \times 7 \times 30$ 的feature map，其中 $30 = 2 * (4 + 1) + 20$ <ul>
<li>$2$ 表示每个单元格预测两个边界框</li>
<li>$4 + 1$ 分别表示边界框的位置以及边界框的置信度（一是该边界框含有目标的可能性大小，0或者1；二是这个边界框的准确度，用IOU衡量）</li>
<li>$20$ 表示最置信的边界框预测该框属于哪个类别的概率。无论单元格预测多少个边界框，都是只取一个边界框来预测类别，即默认每个单元格只有一个物体，这也是v1的缺陷</li>
</ul>
</li>
<li>计算loss，开始训练</li>
</ol>
<p>总结一下，将图片分割成 $S \times S$ 个单元格，每个单元格预测出 $S \times S \times ( B * 5 + C )$ 大小的张量。对应上述流程：$S = 7, B = 2, C = 20$</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><script type="math/tex; mode=display">
\begin{gathered}
\lambda_{\text {coord }} \sum_{i=0}^{S^2} \sum_{j=0}^B \mathbb{1}_{i j}^{\text {obj }}\left[\left(x_i-\hat{x}_i\right)^2+\left(y_i-\hat{y}_i\right)^2\right] \\
+\lambda_{\text {coord }} \sum_{i=0}^{S^2} \sum_{j=0}^B \mathbb{1}_{i j}^{\text {obj }}\left[\left(\sqrt{w_i}-\sqrt{\hat{w}_i}\right)^2+\left(\sqrt{h_i}-\sqrt{\hat{h}_i}\right)^2\right] \\
+\sum_{i=0}^{S^2} \sum_{j=0}^B \mathbb{1}_{i j}^{\text {obj }}\left(C_i-\hat{C}_i\right)^2 \\
+\lambda_{\text {noobj }} \sum_{i=0}^{S^2} \sum_{j=0}^B \mathbb{1}_{i j}^{\text {noobj }}\left(C_i-\hat{C}_i\right)^2 \\
+\sum_{i=0}^{S^2} \mathbb{1}_i^{\text {obj }} \sum_{c \in \text { classes }}\left(p_i(c)-\hat{p}_i(c)\right)^2
\end{gathered}</script><ol>
<li>第一项是边界框中心坐标的误差</li>
<li>第二项是边界框的高与宽的误差</li>
<li>第三项是包含目标的边界框的置信度误差（在训练过程中，如果该边界框包含目标，则置信度取IOU，而不是1，对应下面的代码可以理解）</li>
<li>第四项是不包含目标的边界框的置信度误差</li>
<li>第五项是包含目标的单元格的分类误差</li>
</ol>
<div class="keep-note danger"><p>值得注意的是，在推理过程中，我们是不可能计算出跟gt的IOU，所以取最置信的边界框作为predict label。</p>
</div>
<h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p><a class="link"   href="https://blog.csdn.net/qq_38683460/article/details/129578355#yoloLosspy_758" >yolo loss<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/qq_38683460/article/details/129578355" >YOLOv1代码分析——pytorch版保姆级教程<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/32525231" >目标检测|YOLO原理与实现<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>One-Stage</tag>
      </tags>
  </entry>
  <entry>
    <title>Zero-Inflated Log-Normal Loss</title>
    <url>/2025/06/03/Zero-Inflated-Log-Normal-Loss/</url>
    <content><![CDATA[<p>在营销LTV预测任务中，用户的价值呈现出如下特点：</p>
<ol>
<li>零膨胀（Zero-inflation）：大量用户的LTV为零（比如没有转化、没有付费）</li>
<li>偏态分布：有转化的人群中，LTV的非零值分布通常呈现出右偏重尾（分布的右侧有更长的尾巴，且均值 &gt; 中位数 &gt; 众数），即呈对数正态分布（Log-Normal）</li>
</ol>
<span id="more"></span>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.5fktxue58n.webp"  alt="data distribution"></p>
<p>针对这种数据分布，Google提出了ZILN Loss，用于更真实地拟合这类零膨胀、长尾的数据。</p>
<p>LTV建模如下两个任务：用户是否付费、付多少费，分别对应上述两个问题。</p>
<script type="math/tex; mode=display">
pred\_ltv(x) = pay\_prob(x) \times pay\_amount(x)</script><p>问题1是个二分类任务，问题2则是个回归任务：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    L_{\text {ZILN }}(x ; p, \mu, \sigma) &= L_{\text {CrossEntropy }}\left(\mathbb{1}_{\{x>0\}} ; p\right)+\mathbb{1}_{\{x>0\}} L_{\text {Lognormal }}(x ; \mu, \sigma) \\
    L_{\text {Lognormal }}(x ; \mu, \sigma) &= \log (x \sigma \sqrt{2 \pi})+\frac{(\log x-\mu)^2}{2 \sigma^2}
\end{aligned}</script><p>具体到模型建模，就是学习$p$、$mu$、$sigma$：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.2ks5s2w4g9.webp"  alt="model"></p>
<p>以下是tensorflow的实现代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">zero_inflated_lognormal_loss</span>(<span class="params">labels: tf.Tensor,</span></span><br><span class="line"><span class="params">                                 logits: tf.Tensor</span>) -&gt; tf.Tensor:</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Computes the zero inflated lognormal loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Usage with tf.keras API:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  model = tf.keras.Model(inputs, outputs)</span></span><br><span class="line"><span class="string">  model.compile(&#x27;sgd&#x27;, loss=zero_inflated_lognormal)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Arguments:</span></span><br><span class="line"><span class="string">    labels: True targets, tensor of shape [batch_size, 1].</span></span><br><span class="line"><span class="string">    logits: Logits of output layer, tensor of shape [batch_size, 3].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    Zero inflated lognormal loss value.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># [0, 1.2, 3, 0], 0表示未付费，1.2表示付费金额</span></span><br><span class="line">  labels = tf.convert_to_tensor(labels, dtype=tf.float32)</span><br><span class="line">  positive = tf.cast(labels &gt; <span class="number">0</span>, tf.float32)</span><br><span class="line"></span><br><span class="line">  logits = tf.convert_to_tensor(logits, dtype=tf.float32)</span><br><span class="line">  logits.shape.assert_is_compatible_with(</span><br><span class="line">      tf.TensorShape(labels.shape[:-<span class="number">1</span>].as_list() + [<span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># p</span></span><br><span class="line">  positive_logits = logits[..., :<span class="number">1</span>]</span><br><span class="line">  classification_loss = tf.keras.losses.binary_crossentropy(</span><br><span class="line">      y_true=positive, y_pred=positive_logits, from_logits=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># mu, sigma</span></span><br><span class="line">  loc = logits[..., <span class="number">1</span>:<span class="number">2</span>]</span><br><span class="line">  scale = tf.math.maximum(</span><br><span class="line">      tf.keras.backend.softplus(logits[..., <span class="number">2</span>:]),</span><br><span class="line">      tf.math.sqrt(tf.keras.backend.epsilon()))</span><br><span class="line">  <span class="comment"># 下面两行可以直接改成：log_prob(labels[labels&gt;0])</span></span><br><span class="line">  safe_labels = positive * labels + (</span><br><span class="line">      <span class="number">1</span> - positive) * tf.keras.backend.ones_like(labels)</span><br><span class="line">  regression_loss = -tf.keras.backend.mean(</span><br><span class="line">      positive * tfd.LogNormal(loc=loc, scale=scale).log_prob(safe_labels),</span><br><span class="line">      axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> classification_loss + regression_loss</span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/zhouyc/p/16943042.html" >LTV预估的一些思考<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://liuslevis.github.io/2020/09/17/pltv/" >Google用户付费预估深度模型笔记<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Marketing</category>
      </categories>
      <tags>
        <tag>LTV</tag>
        <tag>ZILN</tag>
      </tags>
  </entry>
  <entry>
    <title>ajax发送请求无法加载等待模态框？</title>
    <url>/2018/07/15/ajax%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82%E6%97%A0%E6%B3%95%E5%8A%A0%E8%BD%BD%E7%AD%89%E5%BE%85%E6%A8%A1%E6%80%81%E6%A1%86%EF%BC%9F/</url>
    <content><![CDATA[<p>虽说现在谈论jQuery已经很low了，但出于维护旧项目的需要，还是重新学习了一遍。当我们向后台发送请求的时候，为了照顾用户体验，需要使用等待模态框框来过渡。今天遇到的一个坑是，无论怎么发送请求，界面都不会出现模态框，即使有也是一闪而过。代码如下：</p>
<span id="more"></span>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>test<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;./js/jquery-3.1.1.js&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">href</span>=<span class="string">&quot;./css/bootstrap.min.css&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/css&quot;</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;./js/bootstrap.min.js&quot;</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">function</span> <span class="title function_">firmSubmit</span>(<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">            $(<span class="string">&quot;#submit&quot;</span>).<span class="title function_">click</span>(<span class="keyword">function</span> (<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="keyword">var</span> sex = <span class="built_in">parseInt</span>($(<span class="string">&#x27;#sex&#x27;</span>).<span class="title function_">val</span>());</span></span><br><span class="line"><span class="language-javascript">                <span class="keyword">if</span> (sex == <span class="number">0</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                    <span class="title function_">alert</span>(<span class="string">&quot;性别不能为空！&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">                    $(<span class="string">&#x27;#sex&#x27;</span>).<span class="title function_">focus</span>();</span></span><br><span class="line"><span class="language-javascript">                    <span class="keyword">return</span> <span class="literal">false</span>;</span></span><br><span class="line"><span class="language-javascript">                &#125;</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">                $.<span class="title function_">ajax</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">type</span>: <span class="string">&quot;post&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">url</span>: <span class="string">&quot;/api&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">dataType</span>: <span class="string">&quot;json&quot;</span>,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">data</span>: &#123; <span class="string">&quot;sex&quot;</span>: sex &#125;,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">beforeSend</span>: <span class="keyword">function</span> (<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                        $(<span class="string">&quot;#loadingModal&quot;</span>).<span class="title function_">modal</span>(<span class="string">&quot;show&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">                    &#125;,</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">success</span>: <span class="keyword">function</span> (<span class="params">json</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                        <span class="keyword">if</span> (json.<span class="property">result</span> == <span class="number">1</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                            <span class="title function_">alert</span>(<span class="string">&quot;提交成功&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">                        &#125;</span></span><br><span class="line"><span class="language-javascript">                        <span class="keyword">else</span> &#123;</span></span><br><span class="line"><span class="language-javascript">                            <span class="title function_">alert</span>(<span class="string">&quot;提交失败，请重新检查！&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">                        &#125;</span></span><br><span class="line"><span class="language-javascript">                    &#125;,</span></span><br><span class="line"><span class="language-javascript">                    <span class="attr">complete</span>: <span class="keyword">function</span> (<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                        $(<span class="string">&quot;#loadingModal&quot;</span>).<span class="title function_">modal</span>(<span class="string">&quot;hide&quot;</span>);</span></span><br><span class="line"><span class="language-javascript">                        <span class="variable language_">window</span>.<span class="property">location</span>.<span class="property">href</span> = <span class="string">&quot;/in&quot;</span>;</span></span><br><span class="line"><span class="language-javascript">                    &#125;</span></span><br><span class="line"><span class="language-javascript">                &#125;);</span></span><br><span class="line"><span class="language-javascript">            &#125;);</span></span><br><span class="line"><span class="language-javascript">        &#125;</span></span><br><span class="line"><span class="language-javascript">        $(<span class="keyword">function</span> (<span class="params"></span>) &#123;</span></span><br><span class="line"><span class="language-javascript">            <span class="comment">//使用getJSON方法读取json数据,</span></span></span><br><span class="line"><span class="language-javascript">            <span class="keyword">var</span> options = [<span class="string">&quot;sex&quot;</span>];</span></span><br><span class="line"><span class="language-javascript">            $.<span class="title function_">ajaxSetup</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="attr">async</span>: <span class="literal">false</span></span></span><br><span class="line"><span class="language-javascript">            &#125;);</span></span><br><span class="line"><span class="language-javascript">            $.<span class="title function_">getJSON</span>(<span class="string">&quot;/getsex&quot;</span>, <span class="keyword">function</span> (<span class="params">data</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; options.<span class="property">length</span>; i++) &#123;</span></span><br><span class="line"><span class="language-javascript">                    $.<span class="title function_">each</span>(data[options[i]], <span class="keyword">function</span> (<span class="params">key, val</span>) &#123;</span></span><br><span class="line"><span class="language-javascript">                        <span class="keyword">var</span> str = <span class="string">&quot;&lt;option value=&quot;</span> + key + <span class="string">&quot;&gt;&quot;</span> + val + <span class="string">&quot;&lt;/option&gt;&quot;</span>;</span></span><br><span class="line"><span class="language-javascript">                        $(<span class="string">&quot;#&quot;</span> + options[i]).<span class="title function_">append</span>(str);</span></span><br><span class="line"><span class="language-javascript">                    &#125;);</span></span><br><span class="line"><span class="language-javascript">                &#125;</span></span><br><span class="line"><span class="language-javascript">            &#125;);</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">            <span class="comment">//表单判空并提交</span></span></span><br><span class="line"><span class="language-javascript">            <span class="title function_">firmSubmit</span>();</span></span><br><span class="line"><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">        &#125;);</span></span><br><span class="line"><span class="language-javascript">    </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"></span></span><br><span class="line"><span class="language-css">        <span class="selector-class">.main</span> &#123;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">width</span>: <span class="number">38%</span>;</span></span><br><span class="line"><span class="language-css">        &#125;</span></span><br><span class="line"><span class="language-css"></span></span><br><span class="line"><span class="language-css">        <span class="selector-class">.commonlabel</span> &#123;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">font-size</span>: <span class="number">16px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">margin-left</span>: <span class="number">5px</span>;</span></span><br><span class="line"><span class="language-css">        &#125;</span></span><br><span class="line"><span class="language-css"></span></span><br><span class="line"><span class="language-css">        <span class="selector-class">.loading</span> &#123;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">height</span>: <span class="number">80px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">width</span>: <span class="number">80px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">background</span>: <span class="built_in">url</span>(<span class="string">&#x27;./img/load.gif&#x27;</span>) no-repeat center;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">opacity</span>: <span class="number">0.7</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">position</span>: fixed;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">left</span>: <span class="number">50%</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">top</span>: <span class="number">50%</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">margin-left</span>: -<span class="number">40px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">margin-top</span>: -<span class="number">40px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">z-index</span>: <span class="number">1001</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">background-color</span>: <span class="number">#dad8d8</span>;</span></span><br><span class="line"><span class="language-css">            -moz-<span class="attribute">border-radius</span>: <span class="number">20px</span>;</span></span><br><span class="line"><span class="language-css">            -webkit-<span class="attribute">border-radius</span>: <span class="number">20px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">border-radius</span>: <span class="number">20px</span>;</span></span><br><span class="line"><span class="language-css">            <span class="attribute">filter</span>: progid:DXImageTransform.Microsoft.<span class="built_in">Alpha</span>(opacity=<span class="number">70</span>);</span></span><br><span class="line"><span class="language-css">        &#125;</span></span><br><span class="line"><span class="language-css">    </span><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;loadingModal&quot;</span> <span class="attr">class</span>=<span class="string">&quot;modal fade&quot;</span> <span class="attr">data-keyboard</span>=<span class="string">&quot;false&quot;</span> <span class="attr">tabindex</span>=<span class="string">&quot;-1&quot;</span> <span class="attr">data-backdrop</span>=<span class="string">&quot;static&quot;</span> <span class="attr">data-role</span>=<span class="string">&quot;dialog&quot;</span> <span class="attr">aria-labelledby</span>=<span class="string">&quot;myModalLabel&quot;</span></span></span><br><span class="line"><span class="tag">        <span class="attr">aria-hidden</span>=<span class="string">&quot;true&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;loading&quot;</span> <span class="attr">class</span>=<span class="string">&quot;loading&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;main center-block&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">style</span>=<span class="string">&quot;height: 30px;&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;form-group&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">&quot;sex&quot;</span> <span class="attr">class</span>=<span class="string">&quot;commonlabel&quot;</span>&gt;</span>性别<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;sex&quot;</span> <span class="attr">class</span>=<span class="string">&quot;form-control&quot;</span> <span class="attr">name</span>=<span class="string">&quot;sex&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">button</span> <span class="attr">id</span>=<span class="string">&quot;submit&quot;</span> <span class="attr">class</span>=<span class="string">&quot;btn btn-primary&quot;</span> <span class="attr">style</span>=<span class="string">&quot;width: 20%; margin: 0 auto; display: block; float: left;&quot;</span>&gt;</span>提交<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>后来才发现是 <code>$.ajaxSetup(&#123;async: false&#125;);</code> 搞的鬼。查了一下官方文档，它是这样解释的：</p>
<blockquote>
<p>async (default: true)<br>Type: Boolean<br>By default, all requests are sent asynchronously (i.e. this is set to true by default). If you need synchronous requests, set this option to false. Cross-domain requests and dataType: “jsonp” requests do not support synchronous operation. Note that synchronous requests may temporarily lock the browser, disabling any actions while the request is active. As of jQuery 1.8, the use of async: false with jqXHR ($.Deferred) is deprecated; you must use the success/error/complete callback options instead of the corresponding methods of the jqXHR object such as jqXHR.done().</p>
</blockquote>
<p>也就是说设置 <code>async: false</code> 会锁住浏览器，禁止浏览器的任何行为。比如用户点击按钮、下拉滚动条等行为浏览器都不会有响应，直到该同步请求完成。所以发送同步请求期间，模态框不会出现。但在请求完成后，会执行：<br><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">complete</span>: <span class="keyword">function</span> (<span class="params"></span>) &#123;</span><br><span class="line">    $(<span class="string">&quot;#loadingModal&quot;</span>).<span class="title function_">modal</span>(<span class="string">&quot;hide&quot;</span>);</span><br><span class="line">    <span class="variable language_">window</span>.<span class="property">location</span>.<span class="property">href</span> = <span class="string">&quot;/in&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>因此会出现模态框一闪而过的情况。解决方法也很简单，在发送请求前将 <code>$.ajaxSetup(&#123;async: true&#125;);</code> 设置回来即可。</p>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>Ajax</tag>
      </tags>
  </entry>
  <entry>
    <title>alfred自定义谷歌翻译workflow</title>
    <url>/2024/01/23/alfred%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91workflow/</url>
    <content><![CDATA[<p>如果要实现自定义workflow，则必须安装付费版的alfred，囊中羞涩的话可以自行淘宝。自定义步骤如下：</p>
<span id="more"></span>
<h2 id="1-新建空的workflow，填写基本信息"><a href="#1-新建空的workflow，填写基本信息" class="headerlink" title="1. 新建空的workflow，填写基本信息"></a>1. 新建空的workflow，填写基本信息</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.2bpwgci5ntes.webp"  alt="blank workflow"></p>
<h2 id="2-开发python脚本"><a href="#2-开发python脚本" class="headerlink" title="2. 开发python脚本"></a>2. 开发python脚本</h2><p>打开该workflow所在目录，进行下面步骤：</p>
<ol>
<li>首先安装谷歌翻译库：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install googletrans==3.1.0a0</span><br></pre></td></tr></table></figure></li>
<li>编写py脚本<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> googletrans <span class="keyword">import</span> Translator</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">input_text</span>):</span><br><span class="line">    d = &#123;</span><br><span class="line">        <span class="string">&quot;en&quot;</span>: <span class="string">&quot;zh-CN&quot;</span>,</span><br><span class="line">        <span class="string">&quot;zh-CN&quot;</span>: <span class="string">&quot;en&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    translator = Translator()</span><br><span class="line">    lang = translator.detect(input_text).lang</span><br><span class="line">    alfred_results = []</span><br><span class="line">    <span class="comment"># 中英文互译</span></span><br><span class="line">    <span class="keyword">if</span> lang <span class="keyword">in</span> d:</span><br><span class="line">        text = translator.translate(input_text, dest=d[lang]).text</span><br><span class="line">        alfred_results.append(&#123;</span><br><span class="line">            <span class="string">&quot;title&quot;</span>: text,</span><br><span class="line">            <span class="string">&quot;arg&quot;</span>: text,    <span class="comment"># 该参数不可省略，将用于后续的剪贴板复制；否则后续动作无法触发</span></span><br><span class="line">            <span class="string">&quot;icon&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;path&quot;</span>: <span class="string">&quot;./google_translate.png&quot;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        alfred_results.append(&#123;</span><br><span class="line">            <span class="string">&quot;title&quot;</span>: <span class="string">&quot;未识别语种&quot;</span>,</span><br><span class="line">            <span class="string">&quot;icon&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;path&quot;</span>: <span class="string">&quot;./google_translate.png&quot;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    <span class="keyword">return</span> json.dumps(&#123;</span><br><span class="line">        <span class="string">&quot;items&quot;</span>: alfred_results</span><br><span class="line">    &#125;, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    resp = <span class="string">&quot;no input text to translate&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sys.argv) &gt;= <span class="number">2</span>:</span><br><span class="line">        input_text = <span class="string">&quot;\t&quot;</span>.join(sys.argv[<span class="number">1</span>:])</span><br><span class="line">        resp = main(input_text)</span><br><span class="line">    sys.stdout.write(resp)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="3-编辑工作流"><a href="#3-编辑工作流" class="headerlink" title="3. 编辑工作流"></a>3. 编辑工作流</h2><ol>
<li>新建script filter：</li>
</ol>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.4s7zzod45jc0.webp"  alt="new scipt filter"></p>
<p>配置信息说明:</p>
<ul>
<li>触发谷歌翻译关键词：<code>tr</code></li>
<li>将输入看做<code>&#123;query&#125;</code></li>
<li>调用python脚本进行翻译：<code>python ./translate.py &quot;&#123;query&#125;&quot;</code></li>
<li>避免一些转义符</li>
</ul>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.2bw7bcg882m8.webp"  alt="config"></p>
<ol>
<li>新增剪贴板</li>
</ol>
<p>在filter后面接一个clipboard：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.723f5xy7w2o0.webp"  alt="clipboard"></p>
<h2 id="4-调试工作流"><a href="#4-调试工作流" class="headerlink" title="4. 调试工作流"></a>4. 调试工作流</h2><p>右侧有个虫子标记，点击。然后调起alfred，输入命令测试，下面的控制台会打印日志信息：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.5cwhhyuhgmg0.webp"  alt="debug"></p>
<p>如果上述步骤一切顺利的话，你的工作流就实现了。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://py-googletrans.readthedocs.io/en/latest/" >googletrans<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://jlovec.net/2020/12/26/alfred-gong-zuo-liu-workflows-shi-li/" >Alfred工作流workflows实例<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>Alfred</tag>
        <tag>Workflow</tag>
      </tags>
  </entry>
  <entry>
    <title>args &amp; kwargs</title>
    <url>/2017/12/29/args%20&amp;%20kwargs/</url>
    <content><![CDATA[<p><code>args</code> 和 <code>kwargs</code> 是python函数中最常用的传参形式，用法非常灵活。</p>
<span id="more"></span>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>Python支持可变参数，最简单的方法莫过于使用默认参数。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_defargs</span>(<span class="params">one, two=<span class="number">2</span></span>):    <span class="comment"># 参数one没有默认值，two的默认值为2</span></span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;Required argument: &#x27;</span>, one)</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;Optional argument: &#x27;</span>, two)</span><br><span class="line"></span><br><span class="line">test_defargs(<span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Required argument: 1</span></span><br><span class="line"><span class="string">Optional argument: 2</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">test_defargs(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Required argument: 1</span></span><br><span class="line"><span class="string">Optional argument: 3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>另外一种达到可变参数 (Variable Argument) 的方法：<br>使用 <code>*args</code> 和 <code>**kwargs</code> 语法。<br><code>*args</code> 是可变的位置参数(positional arguments)列表，<br><code>**kwargs</code> 是可变的关键词参数(keyword arguments)列表。<br>并且规定位置参数必须位于关键词参数之前，即 <code>*args</code> 必须位于 <code>**kwargs</code> 之前。</p>
<h2 id="位置参数"><a href="#位置参数" class="headerlink" title="位置参数"></a>位置参数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_hello</span>(<span class="params">name, sex</span>):</span><br><span class="line">    sex_dict = &#123;<span class="number">0</span>: <span class="string">&#x27;先生&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;女士&#x27;</span>&#125;    <span class="comment"># key: value</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;hello %s %s, welcome to Python world!&#x27;</span> %(name, sex_dict.get(sex%<span class="number">2</span>, <span class="string">&#x27;先生&#x27;</span>)))    <span class="comment"># if no such a key, print &#x27;先生&#x27;</span></span><br><span class="line"></span><br><span class="line">print_hello(<span class="string">&#x27;Chen&#x27;</span>, <span class="number">2</span>)    <span class="comment"># 位置参数要求先后顺序，对应name和sex</span></span><br><span class="line">print_hello(<span class="string">&#x27;Chen&#x27;</span>, <span class="number">3</span>)    <span class="comment"># 两个参数的顺序必须一一对应，且少一个参数都不可以</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hello Chen 女士, welcome to Python world!</span></span><br><span class="line"><span class="string">hello Chen 先生, welcome to Python world!</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="关键字参数"><a href="#关键字参数" class="headerlink" title="关键字参数"></a>关键字参数</h2><p>用于函数调用，通过“键-值”形式加以指定。<br>使用关键词参数可以让函数更加清晰、容易使用，同时也清除了参数的顺序需求。</p>
<p>以下是用关键字参数正确调用函数的实例<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print_hello(<span class="string">&#x27;Chen&#x27;</span>, sex=<span class="number">1</span>)    <span class="comment"># 有位置参数时，位置参数必须在关键字参数的前面</span></span><br><span class="line">print_hello(name=<span class="string">&#x27;Chen&#x27;</span>, sex=<span class="number">1</span>)    <span class="comment"># 关键字参数之间不存在先后顺序的</span></span><br><span class="line">print_hello(sex=<span class="number">1</span>, name=<span class="string">&#x27;Chen&#x27;</span>)</span><br></pre></td></tr></table></figure><br>以下是错误的调用方式:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print_hello(name=<span class="string">&#x27;Chen&#x27;</span>, <span class="number">1</span>)    <span class="comment"># 有位置参数时，位置参数必须在关键字参数的前面</span></span><br><span class="line">print_hello(sex=<span class="number">1</span>, <span class="string">&#x27;Chen&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="默认参数"><a href="#默认参数" class="headerlink" title="默认参数"></a>默认参数</h2><p>使用关键词参数时，可为参数提供默认值，调用函数时可传可不传该默认参数的值（注意：所有位置参数必须出现在默认参数前，包括函数定义和调用）</p>
<p>正确的默认参数定义方式 —&gt; 位置参数在前，默认参数在后:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_hello2</span>(<span class="params">name, sex=<span class="number">1</span></span>):    <span class="comment"># 默认sex=1</span></span><br><span class="line">    sex_dict = &#123;<span class="number">1</span>: <span class="string">&#x27;先生&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;女士&#x27;</span>&#125;    <span class="comment"># key: value</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;hello %s %s, welcome to circus!&#x27;</span> %(name, sex_dict.get(sex, <span class="string">&#x27;先生&#x27;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 错误的定义方式</span></span><br><span class="line"><span class="comment"># def print_hello2(sex=1, name):</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用时不传sex的值，则使用默认值1</span></span><br><span class="line">print_hello2(<span class="string">&#x27;Chen&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hello Chen 先生, welcome to circus!</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用时传入sex的值，并指定为2。无视原来的sex=1，仅本次生效，不会改变函数sex的默认值</span></span><br><span class="line">print_hello2(<span class="string">&#x27;Liu&#x27;</span>, sex=<span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hello Liu 女士, welcome to circus!</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="收集参数-Packing"><a href="#收集参数-Packing" class="headerlink" title="收集参数(Packing)"></a>收集参数(Packing)</h2><blockquote>
<p>被收集到一起的位置参数或关键词参数</p>
</blockquote>
<p>有些时候，我们在定义参数时不确定会传递多少个参数（或者不传递），甚至我们想要根据实际情况传入任意个参数，这个时候 <code>packing</code> 就派上了用场。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_args</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line"></span><br><span class="line">print_args(<span class="number">1</span>)</span><br><span class="line">print_args(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(1,)</span></span><br><span class="line"><span class="string">(1, 2, 3)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>可以看到，结果是作为一个元组(tuple)打印出来的，因为仅传入1时，输出的括号里面有一个逗号。实际上，<code>*args</code> 前的 <code>*</code> 可以理解为将我们需要传入的所有值放置在了同一个元组里面，这就是收集 (packing) 的过程。</p>
<h3 id="位置参数的收集传递-————-packing"><a href="#位置参数的收集传递-————-packing" class="headerlink" title="位置参数的收集传递 ———— *packing"></a>位置参数的收集传递 ———— *packing</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_args2</span>(<span class="params">num, *args</span>):</span><br><span class="line">    <span class="built_in">print</span>(num)</span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line"></span><br><span class="line">print_args2(<span class="number">1</span>)</span><br><span class="line">print_args2(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1    print(num)</span></span><br><span class="line"><span class="string">()    print(args) --&gt; 一个空元组</span></span><br><span class="line"><span class="string">1    print(num)</span></span><br><span class="line"><span class="string">(2, 3)    print(args) --&gt; 剩余的参数组成的元组！</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>那么这么说来 <code>*</code> 的意思就是“把剩余的所有参数收集 packing 起来”！</p>
<h3 id="关键词参数的收集传递-————-packing"><a href="#关键词参数的收集传递-————-packing" class="headerlink" title="关键词参数的收集传递 ———— **packing"></a>关键词参数的收集传递 ———— **packing</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_kargs</span>(<span class="params">**kargs</span>):</span><br><span class="line">    <span class="built_in">print</span>(kargs)</span><br><span class="line"></span><br><span class="line">print_kargs(x=<span class="number">1</span>, y=<span class="number">2</span>, z=<span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;z&#x27;: 3, &#x27;y&#x27;: 2, &#x27;x&#x27;: 1&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>结果输出了一个字典(dic)，双星号 <code>**</code> 收集了所有关键词参数。</p>
<h3 id="分割参数"><a href="#分割参数" class="headerlink" title="分割参数"></a>分割参数</h3><p><code>*</code> 和 <code>**</code> 也可以在函数调用过程中使用，称之为分割 (unpacking)。</p>
<h4 id="在传递元组时，让元组的每一个元素对应一个位置参数"><a href="#在传递元组时，让元组的每一个元素对应一个位置参数" class="headerlink" title="在传递元组时，让元组的每一个元素对应一个位置参数"></a>在传递元组时，让元组的每一个元素对应一个位置参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_hello</span>(<span class="params">name, sex</span>):</span><br><span class="line">    <span class="built_in">print</span>(name, sex)</span><br><span class="line"></span><br><span class="line">args = (<span class="string">&#x27;Chen&#x27;</span>, <span class="string">&#x27;男&#x27;</span>)</span><br><span class="line">print_hello(*args)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Chen 男</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="在传递词典字典时，让词典的每个键值对作为一个关键字参数传递给函数"><a href="#在传递词典字典时，让词典的每个键值对作为一个关键字参数传递给函数" class="headerlink" title="在传递词典字典时，让词典的每个键值对作为一个关键字参数传递给函数"></a>在传递词典字典时，让词典的每个键值对作为一个关键字参数传递给函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_info</span>(<span class="params">**kargs</span>):</span><br><span class="line">    <span class="built_in">print</span>(kargs)</span><br><span class="line"></span><br><span class="line">kargs = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Chen&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>: <span class="string">&#x27;男&#x27;</span>, <span class="string">&#x27;time&#x27;</span>: <span class="string">&#x27;13&#x27;</span>&#125;</span><br><span class="line">print_info(**kargs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;name&#x27;: &#x27;Chen&#x27;, &#x27;sex&#x27;:&#x27;男&#x27;,&#x27;time&#x27;:&#x27;13&#x27;&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="位置参数、关键词参数（默认参数）、收集参数的混合使用"><a href="#位置参数、关键词参数（默认参数）、收集参数的混合使用" class="headerlink" title="位置参数、关键词参数（默认参数）、收集参数的混合使用"></a>位置参数、关键词参数（默认参数）、收集参数的混合使用</h2><p>基本原则是：先位置参数，默认参数，收集位置参数，收集关键字参数(定义和调用都应遵循)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">name, age, sex=<span class="number">1</span>, *args, **kargs</span>):</span><br><span class="line">    <span class="built_in">print</span>(name, age, sex, args, kargs)</span><br><span class="line"></span><br><span class="line">func(<span class="string">&#x27;Chen&#x27;</span>, <span class="number">25</span>, <span class="number">2</span>, <span class="string">&#x27;Geo&#x27;</span>, <span class="string">&#x27;SR&#x27;</span>, level=<span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Chen 25 2 (&#x27;Geo&#x27;, &#x27;SR&#x27;) &#123;&#x27;level&#x27;: 2&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>总结一下就是，星号 <code>*</code> 只有在定义需要使用不定数目的参数的函数，或者调用“分割”字典和序列时才有必要添加。</p>
<h2 id="各类参数混合使用"><a href="#各类参数混合使用" class="headerlink" title="各类参数混合使用"></a>各类参数混合使用</h2><h3 id="EXAMPLE-1"><a href="#EXAMPLE-1" class="headerlink" title="EXAMPLE 1"></a>EXAMPLE 1</h3><h4 id="Tell-a-story"><a href="#Tell-a-story" class="headerlink" title="Tell a story"></a>Tell a story</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">story1</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Once upon a time, there was a %(job)s called %(name)s.&#x27;</span> % kwargs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">story2</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;In the year of our lord %(year)d, there once lived a %(job)s of a royal line.&#x27;</span> % kwargs</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(story1(name=<span class="string">&#x27;Robin&#x27;</span>, job=<span class="string">&#x27;brave knight&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(story1(job=<span class="string">&#x27;king&#x27;</span>, name=<span class="string">&#x27;Charlie&#x27;</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Once upon a time, there was a brave knight called Robin.</span></span><br><span class="line"><span class="string">Once upon a time, there was a king called Charlie.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">kwargs = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Abel&#x27;</span>, <span class="string">&#x27;job&#x27;</span>: <span class="string">&#x27;bard&#x27;</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(story1(**kwargs))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Once upon a time, there was a bard called Abel.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">kwargs.pop(<span class="string">&#x27;job&#x27;</span>)    <span class="comment"># delete key[&#x27;job&#x27;] --&gt; kwargs = &#123;&#x27;name&#x27;: &#x27;Abel&#x27;,&#125;</span></span><br><span class="line"><span class="built_in">print</span>(story1(job=<span class="string">&#x27;witch&#x27;</span>, **kwargs))    <span class="comment"># redefine key[&#x27;job&#x27;]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Once upon a time, there was a witch called Abel.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(story2(year=<span class="number">1239</span>, job=<span class="string">&#x27;prince&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(story2(job=<span class="string">&#x27;princess&#x27;</span>, year=<span class="number">1239</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">In the year of our lord 1239, there once lived a prince of a royal line.</span></span><br><span class="line"><span class="string">In the year of our lord 1239, there once lived a princess of a royal line.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="EXAMPLE-2"><a href="#EXAMPLE-2" class="headerlink" title="EXAMPLE 2"></a>EXAMPLE 2</h3><h4 id="x-to-the-yth-power"><a href="#x-to-the-yth-power" class="headerlink" title="x to the yth power"></a>x to the yth power</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">power</span>(<span class="params">x, y, *args</span>):</span><br><span class="line">    <span class="keyword">if</span> args:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Received redundant parameters:&#x27;</span>, args)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">pow</span>(x, y)    </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(power(<span class="number">2</span>, <span class="number">3</span>))    <span class="comment"># 2 to the 3rd power</span></span><br><span class="line"><span class="built_in">print</span>(power(<span class="number">3</span>, <span class="number">2</span>))    <span class="comment"># 3 to the 2nd power</span></span><br><span class="line"><span class="built_in">print</span>(power(x=<span class="number">2</span>, y=<span class="number">3</span>))    <span class="comment"># 2 to the 3rd power</span></span><br><span class="line"><span class="built_in">print</span>(power(y=<span class="number">3</span>, x=<span class="number">2</span>))    <span class="comment"># no order for keyword arguments</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">8</span></span><br><span class="line"><span class="string">9</span></span><br><span class="line"><span class="string">8</span></span><br><span class="line"><span class="string">8</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">args = (<span class="number">5</span>,) * <span class="number">2</span>    <span class="comment"># &#x27;(5,) * 2&#x27;: copy elements in the tuple 2 times. The outcome is (5, 5)</span></span><br><span class="line"><span class="built_in">print</span>(power(*args))    <span class="comment"># 5 to the 5th power</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">3125</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(power(<span class="number">2</span>, <span class="number">5</span>, <span class="string">&#x27;Keep on trying&#x27;</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Received redundant parameters: (&#x27;Keep on trying&#x27;,)</span></span><br><span class="line"><span class="string">32    # 2 to the 5th power</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="EXAMPLE-3"><a href="#EXAMPLE-3" class="headerlink" title="EXAMPLE 3"></a>EXAMPLE 3</h3><h4 id="Bulid-a-function-to-realize-range-for-step-gt-0"><a href="#Bulid-a-function-to-realize-range-for-step-gt-0" class="headerlink" title="Bulid a function to realize range() for step&gt;0"></a>Bulid a function to realize range() for step&gt;0</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">interval</span>(<span class="params">start=<span class="number">0</span>, stop=<span class="literal">None</span>, step=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">if</span> stop <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        start, stop = <span class="number">0</span>, start</span><br><span class="line">    result = []</span><br><span class="line">    i = start</span><br><span class="line">    <span class="keyword">while</span> i &lt; stop:</span><br><span class="line">        result.append(i)</span><br><span class="line">        i += step</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(interval(<span class="number">10</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(interval(<span class="number">1</span>, <span class="number">6</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[1, 2, 3, 4, 5]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(interval(<span class="number">3</span>, <span class="number">15</span>, <span class="number">3</span>))</span><br><span class="line">[<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">12</span>]</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="POSTSCRIPT"><a href="#POSTSCRIPT" class="headerlink" title="POSTSCRIPT"></a>POSTSCRIPT</h2><p>在Python 3.X当中加入了”部分剩余参数”的概念。举例如下 :<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a,\*b,c = <span class="built_in">range</span>(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><br>中间的 *b 实际上是收集到了3个参数。</p>
<p>下面这个例子更加直观 :<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func1</span>(<span class="params">x, y, *args, z=<span class="number">1</span></span>):    <span class="comment"># z=1 在函数参数中最后定义</span></span><br><span class="line">    <span class="built_in">print</span>(x, y, args, z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">func1(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1 2 (3, 4, 5) 1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>Python 3.X 当中，默认参数 z=1 在函数参数中最后定义，Python 就会知道除了 传给 x 和 y, 以及 z=1 的部分，其他的剩余参数 (3, 4, 5) 都是传入到 *args 当中。</p>
<font color="red">但在Python 2.X 并没有这一特性，所以 Python 2.X 的 z=1 必须在args和*kwargs这种剩余参数收集之前。</font>

<p>默认参数 z=1 不是在函数参数中最后定义时，情况又是怎样？<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func2</span>(<span class="params">x, y, z=<span class="number">1</span>, *args</span>):</span><br><span class="line">    <span class="built_in">print</span>(x, y, z, args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">func2(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1 2 3 (4, 5)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>3传入到 z ，而 args=(4, 5)</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.jianshu.com/p/61507f60fa29">Python中的 <em>args 和 *</em>kwargs</a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>可变参数</tag>
      </tags>
  </entry>
  <entry>
    <title>chmod &amp; chown</title>
    <url>/2017/12/20/chmod%20&amp;%20chown/</url>
    <content><![CDATA[<p>两种都是关于linux权限的命令。</p>
<span id="more"></span>
<h1 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h1><p> <code>chmod</code> 指令是更改文件读写执行权限的。<br>文件权限可以通过 <code>ls -a</code> 或 <code>ll</code> 来看,在每个文件前有10个字符,第一个是 <code>d</code> 是文件夹,否则为 <code>-</code> 。后面三组 <code>rwx</code> ，分别是读取，写入和执行的权限； 三组分别是用户自己,同组以及其他人的相应 <code>rwx</code> 权限。没有执行权限, 脚本和程序也不能直接跑；没有写权限,就没法生成和保存文件；没有读的权限就连访问都难。一般文件权限是 <code>755</code> ，下面将介绍。</p>
<h2 id="权限有两种表示方式"><a href="#权限有两种表示方式" class="headerlink" title="权限有两种表示方式 :"></a>权限有两种表示方式 :</h2><ul>
<li><code>rwx</code> 方式</li>
<li><p>数字方式</p>
<ul>
<li><code>r</code> 权限代表 <code>1</code></li>
<li><code>w</code> 权限代表 <code>2</code></li>
<li><code>x</code> 权限代表 <code>4</code></li>
<li><p>无权限代表 <code>0</code></p>
<p><code>rwx</code> 权限数字的值累加起来，就是一个 <code>用户/组/其余人</code> 的相应权限，例如 <code>775</code> 代表用户和组具有 <code>rwx</code> 权限，而其他人只有 <code>rx</code> 权限没有写权限。</p>
</li>
</ul>
</li>
</ul>
<h2 id="命令格式-chmod-选项-权限模式-文件"><a href="#命令格式-chmod-选项-权限模式-文件" class="headerlink" title="命令格式 : chmod [选项] 权限模式 文件"></a>命令格式 : chmod [选项] 权限模式 文件</h2><h3 id="选项"><a href="#选项" class="headerlink" title="选项"></a>选项</h3><ul>
<li><code>-c or --changes</code> : 效果类似”-v”参数，但仅汇报更改的部分</li>
<li><code>-f or --quiet</code> : 强制执行,不显示错误信息</li>
<li><code>-R or --recursive</code> : 递归处理，将指令目录下的所有文件及子目录一并处理</li>
<li><code>-v or --verbose</code> : 显示指令执行过程</li>
<li><code>--reference=&lt;参考文件或目录&gt;</code> : 把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同</li>
</ul>
<h3 id="权限模式"><a href="#权限模式" class="headerlink" title="权限模式"></a>权限模式</h3><ul>
<li><code>&lt;权限范围&gt;+&lt;权限值&gt;</code> : 开启权限范围的文件或目录的该选项权限设置</li>
<li><code>&lt;权限范围&gt;-&lt;权限值&gt;</code> : 关闭权限范围的文件或目录的该选项权限设置</li>
<li><code>&lt;权限范围&gt;=&lt;权限值&gt;</code> : 指定权限范围的文件或目录的该选项权限设置</li>
</ul>
<h4 id="权限范围"><a href="#权限范围" class="headerlink" title="权限范围"></a>权限范围</h4><ul>
<li><code>u</code> : <code>User</code> , 即文件或目录的拥有者</li>
<li><code>g</code> : <code>Group</code> , 即文件或目录的所属群组</li>
<li><code>o</code> : <code>Other</code> , 除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围</li>
<li><code>a</code> : <code>All</code> , 即全部的用户，包含拥有者，所属群组以及其他用户</li>
</ul>
<h4 id="权限值"><a href="#权限值" class="headerlink" title="权限值"></a>权限值</h4><ul>
<li><code>r</code> : 读取权限，数字代号为 <code>4</code></li>
<li><code>w</code> : 写入权限，数字代号为 <code>2</code></li>
<li><code>x</code> : 执行或切换权限，数字代号为 <code>1</code></li>
<li><code>-</code> : 不具任何权限，数字代号为 <code>0</code></li>
<li><code>s</code> : 特殊功能说明 : 变更文件或目录的权限</li>
<li>不指明权限范围时默认为 <code>All</code> 所有人</li>
</ul>
<h4 id="两种设置方式："><a href="#两种设置方式：" class="headerlink" title="两种设置方式："></a>两种设置方式：</h4><ol>
<li><code>权限范围+/-/=权限值</code> , 例如 <code>u+x</code> 就是用户增加执行权限；不同组别设置使用 <code>,</code> 分隔，例如 <code>u+wx,g+w,o-wx</code> ； 也可以 <code>ug+wx</code> 写。<code>o=r</code> 就是只有读权限 <code>(r–)</code> ; <code>+x</code> 就是三个组都增加执行权限。</li>
<li>三个数字模式，例如 <code>755</code> 代表用户具有 <code>rwx</code> ，组和其他人有 <code>rx</code> 。</li>
</ol>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">chmod u+x,g+w f01　　#为文件f01设置自己可以执行，组员可以写入的权限 </span><br><span class="line">chmod u=rwx,g=rw,o=r f01  #rwxrw-r--</span><br><span class="line">chmod 764 f01    #rwx-wx--x权限</span><br><span class="line">chmod a+x f01　　#对文件f01的u,g,o都增加可执行属性</span><br><span class="line">chmod -R +x DirName #对整个文件夹及里面内容都增加执行权限</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="chown"><a href="#chown" class="headerlink" title="chown"></a>chown</h1><p>chown指令是更改文件归属的，归属哪个用户，用户组是什么。对应将影响chmod里rwx效果。</p>
<p>用户可以是用户或者是用户D，用户组可以是组名或组id。文件名可以使由空格分开的文件列表，在文件名中可以包含通配符。-R选项后可以对整个文件夹操作。 只有文件主(改变自己的文件)和超级用户(改变他人的)才可以便用该命令。非root管理员慎用。</p>
<h2 id="命令格式-chown-选项-用户-组-文件"><a href="#命令格式-chown-选项-用户-组-文件" class="headerlink" title="命令格式: chown [选项] 用户:组 文件"></a>命令格式: chown [选项] 用户:组 文件</h2><ul>
<li><code>-c或--changes</code> : 效果类似 “-v” 参数，但仅汇报更改的部分</li>
<li><code>-f或--quite或--silent</code> : 强制执行,不显示错误信息</li>
<li><code>-h或--no-dereference</code> : 只对符号连接的文件作修改，而不更改其他任何相关文件</li>
<li><code>-R或--recursive</code> : 递归处理，将指定目录下的所有文件及子目录一并处理</li>
<li><code>-v或--version</code> : 显示指令执行过程</li>
<li><code>--dereference</code> : 效果和“-h”参数相同</li>
<li><code>--help</code> : 在线帮助</li>
<li><code>--reference=&lt;参考文件或目录&gt;</code> : 把指定文件或目录的拥有者与所属群组全部设成和参考文件或目录的拥有者与所属群组相同</li>
<li><code>--version</code> : 显示版本信息。</li>
</ul>
<p>记住 <code>用户:组</code> 的写法就可以了。</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title>codebook简史</title>
    <url>/2025/07/14/codebook%E7%AE%80%E5%8F%B2/</url>
    <content><![CDATA[<p><a class="link"   href="https://zhuanlan.zhihu.com/p/2433292582" >一文详解 codebook 技术史（从 VAE 到 VQ/RQ-VAE 到 FSQ）<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>VAE</tag>
        <tag>CodeBook</tag>
      </tags>
  </entry>
  <entry>
    <title>date命令基本使用</title>
    <url>/2022/12/23/date%E5%91%BD%E4%BB%A4%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>在维护一些定时脚本任务的时候，经常需要使用该命令。在此做一个记录：</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!date</span><br></pre></td></tr></table></figure>
<pre><code>Fri Dec 23 00:33:25 CST 2022
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!date +<span class="string">&quot;%Y/%m/%d %H:%M:%S&quot;</span></span><br></pre></td></tr></table></figure>
<pre><code>2022/12/23 00:33:57
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!date -d <span class="string">&quot;1 year ago&quot;</span> +<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span></span><br></pre></td></tr></table></figure>
<pre><code>2021-12-23 00:41:50
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!date -d <span class="string">&quot;20221216 12:17:17 2 minutes ago&quot;</span> +<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span></span><br></pre></td></tr></table></figure>
<pre><code>2022-12-16 12:15:17
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!date -d <span class="string">&quot;20221216 12:17:17 2 minutes&quot;</span> +<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span></span><br></pre></td></tr></table></figure>
<pre><code>2022-12-16 12:19:17
</code></pre>]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>date</tag>
      </tags>
  </entry>
  <entry>
    <title>curl发送post请求存在变量转义的问题</title>
    <url>/2023/03/08/curl%E5%8F%91%E9%80%81post%E8%AF%B7%E6%B1%82%E5%AD%98%E5%9C%A8%E5%8F%98%E9%87%8F%E8%BD%AC%E4%B9%89%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>最近在使用 <code>curl</code> 发送post请求的时候，需要带上自定义的变量，示例如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">name=<span class="string">&quot;Tom&quot;</span></span><br><span class="line">age=18</span><br><span class="line">msg=<span class="string">&quot;my name is <span class="variable">$&#123;name&#125;</span>, age is <span class="variable">$&#123;age&#125;</span>&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;msg&#125;</span></span><br><span class="line">curl -X POST <span class="string">&quot;http://xxx.com&quot;</span> -H <span class="string">&quot;Content-Type:application/json&quot;</span> -d <span class="string">&quot;&#123;\&quot;message\&quot;:&#123;\&quot;header\&quot;:&#123;\&quot;body\&quot;:[&#123;\&quot;type\&quot;:\&quot;TEXT\&quot;,\&quot;content\&quot;:\&quot;<span class="variable">$&#123;msg&#125;</span>\&quot;&#125;]&#125;&#125;&quot;</span></span><br></pre></td></tr></table></figure></p>
<span id="more"></span>
<p>有几点需要注意：</p>
<ul>
<li>json的key必须是双引号 <code>&quot;content&quot;</code> ，所以使用 <code>\</code> 进行转义</li>
<li>如果 <code>-d</code> 后面的使用单引号 <code>&#39;&#123;&quot;message&quot;: ...&#125;&#39;</code> ，那么必须是 <code>&quot;&#39;$&#123;msg&#125;&#39;&quot;</code> 进行转义，但是 <code>msg</code> 里不能有空格等特殊字符，所以建议采用第一点</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="http://www.huamo.online/2017/06/17/curl%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0%E5%BC%95%E7%94%A8%E7%9A%84%E6%96%B9%E5%BC%8F%E5%8F%91%E9%80%81POST%E8%AF%B7%E6%B1%82/" >curl使用参数引用的方式发送POST请求<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>curl</tag>
        <tag>POST</tag>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title>docker volume使用</title>
    <url>/2022/09/27/docker-volume%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p><code>volume</code> 是docker官方提供的一种高级的持久化数据的方法，它比 <code>mount</code> 有如下优点：</p>
<span id="more"></span>
<ul>
<li>更容易备份和迁移</li>
<li>可以使用docker命令行或者api来管理volume</li>
<li>兼容linux和windows容器</li>
<li>可以在多个容器间共享</li>
<li>可加密存储在远程机器或云端</li>
<li>新volume可以被容器预填充</li>
<li>volume性能比mount更高</li>
</ul>
<p><img   src="https://docs.docker.com/storage/images/types-of-mounts-volume.png"  alt="volume"></p>
<p>下面是volume的一些基础命令：</p>
<h2 id="创建volume"><a href="#创建volume" class="headerlink" title="创建volume"></a>创建volume</h2><p><code>docker volume create testvol</code></p>
<h2 id="查看volume信息"><a href="#查看volume信息" class="headerlink" title="查看volume信息"></a>查看volume信息</h2><p><code>docker volume inspect testvol</code></p>
<h2 id="删除volume"><a href="#删除volume" class="headerlink" title="删除volume"></a>删除volume</h2><p><code>docker volume rm myvol</code></p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Volume</tag>
      </tags>
  </entry>
  <entry>
    <title>expand和repeat区别</title>
    <url>/2020/12/08/expand%E5%92%8Crepeat%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p><code>expand</code> 和 <code>repeat</code> 都是对张量进行扩维，这里记录下使用区别。</p>
<span id="more"></span>
<h1 id="expand"><a href="#expand" class="headerlink" title="expand()"></a>expand()</h1><blockquote>
<p>Returns a new view of the :attr:<code>self</code> tensor with singleton dimensions expanded to a larger size.</p>
</blockquote>
<p>将张量<code>=1</code>的维度进行扩展，<code>&gt;1</code>的维度保持不变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">12</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">a.expand(<span class="number">3</span>,-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[12,  3,  4],
        [12,  3,  4],
        [12,  3,  4]])
</code></pre><h1 id="repeat"><a href="#repeat" class="headerlink" title="repeat()"></a>repeat()</h1><blockquote>
<p>Repeats this tensor along the specified dimensions.</p>
</blockquote>
<p>将张量沿着特定维度进行复制。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">b.repeat(<span class="number">3</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1, 2, 4, 1, 2, 4],
        [4, 5, 6, 4, 5, 6],
        [1, 2, 4, 1, 2, 4],
        [4, 5, 6, 4, 5, 6],
        [1, 2, 4, 1, 2, 4],
        [4, 5, 6, 4, 5, 6]])
</code></pre>]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>tensor</tag>
      </tags>
  </entry>
  <entry>
    <title>git tag使用</title>
    <url>/2022/10/27/git-tag%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>git tag主要用来标记某个提交，用法非常简单。</p>
<span id="more"></span>
<h2 id="创建tag"><a href="#创建tag" class="headerlink" title="创建tag"></a>创建tag</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git tag &lt;tagname&gt; -m &lt;message&gt; &lt;commit-id&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>tagname：一般为版本号，比如v1.0</li>
<li>message（可选）：具体描述，类似于 <code>git commit -m &lt;message&gt;</code></li>
<li>commit-id（可选）：默认为当前commit</li>
</ul>
<h2 id="列出tag"><a href="#列出tag" class="headerlink" title="列出tag"></a>列出tag</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git tag</span><br></pre></td></tr></table></figure>
<h2 id="显示某tag详细信息"><a href="#显示某tag详细信息" class="headerlink" title="显示某tag详细信息"></a>显示某tag详细信息</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git show &lt;tagname&gt;</span><br></pre></td></tr></table></figure>
<h2 id="推送tag到远程"><a href="#推送tag到远程" class="headerlink" title="推送tag到远程"></a>推送tag到远程</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git push origin &lt;tagname&gt;</span><br><span class="line"><span class="comment"># git push origin --tags # 推送本地所有的tag</span></span><br></pre></td></tr></table></figure>
<img   src="/2022/10/27/git-tag%E4%BD%BF%E7%94%A8/1.jpg"  class="tag">
<h2 id="删除tag"><a href="#删除tag" class="headerlink" title="删除tag"></a>删除tag</h2><ol>
<li>删除本地tag：<code>git tag -d &lt;tagname&gt;</code></li>
<li>更新远程仓库的tag：<code>git push origin :refs/tags/&lt;tagname&gt;</code></li>
</ol>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.jianshu.com/p/a07777d0b018" >Git的Tag使用<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>VCS</category>
      </categories>
      <tags>
        <tag>tag</tag>
      </tags>
  </entry>
  <entry>
    <title>git合并多个commit</title>
    <url>/2022/10/14/git%E5%90%88%E5%B9%B6%E5%A4%9A%E4%B8%AAcommit/</url>
    <content><![CDATA[<p>有时候为了开发一个新的功能，进行了多次commit，从而导致整个git提交历史显得很冗余。在此记录一下如何合并多个commit：</p>
<span id="more"></span>
<p>我们想把提交历史 <code>A-&gt;B-&gt;C-&gt;D</code> 合并成 <code>A-&gt;B-&gt;C&amp;D</code>：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.491ffzcfhf.webp"  alt="his"></p>
<ol>
<li>回到基线B：<code>git rebase -i 36a1ccf</code>，然后会看到下图：</li>
</ol>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.5tr6fhzx45.webp"  alt="rebase"></p>
<ol>
<li><code>pick</code> 表示选择这条commit，<code>squash</code> 表示将该commit合并到上一个commit，这里我们选择<code>pick C, squash D</code></li>
<li>编辑完上述信息后，退出保存会弹出如下界面。这是因为两个commit合并后会生成一个新的commit，所以要填写message：</li>
</ol>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.45u3xei2e.webp"  alt="update"></p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.6f0u1t4eb0.webp"  alt="msg"></p>
<ol>
<li>编辑完上述信息后，退出保存。这时候再查看log信息，就发现生效了：<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.102bjdq85c.webp"  alt="msg"></li>
</ol>
<p>备注：许多人在用ubuntu系统，默认编辑器是nano，这里可以切换成vim，方便操作：<code>echo export EDITOR=/usr/bin/vim &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc</code></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://kunzhao.org/docs/tutorial/git/merge-multiple-commit/" >Git 多次提交合并成一次提交<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/zhezhebie/article/details/82382984" >修改ubuntu默认编辑器为vim<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>VCS</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>git撤销操作</title>
    <url>/2021/05/10/git%E6%92%A4%E9%94%80%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p><code>git</code>有如下三种状态：</p>
<span id="more"></span>
<img   src="/2021/05/10/git%E6%92%A4%E9%94%80%E6%93%8D%E4%BD%9C/git.jpg"  class="">
<blockquote>
<ul>
<li>Modified: You have changed the file but have not committed it to your database yet.</li>
<li>Staged: You have marked a modified file in its current version to go into your next commit snapshot.</li>
<li>Committed: The data is safely stored in your local database.</li>
</ul>
</blockquote>
<p>现在主要讲述下<code>git</code>的撤销操作：</p>
<h2 id="discard-changes-in-working-directory"><a href="#discard-changes-in-working-directory" class="headerlink" title="discard changes in working directory"></a>discard changes in working directory</h2><p>前提：<code>&lt;file&gt;</code>已经被添加到暂存区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git restore &lt;file&gt;</span><br></pre></td></tr></table></figure>
<h2 id="working-direction-lt-index"><a href="#working-direction-lt-index" class="headerlink" title="working direction &lt;- index"></a>working direction &lt;- index</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git reset HEAD &lt;file&gt;</span><br></pre></td></tr></table></figure>
<h2 id="index-lt-HEAD"><a href="#index-lt-HEAD" class="headerlink" title="index &lt;- HEAD"></a>index &lt;- HEAD</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git reset --soft HEAD^</span><br></pre></td></tr></table></figure>
<h2 id="working-direction-lt-HEAD"><a href="#working-direction-lt-HEAD" class="headerlink" title="working direction &lt;- HEAD"></a>working direction &lt;- HEAD</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git reset --hard HEAD^</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>VCS</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>graphsage解读</title>
    <url>/2023/02/20/graphsage%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>传统的图方法都是直推式(transductive)的，学习到的是结构固定的图模型，一旦有新的节点加入，便需要重新训练整个图网络，泛化性不强。GraphSAGE是归纳式(inductive)的，它学习一种映射：通过采样和聚合邻居节点信息来生成当前节点的表征。GraphSAGE可扩展性更强，对于节点分类和链接预测问题的表现也十分突出。</p>
<span id="more"></span>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><img   src="/2023/02/20/graphsage%E8%A7%A3%E8%AF%BB/1.png"  class="model">
<ol>
<li>采样一跳和二跳的邻居节点</li>
<li>聚合邻居节点的特征信息</li>
<li>预测图上下文和当前节点标签信息</li>
</ol>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><img   src="/2023/02/20/graphsage%E8%A7%A3%E8%AF%BB/2.png"  class="算法">
<p>整体上还是非常通俗易懂的。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ul>
<li>有监督：跟常规的分类任务一样，使用交叉熵损失函数</li>
<li>无监督：根据节点间共现关系来定义损失函数：</li>
</ul>
<script type="math/tex; mode=display">
J_{\mathcal{G}}\left(\mathbf{z}_u\right)=-\log \left(\sigma\left(\mathbf{z}_u^{\top} \mathbf{z}_v\right)\right)-Q \cdot \mathbb{E}_{v_n \sim P_n(v)} \log \left(\sigma\left(-\mathbf{z}_u^{\top} \mathbf{z}_{v_n}\right)\right)</script><ul>
<li>$u$ 表示当前节点，而 $v$ 是跟它在一条随机路径上共现的节点，两者相似，内积很大，则 $-\log \left(\sigma\left(\mathbf{z}_u^{\top} \mathbf{z}_v\right)\right)$ 接近 0</li>
<li>$P_n(v)$ 表示负采样分布，$Q$ 为负样本个数，$u$ 与 $v_n$ 负内积很大，则 $-Q \cdot \mathbb{E}_{v_n \sim P_n(v)} \log \left(\sigma\left(-\mathbf{z}_u^{\top} \mathbf{z}_{v_n}\right)\right)$ 接近 0</li>
</ul>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><img   src="/2023/02/20/graphsage%E8%A7%A3%E8%AF%BB/3.png"  class="result">
<p>可以看到，在三个数据集上大幅领先baseline。</p>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ol>
<li>与GAT的区别？</li>
</ol>
<p>GraphSAGE为每一个节点都抽取一个固定尺寸的节点，在计算的时候不是所有的节点都能参与其中。GAT不需要显式的采样，所有邻居节点都参与到当前节点的注意力计算中。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/367741877" >图神经网络10-GraphSAGE论文全面解读<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/288322305" >Graph neural network综述:从deepwalk到GraphSAGE，GCN，GAT<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Graph</tag>
        <tag>Transductive Learning</tag>
        <tag>Inductive Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo中引入Echarts</title>
    <url>/2022/05/27/hexo%E4%B8%AD%E5%BC%95%E5%85%A5echarts/</url>
    <content><![CDATA[<p>最近发现一个插件<code>hexo-tag-echarts</code>，可以在Hexo中引入Echarts。</p>
<span id="more"></span>
<h2 id="插件安装"><a href="#插件安装" class="headerlink" title="插件安装"></a>插件安装</h2><ol>
<li>npm安装</li>
</ol>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">npm install hexo-tag-echarts --save</span><br></pre></td></tr></table></figure>
<ol>
<li>添加cdn外链</li>
</ol>
<p>在<code>node_modules/hexo-tag-echarts/echarts-template.html</code>文件中添加如下一行：<br><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://github.com/TransformersWsz/TransformersWsz.github.io/releases/download/echarts/echarts.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><div id="echarts4588" style="width: 90%;height: 400px;margin: 0 auto"></div>
<script src="https://github.com/TransformersWsz/TransformersWsz.github.io/releases/download/echarts/echarts.min.js"></script>
<script type="text/javascript">
        // 基于准备好的dom，初始化echarts实例
        var myChart = echarts.init(document.getElementById('echarts4588'));

        // 指定图表的配置项和数据
        var option = {
    tooltip: {
        trigger: 'item',
        formatter: '{a} <br/>{b}: {c} ({d}%)'
    },
    legend: {
        orient: 'vertial',
        left: 10,
        data: ['直接访问', '邮件营销', '联盟广告', '视频广告', '搜索引擎']
    },
    series: [
        {
            name: '访问来源',
            type: 'pie',
            radius: ['50%', '70%'],
            avoidLabelOverlap: false,
            label: {
                show: false,
                position: 'center'
            },
            emphasis: {
                label: {
                    show: true,
                    fontSize: '30',
                    fontWeight: 'bold'
                }
            },
            labelLine: {
                show: false
            },
            data: [
                {value: 335, name: '直接访问'},
                {value: 310, name: '邮件营销'},
                {value: 234, name: '联盟广告'},
                {value: 135, name: '视频广告'},
                {value: 2000, name: '搜索引擎'}
            ]
        }
    ]
};

        // 使用刚指定的配置项和数据显示图表。
        myChart.setOption(option);
</script>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p><code>index.js</code>文件内容如下：<br><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> fs = <span class="built_in">require</span>(<span class="string">&#x27;fs&#x27;</span>),</span><br><span class="line">  path = <span class="built_in">require</span>(<span class="string">&#x27;path&#x27;</span>),</span><br><span class="line">  _ = <span class="built_in">require</span>(<span class="string">&#x27;underscore&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> filePath = path.<span class="title function_">join</span>(__dirname, <span class="string">&#x27;echarts-template.html&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">echartsMaps</span>(<span class="params">args, content</span>) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> template = fs.<span class="title function_">readFileSync</span>(filePath).<span class="title function_">toString</span>(),</span><br><span class="line">    options = &#123;&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (content.<span class="property">length</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> options = content;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Output into </span></span><br><span class="line">  <span class="keyword">return</span> _.<span class="title function_">template</span>(template)(&#123;</span><br><span class="line">    <span class="attr">id</span>: <span class="string">&#x27;echarts&#x27;</span> + ((<span class="title class_">Math</span>.<span class="title function_">random</span>() * <span class="number">9999</span>) | <span class="number">0</span>),</span><br><span class="line">    <span class="attr">option</span>: options,</span><br><span class="line">    <span class="attr">height</span>: args[<span class="number">0</span>] || <span class="number">400</span>,</span><br><span class="line">    <span class="attr">width</span>: args[<span class="number">1</span>] || <span class="string">&#x27;81%&#x27;</span></span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hexo.<span class="property">extend</span>.<span class="property">tag</span>.<span class="title function_">register</span>(<span class="string">&#x27;echarts&#x27;</span>, echartsMaps, &#123;</span><br><span class="line">  <span class="attr">async</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="attr">ends</span>: <span class="literal">true</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>args</code> 接收参数 <code>&#123;% echarts 400 '90%' %&#125;</code></li>
<li><code>content</code> 接收具体的数据信息</li>
</ul>
<p>详细的数据流是这样的：</p>
<p><code>index.js</code>拿到markdown里的参数和数据，进行一些处理，然后将对应的字段渲染到<code>echarts-template.html</code>中。</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Echarts</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo支持latex</title>
    <url>/2022/03/08/hexo%E6%94%AF%E6%8C%81latex/</url>
    <content><![CDATA[<p>最近在新电脑上重新搭了个博客，为了使hexo开启latex支持又踩了一次坑，在此记录一下。</p>
<span id="more"></span>
<h2 id="卸载旧版渲染引擎"><a href="#卸载旧版渲染引擎" class="headerlink" title="卸载旧版渲染引擎"></a>卸载旧版渲染引擎</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm uninstall hexo-math --save</span><br></pre></td></tr></table></figure>
<h2 id="安装新引擎"><a href="#安装新引擎" class="headerlink" title="安装新引擎"></a>安装新引擎</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-kramed --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure>
<h2 id="更改库文件"><a href="#更改库文件" class="headerlink" title="更改库文件"></a>更改库文件</h2><h3 id="node-modules-hexo-renderer-kramed-lib-renderer-js"><a href="#node-modules-hexo-renderer-kramed-lib-renderer-js" class="headerlink" title="node_modules/hexo-renderer-kramed/lib/renderer.js"></a><code>node_modules/hexo-renderer-kramed/lib/renderer.js</code></h3><p>将<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">formatText</span>(<span class="params">text</span>) &#123;</span><br><span class="line">    <span class="comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span></span><br><span class="line">    <span class="keyword">return</span> text.<span class="title function_">replace</span>(<span class="regexp">/`\$(.*?)\$`/g</span>, <span class="string">&#x27;$$$$$1$$$$&#x27;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>修改为：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">formatText</span>(<span class="params">text</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="node-modules-kramed-lib-rules-inline-js"><a href="#node-modules-kramed-lib-rules-inline-js" class="headerlink" title="node_modules/kramed/lib/rules/inline.js"></a><code>node_modules/kramed/lib/rules/inline.js</code></h3><p>latex与markdown语法上有语义冲突，hexo默认的转义规则会将一些字符进行转义，所以我们需要对默认的规则进行修改。更改后为：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> inline = &#123;</span><br><span class="line">  <span class="comment">// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line">  <span class="attr">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br><span class="line">  <span class="attr">autolink</span>: <span class="regexp">/^&lt;([^ &gt;]+(@|:\/)[^ &gt;]+)&gt;/</span>,</span><br><span class="line">  <span class="attr">url</span>: noop,</span><br><span class="line">  <span class="attr">html</span>: <span class="regexp">/^&lt;!--[\s\S]*?--&gt;|^&lt;(\w+(?!:\/|[^\w\s@]*@)\b)*?(?:&quot;[^&quot;]*&quot;|&#x27;[^&#x27;]*&#x27;|[^&#x27;&quot;&gt;])*?&gt;([\s\S]*?)?&lt;\/\1&gt;|^&lt;(\w+(?!:\/|[^\w\s@]*@)\b)(?:&quot;[^&quot;]*&quot;|&#x27;[^&#x27;]*&#x27;|[^&#x27;&quot;&gt;])*?&gt;/</span>,</span><br><span class="line">  <span class="attr">link</span>: <span class="regexp">/^!?\[(inside)\]\(href\)/</span>,</span><br><span class="line">  <span class="attr">reflink</span>: <span class="regexp">/^!?\[(inside)\]\s*\[([^\]]*)\]/</span>,</span><br><span class="line">  <span class="attr">nolink</span>: <span class="regexp">/^!?\[((?:\[[^\]]*\]|[^\[\]])*)\]/</span>,</span><br><span class="line">  <span class="attr">reffn</span>: <span class="regexp">/^!?\[\^(inside)\]/</span>,</span><br><span class="line">  <span class="attr">strong</span>: <span class="regexp">/^__([\s\S]+?)__(?!_)|^\*\*([\s\S]+?)\*\*(?!\*)/</span>,</span><br><span class="line">  <span class="comment">// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line">  <span class="attr">em</span>: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br><span class="line">  <span class="attr">code</span>: <span class="regexp">/^(`+)\s*([\s\S]*?[^`])\s*\1(?!`)/</span>,</span><br><span class="line">  <span class="attr">br</span>: <span class="regexp">/^ &#123;2,&#125;\n(?!\s*$)/</span>,</span><br><span class="line">  <span class="attr">del</span>: noop,</span><br><span class="line">  <span class="attr">text</span>: <span class="regexp">/^[\s\S]+?(?=[\\&lt;!\[_*`$]| &#123;2,&#125;\n|$)/</span>,</span><br><span class="line">  <span class="attr">math</span>: <span class="regexp">/^\$\$\s*([\s\S]*?[^\$])\s*\$\$(?!\$)/</span>,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h3 id="node-modules-hexo-renderer-mathjax-mathjax-html"><a href="#node-modules-hexo-renderer-mathjax-mathjax-html" class="headerlink" title="node_modules/hexo-renderer-mathjax/mathjax.html"></a><code>node_modules/hexo-renderer-mathjax/mathjax.html</code></h3><p>将最后一行改为：<br><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="开启mathjax"><a href="#开启mathjax" class="headerlink" title="开启mathjax"></a>开启mathjax</h2><p>打开<font color="red"><strong>主题</strong></font>目录下的 <code>_config.yml</code> 文件，加入如下：<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>在写博客的时候需要开启latex就加上字段说明：<br><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">title: test</span><br><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><br>但每次都这么加显得很麻烦，可以在 <code>scaffolds/post.md</code> 添加如下模版：<br><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">updated: &#123;&#123; date &#125;&#125;</span><br><span class="line">categories: </span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line"><span class="section">toc: true</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><br>这样就无需每次手写了。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/weixin_44191286/article/details/102702479" >Hexo博客中使用Latex<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title>ipynb导出为markdown</title>
    <url>/2022/10/07/ipynb%E5%AF%BC%E5%87%BA%E4%B8%BAmarkdown/</url>
    <content><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jupyter nbconvert --to markdown pd.ipynb</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>ipynb</tag>
      </tags>
  </entry>
  <entry>
    <title>iOS内存管理</title>
    <url>/2017/08/21/iOS%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<p>现在iOS开发已经是ARC的时代，但是内存管理仍是一个重点关注的问题。它是程序设计中很重要的一部分。程序在运行的过程中消耗内存，运行结束后释放占用的内存。如果程序运行时一直分配内存而不及时释放无用的内存，程序占用的内存会越来越大，直至内存消耗殆尽，程序因无内存可用导致崩溃，这就是所谓的内存泄漏。本文将介绍ObjC的内存管理方式。</p>
<span id="more"></span>
<h2 id="引用计数"><a href="#引用计数" class="headerlink" title="引用计数"></a>引用计数</h2><p>ObjC采用引用计数来进行内存管理:</p>
<ol>
<li>每个对象都有一个关联的整数，称为引用计数器</li>
<li>当代码需要使用该对象时，则将对象的引用计数加1</li>
<li>当代码结束使用该对象时，则将对象的引用计数减1</li>
<li>当引用计数的值变为0时，表示对象没有被任何代码使用，此时对象占用的内存将被释放</li>
</ol>
<p>与之对应的消息发送方法如下:</p>
<ol>
<li>当对象被创建(<code>alloc 、new 、copy</code> 等方法)时，其引用计数初始值为1</li>
<li>给对象发送 <code>retain</code> 消息，其引用计数加1</li>
<li>给对象发送 <code>release</code> 消息，其引用计数减1</li>
<li>当对象引用计数归0时，ObjC给对象发送 <code>dealloc</code> 消息销毁对象</li>
</ol>
<p>下面通过一个例子来说明(关闭Xcode的 Automatic Reference Counting 功能):</p>
<p>场景: 有一个宠物中心（内存），可以派出小动物（对象）陪小朋友们玩耍（对象引用者），现在xiaoming想和小狗一起玩耍。</p>
<p>新建Dog类，重写其创建和销毁的方法</p>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="meta">#import <span class="string">&quot;Dog.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">@implementation</span> <span class="title">Dog</span></span></span><br><span class="line"></span><br><span class="line">- (<span class="keyword">instancetype</span>)init</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">self</span> = [<span class="keyword">super</span> init])</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@&quot;小狗已被派出，%lu&quot;</span>,<span class="keyword">self</span>.retainCount);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">self</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">- (<span class="keyword">void</span>)dealloc</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@&quot;小狗回到宠物中心&quot;</span>);</span><br><span class="line">    [<span class="keyword">super</span> dealloc];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">@end</span></span><br></pre></td></tr></table></figure>
<p>在main方法中创建dog对象，给dog发送消息</p>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="meta">#import <span class="string">&lt;Foundation/Foundation.h&gt;</span></span></span><br><span class="line"><span class="meta">#import <span class="string">&quot;Dog.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> main(<span class="keyword">int</span> argc, <span class="keyword">const</span> <span class="keyword">char</span> * argv[]) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        <span class="comment">//模拟：宠物中心派出小狗</span></span><br><span class="line">        Dog * dog = [[Dog alloc]init];</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//模拟：xiaoming需要和小狗玩耍，需要将其引用计数加1</span></span><br><span class="line">        [dog <span class="keyword">retain</span>];</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//模拟：xiaoming不和小狗玩耍了，需要将其引用计数减1</span></span><br><span class="line">        [dog release];</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//没人需要和小狗玩耍了，将其引用计数减1</span></span><br><span class="line">        [dog release];</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将指针置nil，否则变为野指针</span></span><br><span class="line">        dog = <span class="literal">nil</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出结果为：</p>
<p><img   src="https://img-blog.csdnimg.cn/img_convert/b25cf4b2f5204a84591887f6dd0aef0f.png"  alt="结果"></p>
<p>可以看到，引用计数帮助宠物中心很好的标记了小狗的使用状态，在完成任务的时候及时收回到宠物中心。</p>
<h3 id="注意事项："><a href="#注意事项：" class="headerlink" title="注意事项："></a>注意事项：</h3><ol>
<li>NSString 引用计数问题</li>
</ol>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSString</span>* str = <span class="string">@&quot;hello world!&quot;</span>;</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;%ld&quot;</span>, str.retainCount);</span><br></pre></td></tr></table></figure>
<p>结果输出为-1，这可以理解为str实际上是一个字符串常量，它存储在常量存储区，是没有引用计数的。</p>
<ol>
<li>赋值不会拥有某个对象</li>
</ol>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">Dog* dog1 = dog;</span><br></pre></td></tr></table></figure>
<font color=red>这里仅仅是指针赋值操作，并不会增加dog的引用计数，需要持有对象必须要发送retain消息</font>

<ol>
<li><p>dealloc<br>由于释放对象是会调用dealloc方法，因此重写dealloc方法来查看对象释放的情况，如果没有调用则会造成内存泄露。在上面的例子中我们通过重写dealloc让小狗被释放的时候打印日志来告诉我们已经完成释放。</p>
</li>
<li><p>在上面的例子中，如果再增加这样一个操作:</p>
</li>
</ol>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="comment">//没人需要和小狗玩耍了，将其引用计数减1</span></span><br><span class="line">[dog release];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;%ld&quot;</span>,dog.retainCount);</span><br></pre></td></tr></table></figure>
<p>会发现引用计数为1，为什么不是0呢？这是因为对引用计数为1的对象release时，系统知道该对象将被回收，就不会再对该对象的引用计数进行减1操作，这样可以增加对象回收的效率。</p>
<p>另外，对已释放的对象发送消息是不可取的，因为对象的内存已被回收，如果发送消息时，该内存已经被其他对象使用了，得到的结果是无法确定的，甚至会造成崩溃。</p>
<h2 id="自动释放池"><a href="#自动释放池" class="headerlink" title="自动释放池"></a>自动释放池</h2><p>当不再使用一个对象时应该将其释放，但是在某些情况下，我们很难理清一个对象什么时候不再使用（比如xiaoming和小狗玩耍结束的时间不确定），这应该如何处理？</p>
<p>ObjC提供autorelease方法来解决这个问题，当给一个对象发送autorelease消息时，方法会在未来某个时间给这个对象发送release消息将其释放，在这个时间段内，对象还是可以使用的。</p>
<h3 id="autorelease-的工作原理"><a href="#autorelease-的工作原理" class="headerlink" title="autorelease 的工作原理"></a>autorelease 的工作原理</h3><p>当对象接收到autorelease消息时，它会被添加到了当前的自动释放池中，当自动释放池被销毁时，会给池里所有的对象发送release消息。这里就引出了自动释放池这个概念，顾名思义，就是一个池，这个池可以容纳对象，而且可以自动释放，这就大大增加了我们处理对象的灵活性。</p>
<p>ObjC提供两种方法创建自动释放池：</p>
<ol>
<li>使用NSAutoreleasePool来创建</li>
</ol>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSAutoreleasePool</span> * pool = [[<span class="built_in">NSAutoreleasePool</span> alloc]init];</span><br><span class="line"><span class="comment">//这里写代码</span></span><br><span class="line">[pool release];</span><br></pre></td></tr></table></figure>
<ol>
<li>使用@autoreleasepool创建</li>
</ol>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line"><span class="comment">//这里写代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>自动释放池创建后，就会成为活动的池子，释放池子后，池子将释放其所包含的所有对象。以上两种方法推荐第二种，因为第二种方法效率更高。</p>
<h4 id="自动释放池什么时候创建"><a href="#自动释放池什么时候创建" class="headerlink" title="自动释放池什么时候创建?"></a>自动释放池什么时候创建?</h4><p>自动释放池的销毁时间是确定的，一般是在程序事件处理之后释放，或者由我们自己手动释放。<br>下面举例说明自动释放池的工作流程：<br>场景：现在xiaoming和xiaohong都想和小狗一起玩耍，但是他们的需求不一样，他们的玩耍时间不一样。</p>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建一个自动释放池</span></span><br><span class="line"><span class="built_in">NSAutoreleasePool</span> * pool = [[<span class="built_in">NSAutoreleasePool</span> alloc] init];</span><br><span class="line"><span class="comment">//模拟：宠物中心派出小狗</span></span><br><span class="line">Dog * dog = [[Dog alloc]init];</span><br><span class="line"></span><br><span class="line"><span class="comment">//模拟：xiaoming需要和小狗玩耍，需要将其引用计数加1</span></span><br><span class="line">[dog <span class="keyword">retain</span>];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line"></span><br><span class="line"><span class="comment">//模拟：xiaohong需要和小狗玩耍，需要将其引用计数加1</span></span><br><span class="line">[dog <span class="keyword">retain</span>];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line"></span><br><span class="line"><span class="comment">//模拟：xiaoming确定不想和小狗玩耍了，需要将其引用计数减1</span></span><br><span class="line">[dog release];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line"></span><br><span class="line"><span class="comment">//模拟：xiaohong不确定何时不想和小狗玩耍了，将其设置为自动释放</span></span><br><span class="line">[dog autorelease];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;小狗的引用计数为 %ld&quot;</span>,dog.retainCount);</span><br><span class="line"></span><br><span class="line"><span class="comment">//没人需要和小狗玩耍了，将其引用计数减1</span></span><br><span class="line">[dog release];</span><br><span class="line"></span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;释放池子&quot;</span>);</span><br><span class="line">[pool release];</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img   src="https://img-blog.csdnimg.cn/img_convert/08148f61a2a6260ab599542d6a87b8fe.png"  alt="结果"></p>
<p>可以看到，当池子释放后，dog对象才被释放，因此在池子释放之前，xiaohong都可以尽情地和小狗玩耍。</p>
<h4 id="使用自动释放池需要注意"><a href="#使用自动释放池需要注意" class="headerlink" title="使用自动释放池需要注意"></a>使用自动释放池需要注意</h4><ol>
<li>自动释放池实质上只是在释放的时候給池中所有对象对象发送release消息，不保证对象一定会销毁，如果自动释放池向对象发送release消息后对象的引用计数仍大于1，对象就无法销毁。</li>
<li>自动释放池中的对象会集中同一时间释放，如果操作需要生成的对象较多占用内存空间大，可以使用多个释放池来进行优化。比如在一个循环中需要创建大量的临时变量，可以创建内部的池子来降低内存占用峰值。</li>
<li><font color=red>autorelease不会改变对象的引用计数</font>

</li>
</ol>
<h4 id="自动释放池的常见问题"><a href="#自动释放池的常见问题" class="headerlink" title="自动释放池的常见问题"></a>自动释放池的常见问题</h4><p>在管理对象释放的问题上，自动帮助我们释放池节省了大量的时间，但是有时候它却未必会达到我们期望的效果，比如在一个循环事件中，如果循环次数较大或者事件处理占用内存较大，就会导致内存占用不断增长，可能会导致不希望看到的后果。</p>
<p>示例代码:<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100000</span>; i ++) &#123;</span><br><span class="line">    <span class="built_in">NSString</span> * log  = [<span class="built_in">NSString</span> stringWithFormat:<span class="string">@&quot;%d&quot;</span>, i];</span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@&quot;%@&quot;</span>, log);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>前面讲过，自动释放池的释放时间是确定的，这个例子中自动释放池会在循环事件结束时释放，那问题来了：在这个十万次的循环中，每次都会生成一个字符串并打印，这些字符串对象都放在池子中并直到循环结束才会释放，因此在循环期间内存不增长。</p>
<p>这类问题的解决方案是在循环中创建新的自动释放池，多少个循环释放一次由我们自行决定。</p>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100000</span>; i ++) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        <span class="built_in">NSString</span> * log  = [<span class="built_in">NSString</span> stringWithFormat:<span class="string">@&quot;%d&quot;</span>, i];</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@&quot;%@&quot;</span>, log);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="iOS的内存管理规则"><a href="#iOS的内存管理规则" class="headerlink" title="iOS的内存管理规则"></a>iOS的内存管理规则</h2><h3 id="基本规则"><a href="#基本规则" class="headerlink" title="基本规则"></a>基本规则</h3><ol>
<li>当你通过new、alloc或copy方法创建一个对象时，它的引用计数为1，当不再使用该对象时，应该向对象发送release或者autorelease消息释放对象</li>
<li>当你通过其他方法获得一个对象时，如果对象引用计数为1且被设置为autorelease，则不需要执行任何释放对象的操作</li>
<li>如果你打算取得对象所有权，就需要保留对象并在操作完成之后释放，且必须保证retain和release的次数对等</li>
</ol>
<p>应用到文章开头的例子中，小朋友每申请一个小狗（生成对象），最后都要归还到宠物中心（释放对象），如果只申请而不归还（对象创建了没有释放），那宠物中心的小狗就会越来越少（可用内存越来越少），到最后一个小狗都没有了（内存被耗尽），其他小朋友就再也没有小狗可申请了（无资源可申请使用），因此，必须要遵守规则：申请必须归还（规则1），申请几个必须归还几个（规则3），如果小狗被设定归还时间则不用小朋友主动归还（规则2）</p>
<h3 id="ARC"><a href="#ARC" class="headerlink" title="ARC"></a>ARC</h3><p>在MRC时代，必须严格遵守以上规则，否则内存问题将成为恶魔一样的存在，然而来到ARC时代，事情似乎变得轻松了，不用再写无止尽的ratain和release似乎让开发变得轻松了，对初学者变得更友好。</p>
<p>ObjC2.0引入了垃圾回收机制，然而由于垃圾回收机制会对移动设备产生某些不好的影响（例如由于垃圾清理造成的卡顿），iOS并不支持这个机制，苹果的解决方案就是ARC（自动引用计数）。</p>
<p>iOS5以后，我们可以开启ARC模式，ARC可以理解成一位管家，这个管家会帮我们向对象发送retain和release语句，不再需要我们手动添加了，我们可以更舒心地创建或引用对象，简化内存管理步骤，节省大量的开发时间。</p>
<p>实际上，ARC不是垃圾回收，也并不是不需要内存管理了，它是隐式的内存管理，编译器在编译的时候会在代码插入合适的ratain和release语句，相当于在背后帮我们完成了内存管理的工作。</p>
<p>下面将自动释放池的例子转化为ARC来看看：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line"></span><br><span class="line">    Dog * dog = [[Dog alloc]init];</span><br><span class="line"></span><br><span class="line">    [xiaoming playWithDog:dog];</span><br><span class="line"></span><br><span class="line">    [xiaohong playWithDog:dog];</span><br><span class="line"></span><br><span class="line">    <span class="built_in">NSLog</span>(<span class="string">@&quot;释放池子&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><ul>
<li>如果你的工程历史比较久，可以将其从MRC转换成ARC，跟上时代的步伐更好地维护</li>
<li>如果你的工程引用了某些不支持ARC的库，可以在Build Phases的Compile Sources将对应的m文件的编译器参数配置为-fno-objc-arc</li>
<li><font color=red>ARC能帮我们简化内存管理问题，但不代表它是万能的，还是有它不能处理的情况，这就需要我们自己手动处理，比如循环引用、非ObjC对象、Core Foundation中的malloc()或者free()等等</font>

</li>
</ul>
<h3 id="ARC的修饰符"><a href="#ARC的修饰符" class="headerlink" title="ARC的修饰符"></a>ARC的修饰符</h3><p>ARC提供四种修饰符，分别是：</p>
<ul>
<li><code>strong</code>, </li>
<li><code>weak</code>, </li>
<li><code>autoreleasing</code>, </li>
<li><code>unsafe_unretained</code></li>
</ul>
<p>下面举例说明：</p>
<ul>
<li><code>_strong</code> : 强引用，持有所指向对象的所有权，无修饰符情况下的默认值。如需强制释放，可置nil。</li>
</ul>
<p>比如我们常用的定时器：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSTimer</span> * timer = [<span class="built_in">NSTimer</span> timerWith...];</span><br></pre></td></tr></table></figure><br>相当于<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSTimer</span> * __<span class="keyword">strong</span> timer = [<span class="built_in">NSTimer</span> timerWith...];</span><br></pre></td></tr></table></figure><br>当不需要使用时，强制销毁定时器<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">[timer invalidate];</span><br><span class="line">timer = <span class="literal">nil</span>;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>_weak</code> : 弱引用，不持有所指向对象的所有权，引用指向的对象内存被回收之后，引用本身会置nil，避免野指针。</li>
</ul>
<p>比如避免循环引用的弱引用声明：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">__<span class="keyword">weak</span> __<span class="keyword">typeof</span>(<span class="keyword">self</span>) weakSelf = <span class="keyword">self</span>;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>__autoreleasing</code> : 自动释放对象的引用，一般用于传递参数</li>
</ul>
<p>比如一个读取数据的方法:<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)loadData:(<span class="built_in">NSError</span> **)error;</span><br></pre></td></tr></table></figure><br>当你调用时会发现这样的提示<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSError</span> * error;</span><br><span class="line">[dataTool loadData:(<span class="built_in">NSError</span> *__autoreleasing *)]</span><br></pre></td></tr></table></figure><br>这时编译器自动帮我们插入以下代码：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSError</span> * error;</span><br><span class="line"><span class="built_in">NSError</span> * __autoreleasing tmpErr = error;</span><br><span class="line">[dataTool loadData:&amp;tmpErr];</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>__unsafe_unretained</code> : 为兼容iOS5以下版本的产物，可以理解成MRC下的weak，现在基本用不到，这里不作描述。</li>
</ul>
<h3 id="属性的内存管理"><a href="#属性的内存管理" class="headerlink" title="属性的内存管理"></a>属性的内存管理</h3><p>ObjC2.0引入了@property，提供成员变量访问方法、权限、环境、内存管理类型的声明，下面主要说明ARC中属性的内存管理。</p>
<p>属性的参数分为三类，基本数据类型默认为(atomic,readwrite,assign)，对象类型默认为(atomic,readwrite,strong)，其中第三个参数就是该属性的内存管理方式修饰，修饰词可以是以下之一：</p>
<ul>
<li><p><code>assign</code> : 一般用来修饰基本数据类型</p>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>, <span class="keyword">assign</span>) <span class="built_in">NSInteger</span> count;</span><br></pre></td></tr></table></figure>
<p>当然也可以修饰ObjC对象，但是不推荐，因为被assign修饰的对象释放后，指针还是指向释放前的内存，在后续操作中可能会导致内存问题引发崩溃。</p>
</li>
<li><p><code>retain</code> : release旧值，再retain新值（引用计数＋1）</p>
</li>
</ul>
<p>retain和strong一样，都用来修饰ObjC对象。使用set方法赋值时，实质上是会先保留新值，再释放旧值，再设置新值，避免新旧值一样时导致对象被释放的的问题。</p>
<p>MRC写法如下：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)setCount:(<span class="built_in">NSObject</span> *)count &#123;</span><br><span class="line">    [count <span class="keyword">retain</span>];</span><br><span class="line">    [_count release];</span><br><span class="line">    _count = count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>ARC写法如下：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">- (<span class="keyword">void</span>)setCount:(<span class="built_in">NSObject</span> *)count &#123;</span><br><span class="line">    _count = count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>copy</code> : release旧值，再copy新值（拷贝内容）</li>
</ul>
<p>一般用来修饰String、Dict、Array等需要保护其封装性的对象，尤其是在其内容可变的情况下，因此会拷贝（深拷贝）一份内容給属性使用，避免可能造成的对源内容进行改动。</p>
<p>使用set方法赋值时，实质上是会先拷贝新值，再释放旧值，再设置新值。<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>, <span class="keyword">copy</span>) <span class="built_in">NSString</span>* name;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>weak</code> : ARC新引入修饰词，可代替assign，比assign多增加一个特性（置nil，见上文）。</li>
</ul>
<p>weak和strong一样用来修饰ObjC对象。使用set方法赋值时，实质上不保留新值，也不释放旧值，只设置新值。</p>
<p>比如常用的代理的声明：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@property</span> (<span class="keyword">weak</span>) <span class="keyword">id</span>&lt;MyDelegate&gt; delegate;</span><br></pre></td></tr></table></figure></p>
<p>Xib控件的引用：<br>@property (weak, nonatomic) IBOutlet UIImageView* productImage;</p>
<ul>
<li><code>strong</code> : ARC新引入修饰词，可代替retain</li>
</ul>
<p>可参照retain，这里不再作描述。</p>
<h3 id="block的内存管理"><a href="#block的内存管理" class="headerlink" title="block的内存管理"></a>block的内存管理</h3><p>iOS中使用block必须自己管理内存，错误的内存管理将导致循环引用等内存泄漏问题，这里主要说明在ARC下block声明和使用的时候需要注意的两点：</p>
<ul>
<li>如果你使用@property去声明一个block的时候，一般使用copy来进行修饰（当然也可以不写，编译器自动进行copy操作），尽量不要使用retain。</li>
</ul>
<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">@property</span> (<span class="keyword">nonatomic</span>, <span class="keyword">copy</span>) <span class="keyword">void</span>(^block)(<span class="built_in">NSData</span>* data);</span><br></pre></td></tr></table></figure>
<ul>
<li>block会对内部使用的对象进行强引用，因此在使用的时候应该确定不会引起循环引用，当然保险的做法就是添加弱引用标记。<figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">__<span class="keyword">weak</span> <span class="keyword">typeof</span>(<span class="keyword">self</span>) weakSelf = <span class="keyword">self</span>;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="经典内存泄漏及其解决方案"><a href="#经典内存泄漏及其解决方案" class="headerlink" title="经典内存泄漏及其解决方案"></a>经典内存泄漏及其解决方案</h2><p>虽然ARC好处多多，然而也并无法避免内存泄漏问题，下面介绍在ARC中常见的内存泄漏。</p>
<h3 id="僵尸对象和野指针"><a href="#僵尸对象和野指针" class="headerlink" title="僵尸对象和野指针"></a>僵尸对象和野指针</h3><font color=orange>僵尸对象：内存已经被回收的对象</font>

<font color=orange>野指针：指向僵尸对象的指针，向野指针发送消息会导致崩溃</font>

<p>野指针错误形式在Xcode中通常表现为：Thread 1：EXC_BAD_ACCESS，因为你访问了一块已经不属于你的内存。</p>
<p>例子代码：(没有出现错误的多运行几遍，因为获取野指针指向的结果是不确定的)<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line">Dog * dog = [[Dog alloc]init];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;before&quot;</span>);</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;%s&quot;</span>,object_getClassName(dog));</span><br><span class="line">[dog release];</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;after&quot;</span>);</span><br><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;%s&quot;</span>,object_getClassName(dog));</span><br></pre></td></tr></table></figure><br>运行结果：</p>
<p><img   src="https://img-blog.csdnimg.cn/img_convert/2b5c91ee80037ab5ecc3b6309ed5e131.png"  alt="结果"></p>
<p>可以看到，当运行到第六行的时候崩溃了，并给出了EXC_BAD_ACCESS的提示。</p>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>对象已经被释放后，应将其指针置为空指针（没有指向任何对象的指针，给空指针发送消息不会报错）。</p>
<p>然而在实际开发中实际遇到EXC_BAD_ACCESS错误时，往往很难定位到错误点，幸好Xcode提供方便的工具給我们来定位及分析错误。</p>
<ul>
<li>在product－scheme－edit scheme－diagnostics中将enable zombie objects勾选上，下次再出现这样的错误就可以准确定位了。</li>
<li>在Xcode－open developer tool－Instruments打开工具集，选择Zombies工具可以对已安装的应用进行僵尸对象检测。</li>
</ul>
<h3 id="循环引用"><a href="#循环引用" class="headerlink" title="循环引用"></a>循环引用</h3><p>循环引用是ARC中最常出现的问题，对于可能引发循环引用的一些原因在有一篇文章<a class="link"   href="http://www.jianshu.com/p/bcc0bcaadd6c" >iOS总结篇：影响控制器正常释放的常见问题<i class="fas fa-external-link-alt"></i></a>中有提及。</p>
<p>一般来讲循环引用也是可以使用工具来检测到的，分为两种：</p>
<ul>
<li>在 product－Analyze 中使用静态分析来检测代码中可能存在循环引用的问题。</li>
<li>在 Xcode－open developer tool－Instruments 打开工具集，选择Leaks工具可以对已安装的应用进行内存泄漏检测，此工具能检测静态分析不会提示，但是到运行时才会出现的内存泄漏问题。</li>
</ul>
<p>Leaks工具虽然强大，但是它不能检测到block循环引用导致的内存泄漏，这种情况一般需要自行排查问题（考验你的基本功时候到了）。</p>
<h3 id="循环中对象占用内存大"><a href="#循环中对象占用内存大" class="headerlink" title="循环中对象占用内存大"></a>循环中对象占用内存大</h3><p>这个问题常见于循环次数较大，循环体生成的对象占用内存较大的情景。</p>
<p>例子代码：我需要10000个演员来打仗<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10000</span>; i ++) &#123;</span><br><span class="line">    Person * soldier = [[Person alloc]init];</span><br><span class="line">    [soldier fight];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>该循环内产生大量的临时对象，直至循环结束才释放，可能导致内存泄漏，解决方法和上文中提到的自动释放池常见问题类似：在循环中创建自己的autoReleasePool，及时释放占用内存大的临时变量，减少内存占用峰值。<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10000</span>; i ++) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        Person * soldier = [[Person alloc]init];</span><br><span class="line">        [soldier fight];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然而有时候autoReleasePool也不是万能的：</p>
<p>例子：假如有2000张图片，每张1M左右，现在需要获取所有图片的尺寸，你会怎么做？<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2000</span>; i ++) &#123;</span><br><span class="line">    <span class="built_in">CGSize</span> size = [<span class="built_in">UIImage</span> imageNamed:[<span class="built_in">NSString</span> stringWithFormat:<span class="string">@&quot;%d.jpg&quot;</span>,i]].size;</span><br><span class="line">    <span class="comment">//add size to array</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>用imageNamed方法加载图片占用Cache的内存，autoReleasePool也不能释放，对此问题需要另外的解决方法，当然保险的当然是双管齐下了:<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2000</span>; i ++) &#123;</span><br><span class="line">    <span class="keyword">@autoreleasepool</span> &#123;</span><br><span class="line">        <span class="built_in">CGSize</span> size = [<span class="built_in">UIImage</span> imageWithContentsOfFile:filePath].size;</span><br><span class="line">        <span class="comment">//add siez to array</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="无限循环"><a href="#无限循环" class="headerlink" title="无限循环"></a>无限循环</h3><p>无论你出于什么原因，当你启动了一个无限循环的时候，ARC会默认该方法用不会执行完毕，方法里面的对象就永不释放，内存无限上涨，导致内存泄漏。</p>
<p>例子：<br><figure class="highlight objectivec"><table><tr><td class="code"><pre><span class="line"><span class="built_in">NSLog</span>(<span class="string">@&quot;start !&quot;</span>);</span><br><span class="line"><span class="built_in">dispatch_async</span>(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT, <span class="number">0</span>), ^&#123;</span><br><span class="line">    <span class="built_in">BOOL</span> isSucc = <span class="literal">YES</span>;</span><br><span class="line">    <span class="keyword">while</span> (isSucc) &#123;</span><br><span class="line">        [<span class="built_in">NSThread</span> sleepForTimeInterval:<span class="number">1.0</span>];</span><br><span class="line">        <span class="built_in">NSLog</span>(<span class="string">@&quot;create an obj&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><br>对于这样的问题还是很好解决的，只要设置相关条件判断就能避免。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>关于iOS内存管理的知识点还有很多，在这里先做基础储备，后续在实战中会继续补充。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="http://www.jianshu.com/p/8b1ed04b3ba9" >iOS内功篇：内存管理<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>Objective-C</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>java8 Stream流常用示例</title>
    <url>/2022/09/21/java8-Stream%E6%B5%81%E5%B8%B8%E7%94%A8%E7%A4%BA%E4%BE%8B/</url>
    <content><![CDATA[<p>Java8推出了Stream Api，开发者能够以声明的方式来流式处理数据。Stream可以让臃肿的代码变得更加简洁、高效。</p>
<blockquote>
<p>Stream将要处理的元素集合看作一种流， 流在管道中传输，并且可以在管道的节点上进行处理，比如筛选、排序、聚合等。元素流在管道中经过中间操作的处理，最后由最终操作得到前面处理的结果。</p>
</blockquote>
<span id="more"></span>
<p>下面列举了该Api的常用示例：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Comparator;</span><br><span class="line"><span class="keyword">import</span> java.util.stream.Collectors;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: swift</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@CreateAt</span>: 2022/09/16 17:11</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: Stream 示例</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Temp</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;stream learning&quot;</span>);</span><br><span class="line"></span><br><span class="line">        String[] arr = &#123;<span class="string">&quot;123&quot;</span>, <span class="string">&quot;34&quot;</span>, <span class="string">&quot;23&quot;</span>, <span class="string">&quot;234&quot;</span>&#125;;</span><br><span class="line">        <span class="comment">// 遍历数组</span></span><br><span class="line">        Arrays.asList(arr).forEach(System.out::println);</span><br><span class="line">        <span class="comment">// 将字符串数组转换为整数数组</span></span><br><span class="line">        List&lt;Integer&gt; intList = Arrays.asList(arr)</span><br><span class="line">                                .stream()</span><br><span class="line">                                .map(ele -&gt; Integer.parseInt(ele))</span><br><span class="line">                                .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取字符串数组中最大值：234</span></span><br><span class="line">        <span class="type">Integer</span> <span class="variable">maxValue</span> <span class="operator">=</span> Arrays.asList(arr)</span><br><span class="line">                            .stream()</span><br><span class="line">                            .map(ele -&gt; Integer.parseInt(ele))</span><br><span class="line">                            .max(Comparator.comparing(Integer::intValue))</span><br><span class="line">                            .get();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 或者简写如下(求最小值)：23</span></span><br><span class="line">        Arrays.asList(arr)</span><br><span class="line">                .stream()</span><br><span class="line">                .mapToInt(ele-&gt;Integer.parseInt(ele))</span><br><span class="line">                .min()</span><br><span class="line">                .ifPresent(minValue -&gt; System.out.println(<span class="string">&quot;Min: &quot;</span> + minValue));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 复杂的对象求最大值</span></span><br><span class="line"><span class="comment">//        users.stream()</span></span><br><span class="line"><span class="comment">//                .max(Comparator.comparing(u -&gt; u.getUserName()))</span></span><br><span class="line"><span class="comment">//                .ifPresent(e -&gt; System.out.println(&quot;Max: &quot; + e.getUserName()));</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 过滤非数字字符串，并将剩下的数字字符串排序然后输出：12，24，45，231，2342</span></span><br><span class="line">        String[] data = &#123;<span class="string">&quot;231&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="string">&quot;as&quot;</span>, <span class="string">&quot;12&quot;</span>, <span class="string">&quot;3453&quot;</span>, <span class="string">&quot;asd&quot;</span>, <span class="string">&quot;24&quot;</span>, <span class="string">&quot;2342&quot;</span>, <span class="string">&quot;45&quot;</span>&#125;;</span><br><span class="line">        Arrays.asList(data)</span><br><span class="line">                .stream()</span><br><span class="line">                .filter(ele -&gt; !ele.isEmpty() &amp;&amp; StringUtils.isNumeric(ele))</span><br><span class="line">                .map(ele -&gt; Integer.parseInt(ele))</span><br><span class="line">                .sorted()</span><br><span class="line">                .limit(<span class="number">5</span>)</span><br><span class="line">                .forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.runoob.com/java/java8-streams.html" >Java 8 Stream<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title>js变量提升</title>
    <url>/2019/01/16/js%E5%8F%98%E9%87%8F%E6%8F%90%E5%8D%87/</url>
    <content><![CDATA[<h2 id="例1"><a href="#例1" class="headerlink" title="例1"></a>例1</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line">    <span class="keyword">if</span> (!a) &#123;</span><br><span class="line">        <span class="keyword">var</span> a = <span class="number">200</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">f</span>()</span><br><span class="line"><span class="comment">// undefined</span></span><br><span class="line"><span class="comment">// 200</span></span><br></pre></td></tr></table></figure>
<h2 id="例2"><a href="#例2" class="headerlink" title="例2"></a>例2</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    a = <span class="number">200</span>;</span><br><span class="line">    <span class="keyword">return</span> ;</span><br><span class="line">    <span class="keyword">function</span> <span class="title function_">a</span>(<span class="params"></span>) &#123;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">f</span>();</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line"><span class="comment">// 100</span></span><br></pre></td></tr></table></figure>
<p>如果你习惯了强类型语言的编程方式，那么看到上述输出结果你肯定会大吃一惊。</p>
<h1 id="js-作用域"><a href="#js-作用域" class="headerlink" title="js 作用域"></a><code>js</code> 作用域</h1><p>我们来看一下 <code>C++</code> 的一个例子：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">int</span> x = <span class="number">100</span>;</span><br><span class="line">	cout &lt;&lt; x &lt;&lt; endl;</span><br><span class="line">	<span class="keyword">if</span> (<span class="number">1</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="type">int</span> x = <span class="number">200</span>;</span><br><span class="line">		cout &lt;&lt; x &lt;&lt; endl;</span><br><span class="line">	&#125;</span><br><span class="line">	cout &lt;&lt; x &lt;&lt; endl;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 100</span></span><br><span class="line"><span class="comment">// 200</span></span><br><span class="line"><span class="comment">// 100</span></span><br></pre></td></tr></table></figure>
<p>再来看一个 <code>js</code> 的例子：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line"><span class="keyword">if</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> a = <span class="number">200</span>;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line">&#125;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line"><span class="comment">// 100</span></span><br><span class="line"><span class="comment">// 200</span></span><br><span class="line"><span class="comment">// 200</span></span><br></pre></td></tr></table></figure>
<p> <code>if</code> 代码块中的变量覆盖了全局变量。那是因为 <code>js</code> 只有<span style="color: red; font-size: 20px">全局作用域和函数作用域，没有块作用域。</span>块内的变量 <code>x</code> 影响到了全局变量 <code>x</code> 。</p>
<h2 id="js-实现块级作用域效果"><a href="#js-实现块级作用域效果" class="headerlink" title="js 实现块级作用域效果"></a><code>js</code> 实现块级作用域效果</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> x = <span class="number">100</span>;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(x);</span><br><span class="line">    <span class="keyword">if</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        (<span class="keyword">function</span>(<span class="params"></span>) &#123;</span><br><span class="line">            <span class="keyword">var</span> x = <span class="number">200</span>;</span><br><span class="line">            <span class="variable language_">console</span>.<span class="title function_">log</span>(x);</span><br><span class="line">        &#125;());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(x);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 100</span></span><br><span class="line"><span class="comment">// 200</span></span><br><span class="line"><span class="comment">// 100</span></span><br></pre></td></tr></table></figure>
<p>其本质上利用了 <code>js</code> 的函数作用域来模拟实现块级作用域。</p>
<h1 id="Hoisting-in-js"><a href="#Hoisting-in-js" class="headerlink" title="Hoisting in js"></a>Hoisting in <code>js</code></h1><p>在 <code>js</code> 中，变量进入一个作用域有以下方式：</p>
<ul>
<li>变量定义： <code>var a</code></li>
<li>函数形参：函数的形参存在于作用域中—— <code>function f(a, b) &#123;&#125;</code></li>
</ul>
<p>在代码运行前，函数声明和变量定义通常会被解释器移动到其所在作用域的最顶部。如下：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="title function_">test</span>();</span><br><span class="line">    <span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面代码被解释器解释后，将会变成如下形式：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> a;</span><br><span class="line">    <span class="title function_">test</span>();</span><br><span class="line">    a = <span class="number">100</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><span style="color: green; font-size: 20px"> <code>hoisting</code> 只是将变量的定义上升，但变量的赋值并不会上升。</span></p>
<p>再来看一个例子：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="title function_">f1</span>();</span><br><span class="line">    <span class="title function_">f2</span>();</span><br><span class="line">    <span class="keyword">var</span> f1 = <span class="keyword">function</span> <span class="title function_">f1</span>(<span class="params"></span>) &#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;error&quot;</span>);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">function</span> <span class="title function_">f2</span>(<span class="params"></span>) &#123;</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;normal&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">f</span>();</span><br><span class="line"><span class="comment">// TypeError: f1 is not a function</span></span><br><span class="line"><span class="comment">// normal</span></span><br></pre></td></tr></table></figure>
<p>首先 <code>var f1</code> 会上升到函数顶部，但是此时 <code>f1</code> 为 <code>undefined</code> ，所以执行报错。但对于函数 <code>f2</code> ，函数本身也是一种变量，存在变量上升的现象，也会上升到函数顶部，所以 <code>f2()</code> 能顺利进行。</p>
<h1 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h1><p>例1等同于如下代码：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> a;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line">    <span class="keyword">if</span> (!a) &#123;</span><br><span class="line">       a = <span class="number">200</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">f</span>()</span><br><span class="line"><span class="comment">// undefined</span></span><br><span class="line"><span class="comment">// 200</span></span><br></pre></td></tr></table></figure>
<p>例2等同于如下代码：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">function</span> <span class="title function_">f</span>(<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">function</span> <span class="title function_">a</span>(<span class="params"></span>) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    a = <span class="number">200</span>;</span><br><span class="line">    <span class="keyword">return</span> ;   </span><br><span class="line">&#125;</span><br><span class="line"><span class="title function_">f</span>();</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(a);</span><br><span class="line"><span class="comment">// 100</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://segmentfault.com/a/1190000003114255#articleHeader1" >Javascript作用域和变量提升<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>变量提升</tag>
      </tags>
  </entry>
  <entry>
    <title>keep主题从3.x升级到4.x后GitHub Actions自动部署后文章更新时间异常的问题</title>
    <url>/2024/02/28/keep%E4%B8%BB%E9%A2%98%E4%BB%8E3-x%E5%8D%87%E7%BA%A7%E5%88%B04-x%E5%90%8EGitHub-Actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E5%90%8E%E6%96%87%E7%AB%A0%E6%9B%B4%E6%96%B0%E6%97%B6%E9%97%B4%E5%BC%82%E5%B8%B8%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>keep主题4.x新增了很多功能配置，在升级的过程中遇到了一些问题，在此记录一下：</p>
<span id="more"></span>
<h2 id="GitHub-Actions自动部署后文章更新时间异常"><a href="#GitHub-Actions自动部署后文章更新时间异常" class="headerlink" title="GitHub Actions自动部署后文章更新时间异常"></a>GitHub Actions自动部署后文章更新时间异常</h2><p>在首页展示的所有的文章更新时间都是一样的：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.9kfx1lyvok.webp"  alt="case"></p>
<p>这是因为自动化工作流每次都需要先克隆Hexo博客项目源码，才能进行后续的构建生成和部署等操作。但在Hexo博客中，如果没有在文章的<code>Front-Matter</code>设置<code>updated</code>，Hexo会默认使用文件的最后修改时间作为文章的更新时间，这就是为什么会出现自动部署后所有文章更新时间都一致的真正原因。</p>
<h4 id="两种解决方法"><a href="#两种解决方法" class="headerlink" title="两种解决方法"></a>两种解决方法</h4><ol>
<li><p>在博客的<code>Front-Matter</code>添加<code>updated</code>字段：<br><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.64dl9m683h.webp"  alt="updated"></p>
</li>
<li><p>使用主题作者提供的Github Action工作流</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">Hexo</span> <span class="string">Deploy</span> <span class="string">GitHub</span> <span class="string">Pages</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">main</span></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build-and-deploy:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/checkout@v3</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">fetch-depth:</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Build</span> <span class="string">and</span> <span class="string">Deploy</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">theme-keep/hexo-deploy-github-pages-action@master</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">        <span class="comment"># Your GitHub Token</span></span><br><span class="line">        <span class="attr">PERSONAL_TOKEN:</span> <span class="string">$&#123;&#123;</span> <span class="string">secrets.MYBLOG_TOKEN</span> <span class="string">&#125;&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># The repository the action should deploy to</span></span><br><span class="line">        <span class="attr">PUBLISH_REPOSITORY:</span> <span class="string">TransformersWsz/TransformersWsz.github.io</span></span><br><span class="line">        <span class="attr">BRANCH:</span> <span class="string">main</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li><code>PERSONAL_TOKEN</code>：github token，这样源码仓库<code>TransformersWsz/myblog</code>可以写静态文件仓库<code>TransformersWsz/TransformersWsz.github.io</code>，获取步骤如下：<ol>
<li>创建<a class="link"   href="https://github.com/settings/tokens" >Personal Access Token<i class="fas fa-external-link-alt"></i></a></li>
<li>在<a class="link"   href="https://github.com/TransformersWsz/myblog/settings/secrets/actions" >源码仓库<i class="fas fa-external-link-alt"></i></a>添加token</li>
</ol>
</li>
</ul>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.1aoqdgo4t7.webp"  alt="add_token"></p>
<div class="keep-note danger"><p>注意添加<code>BRANCH: main</code>，否则它会默认推送到静态仓库的<a href="https://github.com/theme-keep/hexo-deploy-github-pages-action/blob/e4c3d8b5150fe5bcc2b643cbd056cd034355442b/entrypoint.sh#L16"><code>gh-pages</code></a>分支</p>
</div>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://keep.xpoet.cn/2023/11/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3-GitHub-Actions-%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E5%90%8E%E6%96%87%E7%AB%A0%E6%9B%B4%E6%96%B0%E6%97%B6%E9%97%B4%E5%BC%82%E5%B8%B8%E7%9A%84%E9%97%AE%E9%A2%98/" >如何解决 GitHub Actions 自动部署后文章更新时间异常的问题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://stackoverflow.com/questions/59261555/you-must-provide-the-action-with-either-a-personal-access-token-or-the-github-to" >You must provide the action with either a Personal Access Token or the GitHub Token secret in order to deploy<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/theme-keep/hexo-deploy-github-pages-action" >hexo-deploy-github-pages-action<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/XPoet/hexo-theme-keep/issues/248" >文章列表时间展示不对 #248<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://gist.github.com/TransformersWsz/1bd68b6c6cc25a1ccf50fc9f12faa1a1" >之前的工作流：deploy.yml<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>CI</tag>
        <tag>theme-keep</tag>
        <tag>Github Action</tag>
      </tags>
  </entry>
  <entry>
    <title>logistic回归参数求解推导过程</title>
    <url>/2021/03/23/logistic%E5%9B%9E%E5%BD%92%E5%8F%82%E6%95%B0%E6%B1%82%E8%A7%A3%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>记录一下逻辑回归的参数求解推导过程：</p>
<span id="more"></span>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>线性回归的表达式为：$f(x) = wx+b$，为了消除后面的$b$，令$\theta = [w \quad b], x = [x \quad 1]^T$，则$f(x) = \theta x$</p>
<p>将其转换为逻辑回归模型：$y=\sigma(f({x}))=\sigma\left({\theta} {x}\right)=\frac{1}{1+e^{-{\theta} {x}}}$</p>
<p>我们把单个样本看作一个事件，那么这个事件发生的概率为：</p>
<script type="math/tex; mode=display">
P(y \mid {x})=\left\{\begin{array}{r}
p, y=1 \\
1-p, y=0
\end{array}\right.</script><p>它等价于：$P\left(y_{i} \mid {x}_{i}\right)=p^{y_{i}}(1-p)^{1-y_{i}}$</p>
<p>如果我们采集到了一组数据一共N个，$\left\{\left({x}_{1}, y_{1}\right),\left({x}_{2}, y_{2}\right),\left({x}_{3}, y_{3}\right) \ldots\left({x}_{N}, y_{N}\right)\right\},$ 这个合成在一起的合事件发生的总概率如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P_{total} &= P(y_1|x_1)P(y_2|x_2)P(y_3|x_3) \ldots P(y_N|x_N) \\
&= \prod_{i=1}^{N} p^{y_{i}}(1-p)^{1-y_{i}} \\
F(\theta) &= ln(P_{total}) = \sum_{i=1}^N ln(p^{y_{i}}(1-p)^{1-y_{i}}) \\
&= \sum_{i=1}^N y_ilnp + (1-y_i)ln(1-p) \\
其中 p &= \frac{1}{1+e^{-{\theta} {x}}}
\end{aligned}</script><p>为了符合损失函数的含义，将其定义为：</p>
<script type="math/tex; mode=display">
L(\theta) = -F(\theta)</script><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><script type="math/tex; mode=display">
\frac{\partial L}{\partial \theta} = \frac{\partial L}{\partial p} \times \frac{\partial p}{\partial \theta}</script><p>先求$\frac{\partial p}{\partial \theta}$ :</p>
<script type="math/tex; mode=display">
\begin{aligned}
p' &= (\frac{1}{1+e^{-\theta x}})' \\
&= \frac{-1}{(1+e^{-\theta x})^2} \cdot e^{-\theta x} \cdot -x \\
&= \frac{1}{1+e^{-\theta x}} \cdot \frac{e^{-\theta x}}{1+e^{-\theta x}} \cdot x \\
&= p(1-p)x
\end{aligned}</script><p>求$\frac{\partial L}{\partial \theta}$ :</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla F(\theta) &= \nabla (\sum_{i=1}^N y_ilnp + (1-y_i)ln(1-p)) \\
&= \sum_{i=1}^N y_i \frac{1}{p} p' + (1-y_i)\frac{-1}{1-p}p' \\
&= \sum_{i=1}^N y_i(1-p)x_i - (1-y_i)px_i \\
&= \sum_{i=1}^N (y_i-p) x_i \\
\end{aligned}</script><p>因此 $\frac{\partial L}{\partial \theta} = \sum_{i=1}^N (p-y_i)x_i$</p>
<h2 id="梯度更新"><a href="#梯度更新" class="headerlink" title="梯度更新"></a>梯度更新</h2><p>通过反向传播，$\theta$ 的更新过程如下：</p>
<script type="math/tex; mode=display">
\theta := \theta - \alpha \sum_{i=1}^N (y_i - \frac{1}{1+e^{-\theta x_i}}) x_i</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/44591359" >逻辑回归 logistics regression 公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>metapath2vec解读</title>
    <url>/2024/02/04/metapath2vec%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>metapath2vec在用在工业界的召回通路中比较多，非常适用于<strong>异构的K部图</strong>。</p>
<p>元路径 $P$ 定义形式如： $V_1 \rightarrow^{R_1} V_2 \rightarrow^{R_2} A_3 \ldots \rightarrow^{R_l} A_{l+1}$ 表示了从 $A_1$ 到 $A_{l+1}$ 的复杂关系。<br>其中 $V_i$ 表示节点类型，$R_i$ 表示节点间的关系。 $R=R_1 \circ R_2 \circ R_3 \circ R_l$，元路径 $P$ 的长度即为关系 $R$ 的个数。</p>
<span id="more"></span>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p><code>APA</code> 就表示两位作者是论文的共同作者：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/a63833f2687f592ccad30d090fab8a88ce76a9c0/image.2squ8k783uy0.png"  alt="example"></p>
<h2 id="不同类型的节点间的转移概率"><a href="#不同类型的节点间的转移概率" class="headerlink" title="不同类型的节点间的转移概率"></a>不同类型的节点间的转移概率</h2><script type="math/tex; mode=display">
p\left(v^{i+1} \mid v_t^i, \mathcal{P}\right)=\left\{\begin{array}{cl}
\frac{1}{\left|N_{t+1}\left(v_t^i\right)\right|} & \left(v^{i+1}, v_t^i\right) \in E, \phi\left(v^{i+1}\right)=t+1 \\
0 & \left(v^{i+1}, v_t^i\right) \in E, \phi\left(v^{i+1}\right) \neq t+1 \\
0 & \left(v^{i+1}, v_t^i\right) \notin E
\end{array}\right.</script><p>其中 $v_t^i$ 表示step $i$ 的类型为$V_t$的节点， $N_{t+1}\left(v_t^i\right)$ 表示 $v_t^i$ 的类型为$V_{t+1}$的邻居节点集合</p>
<blockquote>
<p>具体详细的讲解见：<a class="link"   href="https://aistudio.baidu.com/aistudio/projectdetail/1099287" >PGL系列16：metapath2vec<i class="fas fa-external-link-alt"></i></a></p>
</blockquote>
<h2 id="Heterogeneous-Skip-Gram"><a href="#Heterogeneous-Skip-Gram" class="headerlink" title="Heterogeneous Skip-Gram"></a>Heterogeneous Skip-Gram</h2><script type="math/tex; mode=display">
p\left(c_t \mid v ; \theta\right)=\frac{e^{X_{c_t} \cdot X_v}}{\sum_{u \in V} e^{X_u \cdot X_v}}</script><h2 id="Metapath2vec-框架"><a href="#Metapath2vec-框架" class="headerlink" title="Metapath2vec++框架"></a>Metapath2vec++框架</h2><script type="math/tex; mode=display">
p\left(c_t \mid v ; \theta\right)=\frac{e^{X_{c_t} \cdot X_v}}{\sum_{u t \in V_t} e^{X_{u_t} \cdot X_v}}</script>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Paper Reading</tag>
        <tag>Heterogeneous Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>picture bed</title>
    <url>/2019/09/19/picture%20bed/</url>
    <content><![CDATA[<p>暂时作为一个图片床吧。。。</p>
<span id="more"></span>
<img   src="/2019/09/19/picture%20bed/93.png"  class="">
<img   src="/2019/09/19/picture%20bed/94.png"  class="">
<img   src="/2019/09/19/picture%20bed/95.png"  class="">
<img   src="/2019/09/19/picture%20bed/96.png"  class="">
<img   src="/2019/09/19/picture%20bed/97.png"  class="">
<img   src="/2019/09/19/picture%20bed/98.png"  class="">
<img   src="/2019/09/19/picture%20bed/99.png"  class="">
<img   src="/2019/09/19/picture%20bed/100.png"  class="">
<img   src="/2019/09/19/picture%20bed/101.png"  class="">
<img   src="/2019/09/19/picture%20bed/102.png"  class="">
<img   src="/2019/09/19/picture%20bed/103.png"  class="">
<img   src="/2019/09/19/picture%20bed/104.png"  class="">
<img   src="/2019/09/19/picture%20bed/105.png"  class="">
<img   src="/2019/09/19/picture%20bed/106.png"  class="">
<img   src="/2019/09/19/picture%20bed/107.png"  class="">
<img   src="/2019/09/19/picture%20bed/108.png"  class="">
<img   src="/2019/09/19/picture%20bed/109.png"  class="">
<img   src="/2019/09/19/picture%20bed/110.png"  class="">
<img   src="/2019/09/19/picture%20bed/111.png"  class="">
<img   src="/2019/09/19/picture%20bed/112.png"  class="">
<img   src="/2019/09/19/picture%20bed/113.png"  class="">
<img   src="/2019/09/19/picture%20bed/114.png"  class="">
<img   src="/2019/09/19/picture%20bed/115.png"  class="">
<img   src="/2019/09/19/picture%20bed/116.png"  class="">
<img   src="/2019/09/19/picture%20bed/117.png"  class="">
<img   src="/2019/09/19/picture%20bed/118.png"  class="">
<img   src="/2019/09/19/picture%20bed/119.png"  class="">
<img   src="/2019/09/19/picture%20bed/120.png"  class="">
<img   src="/2019/09/19/picture%20bed/121.png"  class="">
<img   src="/2019/09/19/picture%20bed/122.png"  class="">
<img   src="/2019/09/19/picture%20bed/123.png"  class="">
<img   src="/2019/09/19/picture%20bed/124.png"  class="">
<img   src="/2019/09/19/picture%20bed/125.png"  class="">
<img   src="/2019/09/19/picture%20bed/126.png"  class="">
<img   src="/2019/09/19/picture%20bed/127.png"  class="">
<img   src="/2019/09/19/picture%20bed/128.png"  class="">
<img   src="/2019/09/19/picture%20bed/129.png"  class="">
<img   src="/2019/09/19/picture%20bed/130.png"  class="">
<img   src="/2019/09/19/picture%20bed/131.png"  class="">
<img   src="/2019/09/19/picture%20bed/132.png"  class="">
<img   src="/2019/09/19/picture%20bed/133.png"  class="">
<img   src="/2019/09/19/picture%20bed/134.png"  class="">
<img   src="/2019/09/19/picture%20bed/135.png"  class="">
<img   src="/2019/09/19/picture%20bed/lcs.jpeg"  class="">
<img   src="/2019/09/19/picture%20bed/136.png"  class="">
]]></content>
      <categories>
        <category>personal</category>
      </categories>
      <tags>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title>productivity tools for windows</title>
    <url>/2019/09/19/productivity%20tools%20for%20windows/</url>
    <content><![CDATA[<p>每次重装系统都要装回一大堆软件，但有些软件可能会忘记，在此记录一下：</p>
<span id="more"></span>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">工具名</th>
<th style="text-align:center">简介</th>
<th style="text-align:center">下载地址</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Chrome</td>
<td style="text-align:center">谷歌浏览器，web开发必备</td>
<td style="text-align:center"><a class="link"   href="https://www.google.com/intl/zh-CN/chrome/" >https://www.google.com/intl/zh-CN/chrome/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">PyCharm</td>
<td style="text-align:center">编写python程序的IDE</td>
<td style="text-align:center"><a class="link"   href="https://www.jetbrains.com/pycharm/download/" >https://www.jetbrains.com/pycharm/download/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">IDEA</td>
<td style="text-align:center">编写java程序的IDE</td>
<td style="text-align:center"><a class="link"   href="https://www.jetbrains.com/idea/download/" >https://www.jetbrains.com/idea/download/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">DataGrid</td>
<td style="text-align:center">数据库管理工具</td>
<td style="text-align:center"><a class="link"   href="https://www.jetbrains.com/datagrip/download/" >https://www.jetbrains.com/datagrip/download/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">Android Studio</td>
<td style="text-align:center">官方安卓开发IDE</td>
<td style="text-align:center"><a class="link"   href="https://developer.android.com/studio" >https://developer.android.com/studio<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">Visual Studio</td>
<td style="text-align:center">宇宙最强IDE</td>
<td style="text-align:center"><a class="link"   href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community&amp;rel=16" >https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community&amp;rel=16<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">VS Code</td>
<td style="text-align:center">微软出品的编辑器，适合做前端以及python开发</td>
<td style="text-align:center"><a class="link"   href="https://code.visualstudio.com/Download" >https://code.visualstudio.com/Download<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">Octave</td>
<td style="text-align:center">仿matlab的数学计算工具</td>
<td style="text-align:center"><a class="link"   href="https://www.gnu.org/software/octave/#install" >https://www.gnu.org/software/octave/#install<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">Office</td>
<td style="text-align:center">办公三件套</td>
<td style="text-align:center">可到各学校软件中心下载</td>
</tr>
<tr>
<td style="text-align:center">Everything</td>
<td style="text-align:center">简洁、快速的文件检索工具</td>
<td style="text-align:center"><a class="link"   href="http://www.voidtools.com/downloads/" >http://www.voidtools.com/downloads/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">typora</td>
<td style="text-align:center">markdown编辑器，所见即所得</td>
<td style="text-align:center"><a class="link"   href="https://typora.io/" >https://typora.io/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">MobaXterm</td>
<td style="text-align:center">服务器远程连接工具</td>
<td style="text-align:center"><a class="link"   href="https://mobaxterm.mobatek.net/download.html" >https://mobaxterm.mobatek.net/download.html<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">Rufus</td>
<td style="text-align:center">简洁轻巧的系统盘制作工具</td>
<td style="text-align:center"><a class="link"   href="https://rufus.ie/" >https://rufus.ie/<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">分区助手</td>
<td style="text-align:center">傻瓜式操作的系统分区工具</td>
<td style="text-align:center"><a class="link"   href="https://www.disktool.cn" >https://www.disktool.cn<i class="fas fa-external-link-alt"></i></a></td>
</tr>
<tr>
<td style="text-align:center">石墨文档</td>
<td style="text-align:center">可多人在线协同编辑的云端Office，支持markdown</td>
<td style="text-align:center"><a class="link"   href="https://shimo.im/download" >https://shimo.im/download<i class="fas fa-external-link-alt"></i></a></td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>personal</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>python2.7安装tensorflow1.x</title>
    <url>/2024/12/09/python2-7%E5%AE%89%E8%A3%85tensorflow1-x/</url>
    <content><![CDATA[<p>当前tensorflow官方已不再提供1.x版本的pip安装，尝试了网上多种解决方案后，最简单的就是换源。</p>
<span id="more"></span>
<p>编辑<code>~/.pip/pip.conf</code>，将pip源换成清华源：<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line">[<span class="string">global</span>]</span><br><span class="line"><span class="string">timeout</span> <span class="string">=</span> <span class="number">6000</span></span><br><span class="line"><span class="string">index-url</span> <span class="string">=</span> <span class="string">https://pypi.tuna.tsinghua.edu.cn/simple</span></span><br><span class="line"><span class="string">trusted-host</span> <span class="string">=</span> <span class="string">pypi.tuna.tsinghua.edu.cn</span></span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>python2.7</tag>
        <tag>tensorflow1.x</tag>
      </tags>
  </entry>
  <entry>
    <title>python2字典遍历方法性能对比</title>
    <url>/2022/07/21/python2%E5%AD%97%E5%85%B8%E9%81%8D%E5%8E%86%E6%96%B9%E6%B3%95%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>在公司服务器上跑python2程序时，使用了 <code>tqdm(d.items())</code> 来包裹字典，发现进度条一直卡在0%不动，怀疑是 <code>d.items()</code> 取出所有的元素作为列表返回，而不是迭代器，导致耗时非常长。在此做一下性能测试，代码如下：</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">construct_large_dict</span>(<span class="params">length=<span class="number">25000000</span>, size=<span class="number">3</span></span>):</span><br><span class="line">    d = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(length)):</span><br><span class="line">        l = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">            l.append(random.randint(<span class="number">0</span>, <span class="number">100</span>))</span><br><span class="line">        d[i] = l</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    d = construct_large_dict()</span><br><span class="line"></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> d:</span><br><span class="line">        total += <span class="built_in">sum</span>(d[key])</span><br><span class="line">    end1 = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;method-1 time: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(end1 - start))</span><br><span class="line"></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">        total += <span class="built_in">sum</span>(v)</span><br><span class="line">    end2 = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;method-2 time: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(end2 - end1))</span><br><span class="line"></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> d.iteritems():</span><br><span class="line">        total += <span class="built_in">sum</span>(v)</span><br><span class="line">    end3 = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;method-3 time: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(end3 - end2))</span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<p>结果如下：<br><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.1zhzwi7qjc.webp"  alt="result"></p>
<p>因此在遍历大字典时，推荐使用第一种或者第三种方式，进度条展示的时候也更人性化。</p>
<blockquote>
<p>在python3中，已经用 <code>d.items()</code> 代替了 <code>d.iteritems()</code> ，因此无需再担心性能问题。</p>
</blockquote>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Python2</tag>
        <tag>Dict</tag>
        <tag>tqdm</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch中的二分类及多分类交叉熵损失函数</title>
    <url>/2020/12/08/pytorch%E4%B8%AD%E7%9A%84%E4%BA%8C%E5%88%86%E7%B1%BB%E5%8F%8A%E5%A4%9A%E5%88%86%E7%B1%BB%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>本文主要记录一下PyTorch里面的二分类及多分类交叉熵损失函数的使用。</p>
<span id="more"></span>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">torch.manual_seed(<span class="number">2020</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;torch._C.Generator at 0x7f4e8b3298b0&gt;
</code></pre><h2 id="二分类交叉熵损失函数"><a href="#二分类交叉熵损失函数" class="headerlink" title="二分类交叉熵损失函数"></a>二分类交叉熵损失函数</h2><p>该函数主要用于多标签分类中，针对每个标签进行二分类。</p>
<h4 id="Single"><a href="#Single" class="headerlink" title="Single"></a>Single</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(<span class="built_in">input</span>), target)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">f_output = F.binary_cross_entropy(m(<span class="built_in">input</span>), target)</span><br><span class="line"><span class="built_in">print</span>(f_output)</span><br><span class="line">l_output = nn.BCEWithLogitsLoss()(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(l_output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([ 1.2372, -0.9604,  1.5415], requires_grad=True)
tensor(0.2576, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
tensor(0.2576, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
tensor(0.2576, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;)
</code></pre><h4 id="Batch"><a href="#Batch" class="headerlink" title="Batch"></a>Batch</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">32</span>,<span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">32</span>,<span class="number">5</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(<span class="built_in">input</span>), target)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">f_output = F.binary_cross_entropy(m(<span class="built_in">input</span>), target)</span><br><span class="line"><span class="built_in">print</span>(f_output)</span><br><span class="line">l_output = nn.BCEWithLogitsLoss()(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(l_output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 1.2986,  1.5832, -1.1648,  0.8027, -0.9628],
        [-1.5793, -0.2155,  0.4706, -1.2511,  0.7105],
        [-0.1274, -1.9361,  0.8374,  0.0081, -0.1504],
        [ 0.1521,  1.1443,  0.2171, -1.1438,  0.9341],
        [-3.3199,  1.2998,  0.3918,  0.8327,  1.2411],
        [-0.8507, -0.1016, -1.2434, -0.5755,  0.1871],
        [-0.3064,  1.3751,  1.8478,  0.0326,  0.2032],
        [ 0.1782,  2.3037,  1.5948, -1.4731,  1.5312],
        [-0.9075, -1.7135,  0.4650, -1.7061,  0.0625],
        [-1.1904,  0.1130, -1.6609, -0.2000, -0.1422],
        [ 0.3307, -0.8395, -1.3068, -0.8891,  0.9858],
        [ 0.5484,  0.7461, -1.0738, -2.2162,  0.6801],
        [-0.8803,  0.9934, -1.6438,  0.3860,  0.4111],
        [-1.1078, -0.9629, -0.9534, -0.6207,  0.6885],
        [-0.0175,  1.9496,  0.9740, -0.4687, -0.6127],
        [ 0.3713,  0.8074,  0.3072,  1.1604, -0.2669],
        [-0.1773, -0.2787,  0.1926,  0.7492,  0.7492],
        [-0.3126, -0.3321, -1.7287, -3.0126,  0.1194],
        [ 1.0486, -0.1890, -0.5853,  0.4353,  0.2619],
        [ 1.9726, -0.5510, -0.1826, -0.8600, -0.9906],
        [ 0.7551,  0.8431, -0.8461, -1.2120,  0.2908],
        [-0.0932, -0.7151, -0.0631,  1.7554,  0.7374],
        [-0.1494, -0.6990, -0.1666,  2.0430,  1.3968],
        [ 0.2280, -0.3187,  1.0309, -0.1067,  1.1622],
        [-1.5120, -0.8617,  1.4165, -0.2361, -0.0355],
        [-0.8757, -0.6554,  0.1121, -0.1669, -0.2628],
        [-0.8023,  0.2305, -1.1792,  0.4314, -0.3653],
        [ 0.7487,  0.5358, -0.2677, -0.8128,  0.3029],
        [ 1.4439, -0.5677,  0.5564, -0.2485, -0.3281],
        [-2.0259,  1.1038,  1.0615,  1.7317, -0.0531],
        [ 0.9083, -0.8274,  0.8101, -1.1375, -1.2009],
        [ 0.3300, -0.8760,  1.3459, -1.0209, -0.5313]], requires_grad=True)
tensor(0.8165, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
tensor(0.8165, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)
tensor(0.8165, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;)
</code></pre><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><ul>
<li>二分类交叉熵损失函数的input和target的shape是一致的</li>
<li><code>nn.BCELoss()</code> 与 <code>F.binary_cross_entropy</code> 计算结果是等价的，具体两者差距可见<a class="link"   href="https://www.zhihu.com/question/66782101" >PyTorch 中，nn 与 nn.functional 有什么区别？<i class="fas fa-external-link-alt"></i></a></li>
<li><blockquote>
<p><code>nn.BCEWithLogitsLoss</code>: combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability. 至于为什么更稳定，见 <a class="link"   href="https://blog.csdn.net/u010630669/article/details/105599067" >https://blog.csdn.net/u010630669/article/details/105599067<i class="fas fa-external-link-alt"></i></a></p>
</blockquote>
</li>
</ul>
<h2 id="多分类交叉熵损失函数"><a href="#多分类交叉熵损失函数" class="headerlink" title="多分类交叉熵损失函数"></a>多分类交叉熵损失函数</h2><h4 id="Single-1"><a href="#Single-1" class="headerlink" title="Single"></a>Single</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">f_output = F.cross_entropy(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(f_output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(1.7541, grad_fn=&lt;NllLossBackward&gt;)
tensor(1.7541, grad_fn=&lt;NllLossBackward&gt;)
</code></pre><h4 id="Batch-1"><a href="#Batch-1" class="headerlink" title="Batch"></a>Batch</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">32</span>, <span class="number">10</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">32</span>, <span class="number">5</span>, dtype=torch.long).random_(<span class="number">10</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">f_output = F.cross_entropy(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(f_output)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(2.7944, grad_fn=&lt;NllLoss2DBackward&gt;)
tensor(2.7944, grad_fn=&lt;NllLoss2DBackward&gt;)
</code></pre><h3 id="Note-1"><a href="#Note-1" class="headerlink" title="Note"></a>Note</h3><ul>
<li><code>nn.CrossEntropyLoss</code> 与 <code>F.cross_entropy</code> 计算结果是等价的。两个函数都结合了 <code>LogSoftmax</code> and <code>NLLLoss</code> 运算</li>
<li><a href="https://PyTorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss"><code>nn.CrossEntropyLoss</code></a> 的公式为：<script type="math/tex; mode=display">
\operatorname{loss}(\mathrm{x}, \text { class })=-\log \left(\frac{\exp (\mathrm{x}[\mathrm{class}])}{\sum_{\mathrm{j}} \exp (\mathrm{x}[\mathrm{j}])}\right)=-\mathrm{x}[\mathrm{class}]+\log \left(\sum_{\mathrm{j}} \exp (\mathrm{x}[\mathrm{j}])\right)</script>这与我们平时见到的多分类交叉熵损失函数有点不同，具体的推导过程见<a class="link"   href="https://www.cnblogs.com/marsggbo/p/10401215.html" >Pytorch里的CrossEntropyLoss详解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://PyTorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss" >Docs &gt; torch.nn &gt; CrossEntropyLoss<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://PyTorch.org/docs/stable/generated/torch.nn.BCELoss.html?highlight=bceloss#torch.nn.BCELoss" >Docs &gt; torch.nn &gt; BCELoss<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/marsggbo/p/10401215.html" >Pytorch里的CrossEntropyLoss详解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title>scp使用</title>
    <url>/2021/11/09/scp%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>有的时候本地下载好的文件需要上传到服务器上去，但是需要借助第三方软件，显得非常繁琐。因此就用了一下 <code>scp</code> 命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp local_file username@ip:remote_folder</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title>screen命令使用</title>
    <url>/2022/07/20/screen%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p><code>screen</code> 是一款Linux自带的多重视窗管理工具，功能稍微比<a class="link"   href="https://transformerswsz.github.io/2019/09/12/tmux%20-%20%E7%BB%88%E7%AB%AF%E5%A4%8D%E7%94%A8%E5%B7%A5%E5%85%B7/" >tmux<i class="fas fa-external-link-alt"></i></a>弱些。由于公司服务器上没有管理员权限，无法安装tmux，所以只能使用该命令了。在此记录下基本使用方法。</p>
<span id="more"></span>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><h3 id="新建窗口"><a href="#新建窗口" class="headerlink" title="新建窗口"></a>新建窗口</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">screen -S <span class="built_in">test</span></span><br></pre></td></tr></table></figure>
<h3 id="离开当前会话窗口"><a href="#离开当前会话窗口" class="headerlink" title="离开当前会话窗口"></a>离开当前会话窗口</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">screen -d <span class="built_in">test</span></span><br></pre></td></tr></table></figure>
<p>也可以使用快捷键，先按 <code>ctrl + a</code> ，再按 <code>d</code> 。</p>
<h3 id="列出所有的会话窗口"><a href="#列出所有的会话窗口" class="headerlink" title="列出所有的会话窗口"></a>列出所有的会话窗口</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">screen -<span class="built_in">ls</span></span><br></pre></td></tr></table></figure>
<img   src="/2022/07/20/screen%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/ls.png"  class="ls">
<h3 id="重新进入会话"><a href="#重新进入会话" class="headerlink" title="重新进入会话"></a>重新进入会话</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">screen -r <span class="built_in">test</span></span><br></pre></td></tr></table></figure>
<h3 id="杀死会话窗口"><a href="#杀死会话窗口" class="headerlink" title="杀死会话窗口"></a>杀死会话窗口</h3><ol>
<li>杀死进程：<code>kill -9 5469</code></li>
<li>清除缓存：<code>screen -wipe</code></li>
</ol>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/mchina/archive/2013/01/30/2880680.html" >linux screen 命令详解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>screen</tag>
      </tags>
  </entry>
  <entry>
    <title>set与list性能对比</title>
    <url>/2022/07/24/set%E4%B8%8Elist%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p>当集合中的数据量特别大时，要判断一个元素是否在该集合中，建议使用 <code>set</code> 而不是 <code>list</code> ，两种性能差异非常大。下面做一个测试：</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>
<h2 id="构造长度为-length-的数组"><a href="#构造长度为-length-的数组" class="headerlink" title="构造长度为 length 的数组"></a>构造长度为 <code>length</code> 的数组</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">construct_data</span>(<span class="params">length</span>):</span><br><span class="line">    l = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">        l.append(i)</span><br><span class="line">    random.shuffle(l)</span><br><span class="line">    <span class="keyword">return</span> l, <span class="built_in">set</span>(l)</span><br></pre></td></tr></table></figure>
<h2 id="测试-num-次"><a href="#测试-num-次" class="headerlink" title="测试 num 次"></a>测试 <code>num</code> 次</h2><h4 id="测试-list"><a href="#测试-list" class="headerlink" title="测试 list"></a>测试 <code>list</code></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">length, num = <span class="built_in">int</span>(<span class="number">1e6</span>), <span class="built_in">int</span>(<span class="number">1e4</span>)</span><br><span class="line">l, s = construct_data(length)</span><br><span class="line">start_l = time.time()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num)):</span><br><span class="line">    r = random.randint(<span class="number">0</span>, length-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> r <span class="keyword">in</span> l:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">end_l = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;test list time: &#123;&#125; seconds&quot;</span>.<span class="built_in">format</span>(end_l-start_l))</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 10000/10000 [02:52&lt;00:00, 58.00it/s]

test list time: 172.42421102523804 seconds
</code></pre><h4 id="测试-set"><a href="#测试-set" class="headerlink" title="测试 set"></a>测试 <code>set</code></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">start_s = time.time()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num)):</span><br><span class="line">    r = random.randint(<span class="number">0</span>, length-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> r <span class="keyword">in</span> s:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">end_s = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;test set time: &#123;&#125; seconds&quot;</span>.<span class="built_in">format</span>(end_s-start_s))</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 10000/10000 [00:00&lt;00:00, 343595.45it/s]

test set time: 0.03251051902770996 seconds
</code></pre><p>可以看到，<code>set</code> 的速度实在比 <code>list</code> 快很多。毕竟 <code>set</code> 底层用hash散列实现，查找一个元素理论上只需 <code>O(1)</code> 时间，而 <code>list</code> 则是遍历，需要 <code>O(n)</code> 时间。数据量小的时候，两者看不出差距，数据量稍微大点，差距非常明显。</p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>set</tag>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title>shell中2&gt;&amp;1的含义</title>
    <url>/2021/07/16/shell%E4%B8%AD2_&amp;1%E7%9A%84%E5%90%AB%E4%B9%89/</url>
    <content><![CDATA[<p>在做实验的过程中，经常会看到shell脚本里存在 <code>2&gt;&amp;1</code> 的指令组合，有点懵逼，在此记录一下。</p>
<span id="more"></span>
<h2 id="0-1-2"><a href="#0-1-2" class="headerlink" title="0  |  1  |  2"></a><code>0</code>  |  <code>1</code>  |  <code>2</code></h2><p>这三个数字是linux文件描述符。<br>|   数字   |  含义    |<br>| —— | —— |<br>|   0   |    stdin   |<br>|   1   |    stdout  |<br>|   2   |    stderr  |</p>
<p><code>1</code> 和 <code>2</code> 都是输出到屏幕设备上。</p>
<p>我们平时使用的：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> &gt; test.log</span><br></pre></td></tr></table></figure></p>
<p>等价于：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> 1&gt; test.log</span><br></pre></td></tr></table></figure>
<p>注意 <code>1&gt;</code> 是连在一块的。如果分开，那么写入文件的就是 <font color="red">Hello World 1</font> 。</p>
<h2 id="2-gt-amp-1"><a href="#2-gt-amp-1" class="headerlink" title="2&gt;&amp;1"></a><code>2&gt;&amp;1</code></h2><blockquote>
<p>将标准错误输出重定向到标准输出。</p>
</blockquote>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.stdout.write(<span class="string">&quot;this is stdout\n&quot;</span>)</span><br><span class="line">sys.stderr.write(<span class="string">&quot;this is stderr\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python test.py &gt; test.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
<p>表示将标准错误输出重定向到标准输出，标准输出重定向到 <code>test.log</code> 文件中。</p>
<p>在程序执行过程中，我们希望输出保存到文件中，如果有错误的信息也希保存到文件中，那么就使用上面的命令。</p>
<ul>
<li><code>python test.py &gt; test.log 2&gt;1</code> : 那么标准输出将重定向到 <code>test.log</code> ，而错误将输出到<font color="red">名字为 <code>1</code> 的文件中</font>。这里的 <code>&amp;</code> 可以理解为 <code>1</code> 的引用</li>
<li><code>python test.py &gt; test.log 2 &gt;&amp;1</code> : 那么标准输出将重定向到 <code>test.log</code> ，而错误将输出到屏幕上</li>
<li><code>python test.py &gt; test.log 2&gt;&amp; 1</code> : 等价于 <code>python test.py &gt; test.log 2&gt;&amp;1</code></li>
<li><code>python test.py 1&gt; test.log 2&gt; test.log</code> : 会存在如下两个问题：<ul>
<li><code>stdout</code> 会覆盖 <code>stderr</code></li>
<li><code>test.log</code> 会被打开两次，IO效率低下</li>
</ul>
</li>
</ul>
<h3 id="2-gt-amp-1-为什么放在末尾？"><a href="#2-gt-amp-1-为什么放在末尾？" class="headerlink" title="2&gt;&amp;1 为什么放在末尾？"></a><code>2&gt;&amp;1</code> 为什么放在末尾？</h3><p><code>python test.py &gt; test.log 2&gt;&amp;1</code> 从左往右来看 <code>stdin</code> 定向到 <code>test.log</code>，然后 <code>stderr</code> 定向到 <code>stdin</code>，等于说 <code>stderr</code> 也输入到了 <code>test.log</code> 中。</p>
<p>如果放在中间：<code>python test.py 2&gt;&amp;1 &gt;test.log</code> ，<code>stderr</code> 定向到 <code>stdin</code>，但此时 <code>stdin</code> 指向的是屏幕，所以 <code>stderr</code> 会输出到屏幕。执行到 <code>&gt;test.log</code> 的时候，<code>stdin</code> 定向到 <code>test.log</code>。所以 <code>test.log</code> 文件里只有 <code>this is stdout</code>。</p>
<h3 id="简写"><a href="#简写" class="headerlink" title="简写"></a>简写</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python test.py &gt; test.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
<p>可以简写成：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python test.py &gt;&amp; test.log</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/zhaominpro/article/details/82630528" >Linux shell中2&gt;&amp;1的含义解释<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>shell并发编程</title>
    <url>/2022/11/15/shell%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<p>实际上 <code>&amp;</code> 符号就表示将程序放入后台执行，从而实现多个程序并行。但由于机器资源有限，我们需要控制并发数量。下面是解决方案：</p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="built_in">id</span> = sys.argv[<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Processing task &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">id</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 最大并发数</span></span><br><span class="line">Thread_num=5</span><br><span class="line"><span class="comment"># 命名管道文件</span></span><br><span class="line">Tmp_fifo=/tmp/$$.fifo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建命名管道文件</span></span><br><span class="line"><span class="built_in">mkfifo</span> <span class="variable">$Tmp_fifo</span></span><br><span class="line"><span class="comment"># 用文件句柄(命名随意)打开管道文件</span></span><br><span class="line"><span class="built_in">exec</span> 1000&lt;&gt;<span class="variable">$Tmp_fifo</span></span><br><span class="line"><span class="built_in">rm</span> -f <span class="variable">$Tmp_fifo</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 控制并发数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `<span class="built_in">seq</span> <span class="variable">$Thread_num</span>`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="comment"># 向管道中放入最大并发数个行，供下面read读取</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;&quot;</span> &gt;&amp;1000</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..17&#125;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="comment"># 通过文件句柄读取行，当行取尽时，停止下一步（并发）</span></span><br><span class="line">        <span class="built_in">read</span> -u 1000</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="comment"># 业务代码</span></span><br><span class="line">                python main.py <span class="variable">$&#123;i&#125;</span></span><br><span class="line">                <span class="built_in">sleep</span> 10s</span><br><span class="line">        <span class="comment"># 一个并发执行后要想管道中在加入一个空行，供下次使用</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;&quot;</span> &gt;&amp;1000</span><br><span class="line">        &#125;&amp;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;END&quot;</span></span><br></pre></td></tr></table></figure>
<p>有 <code>17</code> 个任务，控制并发数量为 <code>5</code>。<code>wait</code> 表示等待所有后台进程结束，否则的话会出现如下情况：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Processing task 1</span><br><span class="line">Processing task 5</span><br><span class="line">Processing task 2</span><br><span class="line">Processing task 4</span><br><span class="line">Processing task 3</span><br><span class="line">Processing task 7</span><br><span class="line">Processing task 6</span><br><span class="line">Processing task 8</span><br><span class="line">Processing task 10</span><br><span class="line">Processing task 9</span><br><span class="line">Processing task 15</span><br><span class="line">Processing task 14</span><br><span class="line">Processing task 12</span><br><span class="line">Processing task 13</span><br><span class="line">Processing task 11</span><br><span class="line">END</span><br><span class="line">Processing task 16</span><br><span class="line">Processing task 17</span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/weixin_42170236/article/details/117821258" >shell脚本中多线程和控制并发数量<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>sh脚本运行过程中修改程序引发的问题</title>
    <url>/2022/08/12/sh%E8%84%9A%E6%9C%AC%E8%BF%90%E8%A1%8C%E8%BF%87%E7%A8%8B%E4%B8%AD%E4%BF%AE%E6%94%B9%E7%A8%8B%E5%BA%8F%E5%BC%95%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>在公司运行shell脚本（暂命名为A.sh）的时候，由于要跑多个应用，所以其依赖的其它shell脚本（暂命名为B.sh）都要经过不同的处理。当A.sh运行的时候（命令还没有走到运行B.sh）：</p>
<ul>
<li>修改A.sh里的任一命令时，会报错</li>
<li>修改B.sh里的任一命令时，不会影响A.sh的执行。当执行B.sh的时候，输出的就是更新后的B.sh对应的结果</li>
</ul>
<span id="more"></span>
<p>在编程语言中，不管是动态语言还是静态语言，程序运行前都会经过编译或者解释生成可执行文件，运行起来后修改源代码，都不会影响程序的正常运行。但shell脚本不同，它只是纯粹文本，有系统一行行读取命令执行。下面举例讨论：</p>
<h2 id="例1"><a href="#例1" class="headerlink" title="例1"></a>例1</h2><p><code>A.sh</code> 没有依赖其它脚本：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;start running&quot;</span></span><br><span class="line"><span class="built_in">sleep</span> 30s</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;end running&quot;</span></span><br></pre></td></tr></table></figure><br>当程序休眠30s的时候，修改 <code>A.sh</code> 为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;start running&quot;</span></span><br><span class="line"><span class="built_in">sleep</span> 30s</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;sleep end&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;end running&quot;</span></span><br></pre></td></tr></table></figure><br>这个时候程序会报错！<br><img   src="https://img-blog.csdnimg.cn/img_convert/819c68fdc6a719f382725b790d8d002a.webp?x-oss-process=image/format,png"  alt="error"></p>
<h2 id="例2"><a href="#例2" class="headerlink" title="例2"></a>例2</h2><p><code>A.sh</code> 依赖其它脚本：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;start running&quot;</span></span><br><span class="line"><span class="built_in">sleep</span> 30s</span><br><span class="line">sh B.sh</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;end running&quot;</span></span><br></pre></td></tr></table></figure></p>
<p><code>B.sh</code> 内容为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Here is B&quot;</span></span><br></pre></td></tr></table></figure></p>
<p>当程序休眠30s的时候，修改 <code>B.sh</code> 为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Here is B&quot;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Hello B&quot;</span></span><br></pre></td></tr></table></figure><br>这个时候程序的输出就会受到影响，结果为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start running</span><br><span class="line">Here is B</span><br><span class="line">Hello B</span><br><span class="line">end running</span><br></pre></td></tr></table></figure></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们在更改sh脚本的时候，需要极其小心，以免各应用的运行会互相污染。</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Bash</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>tf1.x实现张量的梯度反转</title>
    <url>/2024/10/17/tf1-x%E5%AE%9E%E7%8E%B0%E5%BC%A0%E9%87%8F%E7%9A%84%E6%A2%AF%E5%BA%A6%E5%8F%8D%E8%BD%AC/</url>
    <content><![CDATA[<p>tensorflow实现梯度反转的方法有两种：</p>
<span id="more"></span>
<h2 id="利用-tf-custom-gradient重写梯度函数"><a href="#利用-tf-custom-gradient重写梯度函数" class="headerlink" title="利用@tf.custom_gradient重写梯度函数"></a>利用<code>@tf.custom_gradient</code>重写梯度函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义反转梯度的操作</span></span><br><span class="line"><span class="meta">@tf.custom_gradient</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_reverse</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">grad</span>(<span class="params">dy</span>):</span><br><span class="line">        <span class="keyword">return</span> -dy  <span class="comment"># 反转梯度</span></span><br><span class="line">    <span class="keyword">return</span> x, grad</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量 w 和 x</span></span><br><span class="line">w = tf.Variable(<span class="number">2.0</span>)  <span class="comment"># 假设 w 的初始值为 2.0</span></span><br><span class="line">x = tf.Variable(<span class="number">3.0</span>)  <span class="comment"># 假设 x 的初始值为 3.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义运算 y = w * x</span></span><br><span class="line">y = w * x</span><br><span class="line">y_reversed = gradient_reverse(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求 y 关于 x 的梯度</span></span><br><span class="line">grad_x = tf.gradients(y, x)</span><br><span class="line">grad_x_reversed = tf.gradients(y_reversed, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># 计算 y 的值和 x 的梯度</span></span><br><span class="line">    y_value, grad_x_value, grad_x_reversed_value = sess.run([y, grad_x, grad_x_reversed])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y: &quot;</span>, y_value)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;x gradient: &quot;</span>, grad_x_value)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;x gradient reversed: &quot;</span>, grad_x_reversed_value)</span><br></pre></td></tr></table></figure>
<h2 id="巧用stop-gradient函数"><a href="#巧用stop-gradient函数" class="headerlink" title="巧用stop_gradient函数"></a>巧用<code>stop_gradient</code>函数</h2><p>以实现<a class="link"   href="https://transformerswsz.github.io/2024/09/24/DANN-GRL/" >DANN<i class="fas fa-external-link-alt"></i></a>为例，特征提取器记作<code>F</code>，域分类器记作<code>D</code>，那么<code>F</code>梯度反转的实现如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feat = F(x)</span><br><span class="line">loss = -D(F(x)) + <span class="number">2</span>*D(stop_gradient(F(x)))</span><br></pre></td></tr></table></figure></p>
<ul>
<li>在前向传播的过程中，<code>stop_gradient</code>不起作用，那么<code>loss = D(stop_gradient(F(x)))</code></li>
<li>在反向传播的过程中，<code>stop_gradient</code>起作用，那么<code>2*stop_gradient(F(x)</code>梯度为0，梯度计算就是<code>2D-D-F=D-F</code>，就实现了<code>F</code>的梯度反转</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/FesianXu/p/13283799.html" >在TensorFlow中自定义梯度的两种方法 <i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Gradient Reversal</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>thread_local用法</title>
    <url>/2022/10/25/thread-local%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<p><a class="link"   href="https://zhuanlan.zhihu.com/p/340201634" >C++11 thread_local用法<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>thread</tag>
      </tags>
  </entry>
  <entry>
    <title>tmux - 终端复用工具</title>
    <url>/2019/09/12/tmux%20-%20%E7%BB%88%E7%AB%AF%E5%A4%8D%E7%94%A8%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<p>当运行一个web应用时，我们希望在退出登录或关闭终端的时候，web应用仍然能够运行，这时候就要用到 <code>nohup</code> 。<code>nohup</code> 有个缺点就是它会将输出重定向到 <code>nohup.out</code> 文件，虽然也有其他方法能够在终端实时查看 <code>nohup</code> 输出，但重新登录服务器的时候，这些输出将会丢失。</p>
<span id="more"></span>
<p>同样的，当我们想在终端进行其他活动时，就必须打开一个新的终端，这显然不够greek。<br>基于上述两个痛点，tmux就派上用场了。tmux主要有如下三大功能：</p>
<ul>
<li>保护现场：即使命令行的工作只进行到一半，关闭终端后还可以重新进入到操作现场，继续工作。对于ssh远程连接而言，即使网络不稳定也没有关系，掉线后重新连接，可以直奔现场，之前运行中的任务，依旧在跑，就好像从来没有离开过一样；特别是在远程服务器上运行耗时的任务，tmux可以帮你一直保持住会话。如此一来，你就可以随时随地放心地进行移动办公，只要你附近的计算机装有tmux（没有你也可以花几分钟装一个），你就能继续刚才的工作。</li>
<li>分屏：tmux窗口中，新开的pane，默认进入到之前的路径，如果是ssh连接，登录状态也依旧保持着，如此一来，我就可以随意的增删pane，非常灵活。</li>
<li>会话共享：将tmux会话的地址分享给他人，这样他们就可以通过 SSH 接入该会话。</li>
</ul>
<h2 id="session-amp-window-amp-pane"><a href="#session-amp-window-amp-pane" class="headerlink" title="session &amp; window &amp; pane"></a>session &amp; window &amp; pane</h2><ul>
<li>一个tmux session（会话）可以包含多个window（窗口），窗口默认充满会话界面，因此这些窗口中可以运行相关性不大的任务。</li>
<li>一个window又可以包含多个pane（面板），窗口下的面板，都处于同一界面下，这些面板适合运行相关性高的任务，以便同时观察到它们的运行情况。</li>
</ul>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo yum install -y tmux</span><br></pre></td></tr></table></figure>
<h3 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h3><h4 id="新建会话"><a href="#新建会话" class="headerlink" title="新建会话"></a>新建会话</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux new -s <span class="built_in">test</span> <span class="comment"># 新建一个名叫test的会话</span></span><br></pre></td></tr></table></figure>
<h4 id="断开当前会话"><a href="#断开当前会话" class="headerlink" title="断开当前会话"></a>断开当前会话</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux detach</span><br></pre></td></tr></table></figure>
<p>也可以使用快捷键，先按 <code>ctrl + b</code> ，再按 <code>d</code> 。</p>
<h4 id="进入之前的会话"><a href="#进入之前的会话" class="headerlink" title="进入之前的会话"></a>进入之前的会话</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux a -t <span class="built_in">test</span> <span class="comment"># 进入名叫test的会话</span></span><br></pre></td></tr></table></figure>
<h4 id="关闭会话"><a href="#关闭会话" class="headerlink" title="关闭会话"></a>关闭会话</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux kill-session -t <span class="built_in">test</span> <span class="comment"># 关闭test会话</span></span><br><span class="line">tmux kill-server <span class="comment"># 关闭服务器，所有会话都将关闭</span></span><br></pre></td></tr></table></figure>
<h4 id="查看会话"><a href="#查看会话" class="headerlink" title="查看会话"></a>查看会话</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux <span class="built_in">ls</span></span><br></pre></td></tr></table></figure>
<p>也可以使用快捷键 <code>ctrl + b + s</code> ，此时tmux将打开会话列表。按上下键可切换会话，按左右键可收起或展开会话。</p>
<h2 id="tmux快捷键"><a href="#tmux快捷键" class="headerlink" title="tmux快捷键"></a>tmux快捷键</h2><ul>
<li>系统指令</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">前缀</th>
<th style="text-align:center">指令</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>?</code></td>
<td style="text-align:center">显示快捷键帮助文档</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>d</code></td>
<td style="text-align:center">断开当前会话</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>D</code></td>
<td style="text-align:center">选择要断开的会话</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>ctrl+z</code></td>
<td style="text-align:center">挂起当前会话</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>r</code></td>
<td style="text-align:center">强制重载当前会话</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>s</code></td>
<td style="text-align:center">显示会话列表用于选择并切换</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>:</code></td>
<td style="text-align:center">进入命令行模式</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>[</code></td>
<td style="text-align:center">进入复制模式</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>]</code></td>
<td style="text-align:center">粘贴复制模式中复制的文本</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>~</code></td>
<td style="text-align:center">列出提示信息缓存</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>窗口(window)命令</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">前缀</th>
<th style="text-align:center">指令</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>c</code></td>
<td style="text-align:center">新建窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>&amp;</code></td>
<td style="text-align:center">关闭当前窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>0~9</code></td>
<td style="text-align:center">切换到指定窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>p</code></td>
<td style="text-align:center">切换到上一窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>n</code></td>
<td style="text-align:center">切换到下一窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>w</code></td>
<td style="text-align:center">打开窗口列表且切换窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>,</code></td>
<td style="text-align:center">重命名当前窗口</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>.</code></td>
<td style="text-align:center">修改当前窗口编号</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>f</code></td>
<td style="text-align:center">快速定位到窗口</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>面板(pane)指令</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">前缀</th>
<th style="text-align:center">指令</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>&quot;</code></td>
<td style="text-align:center">当前面板上下一分为二，下侧新建面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>%</code></td>
<td style="text-align:center">当前面板左右一分为二，右侧新建面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>x</code></td>
<td style="text-align:center">关闭当前面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>z</code></td>
<td style="text-align:center">最大化当前面板，再重复一次按键后恢复正常</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>!</code></td>
<td style="text-align:center">将当前面板移动到新的窗口打开（原窗口中存在两个及以上面板有效）</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>;</code></td>
<td style="text-align:center">切换到最后一次使用的面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>q</code></td>
<td style="text-align:center">显示面板编号，在编号消失前输入对应的数字可切换到相应的面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>&#123;</code></td>
<td style="text-align:center">向前置换当前面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>&#125;</code></td>
<td style="text-align:center">向后置换当前面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>ctrl+o</code></td>
<td style="text-align:center">顺时针旋转当前窗口中的所有面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center">方向键</td>
<td style="text-align:center">移动光标切换面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>o</code></td>
<td style="text-align:center">选择下一面板</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>space</code></td>
<td style="text-align:center">在自带的面板布局中循环切换</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>alt+方向键</code></td>
<td style="text-align:center">以5个单元格为单位调整当前面板边缘</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>ctrl+方向键</code></td>
<td style="text-align:center">以1个单元格为单位调整当前面板边缘（Mac下被系统快捷键覆盖）</td>
</tr>
<tr>
<td style="text-align:center"><code>ctrl+b</code></td>
<td style="text-align:center"><code>t</code></td>
<td style="text-align:center">显示时钟</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p>关于tmux的高阶应用，如个性化配置、保存会话、会话共享等，请参照：</p>
<ul>
<li><a class="link"   href="http://louiszhai.github.io/2017/09/30/tmux/" >Tmux使用手册<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/kaiye/p/6275207.html" >十分钟学会 tmux<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>top命令</title>
    <url>/2024/01/23/top%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>在linux运维中，经常用到 <code>top</code> 命令，详细介绍一下：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/04eb5801268567e6bf9c714fc114282d7d3d36f8/image.4kkhwptk8tg0.png"  alt="top"></p>
<span id="more"></span>
<h2 id="字段介绍"><a href="#字段介绍" class="headerlink" title="字段介绍"></a>字段介绍</h2><ul>
<li><p>时间：</p>
<ul>
<li>当前时间</li>
<li>系统运行时间</li>
<li>当前登录用户数</li>
<li>系统负载，即任务队列的平均长度。三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值</li>
</ul>
</li>
<li><p>任务：</p>
<ul>
<li>进程总数</li>
<li>正在运行的进程数</li>
<li>睡眠的进程数</li>
<li>停止的进程数</li>
<li>僵尸进程数</li>
</ul>
</li>
<li><p>CPU：</p>
<ul>
<li>us用户空间占用CPU百分比</li>
<li>sy内核空间占用CPU百分比</li>
<li>ni用户进程空间内改变过优先级的进程占用CPU百分比</li>
<li>id空闲CPU百分比</li>
<li>wa 等待输入输出的CPU时间百分比</li>
<li>hi硬件CPU中断占用百分比</li>
<li>si软中断占用百分比</li>
<li>st虚拟机占用百分比</li>
</ul>
</li>
<li><p>内存</p>
<ul>
<li>物理内存总量</li>
<li>使用的物理内存总量</li>
<li>空闲内存总量</li>
<li>用作内核缓存的内存量</li>
</ul>
</li>
<li><p>交换区</p>
<ul>
<li>交换区总量</li>
<li>使用的交换区总量</li>
<li>空闲交换区总量</li>
<li>缓冲的交换区总量,内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小,相应的内存再次被换出时可不必再对交换区写入</li>
</ul>
</li>
</ul>
<h2 id="指定字段排序查看"><a href="#指定字段排序查看" class="headerlink" title="指定字段排序查看"></a>指定字段排序查看</h2><ul>
<li>按照内存占用大小倒序：Shift+M</li>
<li>按照CPU占用大小倒序：Shift+P</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/ggjucheng/archive/2012/01/08/2316399.html" >linux的top命令参数详解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>top</tag>
      </tags>
  </entry>
  <entry>
    <title>v-model &amp; v-bind</title>
    <url>/2018/05/08/v-model%20&amp;%20v-bind/</url>
    <content><![CDATA[<p>记录一下两种指令的用法。</p>
<span id="more"></span>
<h2 id="v-model"><a href="#v-model" class="headerlink" title="v-model"></a>v-model</h2><p>我们可以使用 <code>v-model</code> 指令在 <code>&lt;input&gt;</code> (<code>&lt;input&gt;</code> 标签有多种类型，如 <code>button、select</code> 等等)及 <code>&lt;textarea&gt;</code> 元素上进行双向数据绑定。但 <code>v-model</code> 本质上不过是语法糖。它负责监听用户的输入事件以更新数据，并对一些极端场景进行一些特殊处理。</p>
<p><code>v-model</code> 会忽略所有表单元素的 <code>value</code>、<code>checked</code>、<code>selected</code> 特性的初始值而总是将 Vue 实例的数据作为数据来源。你应该通过 JavaScript 在组件的 <code>data</code>选项中声明初始值：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;ie=edge&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cdn.jsdelivr.net/npm/vue@2.5.16/dist/vue.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>vue<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;app&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">input</span> <span class="attr">v-model</span>=<span class="string">&quot;message&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span>&gt;</span>The input value is : &#123;&#123;message&#125;&#125;<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span><br><span class="line"><span class="language-javascript">        <span class="keyword">var</span> app = <span class="keyword">new</span> <span class="title class_">Vue</span>(&#123;</span></span><br><span class="line"><span class="language-javascript">            <span class="attr">el</span>: <span class="string">&#x27;#app&#x27;</span>,</span></span><br><span class="line"><span class="language-javascript">            <span class="attr">data</span>: &#123;</span></span><br><span class="line"><span class="language-javascript">                <span class="attr">message</span>: <span class="string">&#x27;Hello Word!&#x27;</span></span></span><br><span class="line"><span class="language-javascript">            &#125;</span></span><br><span class="line"><span class="language-javascript">        &#125;)</span></span><br><span class="line"><span class="language-javascript">    </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>那么输入框的初始值就是 <font color="green">Hello World!</font> 。</p>
<p>实际上，由于<code>v-model</code> 只是语法糖， <code>&lt;input v-model=&quot;message&quot;&gt;</code> 与下面的两行代码是一致的：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">v-bind:value</span>=<span class="string">&quot;message&quot;</span> <span class="attr">v-on:input</span>=<span class="string">&quot;message = $event.target.value&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">:value</span>=<span class="string">&quot;message&quot;</span> @<span class="attr">input</span>=<span class="string">&quot;message = $event.target.value&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="v-bind"><a href="#v-bind" class="headerlink" title="v-bind"></a>v-bind</h2><p>它的用法是后面加冒号，跟上html元素的属性，例如：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">v-bind:class</span>=<span class="string">&quot;someclass&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>如果不加 <code>v-bind</code> 那么 <code>someclass</code> 就是个常量，没有任何动态数据参与。当加上 <code>v-bind</code> 之后，它的值 <code>someclass</code> 不是字符串，而是vue实例对应的 <code>data.someclass</code> 这个变量。具体传入变量类型可参考 <a class="link"   href="https://cn.vuejs.org/v2/guide/class-and-style.html" >Class与Style绑定<i class="fas fa-external-link-alt"></i></a> 。这非常适合用在通过css来实现动画效果的场合。除了class，其他大部分html原始的属性都可以通过这种方式来绑定，而且为了方便，它可以直接缩写成冒号形式，例如：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> app = <span class="title class_">Vue</span>(&#123;  </span><br><span class="line">    <span class="attr">el</span>: <span class="string">&#x27;#app&#x27;</span>,  </span><br><span class="line">    <span class="attr">template</span>: <span class="string">&#x27;&lt;img  : src=&quot;remoteimgurl&quot; &gt;&#x27;</span>,  </span><br><span class="line">    <span class="attr">data</span>: &#123;    <span class="attr">src</span>: <span class="string">&#x27;&#x27;</span>,  &#125;,  </span><br><span class="line">    <span class="title function_">beforeMount</span>(<span class="params"></span>) &#123;    <span class="title function_">fetch</span>(...).<span class="title function_">then</span>(...).<span class="title function_">then</span>(<span class="function"><span class="params">res</span> =&gt;</span> <span class="variable language_">this</span>.<span class="property">src</span> = res.<span class="property">remoteimgurl</span>) &#125;,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>上面这段代码中，默认情况下 <code>data.src</code> 是空字符串，也就说不会有图片显示出来，但是当从远端获取到图片地址之后，更新了 <code>data.src</code>，图片就会显示出来了。</p>
<h2 id="v-bind与v-model区别"><a href="#v-bind与v-model区别" class="headerlink" title="v-bind与v-model区别"></a>v-bind与v-model区别</h2><p>有一些情况我们需要 <code>v-bind</code> 和 <code>v-model</code> 一起使用：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">:value</span>=<span class="string">&quot;name&quot;</span> <span class="attr">v-model</span>=<span class="string">&quot;body&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>data.name</code> 和 <code>data.body</code>，到底谁跟着谁变呢？甚至，它们会不会产生冲突呢？</p>
<p>实际上它们的关系和上面的阐述是一样的，<code>v-bind</code> 产生的效果不含有双向绑定，所以 <code>:value</code> 的效果就是让 input的value属性值等于 <code>data.name</code> 的值，而 <code>v-model</code> 的效果是使input和 <code>data.body</code> 建立双向绑定，因此首先 <code>data.body</code> 的值会给input的value属性，其次，当input中输入的值发生变化的时候，<code>data.body</code> 还会跟着改变。</p>
<p>上文提到过下面两句是等价的：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">v-model</span>=<span class="string">&quot;message&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">v-bind:value</span>=<span class="string">&quot;message&quot;</span> <span class="attr">v-on:input</span>=<span class="string">&quot;message = $event.target.value&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure>
<p>那么 <code>v-model</code> 其实就是 <code>v-bind</code> 和 <code>v-on</code> 的语法糖。</p>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>JavaScript</tag>
        <tag>Vue</tag>
      </tags>
  </entry>
  <entry>
    <title>wsl2中volume挂载位置的问题</title>
    <url>/2022/09/27/wsl2%E4%B8%ADvolume%E6%8C%82%E8%BD%BD%E4%BD%8D%E7%BD%AE%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>本人电脑环境：win10 + wsl2(Ubuntu 18.04.6 LTS)</p>
<p>运行命令：<code>docker inspect testvol</code></p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;CreatedAt&quot;</span>: <span class="string">&quot;2022-09-27T15:15:09Z&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Driver&quot;</span>: <span class="string">&quot;local&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Labels&quot;</span>: &#123;&#125;,</span><br><span class="line">        <span class="string">&quot;Mountpoint&quot;</span>: <span class="string">&quot;/var/lib/docker/volumes/testvol/_data&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;testvol&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Options&quot;</span>: &#123;&#125;,</span><br><span class="line">        <span class="string">&quot;Scope&quot;</span>: <span class="string">&quot;local&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>虽然docker给出了挂载的数据位置，但实际上该路径并不存在。之前以为是 <code>sudo</code> 权限的问题，还是不能解决。</p>
<p>经过一番摸索后，发现wsl2其实将该路径映射到了windows的路径上。不同的docker版本所映射的路径有所不同：</p>
<ul>
<li>Docker Engine v20.10.17：<code>\\wsl$\docker-desktop-data\data\docker\volumes</code></li>
</ul>
<img   src="/2022/09/27/wsl2%E4%B8%ADvolume%E6%8C%82%E8%BD%BD%E4%BD%8D%E7%BD%AE%E7%9A%84%E9%97%AE%E9%A2%98/example.jpg"  class="example">
<ul>
<li>Docker Engine v19.03: <code>\\wsl$\docker-desktop-data\version-pack-data\community\docker\volumes\</code></li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://stackoverflow.com/questions/43181654/locating-data-volumes-in-docker-desktop-windows/64418064#64418064" >Locating data volumes in Docker Desktop (Windows)<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qqhappy8/article/details/106819429" >Docker Desktop for windows原理<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Volume</tag>
      </tags>
  </entry>
  <entry>
    <title>ε-greedy策略</title>
    <url>/2025/09/10/%CE%B5-greedy%E7%AD%96%E7%95%A5/</url>
    <content><![CDATA[<p><strong>ε-greedy（Epsilon-Greedy）</strong> 是强化学习中最重要、最基础的概念之一，它完美地解决了<strong>探索（Exploration）</strong> 与<strong>利用（Exploitation）</strong> 之间的权衡问题。</p>
<span id="more"></span>
<p>该行为策略的流程如下：</p>
<pre class="mermaid">flowchart TD
    Start[开始] --> Input[生成一个随机数<br>0<=rand<=1]
    Input --> Condition{rand<ε?}
    Condition -->|是| Process[进行探索:<br>完全随机选择一个动作]
    Condition -->|否| Error[进行利用:<br>选择当前Q值最高的动作]
    Process --> Stop[更新状态和Q值]
    Error --> Stop</pre>

<ul>
<li>在需要选择动作时，智能体Agent以 <strong>1 - ε</strong> 的概率选择<strong>当前认为最好的动作</strong>（利用）。</li>
<li>以 <strong>ε</strong> 的概率<strong>完全随机选择一个动作</strong>（探索）。</li>
</ul>
<h3 id="ε-greedy的优势"><a href="#ε-greedy的优势" class="headerlink" title="ε-greedy的优势"></a>ε-greedy的优势</h3><ol>
<li><p>避免局部最优（Local Optimum）：</p>
<ul>
<li>如果一开始就锁定某个看似不错的动作，可能会错过真正全局最优（Global Optimum）的动作。随机探索提供了跳出局部最优的可能性。</li>
<li>示例：一个机器人面前有两个按钮，按左按钮每次给1块钱，按右按钮有10%的概率给100块钱。如果一开始按左按钮得了一次钱，它就认为左按钮好，再也不按右按钮，从而错过了获得100块的机会。</li>
</ul>
</li>
<li><p>动态环境适应：</p>
<ul>
<li>环境可能发生变化，之前最好的动作可能不再是最优。持续的探索能帮助智能体发现这些变化并及时调整策略。</li>
</ul>
</li>
</ol>
<h3 id="ε-值的选择与衰减"><a href="#ε-值的选择与衰减" class="headerlink" title="ε 值的选择与衰减"></a>ε 值的选择与衰减</h3><ul>
<li><p>ε 的选择：</p>
<ul>
<li>ε 太大（如 0.5）：探索过于频繁，智能体像个无头苍蝇，学不到知识，策略收敛慢。</li>
<li>ε 太小（如 0.01）：探索不足，容易过早陷入局部最优，无法发现更好的策略。</li>
<li>常用值：<code>0.1</code> 是一个很好的起点。</li>
</ul>
</li>
<li><p>ε 的衰减（Decay）：</p>
<ul>
<li>在训练初期，智能体对环境一无所知，需要<strong>大量探索</strong>，因此 ε 可以设置得高一些。</li>
<li>随着训练进行，智能体越来越有经验，Q表越来越准，就应该<strong>逐渐减少探索，增加利用</strong>，收敛到最优策略。</li>
<li>实现方法：<code>ε = max(ε_min, ε * decay_rate)</code> 或 <code>ε = 0.1 * (0.99)^episode</code></li>
</ul>
</li>
</ul>
<h3 id="ε-greedy代码示例"><a href="#ε-greedy代码示例" class="headerlink" title="ε-greedy代码示例"></a>ε-greedy代码示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">EPSILON_START = <span class="number">1.0</span>     <span class="comment"># 初始探索概率 (100% 探索)</span></span><br><span class="line">EPSILON_END = <span class="number">0.01</span>      <span class="comment"># 最小探索概率 (1% 探索)</span></span><br><span class="line">EPSILON_DECAY = <span class="number">0.995</span>    <span class="comment"># 衰减率 (每轮乘以0.995)</span></span><br><span class="line">current_epsilon = EPSILON_START</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(TOTAL_EPISODES):</span><br><span class="line">    state = env.reset()  <span class="comment"># 重置环境，获取初始状态</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="comment"># ε-greedy 动作选择 (支持衰减)</span></span><br><span class="line">        <span class="keyword">if</span> random() &lt; current_epsilon:</span><br><span class="line">            action = random_choice(ACTIONS)  <span class="comment"># 随机探索</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = argmax(q_table[state])  <span class="comment"># 选择当前最优动作</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 执行动作，获取新状态和奖励</span></span><br><span class="line">        next_state, reward, done = env.step(action)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Q-Learning 更新</span></span><br><span class="line">        q_table[state][action] += ALPHA * (reward + GAMMA * <span class="built_in">max</span>(q_table[next_state]) - q_table[state][action])</span><br><span class="line">        </span><br><span class="line">        state = next_state  <span class="comment"># 更新状态</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ε 衰减 (指数衰减)</span></span><br><span class="line">    current_epsilon = <span class="built_in">max</span>(EPSILON_END, current_epsilon * EPSILON_DECAY)</span><br><span class="line">    <span class="comment"># 线性衰减</span></span><br><span class="line">    <span class="comment"># current_epsilon = max(EPSILON_END, EPSILON_START - episode * (EPSILON_START - EPSILON_END) / TOTAL_EPISODES)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 可选：打印当前ε值</span></span><br><span class="line">    <span class="keyword">if</span> episode % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Episode: <span class="subst">&#123;episode&#125;</span>, Current ε: <span class="subst">&#123;current_epsilon:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>ε-greedy</tag>
      </tags>
  </entry>
  <entry>
    <title>zsh配置环境变量</title>
    <url>/2022/03/10/zsh%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/</url>
    <content><![CDATA[<p>MacOS现在默认的shell为zsh了，这里以配置node环境变量为例：</p>
<span id="more"></span>
<ol>
<li>打开 <code>~/.zshrc</code></li>
<li>输入如下内容：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">NODE_ENV=~/opt/node/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$NODE_ENV</span>:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure></li>
<li><code>source ~/.zshrc</code></li>
</ol>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>如果只是输入：<code>export NODE_ENV=~/opt/node/bin</code> ，那么终端还是不能识别 <code>node</code> 命令，只能输出 <code>echo $NODE_ENV</code> ，必须要把 <code>NODE_ENV</code> 加入到 <code>PATH</code> 中。</p>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>zsh</tag>
      </tags>
  </entry>
  <entry>
    <title>三体中的常见名词</title>
    <url>/2024/11/04/%E4%B8%89%E4%BD%93%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81%E5%90%8D%E8%AF%8D/</url>
    <content><![CDATA[<p>记录下三体小说里的常见名词：</p>
<span id="more"></span>
<ul>
<li>降临派：对人类的本性绝望的人群，他们认为人类该被彻底毁灭，由三体人来重建地球</li>
<li>拯救派：希望拯救三体人于水火之中，甚至愿意三体人移民地球和人类和谐相处</li>
<li>幸存派：为“侵略者”三体人服务的人，希望在他们降临地球后，自己的后代能够生存下去（可以理解为汉奸）</li>
<li><p>洛希极限：当一个较小的天体（如卫星或彗星）接近一个较大的天体（如行星或恒星）时，如果距离小于某个临界值，较小的天体将因为较大的天体的潮汐力而被撕裂。这个临界距离就是洛希极限</p>
<ul>
<li>潮汐力：由于引力场的梯度（即引力在空间中的变化）产生的力。当一个天体接近另一个更大的天体时，靠近大天体的一侧受到的引力比远离的一侧更强，这种引力差异会导致小天体被拉伸。如果距离足够近，潮汐力会超过小天体自身的引力，导致小天体被撕裂</li>
<li>洛希极限的计算公式：$d = R \cdot \left( 2 \cdot \frac{\rho_M}{\rho_m} \right)^{1/3}$<ul>
<li>$R$：大天体的半径</li>
<li>$\rho_M$：大天体的密度</li>
<li>$\rho_m$：小天体的密度</li>
</ul>
</li>
</ul>
</li>
<li><p>费米悖论：一个关于外星文明存在的著名矛盾问题，由诺贝尔奖得主、物理学家恩里科·费米在1950年提出。它的核心问题是：如果宇宙中存在大量可能的外星文明，为什么我们还没有发现任何证据？</p>
</li>
</ul>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>三体</tag>
      </tags>
  </entry>
  <entry>
    <title>三体问题</title>
    <url>/2025/06/09/%E4%B8%89%E4%BD%93%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>从物理学角度，三体问题之所以不稳定，是因为三个天体在万有引力作用下相互作用，形成一个非线性耦合系统。我们可以从牛顿经典力学出发，列出具体的运动方程，并说明为何这个系统本质上是混沌的，无法得到一般解析解。</p>
<span id="more"></span>
<h2 id="🧭-问题设定：三体系统的基本假设"><a href="#🧭-问题设定：三体系统的基本假设" class="headerlink" title="🧭 问题设定：三体系统的基本假设"></a>🧭 问题设定：三体系统的基本假设</h2><p>假设我们有三个质量分别为 $m_1, m_2, m_3$ 的天体，位置向量分别为：</p>
<ul>
<li>$\mathbf{r}_1(t)$</li>
<li>$\mathbf{r}_2(t)$</li>
<li>$\mathbf{r}_3(t)$</li>
</ul>
<p>在牛顿引力定律下，每个天体受到其余两个天体的引力，总加速度由牛顿第二定律（$\mathbf{F} = m\mathbf{a}$）给出。</p>
<hr>
<h2 id="📐-一般形式的运动方程（牛顿引力-牛顿第二定律）"><a href="#📐-一般形式的运动方程（牛顿引力-牛顿第二定律）" class="headerlink" title="📐 一般形式的运动方程（牛顿引力 + 牛顿第二定律）"></a>📐 一般形式的运动方程（牛顿引力 + 牛顿第二定律）</h2><p>对第一个天体 $m_1$，其加速度为：</p>
<script type="math/tex; mode=display">
m_1 \frac{d^2 \mathbf{r}_1}{dt^2} = G \frac{m_1 m_2}{|\mathbf{r}_2 - \mathbf{r}_1|^3} (\mathbf{r}_2 - \mathbf{r}_1) + G \frac{m_1 m_3}{|\mathbf{r}_3 - \mathbf{r}_1|^3} (\mathbf{r}_3 - \mathbf{r}_1)</script><p>类似地，对 $m_2, m_3$ 分别有：</p>
<script type="math/tex; mode=display">
m_2 \frac{d^2 \mathbf{r}_2}{dt^2} = G \frac{m_2 m_1}{|\mathbf{r}_1 - \mathbf{r}_2|^3} (\mathbf{r}_1 - \mathbf{r}_2) + G \frac{m_2 m_3}{|\mathbf{r}_3 - \mathbf{r}_2|^3} (\mathbf{r}_3 - \mathbf{r}_2)</script><script type="math/tex; mode=display">
m_3 \frac{d^2 \mathbf{r}_3}{dt^2} = G \frac{m_3 m_1}{|\mathbf{r}_1 - \mathbf{r}_3|^3} (\mathbf{r}_1 - \mathbf{r}_3) + G \frac{m_3 m_2}{|\mathbf{r}_2 - \mathbf{r}_3|^3} (\mathbf{r}_2 - \mathbf{r}_3)</script><hr>
<h2 id="⚠️-为什么不稳定？"><a href="#⚠️-为什么不稳定？" class="headerlink" title="⚠️ 为什么不稳定？"></a>⚠️ 为什么不稳定？</h2><h3 id="1-非线性耦合微分方程组"><a href="#1-非线性耦合微分方程组" class="headerlink" title="1. 非线性耦合微分方程组"></a>1. 非线性耦合微分方程组</h3><p>这三组式子彼此高度耦合：每个加速度都依赖于另外两个天体的位置，而且这种依赖是非线性（距离的三次方在分母）。</p>
<p>这是典型的非线性二阶常微分方程组，形式上如下：</p>
<script type="math/tex; mode=display">
\frac{d^2 \mathbf{r}_i}{dt^2} = \sum_{j \ne i} G m_j \frac{\mathbf{r}_j - \mathbf{r}_i}{|\mathbf{r}_j - \mathbf{r}_i|^3}, \quad i = 1,2,3</script><p>这种系统存在如下三大特性：</p>
<ul>
<li>不可分解成单体运动 + 微扰项</li>
<li>无法通过代数方法求出通解</li>
<li>随时间演化会展现出对初始条件的极度敏感</li>
</ul>
<hr>
<h3 id="2-初始条件敏感性（混沌特征）"><a href="#2-初始条件敏感性（混沌特征）" class="headerlink" title="2. 初始条件敏感性（混沌特征）"></a>2. 初始条件敏感性（混沌特征）</h3><p>在数值模拟中，即使两个三体系统初始位置差异为 $10^{-10}$ 米，几十个周期后轨道会完全不同。</p>
<p>这是混沌系统的定义特征：</p>
<blockquote>
<p>初始条件微小扰动会被放大，导致长期行为完全不可预测。</p>
</blockquote>
<hr>
<h3 id="3-守恒定律约束不足以稳定系统"><a href="#3-守恒定律约束不足以稳定系统" class="headerlink" title="3. 守恒定律约束不足以稳定系统"></a>3. 守恒定律约束不足以稳定系统</h3><p>虽然系统满足以下守恒定律：</p>
<ul>
<li><p>总动量守恒：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^3 m_i \frac{d \mathbf{r}_i}{dt} = \text{常量}</script></li>
<li><p>总能量守恒：</p>
<script type="math/tex; mode=display">
E = \sum_{i=1}^3 \frac{1}{2} m_i v_i^2 - \sum_{i<j} \frac{G m_i m_j}{|\mathbf{r}_i - \mathbf{r}_j|}</script></li>
</ul>
<p>但这些守恒量（常数）无法限制轨道在高维空间的发散演化，只是提供一些“整体约束”。</p>
<h2 id="📉-所以为什么不能稳定运行？"><a href="#📉-所以为什么不能稳定运行？" class="headerlink" title="📉 所以为什么不能稳定运行？"></a>📉 所以为什么不能稳定运行？</h2><p>从物理角度总结三点：</p>
<ol>
<li>多体引力是非线性且时变的力学系统；</li>
<li>无解析解（除极少对称情况），必须数值求解；</li>
<li>即使数值解，也极度依赖初始条件，最终表现为混沌轨道；</li>
</ol>
<h3 id="🎯-实际结果是什么？"><a href="#🎯-实际结果是什么？" class="headerlink" title="🎯 实际结果是什么？"></a>🎯 实际结果是什么？</h3><p>小说中三体行星文明面临的核心困境——无法预测三颗恒星的运动。正是这个真实的物理问题，使得三体文明的“乱纪元”和“恒纪元”交替成为必然，也解释了为什么他们需要寻找新的家园。数值模拟证实了这种不稳定性是三体系统的内在属性，而非外部干扰的结果。</p>
<h2 id="三体运动的模拟动画"><a href="#三体运动的模拟动画" class="headerlink" title="三体运动的模拟动画"></a>三体运动的模拟动画</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.animation <span class="keyword">import</span> FuncAnimation</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</span><br><span class="line"><span class="keyword">from</span> matplotlib.widgets <span class="keyword">import</span> Button</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置中文字体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>, <span class="string">&#x27;DejaVu Sans&#x27;</span>]</span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ThreeBodySystem</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, masses, positions, velocities, G=<span class="number">1.0</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        三体系统</span></span><br><span class="line"><span class="string">        masses: [m1, m2, m3] 三个天体的质量</span></span><br><span class="line"><span class="string">        positions: [[x1,y1], [x2,y2], [x3,y3]] 初始位置</span></span><br><span class="line"><span class="string">        velocities: [[vx1,vy1], [vx2,vy2], [vx3,vy3]] 初始速度</span></span><br><span class="line"><span class="string">        G: 引力常数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.masses = np.array(masses)</span><br><span class="line">        self.positions = np.array(positions, dtype=<span class="built_in">float</span>)</span><br><span class="line">        self.velocities = np.array(velocities, dtype=<span class="built_in">float</span>)</span><br><span class="line">        self.G = G</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 轨迹历史记录 - 为每个天体单独记录</span></span><br><span class="line">        self.trail_length = <span class="number">800</span></span><br><span class="line">        self.trails = [deque(maxlen=self.trail_length) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化轨迹记录</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            self.trails[i].append([self.positions[i][<span class="number">0</span>], self.positions[i][<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">        self.dt = <span class="number">0.005</span>  <span class="comment"># 基础时间步长</span></span><br><span class="line">        self.speed_multiplier = <span class="number">1.0</span>  <span class="comment"># 速度倍数</span></span><br><span class="line">        self.time = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 天体属性</span></span><br><span class="line">        self.colors = [<span class="string">&#x27;#FF6B35&#x27;</span>, <span class="string">&#x27;#004E89&#x27;</span>, <span class="string">&#x27;#00A86B&#x27;</span>]  <span class="comment"># 橙色、深蓝、绿色</span></span><br><span class="line">        self.names = [<span class="string">&#x27;恒星Alpha&#x27;</span>, <span class="string">&#x27;恒星Beta&#x27;</span>, <span class="string">&#x27;恒星Gamma&#x27;</span>]</span><br><span class="line">        self.base_sizes = [<span class="number">300</span>, <span class="number">250</span>, <span class="number">280</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 统计数据记录</span></span><br><span class="line">        self.energy_history = deque(maxlen=<span class="number">500</span>)</span><br><span class="line">        self.distance_history = &#123;<span class="string">&#x27;AB&#x27;</span>: deque(maxlen=<span class="number">500</span>),</span><br><span class="line">                                 <span class="string">&#x27;AC&#x27;</span>: deque(maxlen=<span class="number">500</span>),</span><br><span class="line">                                 <span class="string">&#x27;BC&#x27;</span>: deque(maxlen=<span class="number">500</span>)&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_speed_multiplier</span>(<span class="params">self, multiplier</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;设置速度倍数&quot;&quot;&quot;</span></span><br><span class="line">        self.speed_multiplier = <span class="built_in">max</span>(<span class="number">0.1</span>, <span class="built_in">min</span>(<span class="number">10.0</span>, multiplier))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_gravitational_forces</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;计算每个天体受到的引力&quot;&quot;&quot;</span></span><br><span class="line">        forces = np.zeros_like(self.positions)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    <span class="comment"># 位置差矢量: 从i指向j</span></span><br><span class="line">                    r_vec = self.positions[j] - self.positions[i]</span><br><span class="line">                    r_magnitude = np.linalg.norm(r_vec)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 避免数值奇点</span></span><br><span class="line">                    <span class="keyword">if</span> r_magnitude &gt; <span class="number">1e-8</span>:</span><br><span class="line">                        <span class="comment"># 万有引力: F = G*m1*m2/r^2 * 单位矢量</span></span><br><span class="line">                        force_magnitude = self.G * self.masses[i] * self.masses[j] / (r_magnitude ** <span class="number">3</span>)</span><br><span class="line">                        forces[i] += force_magnitude * r_vec</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> forces</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">runge_kutta_4th_order</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;四阶龙格-库塔法数值积分（带速度控制）&quot;&quot;&quot;</span></span><br><span class="line">        effective_dt = self.dt * self.speed_multiplier</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 当前状态</span></span><br><span class="line">        pos = self.positions.copy()</span><br><span class="line">        vel = self.velocities.copy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># k1 计算</span></span><br><span class="line">        k1_vel = vel</span><br><span class="line">        k1_acc = self.compute_gravitational_forces() / self.masses.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># k2 计算</span></span><br><span class="line">        pos_k2 = pos + <span class="number">0.5</span> * effective_dt * k1_vel</span><br><span class="line">        self.positions = pos_k2  <span class="comment"># 临时更新位置用于力计算</span></span><br><span class="line">        k2_vel = vel + <span class="number">0.5</span> * effective_dt * k1_acc</span><br><span class="line">        k2_acc = self.compute_gravitational_forces() / self.masses.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># k3 计算</span></span><br><span class="line">        pos_k3 = pos + <span class="number">0.5</span> * effective_dt * k2_vel</span><br><span class="line">        self.positions = pos_k3</span><br><span class="line">        k3_vel = vel + <span class="number">0.5</span> * effective_dt * k2_acc</span><br><span class="line">        k3_acc = self.compute_gravitational_forces() / self.masses.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># k4 计算</span></span><br><span class="line">        pos_k4 = pos + effective_dt * k3_vel</span><br><span class="line">        self.positions = pos_k4</span><br><span class="line">        k4_vel = vel + effective_dt * k3_acc</span><br><span class="line">        k4_acc = self.compute_gravitational_forces() / self.masses.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终更新</span></span><br><span class="line">        self.positions = pos + effective_dt * (k1_vel + <span class="number">2</span> * k2_vel + <span class="number">2</span> * k3_vel + k4_vel) / <span class="number">6</span></span><br><span class="line">        self.velocities = vel + effective_dt * (k1_acc + <span class="number">2</span> * k2_acc + <span class="number">2</span> * k3_acc + k4_acc) / <span class="number">6</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录轨迹</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            self.trails[i].append([self.positions[i][<span class="number">0</span>], self.positions[i][<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">        self.time += effective_dt</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculate_total_energy</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;计算系统总能量（动能+势能）&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 动能</span></span><br><span class="line">        kinetic_energy = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            v_squared = np.<span class="built_in">sum</span>(self.velocities[i] ** <span class="number">2</span>)</span><br><span class="line">            kinetic_energy += <span class="number">0.5</span> * self.masses[i] * v_squared</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 引力势能</span></span><br><span class="line">        potential_energy = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">                r = np.linalg.norm(self.positions[i] - self.positions[j])</span><br><span class="line">                <span class="keyword">if</span> r &gt; <span class="number">1e-8</span>:</span><br><span class="line">                    potential_energy -= self.G * self.masses[i] * self.masses[j] / r</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> kinetic_energy + potential_energy</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_statistics</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;更新统计数据&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 总能量</span></span><br><span class="line">        energy = self.calculate_total_energy()</span><br><span class="line">        self.energy_history.append(energy)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 天体间距离</span></span><br><span class="line">        r_AB = np.linalg.norm(self.positions[<span class="number">0</span>] - self.positions[<span class="number">1</span>])</span><br><span class="line">        r_AC = np.linalg.norm(self.positions[<span class="number">0</span>] - self.positions[<span class="number">2</span>])</span><br><span class="line">        r_BC = np.linalg.norm(self.positions[<span class="number">1</span>] - self.positions[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        self.distance_history[<span class="string">&#x27;AB&#x27;</span>].append(r_AB)</span><br><span class="line">        self.distance_history[<span class="string">&#x27;AC&#x27;</span>].append(r_AC)</span><br><span class="line">        self.distance_history[<span class="string">&#x27;BC&#x27;</span>].append(r_BC)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpeedController</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;速度控制器类&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, system</span>):</span><br><span class="line">        self.system = system</span><br><span class="line">        self.speed_levels = [<span class="number">0.1</span>, <span class="number">0.25</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">8.0</span>]</span><br><span class="line">        self.current_level = <span class="number">3</span>  <span class="comment"># 默认1.0倍速</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">speed_up</span>(<span class="params">self, event=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;加速&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.current_level &lt; <span class="built_in">len</span>(self.speed_levels) - <span class="number">1</span>:</span><br><span class="line">            self.current_level += <span class="number">1</span></span><br><span class="line">            self.current_level = <span class="built_in">min</span>(self.current_level, <span class="built_in">len</span>(self.speed_levels)-<span class="number">1</span>)</span><br><span class="line">            self.system.set_speed_multiplier(self.speed_levels[self.current_level])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">slow_down</span>(<span class="params">self, event=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;减速&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.current_level &gt; <span class="number">0</span>:</span><br><span class="line">            self.current_level -= <span class="number">1</span></span><br><span class="line">            self.current_level = <span class="built_in">max</span>(self.current_level, <span class="number">0</span>)</span><br><span class="line">            self.system.set_speed_multiplier(self.speed_levels[self.current_level])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_speed</span>(<span class="params">self, event=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;重置到正常速度&quot;&quot;&quot;</span></span><br><span class="line">        self.current_level = <span class="number">3</span></span><br><span class="line">        self.system.set_speed_multiplier(self.speed_levels[self.current_level])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_current_speed_text</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;获取当前速度显示文本&quot;&quot;&quot;</span></span><br><span class="line">        speed = self.speed_levels[self.current_level]</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;播放速度: <span class="subst">&#123;speed:<span class="number">.1</span>f&#125;</span>x&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_three_body_animation</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建带速度控制的三体运动动画&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化系统 - 选择一个展现混沌特性的配置</span></span><br><span class="line">    masses = [<span class="number">1.5</span>, <span class="number">1.0</span>, <span class="number">1.2</span>]  <span class="comment"># 三个不同质量的恒星</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 三角形初始配置，但有适当的速度产生复杂轨道</span></span><br><span class="line">    positions = [</span><br><span class="line">        [-<span class="number">2.0</span>, <span class="number">0.0</span>],  <span class="comment"># Alpha星</span></span><br><span class="line">        [<span class="number">1.0</span>, <span class="number">1.732</span>],  <span class="comment"># Beta星</span></span><br><span class="line">        [<span class="number">1.0</span>, -<span class="number">1.732</span>]  <span class="comment"># Gamma星</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    velocities = [</span><br><span class="line">        [<span class="number">0.2</span>, <span class="number">0.8</span>],  <span class="comment"># Alpha星初始速度</span></span><br><span class="line">        [-<span class="number">0.6</span>, -<span class="number">0.2</span>],  <span class="comment"># Beta星初始速度</span></span><br><span class="line">        [<span class="number">0.4</span>, -<span class="number">0.6</span>]  <span class="comment"># Gamma星初始速度</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    system = ThreeBodySystem(masses, positions, velocities, G=<span class="number">3.0</span>)</span><br><span class="line">    speed_controller = SpeedController(system)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建图形界面</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">19</span>, <span class="number">12</span>), facecolor=<span class="string">&#x27;#0a0a0a&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 主动画区域 (左侧大图)</span></span><br><span class="line">    ax_main = plt.subplot2grid((<span class="number">3</span>, <span class="number">5</span>), (<span class="number">0</span>, <span class="number">0</span>), colspan=<span class="number">3</span>, rowspan=<span class="number">3</span>, facecolor=<span class="string">&#x27;#0a0a0a&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 监控面板 (右侧)</span></span><br><span class="line">    ax_energy = plt.subplot2grid((<span class="number">3</span>, <span class="number">5</span>), (<span class="number">0</span>, <span class="number">3</span>), colspan=<span class="number">2</span>, facecolor=<span class="string">&#x27;#0a0a0a&#x27;</span>)</span><br><span class="line">    ax_distances = plt.subplot2grid((<span class="number">3</span>, <span class="number">5</span>), (<span class="number">1</span>, <span class="number">3</span>), colspan=<span class="number">2</span>, facecolor=<span class="string">&#x27;#0a0a0a&#x27;</span>)</span><br><span class="line">    ax_info = plt.subplot2grid((<span class="number">3</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">3</span>), colspan=<span class="number">2</span>, facecolor=<span class="string">&#x27;#0a0a0a&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在底部为按钮留出空间</span></span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0.15</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建速度控制按钮</span></span><br><span class="line">    button_height = <span class="number">0.04</span></span><br><span class="line">    button_width = <span class="number">0.08</span></span><br><span class="line">    button_y = <span class="number">0.02</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 减速按钮</span></span><br><span class="line">    ax_slow = plt.axes([<span class="number">0.25</span>, button_y, button_width, button_height])</span><br><span class="line">    btn_slow = Button(ax_slow, <span class="string">&#x27;减速&#x27;</span>, color=<span class="string">&#x27;#FF4444&#x27;</span>, hovercolor=<span class="string">&#x27;#FF6666&#x27;</span>)</span><br><span class="line">    btn_slow.label.set_color(<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    btn_slow.label.set_fontweight(<span class="string">&#x27;bold&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重置按钮</span></span><br><span class="line">    ax_reset = plt.axes([<span class="number">0.35</span>, button_y, button_width, button_height])</span><br><span class="line">    btn_reset = Button(ax_reset, <span class="string">&#x27;正常&#x27;</span>, color=<span class="string">&#x27;#4CAF50&#x27;</span>, hovercolor=<span class="string">&#x27;#6BCF7F&#x27;</span>)</span><br><span class="line">    btn_reset.label.set_color(<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    btn_reset.label.set_fontweight(<span class="string">&#x27;bold&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加速按钮</span></span><br><span class="line">    ax_fast = plt.axes([<span class="number">0.45</span>, button_y, button_width, button_height])</span><br><span class="line">    btn_fast = Button(ax_fast, <span class="string">&#x27;加速&#x27;</span>, color=<span class="string">&#x27;#2196F3&#x27;</span>, hovercolor=<span class="string">&#x27;#42A5F5&#x27;</span>)</span><br><span class="line">    btn_fast.label.set_color(<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    btn_fast.label.set_fontweight(<span class="string">&#x27;bold&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 暂停/继续按钮</span></span><br><span class="line">    ax_pause = plt.axes([<span class="number">0.55</span>, button_y, button_width, button_height])</span><br><span class="line">    btn_pause = Button(ax_pause, <span class="string">&#x27;暂停&#x27;</span>, color=<span class="string">&#x27;#FF9800&#x27;</span>, hovercolor=<span class="string">&#x27;#FFB74D&#x27;</span>)</span><br><span class="line">    btn_pause.label.set_color(<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    btn_pause.label.set_fontweight(<span class="string">&#x27;bold&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 速度显示文本</span></span><br><span class="line">    speed_text = fig.text(<span class="number">0.65</span>, button_y + button_height / <span class="number">2</span>,</span><br><span class="line">                          speed_controller.get_current_speed_text(),</span><br><span class="line">                          fontsize=<span class="number">12</span>, color=<span class="string">&#x27;white&#x27;</span>, va=<span class="string">&#x27;center&#x27;</span>, weight=<span class="string">&#x27;bold&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置主画布</span></span><br><span class="line">    ax_main.set_xlim(-<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">    ax_main.set_ylim(-<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">    ax_main.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">    ax_main.set_facecolor(<span class="string">&#x27;#0a0a0a&#x27;</span>)</span><br><span class="line">    ax_main.tick_params(colors=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    ax_main.set_xlabel(<span class="string">&#x27;X 坐标&#x27;</span>, color=<span class="string">&#x27;white&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">    ax_main.set_ylabel(<span class="string">&#x27;Y 坐标&#x27;</span>, color=<span class="string">&#x27;white&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">    ax_main.set_title(<span class="string">&#x27;三体问题：三颗恒星的混沌舞蹈&#x27;</span>, color=<span class="string">&#x27;white&#x27;</span>, fontsize=<span class="number">16</span>, pad=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建星空背景</span></span><br><span class="line">    np.random.seed(<span class="number">123</span>)</span><br><span class="line">    star_x = np.random.uniform(-<span class="number">8</span>, <span class="number">8</span>, <span class="number">200</span>)</span><br><span class="line">    star_y = np.random.uniform(-<span class="number">8</span>, <span class="number">8</span>, <span class="number">200</span>)</span><br><span class="line">    star_sizes = np.random.uniform(<span class="number">0.5</span>, <span class="number">3</span>, <span class="number">200</span>)</span><br><span class="line">    ax_main.scatter(star_x, star_y, c=<span class="string">&#x27;white&#x27;</span>, s=star_sizes, alpha=<span class="number">0.4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加网格</span></span><br><span class="line">    ax_main.grid(<span class="literal">True</span>, alpha=<span class="number">0.15</span>, color=<span class="string">&#x27;gray&#x27;</span>, linestyle=<span class="string">&#x27;-&#x27;</span>, linewidth=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化绘图元素</span></span><br><span class="line">    <span class="comment"># 1. 三个恒星天体</span></span><br><span class="line">    stars = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        star = ax_main.scatter([], [], s=system.base_sizes[i],</span><br><span class="line">                               c=system.colors[i], edgecolors=<span class="string">&#x27;white&#x27;</span>,</span><br><span class="line">                               linewidth=<span class="number">3</span>, alpha=<span class="number">0.95</span>, zorder=<span class="number">20</span>)</span><br><span class="line">        stars.append(star)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 三条轨迹线</span></span><br><span class="line">    orbit_trails = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        trail, = ax_main.plot([], [], color=system.colors[i],</span><br><span class="line">                              linewidth=<span class="number">2.5</span>, alpha=<span class="number">0.8</span>, zorder=<span class="number">10</span>)</span><br><span class="line">        orbit_trails.append(trail)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 引力连接线</span></span><br><span class="line">    gravity_lines = []</span><br><span class="line">    gravity_colors = [<span class="string">&#x27;yellow&#x27;</span>, <span class="string">&#x27;orange&#x27;</span>, <span class="string">&#x27;red&#x27;</span>]</span><br><span class="line">    line_pairs = [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(line_pairs):</span><br><span class="line">        line, = ax_main.plot([], [], color=gravity_colors[i],</span><br><span class="line">                             alpha=<span class="number">0.4</span>, linewidth=<span class="number">2</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, zorder=<span class="number">5</span>)</span><br><span class="line">        gravity_lines.append(line)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 恒星标签</span></span><br><span class="line">    star_labels = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        label = ax_main.text(<span class="number">0</span>, <span class="number">0</span>, system.names[i], fontsize=<span class="number">12</span>, color=<span class="string">&#x27;white&#x27;</span>,</span><br><span class="line">                             ha=<span class="string">&#x27;center&#x27;</span>, va=<span class="string">&#x27;center&#x27;</span>, weight=<span class="string">&#x27;bold&#x27;</span>,</span><br><span class="line">                             bbox=<span class="built_in">dict</span>(boxstyle=<span class="string">&quot;round,pad=0.3&quot;</span>, facecolor=system.colors[i], alpha=<span class="number">0.7</span>))</span><br><span class="line">        star_labels.append(label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 信息显示</span></span><br><span class="line">    status_text = ax_main.text(<span class="number">0.02</span>, <span class="number">0.98</span>, <span class="string">&#x27;&#x27;</span>, transform=ax_main.transAxes,</span><br><span class="line">                               fontsize=<span class="number">12</span>, color=<span class="string">&#x27;white&#x27;</span>, verticalalignment=<span class="string">&#x27;top&#x27;</span>,</span><br><span class="line">                               bbox=<span class="built_in">dict</span>(boxstyle=<span class="string">&quot;round,pad=0.5&quot;</span>,</span><br><span class="line">                                         facecolor=<span class="string">&quot;black&quot;</span>, alpha=<span class="number">0.8</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置监控图表</span></span><br><span class="line">    <span class="keyword">for</span> ax <span class="keyword">in</span> [ax_energy, ax_distances]:</span><br><span class="line">        ax.set_facecolor(<span class="string">&#x27;#0a0a0a&#x27;</span>)</span><br><span class="line">        ax.tick_params(colors=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">        ax.grid(<span class="literal">True</span>, alpha=<span class="number">0.3</span>, color=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 能量图</span></span><br><span class="line">    ax_energy.set_title(<span class="string">&#x27;系统总能量守恒&#x27;</span>, color=<span class="string">&#x27;white&#x27;</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">    ax_energy.set_ylabel(<span class="string">&#x27;总能量&#x27;</span>, color=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    energy_line, = ax_energy.plot([], [], color=<span class="string">&#x27;cyan&#x27;</span>, linewidth=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 距离图</span></span><br><span class="line">    ax_distances.set_title(<span class="string">&#x27;恒星间距离变化&#x27;</span>, color=<span class="string">&#x27;white&#x27;</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">    ax_distances.set_ylabel(<span class="string">&#x27;距离&#x27;</span>, color=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">    ax_distances.set_xlabel(<span class="string">&#x27;时间步&#x27;</span>, color=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    dist_lines = &#123;&#125;</span><br><span class="line">    dist_colors = [<span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;green&#x27;</span>]</span><br><span class="line">    dist_labels = [<span class="string">&#x27;Alpha-Beta&#x27;</span>, <span class="string">&#x27;Alpha-Gamma&#x27;</span>, <span class="string">&#x27;Beta-Gamma&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, key <span class="keyword">in</span> <span class="built_in">enumerate</span>([<span class="string">&#x27;AB&#x27;</span>, <span class="string">&#x27;AC&#x27;</span>, <span class="string">&#x27;BC&#x27;</span>]):</span><br><span class="line">        line, = ax_distances.plot([], [], color=dist_colors[i],</span><br><span class="line">                                  linewidth=<span class="number">2</span>, label=dist_labels[i])</span><br><span class="line">        dist_lines[key] = line</span><br><span class="line">    ax_distances.legend(fontsize=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 信息面板</span></span><br><span class="line">    ax_info.set_facecolor(<span class="string">&#x27;#0a0a0a&#x27;</span>)</span><br><span class="line">    ax_info.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    info_display = ax_info.text(<span class="number">0.05</span>, <span class="number">0.95</span>, <span class="string">&#x27;&#x27;</span>, transform=ax_info.transAxes,</span><br><span class="line">                                fontsize=<span class="number">11</span>, color=<span class="string">&#x27;white&#x27;</span>, verticalalignment=<span class="string">&#x27;top&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 动画状态控制</span></span><br><span class="line">    animation_paused = [<span class="literal">False</span>]  <span class="comment"># 使用列表以便在闭包中修改</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">toggle_pause</span>(<span class="params">event</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;切换暂停/继续&quot;&quot;&quot;</span></span><br><span class="line">        animation_paused[<span class="number">0</span>] = <span class="keyword">not</span> animation_paused[<span class="number">0</span>]</span><br><span class="line">        btn_pause.label.set_text(<span class="string">&#x27;继续&#x27;</span> <span class="keyword">if</span> animation_paused[<span class="number">0</span>] <span class="keyword">else</span> <span class="string">&#x27;暂停&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_speed_display</span>():</span><br><span class="line">        <span class="string">&quot;&quot;&quot;更新速度显示&quot;&quot;&quot;</span></span><br><span class="line">        speed_text.set_text(speed_controller.get_current_speed_text())</span><br><span class="line">        fig.canvas.draw_idle()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_speed_up</span>(<span class="params">event</span>):</span><br><span class="line">        speed_controller.speed_up()</span><br><span class="line">        update_speed_display()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_slow_down</span>(<span class="params">event</span>):</span><br><span class="line">        speed_controller.slow_down()</span><br><span class="line">        update_speed_display()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_reset_speed</span>(<span class="params">event</span>):</span><br><span class="line">        speed_controller.reset_speed()</span><br><span class="line">        update_speed_display()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 连接按钮事件</span></span><br><span class="line">    btn_fast.on_clicked(on_speed_up)</span><br><span class="line">    btn_slow.on_clicked(on_slow_down)</span><br><span class="line">    btn_reset.on_clicked(on_reset_speed)</span><br><span class="line">    btn_pause.on_clicked(toggle_pause)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">animate_frame</span>(<span class="params">frame</span>):</span><br><span class="line">        <span class="comment"># 如果暂停则不更新</span></span><br><span class="line">        <span class="keyword">if</span> animation_paused[<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">return</span> (stars + orbit_trails + gravity_lines + star_labels +</span><br><span class="line">                    [status_text, energy_line] + <span class="built_in">list</span>(dist_lines.values()) + [info_display])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 执行物理计算 - 每帧多步提高精度</span></span><br><span class="line">        steps_per_frame = <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">int</span>(<span class="number">3</span> / <span class="built_in">max</span>(<span class="number">0.1</span>, system.speed_multiplier)))</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(steps_per_frame):</span><br><span class="line">            system.runge_kutta_4th_order()</span><br><span class="line">            system.update_statistics()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新三个恒星的位置</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="comment"># 更新恒星位置</span></span><br><span class="line">            current_pos = system.positions[i].reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">            stars[i].set_offsets(current_pos)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 添加脉动效果</span></span><br><span class="line">            pulse = <span class="number">1</span> + <span class="number">0.2</span> * np.sin(system.time * <span class="number">4</span> + i * <span class="number">2</span>)</span><br><span class="line">            stars[i].set_sizes([system.base_sizes[i] * pulse])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新标签位置</span></span><br><span class="line">            star_labels[i].set_position((system.positions[i][<span class="number">0</span>] + <span class="number">0.5</span>,</span><br><span class="line">                                         system.positions[i][<span class="number">1</span>] + <span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新轨迹</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(system.trails[i]) &gt; <span class="number">1</span>:</span><br><span class="line">                trail_array = np.array(<span class="built_in">list</span>(system.trails[i]))</span><br><span class="line">                orbit_trails[i].set_data(trail_array[:, <span class="number">0</span>], trail_array[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新引力连接线</span></span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(line_pairs):</span><br><span class="line">            pos_a = system.positions[a]</span><br><span class="line">            pos_b = system.positions[b]</span><br><span class="line">            gravity_lines[i].set_data([pos_a[<span class="number">0</span>], pos_b[<span class="number">0</span>]], [pos_a[<span class="number">1</span>], pos_b[<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 根据距离调整线条粗细和透明度</span></span><br><span class="line">            distance = np.linalg.norm(pos_a - pos_b)</span><br><span class="line">            alpha = <span class="built_in">max</span>(<span class="number">0.2</span>, <span class="built_in">min</span>(<span class="number">0.8</span>, <span class="number">5.0</span> / distance))</span><br><span class="line">            linewidth = <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">min</span>(<span class="number">4</span>, <span class="number">8.0</span> / distance))</span><br><span class="line">            gravity_lines[i].set_alpha(alpha)</span><br><span class="line">            gravity_lines[i].set_linewidth(linewidth)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新状态信息</span></span><br><span class="line">        current_energy = system.calculate_total_energy()</span><br><span class="line">        max_velocity = <span class="built_in">max</span>([np.linalg.norm(v) <span class="keyword">for</span> v <span class="keyword">in</span> system.velocities])</span><br><span class="line"></span><br><span class="line">        status_info = (</span><br><span class="line">            <span class="string">f&quot;模拟时间: <span class="subst">&#123;system.time:<span class="number">.2</span>f&#125;</span>\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;播放速度: <span class="subst">&#123;system.speed_multiplier:<span class="number">.1</span>f&#125;</span>x\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;系统总能量: <span class="subst">&#123;current_energy:<span class="number">.4</span>f&#125;</span>\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;最大速度: <span class="subst">&#123;max_velocity:<span class="number">.3</span>f&#125;</span>\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;计算步数: <span class="subst">&#123;frame * steps_per_frame&#125;</span>\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;\n三体位置坐标:\n&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            pos = system.positions[i]</span><br><span class="line">            vel_mag = np.linalg.norm(system.velocities[i])</span><br><span class="line">            status_info += <span class="string">f&quot;<span class="subst">&#123;system.names[i]&#125;</span>: (<span class="subst">&#123;pos[<span class="number">0</span>]:<span class="number">.2</span>f&#125;</span>, <span class="subst">&#123;pos[<span class="number">1</span>]:<span class="number">.2</span>f&#125;</span>)\n&quot;</span></span><br><span class="line">            status_info += <span class="string">f&quot;   速度: <span class="subst">&#123;vel_mag:<span class="number">.3</span>f&#125;</span>\n&quot;</span></span><br><span class="line"></span><br><span class="line">        status_text.set_text(status_info)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新能量图</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(system.energy_history) &gt; <span class="number">1</span>:</span><br><span class="line">            time_points = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(system.energy_history)))</span><br><span class="line">            energy_line.set_data(time_points, <span class="built_in">list</span>(system.energy_history))</span><br><span class="line">            ax_energy.relim()</span><br><span class="line">            ax_energy.autoscale_view()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新距离图</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(system.distance_history[<span class="string">&#x27;AB&#x27;</span>]) &gt; <span class="number">1</span>:</span><br><span class="line">            time_points = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(system.distance_history[<span class="string">&#x27;AB&#x27;</span>])))</span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> [<span class="string">&#x27;AB&#x27;</span>, <span class="string">&#x27;AC&#x27;</span>, <span class="string">&#x27;BC&#x27;</span>]:</span><br><span class="line">                dist_lines[key].set_data(time_points, <span class="built_in">list</span>(system.distance_history[key]))</span><br><span class="line">            ax_distances.relim()</span><br><span class="line">            ax_distances.autoscale_view()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新信息面板</span></span><br><span class="line">        min_dist = <span class="built_in">min</span>([<span class="built_in">list</span>(system.distance_history[key])[-<span class="number">1</span>]</span><br><span class="line">                        <span class="keyword">for</span> key <span class="keyword">in</span> system.distance_history.keys()</span><br><span class="line">                        <span class="keyword">if</span> <span class="built_in">len</span>(system.distance_history[key]) &gt; <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        info_text = (</span><br><span class="line">            <span class="string">&quot;=== 三体系统状态 ===\n\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;恒星质量:\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;  <span class="subst">&#123;system.names[<span class="number">0</span>]&#125;</span>: <span class="subst">&#123;system.masses[<span class="number">0</span>]:<span class="number">.1</span>f&#125;</span>M☉\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;  <span class="subst">&#123;system.names[<span class="number">1</span>]&#125;</span>: <span class="subst">&#123;system.masses[<span class="number">1</span>]:<span class="number">.1</span>f&#125;</span>M☉\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;  <span class="subst">&#123;system.names[<span class="number">2</span>]&#125;</span>: <span class="subst">&#123;system.masses[<span class="number">2</span>]:<span class="number">.1</span>f&#125;</span>M☉\n\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;最近距离: <span class="subst">&#123;min_dist:<span class="number">.3</span>f&#125;</span>\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;引力常数: <span class="subst">&#123;system.G:<span class="number">.1</span>f&#125;</span>\n\n&quot;</span></span><br><span class="line">            <span class="string">&quot;观察要点:\n&quot;</span></span><br><span class="line">            <span class="string">&quot;*  轨道的不规则性\n&quot;</span></span><br><span class="line">            <span class="string">&quot;*  近距离遭遇事件\n&quot;</span></span><br><span class="line">            <span class="string">&quot;*  能量守恒定律\n&quot;</span></span><br><span class="line">            <span class="string">&quot;*  混沌敏感依赖性\n\n&quot;</span></span><br><span class="line">            <span class="string">&quot;控制说明:\n&quot;</span></span><br><span class="line">            <span class="string">&quot;*  加速/减速按钮调节播放速度\n&quot;</span></span><br><span class="line">            <span class="string">&quot;*  暂停按钮停止/继续动画\n&quot;</span></span><br><span class="line">            <span class="string">&quot;*  正常按钮重置为1x速度&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        info_display.set_text(info_text)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (stars + orbit_trails + gravity_lines + star_labels +</span><br><span class="line">                [status_text, energy_line] + <span class="built_in">list</span>(dist_lines.values()) + [info_display])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建动画对象</span></span><br><span class="line">    anim = FuncAnimation(fig, animate_frame, frames=<span class="number">3000</span>,</span><br><span class="line">                         interval=<span class="number">30</span>, blit=<span class="literal">False</span>, repeat=<span class="literal">True</span>, cache_frame_data=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 总标题</span></span><br><span class="line">    fig.suptitle(<span class="string">&#x27;三体问题数值模拟：混沌引力系统（速度可控）&#x27;</span>,</span><br><span class="line">                 fontsize=<span class="number">18</span>, color=<span class="string">&#x27;white&#x27;</span>, y=<span class="number">0.96</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 底部说明</span></span><br><span class="line">    fig.text(<span class="number">0.02</span>, <span class="number">0.085</span>,</span><br><span class="line">             <span class="string">&#x27;物理原理: 三个质量不等的恒星在万有引力作用下的非周期运动 | &#x27;</span></span><br><span class="line">             <span class="string">&#x27;混沌特征: 轨道不可预测、对初始条件敏感、长期行为无法确定 | &#x27;</span></span><br><span class="line">             <span class="string">&#x27;数值方法: 四阶龙格-库塔法求解牛顿引力方程组&#x27;</span>,</span><br><span class="line">             fontsize=<span class="number">10</span>, color=<span class="string">&#x27;white&#x27;</span>,</span><br><span class="line">             bbox=<span class="built_in">dict</span>(boxstyle=<span class="string">&quot;round,pad=0.5&quot;</span>, facecolor=<span class="string">&quot;#1a1a2e&quot;</span>, alpha=<span class="number">0.9</span>))</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.subplots_adjust(top=<span class="number">0.92</span>, bottom=<span class="number">0.18</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> anim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动三体动画</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🌟 启动三体恒星系统动态模拟（带速度控制）...&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n系统配置:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*  三颗质量不等的恒星: Alpha(1.5M☉), Beta(1.0M☉), Gamma(1.2M☉)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*  初始三角形配置，赋予复杂初始速度&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*  高精度数值积分(RK4)确保长期稳定性&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n可视化特征:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;✓ 三个不同颜色的恒星实时运动&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;✓ 彩色轨迹显示历史路径&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;✓ 虚线显示引力相互作用&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;✓ 恒星大小脉动模拟真实效果&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;✓ 实时监控能量守恒和距离变化&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n新增速度控制功能:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🎮 减速按钮: 降低播放速度(0.1x-8x)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🎮 加速按钮: 提高播放速度(0.1x-8x)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🎮 正常按钮: 重置为1x正常速度&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🎮 暂停按钮: 暂停/继续动画播放&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🎮 实时速度显示: 当前播放倍数&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n观察重点:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🔍 轨道的复杂性和不可预测性&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🔍 恒星间距离的剧烈变化&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🔍 近距离遭遇引起的轨道突变&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🔍 系统总能量的精确守恒&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n操作提示:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;💡 使用加速功能快速观察长期行为&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;💡 使用减速功能仔细研究细节变化&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;💡 暂停功能方便截图和观察&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n正在加载动画...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行动画</span></span><br><span class="line">animation = create_three_body_animation()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;🎬 三体动画已启动！使用底部按钮控制播放速度...&quot;</span>)</span><br></pre></td></tr></table></figure>
<div style="position: relative; width: 100%; padding-bottom: 56.25%; margin: 20px 0;">
  <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=114649370462296&bvid=BV1qaTdzWEcD&cid=30395404225&p=1" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none;" allowfullscreen>
    </iframe>
</div>


<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.bilibili.com/video/BV1Fj421Z7MF/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3f2411263f367ccf993c28b58688c0e7" >三体问题究竟是什么？为什么说科学的尽头是神学？<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Three-Body</tag>
      </tags>
  </entry>
  <entry>
    <title>专用于个人简历的latex模板</title>
    <url>/2022/04/29/%E4%B8%93%E7%94%A8%E4%BA%8E%E4%B8%AA%E4%BA%BA%E7%AE%80%E5%8E%86%E7%9A%84latex%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>一份简历模板，fork自<a class="link"   href="https://github.com/hijiangtao/resume" >hijiangtao/resume<i class="fas fa-external-link-alt"></i></a>。自己随意删改了一些东西，地址为：<a class="link"   href="https://github.com/TransformersWsz/wsz_resume" >resume<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><ul>
<li>将整个项目直接上传到overleaf上，使用xelatex编译</li>
<li>使用本地的texstudio编译</li>
</ul>
<h2 id="预览"><a href="#预览" class="headerlink" title="预览"></a>预览</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/resume_preview.2ykrizc4yiu0.png"  alt="resume_preview"></p>
<h2 id="FontAwesome"><a href="#FontAwesome" class="headerlink" title="FontAwesome"></a>FontAwesome</h2><p><code>resume-zh_CN.tex</code> 已经导入了 <code>fontawesome</code> 包（需要注意的是，只有<a class="link"   href="https://fontawesome.com/v4/icons/" >v4.5<i class="fas fa-external-link-alt"></i></a>的图标（不支持 <code>alias</code> 别名）），在网站中查找想使用的图标，然后在 <code>fontawesome.sty</code> 中找到相应的宏, 将其作为普通文本一样使用：<br><figure class="highlight latex"><table><tr><td class="code"><pre><span class="line"><span class="keyword">\faGithub</span>  <span class="keyword">\;</span> GitHub @hijiangtao</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>latex</tag>
        <tag>简历</tag>
      </tags>
  </entry>
  <entry>
    <title>两种神经网络参数初始化方法</title>
    <url>/2024/06/21/%E4%B8%A4%E7%A7%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>重点介绍一下Xavier和Kaiming初始化：</p>
<span id="more"></span>
<h2 id="Xavier"><a href="#Xavier" class="headerlink" title="Xavier"></a>Xavier</h2><blockquote>
<p>为了使得网络中信息更好的流动，每一层输出的方差应该尽量相等。</p>
</blockquote>
<h4 id="正态分布参数初始化"><a href="#正态分布参数初始化" class="headerlink" title="正态分布参数初始化"></a>正态分布参数初始化</h4><script type="math/tex; mode=display">
\mathcal{N}\left(0, \frac{2}{n_{\text {in }}+n_{\text {out }}}\right)</script><h4 id="均匀分布参数初始化"><a href="#均匀分布参数初始化" class="headerlink" title="均匀分布参数初始化"></a>均匀分布参数初始化</h4><script type="math/tex; mode=display">
\mathcal{U}\left(-\sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}}, \sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}}\right)</script><h2 id="Kaiming"><a href="#Kaiming" class="headerlink" title="Kaiming"></a>Kaiming</h2><p>Xavier初始化的问题在于，它只适用于线性激活函数，但实际上，对于深层神经网络来说，线性激活函数是没有价值，神经网络需要非线性激活函数(例如ReLU)来构建复杂网络。</p>
<blockquote>
<p>前向传播时每层的方差都是1</p>
<p>反向传播时梯度的方差都是1</p>
</blockquote>
<h4 id="正态分布参数初始化-1"><a href="#正态分布参数初始化-1" class="headerlink" title="正态分布参数初始化"></a>正态分布参数初始化</h4><script type="math/tex; mode=display">
\mathcal{N}\left(0, \frac{2}{n_{\text {in }}}\right)</script><h4 id="均匀分布参数初始化-1"><a href="#均匀分布参数初始化-1" class="headerlink" title="均匀分布参数初始化"></a>均匀分布参数初始化</h4><script type="math/tex; mode=display">
\mathcal{U}\left(-\sqrt{\frac{6}{n_{\text {in }}}}, \sqrt{\frac{6}{n_{\text {in }}}}\right)</script><p>$n_{in}$表示每层输入的神经元数量</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://chatgpt.com/share/6c5856fb-e3d3-4ae0-8d1a-21a2b562888d" >ChatGPT: 什么是xavier和kaiming初始化，给出公式和详细解释<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/IronmanJay/article/details/128888954" >Xavier参数初始化方法和Kaiming参数初始化方法详细介绍及其原理详解<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>从loss角度理解LLM涌现能力</title>
    <url>/2024/05/12/%E4%BB%8Eloss%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3LLM%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/</url>
    <content><![CDATA[<p>如今的很多研究都表明小模型也能出现涌现能力，本文的作者团队通过大量实验发现模型的涌现能力与模型大小、训练计算量无关，只与预训练loss相关。</p>
<p>作者团队惊奇地发现，不管任何下游任务，不管模型大小，模型出现涌现能力都不约而同地是在预训练loss降低到 <code>2.2</code> 以下后。</p>
<span id="more"></span>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.4913c8bgrx.png"  alt="ViT"></p>
<p>在 <code>2.2</code> 之前，模型的表现跟一般模型无异。在 <code>2.2</code> 之后，模型的性能显著上升。</p>
<h2 id="数学建模"><a href="#数学建模" class="headerlink" title="数学建模"></a>数学建模</h2><p>模型涌现能力与预训练loss的关系，公式化如下：</p>
<script type="math/tex; mode=display">
\begin{cases}f(L) & \text { if } L<\eta \\ 0 & \text { otherwise }\end{cases}</script><p>$f(L)$ 是个单调递减函数，$L$ 越大，其值越小。$\eta$ 是个loss阈值，比如 <code>2.2</code> 。</p>
<p>预训练loss与模型大小 $N$ 关系如下：</p>
<script type="math/tex; mode=display">
L(N)=L_{\infty}+\left(\frac{N_0}{N}\right)^{\alpha_N}</script><p>因此涌现能力与模型大小的关系如下：</p>
<script type="math/tex; mode=display">
\begin{cases}f\left(L_{\infty}+\left(\frac{N_0}{N}\right)^{\alpha_N}\right) & \text { if } N \geq N_0 \cdot\left(\eta-L_{\infty}\right)^{-\frac{1}{\alpha_N}} \\ 0 & \text { otherwise }\end{cases}</script><p>当模型大小超过$N_0 \cdot\left(\eta-L_{\infty}\right)^{-\frac{1}{\alpha_N}}$，才会出现涌现能力，否则与普通模型无异。随着模型尺寸变大，预训练loss减少，则模型性能提升。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文从预训练loss角度观察了模型涌现能力是如何发生的。其结论也给业界评估模型在下游任务上的性能提供了全新的视角，即预训练loss，而不是模型参数量、数据量、训练计算量。</p>
<p>但本文并未从理论角度解释loss与涌现能力的关系，更多地是根据后验进行启发式分析，也未给出 <code>2.2</code> 的合理说明。但DL一直这么玄学，不是吗？</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://arxiv.org/pdf/2403.15796" >Understanding Emergent Abilities of Language Models from the Loss Perspective<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>涌现能力</tag>
        <tag>Pretraining Loss</tag>
      </tags>
  </entry>
  <entry>
    <title>从有序数组中查找不小于（不大于）某数的第一个（最后一个）元素</title>
    <url>/2021/03/12/%E4%BB%8E%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E6%9F%A5%E6%89%BE%E4%B8%8D%E5%B0%8F%E4%BA%8E%EF%BC%88%E4%B8%8D%E5%A4%A7%E4%BA%8E%EF%BC%89%E6%9F%90%E6%95%B0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%EF%BC%88%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%EF%BC%89%E5%85%83%E7%B4%A0/</url>
    <content><![CDATA[<p>记录一下二分查找的变形场景：</p>
<span id="more"></span>
<h2 id="不小于某数的第一个元素"><a href="#不小于某数的第一个元素" class="headerlink" title="不小于某数的第一个元素"></a>不小于某数的第一个元素</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_search_notlessthan_first</span>(<span class="params">arr, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(arr)-<span class="number">1</span></span><br><span class="line">    res = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">        mid = (low+high)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> target &lt;= arr[mid]:</span><br><span class="line">            res = mid</span><br><span class="line">            high = mid-<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            low = mid+<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>如果 <code>res==-1</code> ，说明 <code>any(arr) &lt; target</code></p>
<hr>
<h2 id="不大于某数的最后一个元素"><a href="#不大于某数的最后一个元素" class="headerlink" title="不大于某数的最后一个元素"></a>不大于某数的最后一个元素</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_search_notgreaterthan_last</span>(<span class="params">arr, target</span>):</span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(arr)-<span class="number">1</span></span><br><span class="line">    res = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">        mid = (low + high) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> arr[mid] &lt;= target:</span><br><span class="line">            res = mid</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            high = mid - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>如果 <code>res==-1</code> ，说明 <code>any(arr) &gt; target</code></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>二分查找</tag>
      </tags>
  </entry>
  <entry>
    <title>光锥</title>
    <url>/2025/06/18/%E5%85%89%E9%94%A5/</url>
    <content><![CDATA[<p>在物理学中，<strong>光锥（Light Cone）</strong>是描述时空中因果关系的一个几何结构，来源于相对论，尤其是狭义相对论和广义相对论中的时空图像。</p>
<span id="more"></span>
<hr>
<h3 id="🔷-简要定义"><a href="#🔷-简要定义" class="headerlink" title="🔷 简要定义"></a>🔷 简要定义</h3><blockquote>
<p>光锥是以某个事件为顶点，以光速传播形成的时空区域边界。</p>
</blockquote>
<p>设想一个时空中的事件 $E$，比如你在某一时刻某个地点打了个响指，那么：</p>
<ul>
<li><p><strong>未来光锥（Future Light Cone）</strong>：是从事件 $E$ 向未来扩展的所有可能被你这次响指影响到的事件区域（以光速或更慢传播）。</p>
</li>
<li><p><strong>过去光锥（Past Light Cone）</strong>：是能影响你打响指的这个事件的所有过去事件区域（信号从它们出发，以光速或更慢速度传到你这里）。</p>
</li>
<li><p><strong>光锥外部（Elsewhere）</strong>：是那些你无法影响，也无法影响你的事件。因为它们之间的传播所需速度超过光速，不符合因果律。</p>
</li>
</ul>
<hr>
<h3 id="🔷-数学表达"><a href="#🔷-数学表达" class="headerlink" title="🔷 数学表达"></a>🔷 数学表达</h3><p>在四维闵可夫斯基时空中（单位设置为光速 $c = 1$），设某事件的时空坐标是 $(t, x, y, z)$，则：</p>
<ul>
<li><p><strong>光锥边界</strong>满足：</p>
<script type="math/tex; mode=display">
s^2 = c^2t^2 - x^2 - y^2 - z^2 = 0</script></li>
<li><p><strong>时间类（timelike）区域</strong>： $s^2 &gt; 0$：可以有因果影响。</p>
</li>
<li><p><strong>光类（lightlike）区域</strong>： $s^2 = 0$：光传播路径。</p>
</li>
<li><p><strong>空间类（spacelike）区域</strong>： $s^2 &lt; 0$：超光速，需要虫洞或类星门才能通信（违反相对论）。</p>
</li>
</ul>
<hr>
<h3 id="🔷-图示（直观理解）"><a href="#🔷-图示（直观理解）" class="headerlink" title="🔷 图示（直观理解）"></a>🔷 图示（直观理解）</h3><p>在二维时空图（1个时间轴 + 1个空间轴）中：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">      t</span><br><span class="line">      |</span><br><span class="line">      |</span><br><span class="line">     /\</span><br><span class="line">    /  \</span><br><span class="line">   /    \</span><br><span class="line">  /  E   \    ← 未来光锥</span><br><span class="line"> /        \</span><br><span class="line">-------------&gt; x</span><br><span class="line"> \        /</span><br><span class="line">  \  E   /    ← 过去光锥</span><br><span class="line">   \    /</span><br><span class="line">    \  /</span><br><span class="line">     \/</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="🔷-物理意义"><a href="#🔷-物理意义" class="headerlink" title="🔷 物理意义"></a>🔷 物理意义</h3><ol>
<li><strong>因果律基础</strong>：任何信息或影响不能超光速传递，所以所有因果关系都必须在光锥内。</li>
<li><strong>黑洞视界就是一种光锥结构</strong>：事件视界就是无法逃离的未来光锥边界。</li>
<li><strong>宇宙学中的可观测宇宙</strong>：你现在能观测到的宇宙范围就是你所在事件的过去光锥。</li>
</ol>
<h3 id="🔷-现实意义"><a href="#🔷-现实意义" class="headerlink" title="🔷 现实意义"></a>🔷 现实意义</h3><ol>
<li><strong>宇宙视界（Cosmic Horizon）</strong>：宇宙学中，也有类似的说法——我们只能观测到与我们光锥有交集的部分宇宙，其余部分，即便存在，也永远无法观测。</li>
<li><strong>科幻限制</strong>：也是为什么穿越、预知未来、逆时间传输等，在现代物理框架下不成立的根本原因。</li>
</ol>
<h2 id="为什么人类无法观测光锥外的事件？"><a href="#为什么人类无法观测光锥外的事件？" class="headerlink" title="为什么人类无法观测光锥外的事件？"></a>为什么人类无法观测光锥外的事件？</h2><h4 id="1-因果律限制"><a href="#1-因果律限制" class="headerlink" title="1. 因果律限制"></a>1. <strong>因果律限制</strong></h4><p>根据相对论，任何信息传播速度不能超过光速。因此：</p>
<ul>
<li>光锥外的事件无法通过任何物理方式影响我们当前的位置和时间。</li>
<li>它们既不在我们的过去光锥中（无法影响我们），也不在我们的未来光锥中（我们也无法影响它们）。</li>
</ul>
<p>综上，无因果联系，也就意味着不可观测、不可验证。</p>
<h4 id="2-信息传播限制"><a href="#2-信息传播限制" class="headerlink" title="2. 信息传播限制"></a>2. <strong>信息传播限制</strong></h4><p>比如，你在地球上看一个遥远星系正在爆炸（超新星事件），你看到的其实是光到达你眼睛的时刻所对应的过去事件。这是光锥内的情况。</p>
<p>但如果某事件距离你非常远，甚至必须以超光速才能把信息传来——那它就在你的光锥外。你无论等多久、用什么方法，都无法获得它的信息。</p>
<h4 id="3-违反相对性原理"><a href="#3-违反相对性原理" class="headerlink" title="3. 违反相对性原理"></a>3. <strong>违反相对性原理</strong></h4><p>如果人类能够观察光锥外的事件，就等于能获得超光速信息，这将违反狭义相对论的基本原理，导致因果倒置（“效果先于原因”），从而破坏整个物理逻辑。</p>
<hr>
<h3 id="🔷-小结："><a href="#🔷-小结：" class="headerlink" title="🔷 小结："></a>🔷 小结：</h3><div class="table-container">
<table>
<thead>
<tr>
<th>区域</th>
<th>是否有因果联系</th>
<th>是否可传递信息</th>
</tr>
</thead>
<tbody>
<tr>
<td>光锥内</td>
<td>✅ 是</td>
<td>✅ 是</td>
</tr>
<tr>
<td>光锥边界（光速）</td>
<td>✅ 是</td>
<td>✅ 是</td>
</tr>
<tr>
<td>光锥外</td>
<td>❌ 否</td>
<td>❌ 否</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.zhihu.com/question/595987897/answer/2988187264" >光锥究竟是什么？<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Light Cone</tag>
      </tags>
  </entry>
  <entry>
    <title>关于mac上outlook2016无法打开的问题</title>
    <url>/2021/10/10/%E5%85%B3%E4%BA%8Emac%E4%B8%8Aoutlook2016%E6%97%A0%E6%B3%95%E6%89%93%E5%BC%80%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>本人安装了校园网站的office2016，word，ppt，excel都能正常打开，但是outlook却遇到了问题：</p>
<font color="red">You'll need to use the latest version of Outlook to use this database</font>

<span id="more"></span>
<p>解决方案如下：</p>
<p><a class="link"   href="https://answers.microsoft.com/en-us/mac/forum/macoffice2016-macoutlook/upgrade-outlook-database/3bc4c6ea-830a-4d1f-805c-a33018d08e2e" >Upgrade Outlook Database?<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>Outlook</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2021/04/07/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>决策树的定义和示例见西瓜书P73~P74，下面主要介绍决策树的构造算法：</p>
<span id="more"></span>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>信息熵是衡量样本集合纯度最常用的一种指标。假定当前样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k(k=1,2,\cdots,|\gamma|)$ ，那么 $D$ 的信息熵为：</p>
<script type="math/tex; mode=display">
Ent(D) = - \sum_{k=1}^{|\gamma|} p_k log_2 p_k</script><p>$Ent(D)$ 越小，则 $D$ 纯度越高。</p>
<h2 id="构造算法"><a href="#构造算法" class="headerlink" title="构造算法"></a>构造算法</h2><h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h3><script type="math/tex; mode=display">
Gain(D, a) = Ent(D) - \sum_{v=1}^V \frac{|D^v|}{|D|} Ent(D^v)</script><p>$V$ 表示属性 $a$ 的取值范围。信息增益越大，表示使用属性 $a$ 进行划分所获得的“纯度提升”越大。</p>
<p>根据 ID3 算法的核心思想，只要在每次决策树非叶子节点划分之前，计算出每一个属性所带来的信息增益，选择最大信息增益的属性来划分，就可以让本次划分更优，因此整个 ID3 实际上是一个贪心算法。</p>
<p>以西瓜数据集2.0来说，“纹理”的信息增益最大，于是它被选为划分属性。后续对其它属性进行特征选择。</p>
<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><p>C4.5 对 ID3 算法最大的改进就是在获取最优分类特征的时候，将 ID3 所使用的信息增益换成了信息增益比。</p>
<p>如果我们将每个西瓜样本的“编号”作为候选划分属性，那么它的信息增益最大。因为“编号”将产生17个分支，每个分支仅包含一个样本，这些分支结点的纯度达到最大。但这样的决策树不具有泛化能力，无法对新样本有效预测。</p>
<p>实际上，信息增益准则对取值较多的属性有所偏好，为了减少这种偏好带来的不利影响，C4.5采用“增益率”来选择最优划分属性：</p>
<script type="math/tex; mode=display">
Gain\_ratio = \frac{Gain(D,a)}{IV(a)} \\
IV(a) = -\sum_{v=1}^V \frac{D^v}{D} log_2 \frac{D^v}{D}</script><p>$IV(a)$ 称为属性 $a$ 的“固有值”。$V$ 越大，$IV(a)$ 越大。</p>
<p>需注意的是，增益率对 $V$ 较小的属性 $a$ 有所偏好。因此，C4.5算法并不直接选择增益率最大的候选划分属性，而使用了一个启发式策略：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p>CART决策树使用“基尼指数”来选择划分属性。数据集 $D$ 的纯度可用基尼值来度量：</p>
<script type="math/tex; mode=display">
Gini(D) = \sum_{k=1}^{|\gamma|} \sum_{k' \ne k} p_k p_{k'} = 1 - \sum_{k=1}^{|\gamma|} p_k^2</script><p>直观来说，$Gini(D)$ 反映了从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(D)$ 越小，则数据集 $D$ 的纯度越高。</p>
<p>属性 $a$ 的基尼指数定义为：</p>
<script type="math/tex; mode=display">
Gini\_index(D,a) = \sum_{v=1}^V \frac{|D^v|}{|D|} Gini(D^v)</script><p>在候选属性集合 $A$ 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即：</p>
<script type="math/tex; mode=display">
a_{*} = argmax_{a \in A} Gini\_ index(D,a)</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li>西瓜书P73-P79</li>
<li><a class="link"   href="https://techlog.cn/article/list/10183264#" >决策树的构建算法 — ID3 与 C4.5 算法<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/139523931" >决策树算法—CART分类树算法<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式训练</title>
    <url>/2021/08/08/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<p>在面试中，遇到有些面试官会问分布式训练的有关问题，在此总结一下。</p>
<span id="more"></span>
<p>分布式训练的并行方式主要分如下两种：</p>
<ul>
<li>数据并行：将数据集切分放到各计算节点，每个计算节点的计算内容完全一致，并在多个计算节点之间传递模型参数。数据并行可以解决数据集过大无法在单机高效率训练的问题，也是工业生产中最常用的并行方法。</li>
<li>模型并行：通常指将模型单个算子计算分治到多个硬件设备上并发计算，以达到计算单个算子计算速度的目的。一般会将单个算子的计算，利用模型并行的方式分配在配置相同的几个硬件上，进行模型存储和计算，以保证计算步调一致。</li>
</ul>
<p>这里详细介绍数据并行的两种训练架构：</p>
<ul>
<li>Parameter Server：该架构可以对模型参数的存储进行分布式保存，因此对于存储超大规模模型参数的训练场景十分友好。因此在个性化推荐场景中（任务需要保存海量稀疏特征对应的模型参数）应用广泛。</li>
<li>Collective：多被用于视觉、自然语言处理等需要复杂网络计算（计算密集型）的模型训练任务场景。</li>
</ul>
<h2 id="Parameter-Server"><a href="#Parameter-Server" class="headerlink" title="Parameter Server"></a>Parameter Server</h2><img   src="/2021/08/08/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/ps.jpeg"  class="">
<p>PS分为两大部分：server group和多个worker group，另外resource manager负责总体的资源分配调度。</p>
<ul>
<li>server group内部包含多个server node，每个server node负责维护一部分参数，server manager负责维护和分配server资源；</li>
<li>每个worker group对应一个application（即一个模型训练任务），worker group之间以及worker group内部的worker node互相之间并不通信，worker node只与server通信。</li>
</ul>
<p>具体的架构详解可见：<a class="link"   href="https://zhuanlan.zhihu.com/p/82116922" >一文读懂「Parameter Server」的分布式机器学习训练原理<i class="fas fa-external-link-alt"></i></a></p>
<p>总结一下Parameter Server实现分布式机器学习模型训练的要点：</p>
<ul>
<li>用异步非阻断式的分布式梯度下降策略替代同步阻断式的梯度下降策略；</li>
<li>实现多server节点的架构，避免了单master节点带来的带宽瓶颈和内存瓶颈；</li>
<li>使用一致性哈希，range pull和range push等工程手段实现信息的最小传递，避免广播操作带来的全局性网络阻塞和带宽浪费。</li>
</ul>
<h2 id="Collective"><a href="#Collective" class="headerlink" title="Collective"></a>Collective</h2><p>主要有 <code>TreeAllReduce</code> 和 <code>RingAllReduce</code> 两种。</p>
<h4 id="TreeAllReduce"><a href="#TreeAllReduce" class="headerlink" title="TreeAllReduce"></a>TreeAllReduce</h4><img   src="/2021/08/08/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/TreeAllReduce.jpeg"  class="">
<p>该架构已被抛弃，存在如下两个问题：</p>
<ul>
<li>每一轮的训练迭代都需要所有卡都将数据同步完做一次Reduce才算结束。如果并行的卡很多的时候，就涉及到计算快的卡需要去等待计算慢的卡的情况，造成计算资源的浪费。</li>
<li>每次迭代所有的GPU卡都需要针对全部的模型参数跟Reduce卡进行通信，如果参数的数据量大的时候，那么这种通信开销也是非常庞大，而且这种开销会随着卡数的增加而线性增长。</li>
</ul>
<h4 id="RingAllReduce"><a href="#RingAllReduce" class="headerlink" title="RingAllReduce"></a>RingAllReduce</h4><p>与 <code>TreeAllReduce</code> 不同， <code>RingAllreduce</code> 算法的每次通信成本是恒定的，与系统中GPU的数量无关，完全由系统中gpu之间最慢的连接决定。</p>
<img   src="/2021/08/08/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/RingAllReduce.png"  class="">
<p><code>RingAllReduce</code> 中的GPU排列在一个逻辑环中。 每个GPU应该有一个左邻居和一个右邻居。它只会向其右邻居发送数据，并从其左邻居接收数据。</p>
<p>该算法分两步进行：</p>
<ol>
<li>scatter-reduce：GPU交换数据，使得每个GPU最终得到最终结果的一部分；</li>
<li>all-gather：GPU交换这些块，以便所有GPU最终得到完整的最终结果。</li>
</ol>
<p>具体示例可见：<a class="link"   href="https://blog.csdn.net/lj2048/article/details/108322931" >分布式训练-Ring AllReduce<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://PyTorch.org/docs/master/notes/ddp.html#ddp" >DISTRIBUTED DATA PARALLEL<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/wujianming-110117/p/14398483.html" >分布式训练基本原理<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/82116922" >一文读懂「Parameter Server」的分布式机器学习训练原理<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.zhihu.com/question/57799212/answer/292494636" >ring allreduce和tree allreduce的具体区别是什么？<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/lj2048/article/details/108322931" >分布式训练-Ring AllReduce<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>利用Github Action来自动化部署Hexo博客</title>
    <url>/2022/04/24/%E5%88%A9%E7%94%A8Github%20Action%E6%9D%A5%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2Hexo%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p>这两天尝试了使用Github Action来自动化部署博客，踩了一些坑，在此记录一下。</p>
<span id="more"></span>
<h2 id="新建两个仓库"><a href="#新建两个仓库" class="headerlink" title="新建两个仓库"></a>新建两个仓库</h2><ul>
<li>存放博客源文章的仓库（Source Repo），命名随意，即本地源码push的目标仓库</li>
<li>存放编译后生成的静态文件的仓库（Page Repo），命名 <code>username.github.io</code></li>
</ul>
<h2 id="配置部署密钥"><a href="#配置部署密钥" class="headerlink" title="配置部署密钥"></a>配置部署密钥</h2><p>利用 <code>ssh-keygen</code> 来生成公钥和私钥：</p>
<ul>
<li><p>私钥放于Source仓库的 <code>Settings -&gt; Secrets -&gt; Actions</code> ，新建一个secret，命名为 <code>HEXO_DEPLOY_PRI</code>：<br><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/pri.614aag5in440.png"  alt="pri"></p>
</li>
<li><p>公钥放于Page仓库的 <code>Settings -&gt; Deploy Keys</code> ，新建一个deploy key，命名随意：<br><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/pub.17ltsxgf236k.jpg"  alt="pub"></p>
</li>
</ul>
<h2 id="编写Action"><a href="#编写Action" class="headerlink" title="编写Action"></a>编写Action</h2><p>整个Source仓库的结构如下所示：<br><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/tree.72dyffzppe00.jpg"  alt="tree"><br>只需要保留源文件就行了，其它的依赖交给Action来安装。</p>
<p>在 <code>.github/workflows</code> 新建 <code>deploy.yml</code> 文件，内容如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># workflow name</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">Hexo</span> <span class="string">Blog</span> <span class="string">CI</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># master branch on push, auto run</span></span><br><span class="line"><span class="attr">on:</span> </span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">main</span></span><br><span class="line">      </span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span> </span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span> </span><br><span class="line">        </span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="comment"># check it to your workflow can access it</span></span><br><span class="line">    <span class="comment"># from: https://github.com/actions/checkout</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">Repository</span> <span class="string">master</span> <span class="string">branch</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/checkout@master</span> </span><br><span class="line">      </span><br><span class="line">    <span class="comment"># from: https://github.com/actions/setup-node  </span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Node.js</span> <span class="number">16.</span><span class="string">x</span> </span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/setup-node@master</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">node-version:</span> <span class="string">&quot;16.14.0&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Cache</span> <span class="string">node</span> <span class="string">modules</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/cache@v1</span>    <span class="comment"># 缓存node_modules，避免每次跑action都要重新下载</span></span><br><span class="line">      <span class="attr">id:</span> <span class="string">cache</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">node_modules</span></span><br><span class="line">        <span class="attr">key:</span> <span class="string">$&#123;&#123;</span> <span class="string">runner.os</span> <span class="string">&#125;&#125;-node-$&#123;&#123;</span> <span class="string">hashFiles(&#x27;**/package-lock.json&#x27;)</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">restore-keys:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          $&#123;&#123; runner.os &#125;&#125;-node-</span></span><br><span class="line"><span class="string"></span>    </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Hexo</span> <span class="string">Dependencies</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        npm install hexo-cli -g</span></span><br><span class="line"><span class="string">        npm install</span></span><br><span class="line"><span class="string"></span>    </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">update</span> <span class="string">mathjax</span>    <span class="comment"># kramed引擎有点问题，将其部分文件替换掉</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp correct/hexo-renderer-kramed/renderer.js node_modules/hexo-renderer-kramed/lib/</span></span><br><span class="line"><span class="string">        cp correct/kramed/inline.js node_modules/kramed/lib/rules/</span></span><br><span class="line"><span class="string">        cp correct/hexo-renderer-mathjax/mathjax.html node_modules/hexo-renderer-mathjax</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Deploy</span> <span class="string">Private</span> <span class="string">Key</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">        <span class="attr">HEXO_DEPLOY_PRIVATE_KEY:</span> <span class="string">$&#123;&#123;</span> <span class="string">secrets.HEXO_DEPLOY_PRI</span> <span class="string">&#125;&#125;</span>    <span class="comment"># 这个就是Source仓库的私钥</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        mkdir -p ~/.ssh/</span></span><br><span class="line"><span class="string">        echo &quot;$HEXO_DEPLOY_PRIVATE_KEY&quot; &gt; ~/.ssh/id_rsa </span></span><br><span class="line"><span class="string">        chmod 600 ~/.ssh/id_rsa</span></span><br><span class="line"><span class="string">        ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts</span></span><br><span class="line"><span class="string"></span>        </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Git</span> <span class="string">Infomation</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">| </span></span><br><span class="line"><span class="string">        git config --global user.name &quot;TransformersWsz&quot;</span></span><br><span class="line"><span class="string">        git config --global user.email &quot;3287124026@qq.com&quot;</span></span><br><span class="line"><span class="string"></span>    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Deploy</span> <span class="string">Hexo</span> </span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">        hexo clean</span></span><br><span class="line"><span class="string">        hexo generate </span></span><br><span class="line"><span class="string">        hexo deploy</span></span><br></pre></td></tr></table></figure>
<h3 id="相关字段说明"><a href="#相关字段说明" class="headerlink" title="相关字段说明"></a>相关字段说明</h3><ul>
<li><code>use</code>：引用现有的第三方的action，这样就无需自己写流程了</li>
<li><code>run</code>：运行命令，用法跟linux一致</li>
</ul>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><h3 id="1-自己使用的主题未生效？"><a href="#1-自己使用的主题未生效？" class="headerlink" title="1. 自己使用的主题未生效？"></a>1. 自己使用的主题未生效？</h3><ul>
<li>原因：由于主题是 <code>git clone</code> 下来的，主题目录下生成了 <code>.git</code> 目录，导致和 hexo根目录下 <code>.git</code> 冲突了，commit时没有把主题push上去导致的。</li>
<li>解决办法： 删掉主题下的 <code>.git</code> 文件夹，重新提交，目的是把主题文件夹提交上去（删掉 <code>.git</code> 文件夹后git commit依然没有提交上，需要把主题文件夹剪切出来后 <code>git add &amp;&amp; git commit &amp;&amp; git push</code> 后，再把主题文件夹拷贝回来，再 <code>git add &amp;&amp; git commit &amp;&amp; git push</code> 就可以提交成功了）</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/liuhp123/article/details/114040409" >Github action自动部署Hexo Next<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://sanonz.github.io/2020/deploy-a-hexo-blog-from-github-actions/" >利用 Github Actions 自动部署 Hexo 博客<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://sujie-168.top/2021/05/24/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Github-Actions%E5%AE%9E%E7%8E%B0Hexo%E5%8D%9A%E5%AE%A2%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/" >如何使用Github+Actions实现Hexo博客自动化部署<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>CI</tag>
      </tags>
  </entry>
  <entry>
    <title>千卡GPU训练难点</title>
    <url>/2024/08/05/%E5%8D%83%E5%8D%A1GPU%E8%AE%AD%E7%BB%83%E9%9A%BE%E7%82%B9/</url>
    <content><![CDATA[<p>没吃过猪肉，但也要见识下猪跑：<a class="link"   href="https://www.zhihu.com/question/650979052/answer/3501160453" >你的真实姓名的回答<i class="fas fa-external-link-alt"></i></a></p>
<p>千卡训练经验的含金量：<a class="link"   href="https://www.zhihu.com/question/650979052/answer/3454365035" >Frossmann的回答<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>反向传播</title>
    <url>/2019/05/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    <content><![CDATA[<p>误差反向传播算法简称反向传播算法(Back Propagation)。使用反向传播算法的多层感知器又称为BP神经网络。</p>
<span id="more"></span>
<p>BP算法是一个迭代算法，它的基本思想如下：</p>
<ol>
<li>将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。</li>
<li>由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；</li>
<li>在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。</li>
<li>迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。</li>
</ol>
<h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><p>有如下一个神经网络：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224358906.1bsruizweubk.webp"  alt="1.png"></p>
<p>第一层是输入层，包含两个神经元 $i_1$，$i_2$ 和偏置项 $b_1$；第二层是隐藏层，包含两个神经元 $h_1$，$h_2$ 和偏置项 $b_2$；第三层是输出 $o_1$，$o_2$。每条线上标的 $w_i$ 是层与层之间连接的权重。激活函数是 $sigmod$ 函数。我们用 $z$ 表示某神经元的加权输入和；用 $a$ 表示某神经元的输出。</p>
<p>上述各参数赋值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$i_1$</td>
<td style="text-align:center">0.05</td>
</tr>
<tr>
<td style="text-align:center">$i_2$</td>
<td style="text-align:center">0.10</td>
</tr>
<tr>
<td style="text-align:center">$w_1$</td>
<td style="text-align:center">0.15</td>
</tr>
<tr>
<td style="text-align:center">$w_2$</td>
<td style="text-align:center">0.20</td>
</tr>
<tr>
<td style="text-align:center">$w_3$</td>
<td style="text-align:center">0.25</td>
</tr>
<tr>
<td style="text-align:center">$w_4$</td>
<td style="text-align:center">0.30</td>
</tr>
<tr>
<td style="text-align:center">$w_5$</td>
<td style="text-align:center">0.40</td>
</tr>
<tr>
<td style="text-align:center">$w_6$</td>
<td style="text-align:center">0.45</td>
</tr>
<tr>
<td style="text-align:center">$w_7$</td>
<td style="text-align:center">0.50</td>
</tr>
<tr>
<td style="text-align:center">$w_8$</td>
<td style="text-align:center">0.55</td>
</tr>
<tr>
<td style="text-align:center">$b_1$</td>
<td style="text-align:center">0.35</td>
</tr>
<tr>
<td style="text-align:center">$b_2$</td>
<td style="text-align:center">0.60</td>
</tr>
<tr>
<td style="text-align:center">$o_1$</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">$o_2$</td>
<td style="text-align:center">0.99</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Step-1-前向传播"><a href="#Step-1-前向传播" class="headerlink" title="Step 1 前向传播"></a>Step 1 前向传播</h2><h3 id="输入层-—-gt-隐藏层"><a href="#输入层-—-gt-隐藏层" class="headerlink" title="输入层 —-&gt; 隐藏层"></a>输入层 —-&gt; 隐藏层</h3><p>神经元 $h_1$ 的输入加权和：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{h 1} & =w_1 * i_1+w_2 * i_2+b_1 * 1 \\
   & =0.15 * 0.05+0.2 * 0.1+0.35 * 1 \\
   & =0.3775
\end{aligned}</script><p>神经元 $h_1$ 的输出 $a_{h1}$ ：</p>
<script type="math/tex; mode=display">
a_{h1} = \frac{1}{1+e^{-z_{h1}}} = \frac{1}{1+e^{-0.3775}} = 0.593269992</script><p>同理可得，神经元 $h_2$ 的输出 $a_{h2}$ ：</p>
<script type="math/tex; mode=display">
a_{h2} = 0.596884378</script><h3 id="隐藏层-—-gt-输出层"><a href="#隐藏层-—-gt-输出层" class="headerlink" title="隐藏层 —-&gt; 输出层"></a>隐藏层 —-&gt; 输出层</h3><p>计算输出层神经元 $o1$ 和 $o2$ 的值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   z_{o 1} & =w_5 * a_{h 1}+w_6 * a_{h 2}+b_2 * 1 \\
   & =0.4 * 0.593269992+0.45 * 0.596884378+0.6 * 1 \\
   & =1.105905967 \\
   a_{o 1} & =\frac{1}{1+e^{-z_{o 1}}} \\
   & =\frac{1}{1+e^{-1.105905967}} \\
   & =0.751365069 \\
   a_{o 2} & =0.772928465
\end{aligned}</script><p>前向传播的过程就结束了，我们得到的输出值是 $[0.751365069, 0.772928465]$ ，与实际值 $[0.01, 0.99]$ 相差还很远。接下来我们对误差进行反向传播，更新权值，重新计算输出。</p>
<h2 id="Step-2-反向传播"><a href="#Step-2-反向传播" class="headerlink" title="Step 2 反向传播"></a>Step 2 反向传播</h2><ol>
<li>计算损失函数：</li>
</ol>
<script type="math/tex; mode=display">
E_{total} = \sum\frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $o_1$ 和 $o_2$ 的损失值，总误差为两者之和：</p>
<script type="math/tex; mode=display">
E_{o_1} = \frac {1}{2}(0.01 - 0.751365069)^2 = 0.274811083 \\
E_{o_2} = \frac {1}{2}(0.99 - 0.772928465)^2 = 0.023560026 \\
E_{total} = E_{o_1} + E_{o_2} = 0.274811083 + 0.023560026 = 0.298371109</script><ol>
<li>隐藏层 —-&gt; 输出层的权值更新</li>
</ol>
<p>以权重参数 $w_5$ 为例，如果我们想知道 $w_5$ 对整体损失产生了多少影响，可以用整体损失对 $w_5$ 求偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w_5} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} }*{ \frac {\partial z_{o_1}} {\partial w_5} }</script><p>下面的图可以更直观了解误差是如何反向传播的：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224659584.31gxu503n220.webp"  alt="2.png"></p>
<p>我们现在分别来计算每个式子的值：</p>
<p>计算 $\frac {\partial E_{total}} {\partial a_{o_1}}$ ：</p>
<script type="math/tex; mode=display">
E_{total} = \frac {1}{2}(target_{o_1} - a_{o_1})^2 + \frac {1}{2}(target_{o_2} - a_{o_1})^2 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = 2 * \frac {1}{2} (target_{o_1} - a_{o_1})*-1 \\
\frac {\partial E_{total}} {\partial a_{o_1}} = -(target_{o_1} - a_{o_1}) = 0.751365069-0.01=0.741365069 \\</script><p>计算 $\frac {\partial E_{total}} {\partial a_{o_1}}$ ：</p>
<script type="math/tex; mode=display">
a_{o_1} = \frac {1}{1+e^{-z_{o_1}}} \\
\frac {\partial a_{o_1}} {\partial z_{o_1}} = a_{o_1}*(1-a_{o_1}) = 0.751365069*(1-0.751365069) = 0.186815602</script><p>计算 $\frac {\partial z_{o_1}} {\partial w_5}$ ：</p>
<script type="math/tex; mode=display">
z_{o_1} = w_5*a_{h1} + w_6*a_{h2} + b_2*1 \\
\frac {\partial z_{o_1}} {\partial w_5} = a_{h_1} = 0.593269992</script><p>最后三者相乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = 0.741365069*0.186815602*0.593269992 = 0.082167041</script><p>这样我们就算出整体损失 $E_{total}$ 对 $w_5$ 的偏导值。</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_5} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1}) * a_{h_1}</script><p>针对上述公式，为了表达方便，使用 $\delta_{o_1}$ 来表示输出层的误差：</p>
<script type="math/tex; mode=display">
\delta_{o_1} = {\frac {\partial E_{total}}{\partial a_{o_1}}}*{\frac {\partial a_{o_1}}{\partial z_{o_1}} } = \frac {\partial E_{total}} {\partial z_{o_1}} \\
\delta_{o_1} = -(target_{o_1} - a_{o_1}) * a_{o_1}*(1-a_{o_1})</script><p>因此整体损失 $E_{total}$ 对 $w_5$ 的偏导值可以表示为：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}}{\partial w_5} = \delta_{o_1}*a_{h_1}</script><p>最后我们来更新 $w_5$ 的值：</p>
<script type="math/tex; mode=display">
w_5^+ = w_5 - \eta * \frac {\partial E_{total}} {\partial w_5} = 0.4 - 0.5*0.082167041 = 0.35891648 \qquad \eta: 学习率</script><p>同理可更新 $w_6, w_7, w_8$ ：</p>
<script type="math/tex; mode=display">
w_6^+ = 0.408666186 \\
w_7^+ = 0.511301270 \\
w_8^+ = 0.561370121</script><ol>
<li>隐藏层 —-&gt; 隐藏层的权值更新：</li>
</ol>
<p>计算 $\frac {\partial E_{total}} {\partial w_1}$ 与上述方法类似，但需要注意下图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518224736113.1umei2jgzokg.webp"  alt="3.png"></p>
<p>计算 $\frac {\partial E_{total}} {\partial a_{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = \frac {\partial E_{o_1}} {\partial a_{h_1}} + \frac {\partial E_{o_2}} {\partial a_{h_1}}</script><p>先计算 $\frac {\partial E_{o_1}} {\partial a_{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{o_1}}{\partial a_{h_1}} & =\frac{\partial E_{o_1}}{\partial a_{o_1}} * \frac{\partial a_{o_1}}{\partial z_{o_1}} * \frac{\partial z_{o_1}}{\partial a_{h_1}} \\
   & =0.741365069 * 0.186815602 * 0.4 \\
   & =0.055399425
\end{aligned}</script><p>同理可得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{o_2}} {\partial a_{h_1}} = -0.019049119</script><p>两者相加得：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial a_{h_1}} = 0.055399425 - 0.019049119 = 0.036350306</script><p>计算 $\frac {a_{h_1}} {z_{h_1}}$ ：</p>
<script type="math/tex; mode=display">
\frac {a_{h_1}} {z_{h_1}} = a_{h_1} * (1-a_{h_1}) = 0.593269992*(1-0.593269992) = 0.2413007086</script><p>计算 $\frac {\partial z_{h_1}} {\partial w_1}$</p>
<script type="math/tex; mode=display">
\frac {\partial z_{h_1}} {\partial w_1} = i_1 = 0.05</script><p>最后三者相互乘：</p>
<script type="math/tex; mode=display">
\frac {\partial E_{total}} {\partial w_1} = 0.036350306 * 0.2413007086 * 0.05 = 0.000438568</script><p>为了简化公式，用 $\delta_{h_1}$ 表示隐藏层单元 $h_1$ 的误差： </p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial E_{\text {total }}}{\partial w_1} & =\left(\sum_i \frac{\partial E_{\text {total }}}{\partial a_i} * \frac{\partial a_i}{\partial z_i} * \frac{\partial z_i}{\partial h_1}\right) * \frac{\partial a_{h_1}}{\partial z_{h_1}} * \frac{\partial z_{h_1}}{\partial w_1} \\
   & =\left(\sum_i \delta_i * w_{h_i}\right) * a_{h_1} *\left(1-a_{h_1}\right) * i_1 \\
   & =\delta_{h_1} * i_1
\end{aligned}</script><p>最后更新 $w_1$ 的权值：</p>
<script type="math/tex; mode=display">
w_1^+ = w_1 - \eta * \frac {\partial E_{total}} {\partial w_1} = 0.15 - 0.5*0.000438568 = 0.149780716</script><p>同理，更新 $w_2, w_3, w_4$ 权值：</p>
<script type="math/tex; mode=display">
w_2^+ = 0.19956143 \\
w_3^+ = 0.24975114 \\
w_4^+ = 0.29950229</script><p>这样，反向传播算法就完成了，最后我们再把更新的权值重新计算，不停地迭代。在这个例子中第一次迭代之后，总误差 $E_{total}$ 由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为$[0.015912196,0.984065734](原输入为[0.01,0.99]$ ，证明效果还是不错的。</p>
<h1 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h1><p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225132914.jrimp1wc9o0.webp"  alt="4.png"></p>
<h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$n_l$</td>
<td style="text-align:center">网络层数</td>
</tr>
<tr>
<td style="text-align:center">$y_j$</td>
<td style="text-align:center">输出层第 $j$ 类标签</td>
</tr>
<tr>
<td style="text-align:center">$S_l$</td>
<td style="text-align:center">第 $l$ 层神经元个数（不包括偏置项）</td>
</tr>
<tr>
<td style="text-align:center">$g(x)$</td>
<td style="text-align:center">激活函数</td>
</tr>
<tr>
<td style="text-align:center">$w_{ij}^{l}$</td>
<td style="text-align:center">第 $l-1$ 层的第 $j$ 个神经元连接到第 $l$ 层第 $i$ 个神经元的权重</td>
</tr>
<tr>
<td style="text-align:center">$b_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的偏置</td>
</tr>
<tr>
<td style="text-align:center">$z_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输入加权和</td>
</tr>
<tr>
<td style="text-align:center">$a_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元的输出（激活值）</td>
</tr>
<tr>
<td style="text-align:center">$\delta_i^{l}$</td>
<td style="text-align:center">第 $l$ 层的第 $i$ 个神经元产生的错误</td>
</tr>
</tbody>
</table>
</div>
<h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><h3 id="基本公式"><a href="#基本公式" class="headerlink" title="基本公式"></a>基本公式</h3><script type="math/tex; mode=display">
\begin{aligned}
   z_i^l & =\sum_k w_{i k}^l * a_k^{l-1}+b_j^l \\
   g(x) & =\frac{1}{1+e^{-x}} \\
   a_i^l & =g\left(z_i^l\right) \\
   J(\theta) & =\frac{1}{2} \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2 \\
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l}
\end{aligned}</script><h3 id="梯度方向传播公式推导"><a href="#梯度方向传播公式推导" class="headerlink" title="梯度方向传播公式推导"></a>梯度方向传播公式推导</h3><h4 id="初始条件"><a href="#初始条件" class="headerlink" title="初始条件"></a>初始条件</h4><p>以一个输入样本为例：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^{n l} & =\frac{\partial J(\theta)}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-a_j^{n l}\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} * \frac{\partial \sum_{j=1}^{S_{n l}}\left(y_j-g\left(z_j^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =\frac{1}{2} \frac{\partial\left(y_i-g\left(z_i^{n l}\right)\right)^2}{\partial z_i^{n l}} \\
   & =-\left(y_i-a_i^{n l}\right) g^{\prime}\left(z_i^{l l}\right)
\end{aligned}</script><h4 id="递推公式"><a href="#递推公式" class="headerlink" title="递推公式"></a>递推公式</h4><script type="math/tex; mode=display">
\begin{aligned}
   \delta_i^l & =\frac{\partial J(\theta)}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial z_j^{l+1}}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \frac{\partial J(\theta)}{\partial z_j^{l+1}} * \frac{\partial\left(\sum_k w_{j k}^{l+1} * a_k^l+b_j^{l+1}\right)}{\partial a_i^l} * \frac{\partial a_i^l}{\partial z_i^l} \\
   & =\sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} * g^{\prime}\left(z_i^l\right) \\
   & =g^{\prime}\left(z_i^l\right) * \sum_{j=1}^{S_{l+1}} \delta_j^{l+1} * w_{j i}^{l+1} \\
   \frac{\partial J(\theta)}{\partial w_{i j}^l} & =\frac{\partial J(\theta)}{\partial z_i^l} * \frac{\partial z_i^l}{\partial w_{i j}^l} \\
   & =\delta_i^l * a_j^{l-1} \\
   \frac{\partial J(\theta)}{\partial b_i^l} & =\delta_i^l
\end{aligned}</script><h2 id="反向传播伪代码"><a href="#反向传播伪代码" class="headerlink" title="反向传播伪代码"></a>反向传播伪代码</h2><ol>
<li>输入训练集。</li>
<li>对于训练集的每个样本 $\vec x$ ，设输入层对应的激活值为 $a^l$ ：<ul>
<li>前向传播：$z^l = w^l*a^{l-1}+b^l, a^l = g(z^l)$</li>
<li>计算输出层产生的误差：$\delta^L = \frac {\partial J(\theta)} {\partial a^L} \odot g’(z^L)$</li>
<li>反向传播错误：$\delta^l = ((w^{l+1})^T*\delta^{l+1}) \odot g’(z^l)$</li>
</ul>
</li>
<li>使用梯度下降训练参数：<ul>
<li>$w^l \dashrightarrow w^l - \frac {\alpha} {m} \sum_x\delta^{x, l}*(a^{x, l-1})^T$</li>
<li>$b^l \dashrightarrow  b^l - \frac {\eta} {m} \sum_x\delta^{x, l}$</li>
</ul>
</li>
</ol>
<h1 id="交叉熵损失函数推导"><a href="#交叉熵损失函数推导" class="headerlink" title="交叉熵损失函数推导"></a>交叉熵损失函数推导</h1><p>对于多分类问题，$softmax$ 函数可以将神经网络的输出变成一个概率分布。它只是一个额外的处理层，下图展示了加上了 $softmax$ 回归的神经网络结构图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/20190518225454143.6l2hdid9suw0.webp"  alt="softmax"></p>
<p>递推公式仍然和上述递推公式保持一致。初始条件如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   J(\theta) & =-\sum_{i=1}^{S_{n l}} y^t * \ln y^p \\
   y_j^p & =\frac{e^{a_j^{n l}}}{\sum_{k=1}^{S_{n l}} e^{a_k^{n l}}}
\end{aligned}</script><p>$softmax$ 偏导数计算：</p>
<script type="math/tex; mode=display">
\frac {\partial y_j^p} {\partial a_i^{nl}} =
\left\{
\begin{aligned}
-y_i^p*y_j^p \qquad i \neq j \\
y_i^p*(1-y_i^p) i = j
\end{aligned}
\right.</script><h2 id="推导过程-1"><a href="#推导过程-1" class="headerlink" title="推导过程"></a>推导过程</h2><script type="math/tex; mode=display">
\begin{aligned}
   & \frac{\partial J(\theta)}{\partial z_i^{n l}}=\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial a_i^{n l}}=\sum_{j=1}^{S_{n l}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & \frac{\partial J(\theta)}{\partial y_j^p}=-\frac{y_j^t}{y_j^p}
\end{aligned}</script><p>   由上可知:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial a_i^{n l}} & =\frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}}+\sum_{j \neq i}^{S_{n t}} \frac{\partial J(\theta)}{\partial y_j^p} * \frac{\partial y_j^p}{\partial a_i^{n l}} \\
   & =-\frac{y_i^t}{y_i^p} * y_i^p *\left(1-y_i^p\right)+\sum_{j \neq i}^{S_{n l}}-\frac{y_j^t}{y_j^p} *\left(-y_j^p * y_i^p\right) \\
   & =-y_i^t+y_i^t * y_i^p+\sum_{j \neq i}^{S_{n l l}}\left(y_j^t * y_i^p\right) \\
   & =-y_i^t+\sum_j^{S_{n+1}}\left(y_j^t * y_i^p\right) \\
   & =y_i^p-y_i^t
\end{aligned}</script><p>   $\therefore$ 反向传播选代算法的初始值为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \frac{\partial J(\theta)}{\partial z_i^{n l}} & =\frac{\partial J(\theta)}{\partial a_i^{n l}} * \frac{\partial a_i^{n l}}{\partial z_i^{n l}} \\
   & =\left(y_i^p-y_i^t\right) * a_i^{n l} *\left(1-a_i^{n l}\right)
\end{aligned}</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/charlotte77/p/5629865.html?tdsourcetag=s_pcqq_aiomsg" >一文弄懂神经网络中的反向传播法——BackPropagation<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/u014313009/article/details/51039334" >反向传播算法（过程及公式推导）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/nowgood/p/backprop2.html?tdsourcetag=s_pcqq_aiomsg" >反向传播公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>古代万物称呼究竟可以有多美？</title>
    <url>/2023/04/27/%E5%8F%A4%E4%BB%A3%E4%B8%87%E7%89%A9%E7%A7%B0%E5%91%BC%E7%A9%B6%E7%AB%9F%E5%8F%AF%E4%BB%A5%E6%9C%89%E5%A4%9A%E7%BE%8E%EF%BC%9F/</url>
    <content><![CDATA[<p><a class="link"   href="https://zhuanlan.zhihu.com/p/561800590" >古代万物称呼究竟可以有多美？<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>文学</tag>
      </tags>
  </entry>
  <entry>
    <title>古诗句</title>
    <url>/2023/07/02/%E5%8F%A4%E8%AF%97%E5%8F%A5/</url>
    <content><![CDATA[<p>记录一些古诗：<br><span id="more"></span></p>
<ul>
<li>春风若有怜花意，可否许我再少年</li>
<li>试问闲愁都几许，一川烟草，满城风絮，梅子黄诗雨</li>
<li>最是人间留不住，朱颜辞镜花辞树</li>
<li>若教眼底无离恨，不信人间有白头</li>
<li>满堂花醉三千客，一剑霜寒十四州</li>
<li>为当梦是浮生事，为复浮生是梦中<ul>
<li>难道说梦境就是人生的真相吗？还是说人生本身就是一场梦？</li>
</ul>
</li>
<li>何须更问浮生事，只此浮生是梦中<ul>
<li>何必在意这空虚不实的人生中的事情，这大千世界、人生都像是一场梦。</li>
</ul>
</li>
<li>荼蘼不争春，寂寞开最晚</li>
<li>红莲相倚浑如醉，百鸟无言定自愁</li>
</ul>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>古诗</tag>
      </tags>
  </entry>
  <entry>
    <title>召回和排序的样本构造问题</title>
    <url>/2023/09/14/%E5%8F%AC%E5%9B%9E%E5%92%8C%E6%8E%92%E5%BA%8F%E7%9A%84%E6%A0%B7%E6%9C%AC%E6%9E%84%E9%80%A0%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>简单介绍一下搜广推系统中的正负样本构造问题。</p>
<span id="more"></span>
<h2 id="精排"><a href="#精排" class="headerlink" title="精排"></a>精排</h2><ul>
<li>正样本：曝光点击</li>
<li>负样本：曝光未点击</li>
</ul>
<h2 id="粗排"><a href="#粗排" class="headerlink" title="粗排"></a>粗排</h2><ul>
<li>正样本：曝光点击</li>
<li>负样本：如果只复用精排的负样本，粗排模型对精排模型的拟合就会出现比较大的偏差。因为粗排打分高的item可能会被精排打低分，导致不能下发曝光。而精排的正负样本量很少，粗排只见到了精排的样本，对于自己打分高的item，并不知道其正负属性，在下一次打分中，可能仍然会对其打高分。因此除了精排的负样本，仍然需要从精排未下发的item中负采样一部分，作为粗排的负样本。</li>
</ul>
<h2 id="召回"><a href="#召回" class="headerlink" title="召回"></a>召回</h2><ul>
<li>正样本：曝光点击</li>
<li>负样本：曝光未点击+全库随机负采样。召回线上面临的环境，是从全库良莠不齐的物料中找到用户可能感兴趣的item，而如果只拿曝光未点击做负样本训练会导致样本选择偏差。全库随机负采样可以模拟线上这一分布，大部分物料是跟用户打不着的。具体的采样策略可以见：<a class="link"   href="https://zhuanlan.zhihu.com/p/165064102" >拿随机采样做负样本<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/352961688" >召回和粗排负样本构造问题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/165064102" >负样本为王：评Facebook的向量化召回算法<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/453776850" >模型利器 - 召回/排序中的负样本优化方法<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>召回</tag>
        <tag>粗排</tag>
        <tag>精排</tag>
        <tag>正样本</tag>
        <tag>负样本</tag>
      </tags>
  </entry>
  <entry>
    <title>各开源协议一览</title>
    <url>/2025/04/07/%E5%90%84%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE%E4%B8%80%E8%A7%88/</url>
    <content><![CDATA[<p>在 GitHub 上，开源项目通常会使用一些常见的开源协议来定义项目的使用、修改和分发规则。以下是目前 GitHub 上最常见的几种开源协议及其差异和示例说明：</p>
<span id="more"></span>
<hr>
<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a><strong>TL;DR</strong></h2><div class="table-container">
<table>
<thead>
<tr>
<th><strong>协议</strong></th>
<th><strong>宽松程度</strong></th>
<th><strong>是否强制开源</strong></th>
<th><strong>专利保护</strong></th>
<th><strong>适用场景</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MIT</strong></td>
<td>最宽松</td>
<td>否</td>
<td>无</td>
<td>希望代码被广泛使用</td>
</tr>
<tr>
<td><strong>Apache 2.0</strong></td>
<td>宽松</td>
<td>否</td>
<td>有</td>
<td>希望提供专利保护</td>
</tr>
<tr>
<td><strong>GPL</strong></td>
<td>严格</td>
<td>是</td>
<td>无</td>
<td>希望确保代码始终开源</td>
</tr>
<tr>
<td><strong>LGPL</strong></td>
<td>较宽松</td>
<td>部分是</td>
<td>无</td>
<td>希望代码被更广泛集成</td>
</tr>
<tr>
<td><strong>BSD</strong></td>
<td>宽松</td>
<td>否</td>
<td>无</td>
<td>希望代码被广泛使用且简单</td>
</tr>
<tr>
<td><strong>MPL 2.0</strong></td>
<td>中等</td>
<td>部分是</td>
<td>无</td>
<td>希望代码部分开源但允许混合</td>
</tr>
</tbody>
</table>
</div>
<h2 id="协议详解"><a href="#协议详解" class="headerlink" title="协议详解"></a><strong>协议详解</strong></h2><h3 id="1-MIT-License"><a href="#1-MIT-License" class="headerlink" title="1. MIT License"></a><strong>1. MIT License</strong></h3><ul>
<li><strong>特点</strong>：<ul>
<li>非常宽松，几乎没有任何限制。</li>
<li>允许用户自由使用、复制、修改、合并、发布、分发、再授权甚至用于商业用途。</li>
<li>唯一要求是保留原始版权声明和许可声明。</li>
</ul>
</li>
<li><strong>适用场景</strong>：<ul>
<li>适合希望代码被广泛使用的开发者。</li>
<li>示例：<a class="link"   href="https://github.com/vuejs/vue" >Vue.js<i class="fas fa-external-link-alt"></i></a> 使用了 MIT License 。</li>
</ul>
</li>
<li><strong>差异</strong>：<ul>
<li>相较于其他协议（如 GPL），MIT 不强制要求衍生作品也必须开源。</li>
</ul>
</li>
</ul>
<h3 id="2-Apache-License-2-0"><a href="#2-Apache-License-2-0" class="headerlink" title="2. Apache License 2.0"></a><strong>2. Apache License 2.0</strong></h3><ul>
<li><strong>特点</strong>：<ul>
<li>提供了明确的专利授权条款，保护用户免受潜在的专利诉讼。</li>
<li>允许用户自由使用、修改和分发代码，但需要保留版权声明和许可证文件。</li>
<li>明确限制商标使用，不允许用原作者的商标进行宣传。</li>
</ul>
</li>
<li><strong>适用场景</strong>：<ul>
<li>适合希望保护知识产权并提供专利保障的项目。</li>
<li>示例：<a class="link"   href="https://github.com/apache/kafka" >Apache Kafka<i class="fas fa-external-link-alt"></i></a> 使用了 Apache License 2.0 。</li>
</ul>
</li>
<li><strong>差异</strong>：<ul>
<li>比 MIT 更加详细，特别是关于专利和商标的规定。</li>
</ul>
</li>
</ul>
<h3 id="3-GNU-General-Public-License-GPL"><a href="#3-GNU-General-Public-License-GPL" class="headerlink" title="3. GNU General Public License (GPL)"></a><strong>3. GNU General Public License (GPL)</strong></h3><ul>
<li><strong>特点</strong>：<ul>
<li>强制性开源，任何基于 GPL 代码的衍生作品也必须以 GPL 协议发布。</li>
<li>用户可以自由使用、修改和分发代码，但必须公开源码。</li>
</ul>
</li>
<li><strong>适用场景</strong>：<ul>
<li>适合希望确保代码始终开源的项目。</li>
<li>示例：<a class="link"   href="https://github.com/torvalds/linux" >Linux Kernel<i class="fas fa-external-link-alt"></i></a> 使用了 GPL 。</li>
</ul>
</li>
<li><strong>差异</strong>：<ul>
<li>相较于 MIT 和 Apache，GPL 对衍生作品有更强的约束力。</li>
</ul>
</li>
</ul>
<h3 id="4-Lesser-GNU-General-Public-License-LGPL"><a href="#4-Lesser-GNU-General-Public-License-LGPL" class="headerlink" title="4. Lesser GNU General Public License (LGPL)"></a><strong>4. Lesser GNU General Public License (LGPL)</strong></h3><ul>
<li><strong>特点</strong>：<ul>
<li>是 GPL 的一个变种，允许将 LGPL 代码作为库链接到闭源项目中。</li>
<li>衍生作品如果是独立模块，可以不公开源码；但如果修改了 LGPL 库本身，则必须公开修改后的代码。</li>
</ul>
</li>
<li><strong>适用场景</strong>：<ul>
<li>适合希望代码被更广泛地集成到商业项目中的库类项目。</li>
<li>示例：<a class="link"   href="https://www.gnu.org/software/libc/" >GNU C Library (glibc)<i class="fas fa-external-link-alt"></i></a> 使用了 LGPL 。</li>
</ul>
</li>
<li><strong>差异</strong>：<ul>
<li>比 GPL 更宽松，但仍要求对库本身的修改保持开源。</li>
</ul>
</li>
</ul>
<h3 id="5-BSD-License"><a href="#5-BSD-License" class="headerlink" title="5. BSD License"></a><strong>5. BSD License</strong></h3><ul>
<li><strong>特点</strong>：<ul>
<li>类似于 MIT，非常宽松。</li>
<li>分为两种主要版本：2-Clause（简化版）和 3-Clause（禁止用项目名称做广告）。</li>
<li>要求保留版权声明和许可证文件。</li>
</ul>
</li>
<li><strong>适用场景</strong>：<ul>
<li>适合希望代码被广泛使用且不介意闭源衍生作品的项目。</li>
<li>示例：<a class="link"   href="https://github.com/freebsd/freebsd-src" >FreeBSD<i class="fas fa-external-link-alt"></i></a> 使用了 BSD License 。</li>
</ul>
</li>
<li><strong>差异</strong>：<ul>
<li>相较于 MIT，3-Clause 版本增加了对广告的限制。</li>
</ul>
</li>
</ul>
<h3 id="6-Mozilla-Public-License-2-0-MPL-2-0"><a href="#6-Mozilla-Public-License-2-0-MPL-2-0" class="headerlink" title="6. Mozilla Public License 2.0 (MPL 2.0)"></a><strong>6. Mozilla Public License 2.0 (MPL 2.0)</strong></h3><ul>
<li><strong>特点</strong>：<ul>
<li>是 BSD 系协议和 GPL 系协议的折中。</li>
<li>要求对 MPL 覆盖的代码部分保持开源，但允许与闭源代码混合。</li>
<li>必须保留版权信息，并公开对覆盖代码的修改。</li>
</ul>
</li>
<li><strong>适用场景</strong>：<ul>
<li>适合希望代码部分开源但允许与其他闭源代码协作的项目。</li>
<li>示例：<a class="link"   href="https://github.com/mozilla/gecko-dev" >Firefox<i class="fas fa-external-link-alt"></i></a> 使用了 MPL 2.0 。</li>
</ul>
</li>
<li><strong>差异</strong>：<ul>
<li>比 GPL 更宽松，但比 MIT 和 BSD 更严格。</li>
</ul>
</li>
</ul>
<h3 id="7-Creative-Commons-CC"><a href="#7-Creative-Commons-CC" class="headerlink" title="7. Creative Commons (CC)"></a><strong>7. Creative Commons (CC)</strong></h3><ul>
<li><strong>特点</strong>：<ul>
<li>主要用于非代码内容（如文档、图片、音乐等）。</li>
<li>提供多种版本，包括 CC0（完全放弃版权）、CC BY（署名即可使用）等。</li>
</ul>
</li>
<li><strong>适用场景</strong>：<ul>
<li>适合非软件项目或需要灵活授权的内容。</li>
<li>示例：<a class="link"   href="https://en.wikipedia.org/" >Wikipedia<i class="fas fa-external-link-alt"></i></a> 的部分内容使用了 CC BY-SA 。</li>
</ul>
</li>
<li><strong>差异</strong>：<ul>
<li>不适用于传统意义上的代码项目。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Open Source License</tag>
      </tags>
  </entry>
  <entry>
    <title>四种常见的POST提交数据方式</title>
    <url>/2017/12/15/%E5%9B%9B%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84POST%E6%8F%90%E4%BA%A4%E6%95%B0%E6%8D%AE%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<p>HTTP/1.1 协议规定的 HTTP 请求方法有 <code>OPTIONS、GET、HEAD、POST、PUT、DELETE、TRACE、CONNECT</code> 这几种。其中 POST 一般用来向服务端提交数据，本文主要讨论 POST 提交数据的几种方式。</p>
<span id="more"></span>
<p>我们知道，HTTP 协议是以 ASCII 码传输，建立在 TCP/IP 协议之上的应用层规范。规范把 HTTP 请求分为三个部分：<font color="green">状态行、请求头、消息主体</font>。类似于下面这样：<br><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">method</span>&gt;</span> <span class="tag">&lt;<span class="name">request-URL</span>&gt;</span> <span class="tag">&lt;<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">headers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">entity-body</span>&gt;</span></span><br></pre></td></tr></table></figure><br>协议规定 POST 提交的数据必须放在消息主体（entity-body）中，但协议并没有规定数据必须使用什么编码方式。实际上，开发者完全可以自己决定消息主体的格式，只要最后发送的 HTTP 请求满足上面的格式就可以。</p>
<p>但是，数据发送出去，还要服务端解析成功才有意义。一般服务端语言如 php、python 等，以及它们的 framework，都内置了自动解析常见数据格式的功能。服务端通常是根据请求头（headers）中的 Content-Type 字段来获知请求中的消息主体是用何种方式编码，再对主体进行解析。所以说到 POST 提交数据方案，包含了 Content-Type 和消息主体编码方式两部分。下面就正式开始介绍它们。</p>
<h2 id="application-x-www-form-urlencoded"><a href="#application-x-www-form-urlencoded" class="headerlink" title="application/x-www-form-urlencoded"></a><font color="blue">application/x-www-form-urlencoded</font></h2><p>这应该是最常见的 POST 提交数据的方式了。浏览器的原生 <code>&lt;form&gt;</code> 表单，如果不设置 <code>enctype</code> 属性，那么最终就会以 <code>application/x-www-form-urlencoded</code> 方式提交数据。请求类似于下面这样（无关的请求头在本文中都省略掉了）：<br><figure class="highlight html"><table><tr><td class="code"><pre><span class="line">POST http://www.example.com HTTP/1.1</span><br><span class="line">Content-Type: application/x-www-form-urlencoded;charset=utf-8</span><br><span class="line"></span><br><span class="line">title=test&amp;sub%5B%5D=1&amp;sub%5B%5D=2&amp;sub%5B%5D=3</span><br></pre></td></tr></table></figure><br>首先，<code>Content-Type</code> 被指定为 <code>application/x-www-form-urlencoded</code>；其次，提交的数据按照 <code>key1=val1&amp;key2=val2</code> 的方式进行编码，key 和 val 都进行了 URL 转码。大部分服务端语言都对这种方式有很好的支持。例如 PHP 中，<code>$_POST[&#39;title&#39;]</code> 可以获取到 title 的值，<code>$_POST[&#39;sub&#39;]</code> 可以得到 sub 数组。</p>
<p>很多时候，我们用 Ajax 提交数据时，也是使用这种方式。例如 JQuery 和 QWrap 的 Ajax，Content-Type 默认值都是<code>application/x-www-form-urlencoded;charset=utf-8</code>。</p>
<h2 id="multipart-form-data"><a href="#multipart-form-data" class="headerlink" title="multipart/form-data"></a><font color="blue">multipart/form-data</font></h2><p>这又是一个常见的 POST 数据提交的方式。我们使用表单上传文件时，必须让 <code>&lt;form&gt;</code> 表单的 <code>enctype</code> 等于 <code>multipart/form-data</code>。直接来看一个请求示例：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">POST http://www.example.com HTTP/1.1</span><br><span class="line">Content-Type:multipart/form-data; boundary=----WebKitFormBoundaryrGKCBY7qhFd3TrwA</span><br><span class="line"></span><br><span class="line">------WebKitFormBoundaryrGKCBY7qhFd3TrwA</span><br><span class="line">Content-Disposition: form-data; name=&quot;text&quot;</span><br><span class="line"></span><br><span class="line">title</span><br><span class="line">------WebKitFormBoundaryrGKCBY7qhFd3TrwA</span><br><span class="line">Content-Disposition: form-data; name=&quot;file&quot;; filename=&quot;chrome.png&quot;</span><br><span class="line">Content-Type: image/png</span><br><span class="line"></span><br><span class="line">PNG ... content of chrome.png ...</span><br><span class="line">------WebKitFormBoundaryrGKCBY7qhFd3TrwA--</span><br></pre></td></tr></table></figure>
<p>这个例子稍微复杂点。首先生成了一个 <code>boundary</code> 用于分割不同的字段，为了避免与正文内容重复，<code>boundary</code> 很长很复杂。然后 <code>Content-Type</code> 里指明了数据是以 <code>multipart/form-data</code> 来编码，本次请求的 <code>boundary</code> 是什么内容。消息主体里按照字段个数又分为多个结构类似的部分，每部分都是以 <code>--boundary</code> 开始，紧接着是内容描述信息，然后是回车，最后是字段具体内容（文本或二进制）。如果传输的是文件，还要包含文件名和文件类型信息。消息主体最后以 <code>--boundary--</code> 标示结束。关于 <code>multipart/form-data</code> 的详细定义，请前往 <a class="link"   href="http://www.ietf.org/rfc/rfc1867.txt" >rfc1867<i class="fas fa-external-link-alt"></i></a> 查看。</p>
<p>这种方式一般用来上传文件，各大服务端语言对它也有着良好的支持。</p>
<p>上面提到的这两种 POST 数据的方式，都是浏览器原生支持的，而且现阶段标准中原生 <code>&lt;form&gt;</code> 表单也只支持这两种方式（通过 <code>&lt;form&gt;</code> 元素的 <code>enctype</code> 属性指定，默认为 <code>application/x-www-form-urlencoded</code>。其实 <code>enctype</code> 还支持 <code>text/plain</code>，不过用得非常少）。</p>
<p>随着越来越多的 Web 站点，尤其是 WebApp，全部使用 Ajax 进行数据交互之后，我们完全可以定义新的数据提交方式，给开发带来更多便利。</p>
<h2 id="application-json"><a href="#application-json" class="headerlink" title="application/json"></a><font color="blue">application/json</font></h2><p><code>application/json</code> 这个 <code>Content-Type</code> 作为响应头大家肯定不陌生。实际上，现在越来越多的人把它作为请求头，用来告诉服务端消息主体是序列化后的 JSON 字符串。由于 JSON 规范的流行，除了低版本 IE 之外的各大浏览器都原生支持 <code>JSON.stringify</code>，服务端语言也都有处理 JSON 的函数，使用 JSON 不会遇上什么麻烦。</p>
<p>JSON 格式支持比键值对复杂得多的结构化数据，这一点也很有用。记得我几年前做一个项目时，需要提交的数据层次非常深，我就是把数据 JSON 序列化之后来提交的。不过当时我是把 JSON 字符串作为 val，仍然放在键值对里，以 <code>x-www-form-urlencoded</code> 方式提交。</p>
<p>Google 的 <a class="link"   href="http://angularjs.org/" >AngularJS<i class="fas fa-external-link-alt"></i></a> 中的 Ajax 功能，默认就是提交 JSON 字符串。例如下面这段代码：<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> data = &#123;<span class="string">&#x27;title&#x27;</span>:<span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;sub&#x27;</span> : [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]&#125;;</span><br><span class="line">$http.<span class="title function_">post</span>(url, data).<span class="title function_">success</span>(<span class="keyword">function</span>(<span class="params">result</span>) &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<p>最终发送的请求是：<br><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">POST http<span class="punctuation">:</span><span class="comment">//www.example.com HTTP/1.1 </span></span><br><span class="line">Content-Type<span class="punctuation">:</span> application/json;charset=utf<span class="number">-8</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;title&quot;</span><span class="punctuation">:</span><span class="string">&quot;test&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sub&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><br>这种方案，可以方便的提交复杂的结构化数据，特别适合 <code>RESTful</code> 的接口。各大抓包工具如 Chrome 自带的开发者工具、Firebug、Fiddler，都会以树形结构展示 JSON 数据，非常友好。但也有些服务端语言还没有支持这种方式，例如 php 就无法通过 <code>$_POST</code> 对象从上面的请求中获得内容。这时候，需要自己动手处理下：在请求头中 <code>Content-Type</code> 为 <code>application/json</code> 时，从 <code>php://input</code> 里获得原始输入流，再 <code>json_decode</code> 成对象。一些 php 框架已经开始这么做了。</p>
<p>当然 <a class="link"   href="http://angularjs.org/" >AngularJS<i class="fas fa-external-link-alt"></i></a> 也可以配置为使用 <code>x-www-form-urlencoded</code> 方式提交数据。如有需要，可以参考 <a class="link"   href="http://victorblog.com/2012/12/20/make-angularjs-http-service-behave-like-jquery-ajax/" >这篇文章<i class="fas fa-external-link-alt"></i></a>。</p>
<h2 id="text-xml"><a href="#text-xml" class="headerlink" title="text/xml"></a><font color="blue">text/xml</font></h2><p>我的博客之前提到过 <a class="link"   href="http://www.imququ.com/post/64.html" >XML-RPC (XML Remote Procedure Call)<i class="fas fa-external-link-alt"></i></a>。它是一种使用 HTTP 作为传输协议，XML 作为编码方式的远程调用规范。典型的 XML-RPC 请求是这样的：<br><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">POST http://www.example.com HTTP/1.1 </span><br><span class="line">Content-Type: text/xml</span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">methodCall</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">methodName</span>&gt;</span>examples.getStateName<span class="tag">&lt;/<span class="name">methodName</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">params</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span><span class="tag">&lt;<span class="name">i4</span>&gt;</span>41<span class="tag">&lt;/<span class="name">i4</span>&gt;</span><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">param</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">params</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">methodCall</span>&gt;</span></span><br></pre></td></tr></table></figure><br>XML-RPC 协议简单、功能够用，各种语言的实现都有。它的使用也很广泛，如 WordPress 的 <a class="link"   href="http://codex.wordpress.org/XML-RPC_WordPress_API" >XML-RPC Api<i class="fas fa-external-link-alt"></i></a>，搜索引擎的 <a class="link"   href="http://help.baidu.com/question?prod_en=master&amp;class=476&amp;id=1000423" >ping 服务<i class="fas fa-external-link-alt"></i></a> 等等。JavaScript 中，也有 <a class="link"   href="http://plugins.jquery.com/xmlrpc/" >现成的库<i class="fas fa-external-link-alt"></i></a> 支持以这种方式进行数据交互，能很好的支持已有的 XML-RPC 服务。不过，我个人觉得 XML 结构还是过于臃肿，一般场景用 JSON 会更灵活方便。</p>
<hr>
<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><ul>
<li><a class="link"   href="https://imququ.com/post/four-ways-to-post-data-in-http.html" >四种常见的 POST 提交数据方式<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Software Engineering</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>POST</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title>图像生成评估指标IS&amp;FID</title>
    <url>/2025/09/18/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87IS-FID/</url>
    <content><![CDATA[<p><strong>IS（Inception Score）</strong> 和 <strong>FID（Fréchet Inception Distance）</strong> 是评估生成模型（特别是GAN、Diffusion等）最常见的两个指标</p>
<span id="more"></span>
<h2 id="Inception-Score-IS"><a href="#Inception-Score-IS" class="headerlink" title="Inception Score (IS)"></a>Inception Score (IS)</h2><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li>希望生成的图片<strong>清晰且多样</strong>。</li>
<li>用预训练好的<strong>Inception v3 分类网络</strong>来评估生成图片的质量。</li>
</ul>
<h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>对生成图片$x$，Inception网络输出类别分布$p(y|x)$。</p>
<ul>
<li><strong>清晰度</strong>：若图片清晰，$p(y|x)$ 应该高度集中（熵低）。</li>
<li><strong>多样性</strong>：若生成结果多样，整体类别分布 $p(y) = \int_x p(y|x) dx$ 应该接近均匀（熵高）。</li>
</ul>
<p>定义 IS：</p>
<script type="math/tex; mode=display">
IS = \exp\left(\mathbb{E}_{x} \left[ D_{KL}(p(y|x) \parallel p(y)) \right]\right)</script><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li><strong>优点</strong>：简单直观，广泛使用。</li>
<li><p><strong>缺点</strong>：</p>
<ol>
<li>依赖 Inception v3 分类器，不一定适用于非 ImageNet 数据集。</li>
<li>只看类别分布，不直接衡量“真实分布的接近度”。</li>
</ol>
</li>
</ul>
<h2 id="Frechet-Inception-Distance-FID"><a href="#Frechet-Inception-Distance-FID" class="headerlink" title="Fréchet Inception Distance (FID)"></a>Fréchet Inception Distance (FID)</h2><h3 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h3><ul>
<li>用统计方式比较<strong>真实图片分布</strong>与<strong>生成图片分布</strong>的接近程度。</li>
<li>在Inception v3特征空间里，把数据分布近似成高斯分布，然后计算两者的Fréchet 距离（Wasserstein-2 距离）。</li>
</ul>
<h3 id="公式-1"><a href="#公式-1" class="headerlink" title="公式"></a>公式</h3><p>设真实图片特征的分布为 $\mathcal{N}(\mu_r, \Sigma_r)$，生成图片特征的分布为$\mathcal{N}(\mu_g, \Sigma_g)$。<br>FID 定义为：</p>
<script type="math/tex; mode=display">
FID = \|\mu_r - \mu_g\|^2 + \mathrm{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)</script><h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p><strong>优点</strong>：</p>
<ol>
<li>能综合反映图像的质量和多样性。</li>
<li>与人类感知一致性更高。</li>
<li>可以比较不同数据集。</li>
</ol>
</li>
<li><p><strong>缺点</strong>：</p>
<ol>
<li>特征分布假设为高斯，近似可能不准。</li>
<li>计算时需要足够样本，否则估计不稳定。</li>
</ol>
</li>
</ul>
<h2 id="对比总结"><a href="#对比总结" class="headerlink" title="对比总结"></a>对比总结</h2><div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>思想</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>IS</strong></td>
<td>用 KL 散度衡量单图置信度与整体多样性</td>
<td>简单、计算快</td>
<td>依赖 Inception，不能直接衡量与真实分布的差距</td>
</tr>
<tr>
<td><strong>FID</strong></td>
<td>在特征空间拟合高斯，计算两分布差异</td>
<td>更符合人类感知，能比较生成与真实数据</td>
<td>需要更多样本，假设近似可能偏差</td>
</tr>
</tbody>
</table>
</div>
<p>👉 直观理解：</p>
<ul>
<li><strong>IS 高 → 图像清晰且类别多样</strong></li>
<li><strong>FID 低 → 生成分布接近真实分布</strong></li>
</ul>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>基于特征函数的数据蒸馏方法</title>
    <url>/2025/04/02/%E5%9F%BA%E4%BA%8E%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E8%92%B8%E9%A6%8F%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>本篇是CVPR满分作文，聚焦于数据蒸馏工作，创新点在于将数据合成问题建模成对抗性的minmax优化问题。</p>
<span id="more"></span>
<p>具体而言，本文引入了基于特征函数的分布差异度量方法，该方法能够完整地刻画一个分布的所有信息（相位和幅度）。利用特征函数的性质，最小化合成数据与真实数据的分布差异实现仿真，优化采样策略来最大化两者分布差异实现多样性和鲁棒性。</p>
<h2 id="方法对比"><a href="#方法对比" class="headerlink" title="方法对比"></a>方法对比</h2><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.41y8d5i61s.webp"  alt="example"></p>
<h2 id="方法详解"><a href="#方法详解" class="headerlink" title="方法详解"></a>方法详解</h2><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.pfiis4bti.webp"  alt="model"></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.26lnkjaker.webp"  alt="result"></p>
<p>各项实验指标确实提升很明显。</p>
<hr>
<ul>
<li><a class="link"   href="https://arxiv.org/pdf/2502.20653" >Dataset Distillation with Neural Characteristic Function: A Minmax Perspective<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/LhrnQnyA3dlf_bHm98_iLA" >CVPR 2025 满分论文，极简的数据蒸馏！<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Data Distillation</tag>
      </tags>
  </entry>
  <entry>
    <title>多标签分类新建模方法</title>
    <url>/2024/03/18/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E6%96%B0%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>常见的多标签分类方法是同时生成多个标签的logits，然后接一个sigmoid激活函数做二分类。该方法简单直接，但忽略了标签之间的相关性。虽然业界针对该问题提出了很多解决思路，但大多是任务特定，通用性不强，也不够优雅。</p>
<span id="more"></span>
<p>Transformer decoder倒是可以序列输出多个标签，但却加入了位置偏差。而标签之间是没有位置关系的，谁先谁后无所谓，只要输出全就行。这样也导致数据集不好构造。</p>
<h2 id="C-Tran"><a href="#C-Tran" class="headerlink" title="C-Tran"></a>C-Tran</h2><p><a class="link"   href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lanchantin_General_Multi-Label_Image_Classification_With_Transformers_CVPR_2021_paper.pdf" >General Multi-label Image Classification with Transformers<i class="fas fa-external-link-alt"></i></a> 这篇论文提供了新思路，类似BERT的MLM预训练任务：通过在输入端对多个标签做随机mask，然后预测被mask的标签，从而强制模型去学习标签之间的依赖关系：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.92pw1se1sg.png"  alt="model"></p>
<p>模型细节：<br><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.b8nozrlq2.webp"  alt="detail"></p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.7p3cxrf3jv.webp"  alt="params"></p>
<ul>
<li>Label Embeddings: 可学习的参数矩阵，由模型隐式学习到标签的语义信息和标签间依赖。有点像DETR的query</li>
<li>State Embeddings: 控制标签的mask比例，这样就跟标签学习实现了解耦，也方便在推理阶段注入全比例mask</li>
</ul>
<h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>不说了，全是sota：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.92pw1sv1ar.webp"  alt="exp"></p>
<hr>
<ul>
<li>旷视用gcn来建模多标签方法(被C-Tran超越了，建模思路可以学习)：<a class="link"   href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Multi-Label_Image_Recognition_With_Graph_Convolutional_Networks_CVPR_2019_paper.pdf" >Multi-Label Image Recognition with Graph Convolutional Networks<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>Multi-label Classification</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态大模型-从BLIP到LLaVA</title>
    <url>/2024/08/25/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E4%BB%8EBLIP%E5%88%B0LLaVA/</url>
    <content><![CDATA[<p>多模态大模型方面的经典工作：<a class="link"   href="https://mp.weixin.qq.com/s/2TrRO1QZtlL0T0PjuQoRhg" >多模态大模型: 盘点&amp;Highlights part1——从BLIP到LLaVA<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Multi-Modal</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型融合方法-DARE</title>
    <url>/2024/03/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95-DARE/</url>
    <content><![CDATA[<p>LLM在SFT之后会产生大量的冗余参数(delta参数)，阿里团队提出DARE方法来消除delta参数，并将其合并到PRE模型中，从而实现多源模型能力的吸收。</p>
<p>DARE无需GPU重新训练，其思路非常简单，就跟dropout类似：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\boldsymbol{m}^t \sim \operatorname{Bernoulli}(p) \\
\widetilde{\boldsymbol{\delta}}^t=\left(\mathbf{1}-\boldsymbol{m}^t\right) \odot \boldsymbol{\delta}^t \\
\hat{\boldsymbol{\delta}}^t=\widetilde{\boldsymbol{\delta}}^t /(1-p)  \\
\boldsymbol{\theta}_{\mathrm{DARE}}^t=\hat{\boldsymbol{\delta}}^t+\boldsymbol{\theta}_{\mathrm{PRE}}
\end{gathered}</script><p>两个步骤：</p>
<ol>
<li>drop：随机mask参数为0</li>
<li>rescale：对保存的参数rescale，这样可以保证神经元期望值不变：$E_{not_{mask}}=x,E_{mask}=\frac{p*x}{p}$</li>
</ol>
<p>传统的模型融合只是对神经元进行加权求和，这样会导致模型能力骤降。DARE方法通过dropout避免了这种问题。</p>
<h2 id="多源模型融合"><a href="#多源模型融合" class="headerlink" title="多源模型融合"></a>多源模型融合</h2><script type="math/tex; mode=display">
\begin{gathered}
\boldsymbol{\theta}_{\mathrm{DARE}}^{t_k}=\operatorname{DARE}\left(\boldsymbol{\theta}_{\mathrm{SFT}}^{t_k}, \boldsymbol{\theta}_{\mathrm{PRE}}\right), \text { for } 1 \leq k \leq K, \\
\boldsymbol{\theta}_{\mathrm{M}}=\boldsymbol{\theta}_{\mathrm{PRE}}+\lambda \cdot \sum_{k=1}^K \hat{\boldsymbol{\delta}}^{t_k}=\boldsymbol{\theta}_{\mathrm{PRE}}+\lambda \cdot \sum_{k=1}^K\left(\boldsymbol{\theta}_{\mathrm{DARE}}^{t_k}-\boldsymbol{\theta}_{\mathrm{PRE}}\right) .
\end{gathered}</script><p>流程图：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.3d4k8ni4bl.png"  alt="procedure"></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.5q76puxcia.webp"  alt="result"></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/YiqWovBUXIbzmUbL6uT-8g" >丢弃99%的参数！阿里团队提出语言模型合体术，性能暴涨且无需重新训练和GPU<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/yule-BUAA/MergeLM" >MergeLM<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>模型融合</tag>
      </tags>
  </entry>
  <entry>
    <title>对数正态分布LogNormal</title>
    <url>/2025/06/03/%E5%AF%B9%E6%95%B0%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/</url>
    <content><![CDATA[<p>如果$\ln X \sim \mathcal{N}(\mu, \sigma^2)$，那么$X$服从对数正态分布，它的PDF是：$\frac{1}{x \sigma \sqrt{2\pi}} \exp \left( -\frac{(\ln x - \mu)^2}{2\sigma^2} \right)$</p>
<span id="more"></span>
<h2 id="图例"><a href="#图例" class="headerlink" title="图例"></a>图例</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm, lognorm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">mu = <span class="number">0.0</span><span class="comment"># 正态分布的均值</span></span><br><span class="line">sigma = <span class="number">0.5</span> <span class="comment"># 正态分布的标准差</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成x轴数据</span></span><br><span class="line">x_normal = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">500</span>)<span class="comment"># 正态分布定义域：实数范围</span></span><br><span class="line">x_lognormal = np.linspace(<span class="number">0.01</span>, <span class="number">5</span>, <span class="number">500</span>) <span class="comment"># 对数正态定义域：x &gt; 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算PDF</span></span><br><span class="line">pdf_normal = norm.pdf(x_normal, mu, sigma) <span class="comment"># 正态分布PDF</span></span><br><span class="line">pdf_lognormal = lognorm.pdf(x_lognormal, s=sigma, scale=np.exp(mu))<span class="comment"># 对数正态PDF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制图形</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子图1：正态分布</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(x_normal, pdf_normal, <span class="string">&#x27;b-&#x27;</span>, lw=<span class="number">2</span>, label=<span class="string">f&#x27;N(μ=<span class="subst">&#123;mu&#125;</span>, σ=<span class="subst">&#123;sigma&#125;</span>)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Normal Distribution PDF&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability Density&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子图2：对数正态分布</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(x_lognormal, pdf_lognormal, <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">2</span>, label=<span class="string">f&#x27;Log-N(μ=<span class="subst">&#123;mu&#125;</span>, σ=<span class="subst">&#123;sigma&#125;</span>)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Log-Normal Distribution PDF&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x (x &gt; 0)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability Density&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.51ee7115w7.webp"  alt="case"></p>
<h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><p>关键在于理解概率密度变换（Probability Density Transformation）的数学原理。重点解释为什么分母是$x$而不是$\ln x$。</p>
<h4 id="核心问题"><a href="#核心问题" class="headerlink" title="核心问题"></a>核心问题</h4><p>已知$\ln X \sim \mathcal{N}(\mu, \sigma^2)$，即$Y = \ln X$服从正态分布，其概率密度函数（PDF）为：</p>
<script type="math/tex; mode=display">
f_Y(y) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right), \quad y \in \mathbb{R}</script><p>但我们需要的是$X = e^Y$的分布，即$X$的PDF$f_X(x)$。</p>
<h4 id="概率密度变换的推导"><a href="#概率密度变换的推导" class="headerlink" title="概率密度变换的推导"></a>概率密度变换的推导</h4><p>从$Y$到$X$的变换是一个非线性变换（ $X = e^Y$ ），因此需要用到 变量替换定理（Change of Variables Theorem）。具体步骤如下：</p>
<h5 id="1-变换关系"><a href="#1-变换关系" class="headerlink" title="1. 变换关系"></a>1. 变换关系</h5><p>$Y = \ln X$ ，即$X = e^Y$。变换的雅可比行列式（Jacobian）为：</p>
<script type="math/tex; mode=display">
\left|\frac{d y}{d x}\right|=\frac{1}{x} \quad(\text { 因为 } y=\ln x \Longrightarrow d y / d x=1 / x)</script><h5 id="2-PDF变换公式"><a href="#2-PDF变换公式" class="headerlink" title="2. PDF变换公式"></a>2. PDF变换公式</h5><p>对于单调变换$X = g(Y)$，概率密度满足：</p>
<script type="math/tex; mode=display">
f_X(x) = f_Y(y) \cdot \left|\frac{dy}{dx} \right|</script><p>将$y = \ln x$和雅可比行列式代入：</p>
<script type="math/tex; mode=display">
f_X(x) = f_Y(\ln x) \cdot \frac{1}{x}</script><h5 id="3-代入正态分布PDF"><a href="#3-代入正态分布PDF" class="headerlink" title="3. 代入正态分布PDF"></a>3. 代入正态分布PDF</h5><p>将 $f_Y(y)$ 的表达式代入：</p>
<script type="math/tex; mode=display">
f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(\ln x - \mu)^2}{2\sigma^2} \right) \cdot \frac{1}{x}</script><p>合并后即得到对数正态分布的PDF：</p>
<script type="math/tex; mode=display">
f_X(x) = \frac{1}{x \sigma \sqrt{2\pi}} \exp \left( -\frac{(\ln x - \mu)^2}{2\sigma^2} \right), \quad x > 0</script><p>为什么分母是$x$而不是$\ln x$？</p>
<p>关键原因：分母的$x$来自 雅可比行列式$\frac{dy}{dx} = \frac{1}{x}$，它是对数变换$Y = \ln X$的导数。</p>
<p>如果强行改为$\ln x$，会破坏概率密度的积分性质（即 $\int f_X(x) dx = 1$），导致分布不合法。</p>
<p>物理意义：$x$是原始变量，而$\ln x$是变换后的变量。PDF必须反映原始变量的概率密度，因此需要乘以$\frac{1}{x}$来修正缩放比例。</p>
<h4 id="4-验证积分是否为1"><a href="#4-验证积分是否为1" class="headerlink" title="4. 验证积分是否为1"></a>4. 验证积分是否为1</h4><p>可以验证$f_X(x)$的积分：</p>
<script type="math/tex; mode=display">
\int_0^\infty \frac{1}{x \sigma \sqrt{2\pi}} \exp \left( -\frac{(\ln x - \mu)^2}{2\sigma^2} \right) dx = 1</script><p>通过变量替换$u = \ln x ， du = \frac{1}{x} dx$ ，积分变为：</p>
<script type="math/tex; mode=display">
\int_{-\infty}^\infty \frac{1}{\sigma \sqrt{2\pi}} e^{-(u - \mu)^2 / 2\sigma^2} du = 1</script><p>这正是标准正态分布的积分性质。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>分母的$x$是数学推导的必然结果，源于概率密度变换的雅可比行列式。它保证了$f_X(x)$是一个合法的概率密度函数（积分为1）。若替换为$\ln x$，会破坏分布的正确性。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>LogNormal</tag>
        <tag>PDF</tag>
      </tags>
  </entry>
  <entry>
    <title>寻找两个正序数组的中位数</title>
    <url>/2021/03/12/%E5%AF%BB%E6%89%BE%E4%B8%A4%E4%B8%AA%E6%AD%A3%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/</url>
    <content><![CDATA[<p>这道题题目描述很简单，但却是leetcode <code>hard</code>难度。如果用传统的二分查找方法来做，那么边界情况将非常多。</p>
<span id="more"></span>
<p>本题将寻找两个有序数组的中位数看作是从两个有序数组中查找第<code>k</code>小元素，具体讲解见：<a class="link"   href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by-w-2/" >详细通俗的思路分析，多解法（解法三）<i class="fas fa-external-link-alt"></i></a>。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMedianSortedArrays</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">getKthElement</span>(<span class="params">k</span>):</span><br><span class="line">            idx1, idx2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                <span class="keyword">if</span> idx1 == m:</span><br><span class="line">                    <span class="keyword">return</span> nums2[idx2+k-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> idx2 == n:</span><br><span class="line">                    <span class="keyword">return</span> nums1[idx1+k-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> k == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="built_in">min</span>(nums1[idx1], nums2[idx2])</span><br><span class="line">                </span><br><span class="line">                newidx1 = <span class="built_in">min</span>(idx1 + k//<span class="number">2</span> -<span class="number">1</span>, m-<span class="number">1</span>)</span><br><span class="line">                newidx2 = <span class="built_in">min</span>(idx2 + k//<span class="number">2</span> -<span class="number">1</span>, n-<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> nums1[newidx1] &gt;= nums2[newidx2]:</span><br><span class="line">                    k -= newidx2-idx2+<span class="number">1</span></span><br><span class="line">                    idx2 = newidx2+<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    k -= newidx1-idx1+<span class="number">1</span></span><br><span class="line">                    idx1 = newidx1+<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        m = <span class="built_in">len</span>(nums1)</span><br><span class="line">        n = <span class="built_in">len</span>(nums2)</span><br><span class="line">        <span class="keyword">if</span> (m+n)%<span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> getKthElement((m+n)//<span class="number">2</span>+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (getKthElement((m+n)//<span class="number">2</span>) + getKthElement( (m+n)//<span class="number">2</span>+<span class="number">1</span> )) / <span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>二分查找</tag>
      </tags>
  </entry>
  <entry>
    <title>小天狼星</title>
    <url>/2025/03/02/%E5%B0%8F%E5%A4%A9%E7%8B%BC%E6%98%9F/</url>
    <content><![CDATA[<p><strong>小天狼星·布莱克</strong>（Sirius Black）是《哈利·波特》系列中的一个重要角色，他是哈利·波特的教父，也是哈利父亲詹姆·波特（James Potter）最好的朋友之一。小天狼星是一个复杂而悲剧性的人物，他的经历和命运贯穿了整个系列。</p>
<span id="more"></span>
<h3 id="背景与早期经历"><a href="#背景与早期经历" class="headerlink" title="背景与早期经历"></a>背景与早期经历</h3><ol>
<li><p><strong>出身与家族</strong><br>小天狼星出生于纯血统巫师家族布莱克家族，这是一个极端崇尚纯血统、鄙视麻瓜和混血巫师的家族。然而，小天狼星从小就对家族的观念感到厌恶，并在进入霍格沃茨后被分到格兰芬多学院，这进一步加深了他与家族的矛盾。</p>
</li>
<li><p><strong>与詹姆·波特的友谊</strong><br>在霍格沃茨期间，小天狼星与詹姆·波特、莱姆斯·卢平（Remus Lupin）和小矮星彼得（Peter Pettigrew）成为最好的朋友。他们四人组成了“掠夺者”（Marauders），并共同制作了活点地图（Marauders’ Map）。</p>
</li>
<li><p><strong>对抗伏地魔</strong><br>在第一次巫师战争中，小天狼星加入了凤凰社（Order of the Phoenix），与詹姆等人一起对抗伏地魔（Lord Voldemort）。然而，由于小矮星彼得的背叛，詹姆和莉莉·波特被伏地魔杀害，而小天狼星被错误地指控为凶手。</p>
</li>
</ol>
<h3 id="被诬陷与逃亡"><a href="#被诬陷与逃亡" class="headerlink" title="被诬陷与逃亡"></a>被诬陷与逃亡</h3><ol>
<li><p><strong>被诬陷与入狱</strong><br>小天狼星被指控为杀害詹姆和莉莉的凶手，并被认为是出卖他们的叛徒。他被捕后未经审判就被关进了阿兹卡班（Azkaban）监狱，在那里度过了12年。</p>
</li>
<li><p><strong>越狱与真相</strong><br>小天狼星在得知小矮星彼得还活着后，成功越狱并开始寻找彼得。在《哈利·波特与阿兹卡班的囚徒》中，他与哈利相遇，并揭露了真相：真正的叛徒是小矮星彼得，而非小天狼星。</p>
</li>
</ol>
<h3 id="与哈利的关系"><a href="#与哈利的关系" class="headerlink" title="与哈利的关系"></a>与哈利的关系</h3><ol>
<li><p><strong>教父与监护人</strong><br>小天狼星是哈利的教父，他对哈利充满了关爱和责任感。他多次冒着生命危险帮助哈利，并试图成为哈利的监护人。</p>
</li>
<li><p><strong>格里莫广场12号</strong><br>在《哈利·波特与凤凰社》中，小天狼星将家族的老宅格里莫广场12号（12 Grimmauld Place）作为凤凰社的总部，为对抗伏地魔提供了重要支持。</p>
</li>
</ol>
<h3 id="结局"><a href="#结局" class="headerlink" title="结局"></a>结局</h3><ol>
<li><p><strong>在神秘事务司的战斗</strong><br>在《哈利·波特与凤凰社》的结尾，小天狼星在神秘事务司（Department of Mysteries）的战斗中与食死徒（Death Eaters）对抗。他在与堂姐贝拉特里克斯·莱斯特兰奇（Bellatrix Lestrange）的决斗中被她的咒语击中，跌入了帷幔（Veil）中，不幸身亡。</p>
</li>
<li><p><strong>哈利的悲痛</strong><br>小天狼星的死对哈利造成了巨大的打击，因为他是哈利最亲近的亲人之一，也是哈利对抗伏地魔的重要支持者。</p>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>小天狼星·布莱克是一个忠诚、勇敢但命运多舛的角色。他的一生充满了悲剧：被家族排斥、被朋友背叛、被错误地关押，最终在战斗中牺牲。然而，他对哈利无私的爱和对正义的坚持使他成为《哈利·波特》系列中最受喜爱的角色之一。他的死亡不仅让哈利失去了重要的亲人，也加深了故事的情感深度。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>哈利·波特</tag>
        <tag>小天狼星</tag>
      </tags>
  </entry>
  <entry>
    <title>布隆过滤器误判率计算</title>
    <url>/2023/07/12/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E8%AF%AF%E5%88%A4%E7%8E%87%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<p>记录一下布隆过滤器误判率的计算过程：</p>
<span id="more"></span>
<p>假设布隆过滤器的长度为$m$，需要插入的元素个数为$n$，哈希函数个数为$k$：</p>
<ol>
<li>先插入一个元素，某个位置没有被置为1的概率为：$1-\frac{1}{m}$</li>
<li>经过$k$个哈希函数后，仍然没有被置为1的概率：$(1-\frac{1}{m})^k$</li>
<li>插入了$n$个元素，仍然没有被置为1的概率：$(1-\frac{1}{m})^{kn}$；反之被置为1的概率：$1-(1-\frac{1}{m})^{kn}$</li>
<li>现处于query阶段，来了一个元素待插入到过滤器中，如果插入的位置全部为1，则会产生误判，其概率为：<script type="math/tex; mode=display">
(1-(1-\frac{1}{m})^{kn})^k</script></li>
</ol>
<p>根据定理：$当\mathrm{x} \rightarrow 0时， (1+\mathrm{x})^{\frac{1}{x}} \sim e$，进一步推导：</p>
<script type="math/tex; mode=display">
\begin{align}
(1-(1-\frac{1}{m})^{kn})^k &= \left(1-\left(1-\frac{1}{m}\right)^{-m \cdot \frac{-k n}{m}}\right)^k \\
&\sim \left(1-e^{-\frac{n k}{m}}\right)^k
\end{align}</script><p>最终：$P_{error} = \left(1-e^{-\frac{n k}{m}}\right)^k$</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/gaoyueace/article/details/90410735" >布隆过滤器概念及其公式推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Bloom Filter</tag>
      </tags>
  </entry>
  <entry>
    <title>常用排序算法的比较</title>
    <url>/2021/04/09/%E5%B8%B8%E7%94%A8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83/</url>
    <content><![CDATA[<p>记录一下各种常见排序算法的比较。</p>
<span id="more"></span>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">思想</th>
<th style="text-align:center">最好时间复杂度</th>
<th style="text-align:center">最坏时间复杂度</th>
<th>平均时间复杂度</th>
<th style="text-align:center">空间复杂度</th>
<th style="text-align:center">是否稳定</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">冒泡排序</td>
<td style="text-align:center">两两比较相邻记录的关键字，如果反序则交换，直到没有反序的记录为止</td>
<td style="text-align:center">$O(n)$</td>
<td style="text-align:center">$O(n^2)$</td>
<td>$O(n^2)$</td>
<td style="text-align:center">$O(1)$</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">插入排序</td>
<td style="text-align:center">把$n$个待排序的元素看成为一个有序表和一个无序表。开始时有序表中只包含1个元素，无序表中包含有$n-1$个元素，排序过程中每次从无序表中取出第一个元素，将它插入到有序表中的适当位置，使之成为新的有序表，重复$n-1$次可完成排序过程</td>
<td style="text-align:center">$O(n)$</td>
<td style="text-align:center">$O(n^2)$</td>
<td>$O(n^2)$</td>
<td style="text-align:center">$O(1)$</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">简单选择排序</td>
<td style="text-align:center">通过$n-i$次关键字之间的比较，从$n-i+1$个记录中选择关键字最小的记录，并和第$i(1 \le i \le n)$个记录交换之</td>
<td style="text-align:center">$O(n^2)$</td>
<td style="text-align:center">$O(n^2)$</td>
<td>$O(n^2)$</td>
<td style="text-align:center">$O(1)$</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">归并排序</td>
<td style="text-align:center">分治算法，是建立在归并操作上的一种有效的排序算法。常用的2路归并排序假设初始序列有$n$个记录，可以看成是$n$个长度为1的子序列，进行两两归并，可以得到$\frac{n}{2}$个长度为2的子序列；再两两归并,直到得到一个长度为$n$的有序序列为止</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">$O(nlogn)$</td>
<td>$O(nlogn)$</td>
<td style="text-align:center">$O(n)$</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">堆排序</td>
<td style="text-align:center">把待排序的序列构造成一个大顶堆，此时序列的最大值就是队顶元素，把该元素放在最后，然后对剩下的$n-1$个元素继续构造大顶堆，直到排序完成</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">$O(nlogn)$</td>
<td>$O(nlogn)$</td>
<td style="text-align:center">$O(1)$</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">快速排序</td>
<td style="text-align:center">通过一趟排序将待排记录分割成独立的两部分，其中一部分的记录都比另一部分小，然后再分别对这两个部分进行快速排序，最终实现整个序列的排序</td>
<td style="text-align:center">$O(nlogn)$</td>
<td style="text-align:center">$O(n^2)$</td>
<td>$O(nlogn)$</td>
<td style="text-align:center">$O(logn)$</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>常见NLP面试问答</title>
    <url>/2021/03/30/%E5%B8%B8%E8%A7%81NLP%E9%9D%A2%E8%AF%95%E9%97%AE%E7%AD%94/</url>
    <content><![CDATA[<p>NLP面试中的经典八股文。</p>
<span id="more"></span>
<h2 id="1-HMM-vs-MEMM-vs-CRF"><a href="#1-HMM-vs-MEMM-vs-CRF" class="headerlink" title="1. HMM vs MEMM vs CRF"></a>1. HMM vs MEMM vs CRF</h2><h4 id="HMM-gt-MEMM"><a href="#HMM-gt-MEMM" class="headerlink" title="HMM -&gt; MEMM"></a>HMM -&gt; MEMM</h4><p>HMM模型中存在两个假设：</p>
<ol>
<li>输出观察值之间严格独立。MEMM解决了HMM输出独立性假设的问题。因为HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。</li>
<li>状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。</li>
</ol>
<h4 id="MEMM-gt-CRF"><a href="#MEMM-gt-CRF" class="headerlink" title="MEMM -&gt; CRF:"></a>MEMM -&gt; CRF:</h4><ul>
<li>CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。</li>
<li>HMM、MEMM属于有向图，所以考虑了x与y的影响，但没将x当做整体考虑进去（这点问题应该只有HMM）。CRF属于无向图，没有这种依赖性，克服此问题。</li>
</ul>
<hr>
<h2 id="2-常见的几种优化器"><a href="#2-常见的几种优化器" class="headerlink" title="2. 常见的几种优化器"></a>2. 常见的几种优化器</h2><ol>
<li>SGD</li>
</ol>
<script type="math/tex; mode=display">
\theta \leftarrow \theta-\eta \nabla_{\theta} J(\theta)</script><p>$\eta$ 是学习率，$J(\theta)$ 是损失函数</p>
<ol>
<li>Momentum</li>
</ol>
<script type="math/tex; mode=display">
\begin{array}{l}
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J(\theta) \\
\theta=\theta-v_{t}
\end{array}</script><p>当我们将一个小球从山上滚下来时，没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。<br>加入的这一项，可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。</p>
<p><strong>超参数设定值:  一般 γ 取值 0.9 左右。</strong></p>
<ol>
<li>Nesterov</li>
</ol>
<script type="math/tex; mode=display">
\begin{array}{l}
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right) \\
\theta=\theta-v_{t}
\end{array}</script><p>用 $\theta-\gamma v_{t-1}$ 来近似当做参数下一步会变成的值，则在计算梯度时，不是在当前位置，而是未来的位置上。</p>
<ol>
<li>AdaGrad</li>
</ol>
<p>这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性。</p>
<script type="math/tex; mode=display">
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i}</script><p>其中 $g_{t,i}$ 是 $t$ 时刻参数 $\theta_{i}$ 的梯度，$G_{t, ii}$ (对角矩阵 $G_t$ 的 $(i,i)$ 元素)就是 $t$ 时刻参数 $\theta_i$ 的梯度平方和。</p>
<p>超参数设定值：一般 $\eta$ 选取0.01</p>
<ol>
<li>RMSprop</li>
</ol>
<p>RMSprop 都是为了解决 Adagrad 学习率急剧下降问题的：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
E\left[g^{2}\right]_{t}=0.9 E\left[g^{2}\right]_{t-1}+0.1 g_{t}^{2} \\
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t}
\end{array}</script><p>使用的是指数加权平均，旨在消除梯度下降中的摆动，与Momentum的效果一样，某一维度的导数比较大，则指数加权平均就大，某一维度的导数比较小，则其指数加权平均就小，这样就保证了各维度导数都在一个量级，进而减少了摆动。允许使用一个更大的学习率 $\eta$ 。</p>
<p>超参数设定值： $\gamma$ 为 0.9，$\eta$ 为 0.001</p>
<ol>
<li>Adam</li>
</ol>
<p>相当于 RMSprop + Momentum：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
m_{t}=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\
v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}
\end{array}</script><p>除了像 momentum 一样保持了过去梯度 $m_t$ 的指数衰减平均值， 也像Adadelta 和 RMSprop 一样存储了过去梯度的平方 $v_t$ 的指数衰减平均值。</p>
<p>如果 $m_t$ 和 $v_t$ 都被初始化为0，那么它们会向0偏置，要做偏差纠正。通过计算偏差校正后的 $m_t$ 和 $v_t$ 来抵消这些偏差：</p>
<script type="math/tex; mode=display">
\hat{m}_t = \frac{m_t} {1-\beta_1^t} \\
\hat{v}_t = \frac{v_t} {1-\beta_2^t}</script><p>梯度更新规则：</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon} \hat{m}_t</script><p>超参数设定值： $\beta_1$ 为 0.9，$\beta_2$ 为0.999，$\epsilon$ 为 $10^{-8}$</p>
<hr>
<h2 id="3-Self-Attention添加head数量是否会增加计算复杂度？"><a href="#3-Self-Attention添加head数量是否会增加计算复杂度？" class="headerlink" title="3. Self-Attention添加head数量是否会增加计算复杂度？"></a>3. Self-Attention添加head数量是否会增加计算复杂度？</h2><p>不会。self-attention的时间复杂度为$O(n^2 \times d)$，$n$ 为序列长度，$d$ 为维度。假设分成 $h$ 个头，那么张量shape为 $h \times n \times m$ 。其中 $d = h \times m$ 。每个头做self-attention的时间复杂度为 $O(n^2 \times m)$ ，那么 $h$ 个头的总时间复杂度为 $O(h \times n^2 \times m) = O(n^2 \times d)$ 。因此增加头的数量不会导致计算复杂度增加。</p>
<hr>
<h2 id="4-L1和L2正则化"><a href="#4-L1和L2正则化" class="headerlink" title="4. L1和L2正则化"></a>4. L1和L2正则化</h2><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><ul>
<li>优点：输出具有稀疏性，即产生一个稀疏模型，进而可以用于特征选择；一定程度上，L1可以防止过拟合</li>
<li>缺点：非稀疏情况下计算效率低</li>
</ul>
<h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><ul>
<li>优点：计算效率高（因为存在解析解）；可以防止模型过拟合</li>
<li>缺点：非稀疏输出；无特征选择</li>
</ul>
<hr>
<h2 id="5-方差与偏差"><a href="#5-方差与偏差" class="headerlink" title="5. 方差与偏差"></a>5. 方差与偏差</h2><blockquote>
<p>偏差：用所有可能的训练数据集训练出的所有模型的输出的平均值与真实模型的输出值之间的差异。<br>方差：不同的训练数据集训练出的模型输出值之间的差异。</p>
</blockquote>
<ul>
<li>欠拟合：高偏差，低方差</li>
<li>过拟合：高偏差，高方差</li>
</ul>
<hr>
<h2 id="6-正负样例分布不均衡解决办法"><a href="#6-正负样例分布不均衡解决办法" class="headerlink" title="6. 正负样例分布不均衡解决办法"></a>6. 正负样例分布不均衡解决办法</h2><ol>
<li>过采样与欠采样：<ul>
<li>过抽样：通过增加分类中少数类样本的数量来实现样本均衡</li>
<li>欠抽样：通过减少分类中多数类样本的数量来实现样本均衡</li>
</ul>
</li>
<li>通过正负样本的惩罚权重解决样本不均衡：对于分类中不同样本数量的类别分别赋予不同的权重，一般是小样本量类别权重高，大样本量类别权重低。</li>
</ol>
<hr>
<h2 id="7-词汇表太大，softmax计算如何优化？"><a href="#7-词汇表太大，softmax计算如何优化？" class="headerlink" title="7. 词汇表太大，softmax计算如何优化？"></a>7. 词汇表太大，softmax计算如何优化？</h2><p>Hierarchical Softmax根据单词出现的频率来构建一颗霍夫曼树。树的叶子结点代表一个单词，在每一个非叶子节点处都需要作一次二分类，走左边的概率和走右边的概率，这里用逻辑回归的公式表示：</p>
<ul>
<li>正类别：$\sigma\left(X_{i} \theta\right)=\frac{1}{1+e^{-x_{i} \theta}}$</li>
<li>负类别：$1-\sigma\left(X_{i} \theta\right)$<br>每个词都会有一条路径，根据训练样本的特征向量 $X_i$ 预测目标label词 $Y_i$ 的概率为：<script type="math/tex; mode=display">
P\left(Y_{i} \mid X_{i}\right)=\prod_{j=2}^{l} P\left(d_{j} \mid X_{i}, \theta_{j-1}\right) \\
P\left(d_{j} \mid X_{i}, \theta_{j-1}\right)=\left\{\begin{array}{ll}
\sigma\left(X_{i} \theta\right), & \text { if } \mathrm{d_j}=1 \\
1-\sigma\left(X_{i} \theta\right), & \text { if } \mathrm{d_j}=0
\end{array}\right.</script>详细见：</li>
<li><a class="link"   href="https://www.cnblogs.com/eniac1946/p/8818892.html" >层次softmax函数（hierarchical softmax）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/56139075" >Hierarchical Softmax（层次Softmax）<i class="fas fa-external-link-alt"></i></a><h2 id="8-LSTM如何解决梯度弥散或爆炸？"><a href="#8-LSTM如何解决梯度弥散或爆炸？" class="headerlink" title="8. LSTM如何解决梯度弥散或爆炸？"></a>8. LSTM如何解决梯度弥散或爆炸？</h2>LSTM的介绍见：<a class="link"   href="https://zhuanlan.zhihu.com/p/44124492" >LSTM：RNN最常用的变体<i class="fas fa-external-link-alt"></i></a><br>梯度问题见：</li>
<li><a class="link"   href="https://www.cnblogs.com/bonelee/p/10475453.html" >LSTM如何解决梯度消失或爆炸的？<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.zhihu.com/question/34878706" >LSTM如何来避免梯度弥散和梯度爆炸？<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="9-简述EM算法的流程"><a href="#9-简述EM算法的流程" class="headerlink" title="9. 简述EM算法的流程"></a>9. 简述EM算法的流程</h2><p>输入：观察数据 $x=\left(x^{(1)}, x^{(2)}, \ldots x^{(m)}\right),$ 联合分布 $p(x, z ; \theta),$ 条件分布 $p(z \mid x ; \theta),$ 最大迭代次数 $J$</p>
<ol>
<li>随机初始化模型参数 $\theta$ 的初值 $\theta^{0}$</li>
<li>$for \quad j \quad from \quad 1 \quad to \quad j$:<br>a) E步。计算联合分布的条件概率期望：<script type="math/tex; mode=display">
\begin{array}{c}
Q_{i}\left(z^{(i)}\right)=P\left(z^{(i)} \mid x^{(i)}, \theta^{j}\right) \\
L\left(\theta, \theta^{j}\right)=\sum_{i=1}^{m} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log P\left(x^{(i)}, z^{(i)} ; \theta\right)
\end{array}</script>b) M步。极大化 $L\left(\theta, \theta^{j}\right),$ 得到 $\theta^{j+1}$ :<script type="math/tex; mode=display">
\theta^{j+1}=\underset{\theta}{\arg \max } L\left(\theta, \theta^{j}\right)</script>c) 如果 $\theta^{j+1}$ 收敛, 则算法结束。否则继续回到步骤 a) 进行E步迭代<br>输出：模型参数 $\theta$ 。<h3 id="具体示例可见："><a href="#具体示例可见：" class="headerlink" title="具体示例可见："></a>具体示例可见：</h3></li>
</ol>
<ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/36331115" >人人都懂EM算法<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/v_JULY_v/article/details/81708386" >如何通俗理解EM算法<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/pinard/p/6912636.html" >EM算法原理总结<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="10-LR、SVM、决策树的对比"><a href="#10-LR、SVM、决策树的对比" class="headerlink" title="10. LR、SVM、决策树的对比"></a>10. LR、SVM、决策树的对比</h2><h3 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li>实现简单高效</li>
<li>对观测样本概率输出<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4></li>
<li>特征空间太大时表现不太好</li>
<li>对于非线性特征须要作特征变换</li>
<li>需要额外添加正则项<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4></li>
<li>能够处理高维特征 </li>
<li>自带正则项</li>
<li>使用核函数轻松应对非线性特征空间</li>
<li>分类面不依赖于全部数据<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4></li>
<li>核函数选择较难</li>
<li>样本量非常大，核函数映射维度非常高时，计算量过大</li>
<li>对缺失数据敏感</li>
</ol>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ol>
<li>决策过程直观</li>
<li>可以处理非线性特征<h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4></li>
<li>容易过拟合</li>
<li>无法输出概率，只能输出分类结果</li>
</ol>
<hr>
<h2 id="11-SVM常用核函数"><a href="#11-SVM常用核函数" class="headerlink" title="11. SVM常用核函数"></a>11. SVM常用核函数</h2><ol>
<li>线性核函数</li>
<li>多项式核函数</li>
<li>高斯核函数</li>
<li>sigmoid核函数</li>
</ol>
<p>详细见：<a class="link"   href="https://blog.csdn.net/batuwuhanpei/article/details/52354822" >svm常用核函数<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h2 id="12-k-means与EM联系与区别"><a href="#12-k-means与EM联系与区别" class="headerlink" title="12. k-means与EM联系与区别"></a>12. k-means与EM联系与区别</h2><blockquote>
<p>两者都是无监督学习。</p>
</blockquote>
<p>k-means可以看成是两阶段的：</p>
<ul>
<li>第一阶段，确定每一个样本所属的聚类，在这个过程中，聚类的中心保持不变。可以看作EM的E步。</li>
<li>第二阶段，确定聚类中心，在这个过程中，每一个样本所属的类别保持不变。可以看作EM的M步。</li>
</ul>
<p>EM算法和K-Means算法的迭代过程比较类似，不同的是K-Means算法中每次对参数的更新是硬猜测，而EM中每次对参数的更新是软猜测；相同的是，两个算法都可能得到局部最优解，采用不同的初始参数迭代会有利于得到全局最优解。</p>
<p>详细见：</p>
<ul>
<li><a class="link"   href="https://www.jianshu.com/p/2c42c567e893" >机器学习笔记11: K-Means算法和EM算法<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.cnblogs.com/youyouzaLearn/p/9471409.html" >k-Means与EM之间的关系<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="13-Xavier原理"><a href="#13-Xavier原理" class="headerlink" title="13. Xavier原理"></a>13. Xavier原理</h2><blockquote>
<p>为了使得网络中信息更好的流动，每一层输出的方差应该尽量相等。</p>
</blockquote>
<p>先贴结论：</p>
<script type="math/tex; mode=display">
w \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_{i n}+n_{\text {out }}}}, \frac{\sqrt{6}}{\sqrt{n_{\text {in }}+n_{\text {out }}}}\right]</script><p>具体的公式推导见：</p>
<ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/27919794" >深度前馈网络与Xavier初始化原理
<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/VictoriaW/article/details/73000632" >深度学习之参数初始化（一）——Xavier初始化<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.jianshu.com/p/f2d800388d1c" >一文搞懂深度网络初始化（Xavier and Kaiming initialization）<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="14-目标检测综述"><a href="#14-目标检测综述" class="headerlink" title="14. 目标检测综述"></a>14. 目标检测综述</h2><h3 id="Two-stage方法"><a href="#Two-stage方法" class="headerlink" title="Two-stage方法"></a>Two-stage方法</h3><h4 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h4><ol>
<li>通过Selective Search（SS）方法筛选出一些备选的区域框（Region proposal）；</li>
<li>CNN提取特征，SVM分类；</li>
<li>分类完成后，对bbox进行回归，修正bbox中的坐标的值，得到更精确的bbox。</li>
</ol>
<h4 id="SPP-net"><a href="#SPP-net" class="headerlink" title="SPP-net"></a>SPP-net</h4><ul>
<li>R-CNN中，每个区域都要过一次CNN 提取特征。而SPP-net中，一张图片只需要过一次CNN，特征提取是针对整张图进行的，候选区域的框定以及特征向量化是在CNN的feature map层面进行的。</li>
<li>提出自适应池化的方法，它分别对输入的feature map（可以由不定尺寸的输入图像进CNN得到，也可由region proposal 框定后进CNN 得到）进行多个尺度（实际上就是改变pooling 的size 和stride）的池化，分别得到特征，并进行向量化后拼接起来。无需像R-CNN一样对所有的Region proposal进行缩放得到相同的大小。</li>
</ul>
<h4 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h4><ul>
<li>提出了ROI pooling 的结构，实际上就是一种特殊的SPP（相当于SPP 的金字塔层数设置为了1，即只计算一次池化）。</li>
<li>将最终的SVM分类去掉了，直接做成了端到端的一个网络结构。对这个网络进行多任务训练，即分类和回归，得到物体类别和bbox的位置。</li>
</ul>
<h4 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h4><p>提出RPN网络：利用一个与检测器共享部分权重的RPN 网络来直接对图片生成候选框，然后基于RPN 得到的候选框进行分类和位置回归：</p>
<blockquote>
<p>定义anchor box 的尺寸（scale）和比例（aspect ratio）。按上图，预先定义了k个anchor box。在实际的RPN网络实现中，共采用了3个不同的scale（大中小）和3种不同的比例（宽中窄）。然后通过组合，得到了9个anchor box，即 $k=9$ 。在训练RPN的过程中，对于每个feature map上的像素点，都生成 $k$ 个anchor box 的预测。由于预测需要有两个输出用来分类（前景/背景），以及4个用来定位 $(x, y, w, h)$ ，所以RPN的分类层生成的是 $2k$ 维度的向量，RPN的回归层生成的是 $4k$ 维度的向量。</p>
</blockquote>
<h3 id="One-stage方法"><a href="#One-stage方法" class="headerlink" title="One-stage方法"></a>One-stage方法</h3><h4 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h4><p>YOLO的过程如下：首先，将整个图像分成 $S \times S$ 的小格子（cell），对于每个格子，分别预测 $B$ 个bbox，以及 $C$ 个类别的条件概率（注意是条件概率，即已经确定有目标的情况下，该目标属于哪个类别的概率，因此不需要对每个bbox分别预测类别，每个格子只预测一个概率向量即可）。每个bbox都有5个变量，分别是四个描述位置坐标的值，以及一个objectness，即是否有目标（相当于RPN 网络里的那个前景/背景预测）。这样一来，每个格子需要输出 $5B+C$ 维度的向量，因此，CNN最终的输出的tensor的形态为 $S \times S \times (5B + C)$ 。</p>
<p>YOLO的训练过程如下：对于每个GT bbox，找到它的中心位置，该中心位置所在的cell负责该物体的预测。因此，对于该cell 中的输出，其objectness应该尽可能的增加，同时其位置坐标尽可能拟合GTbbox（注意，由于每个cell可以输出多个备选的bbox，因此这里需要选择和GT最相近的那个预测的bbox进行调优）。另外，根据其实际的类别，对类别概率向量进行优化，使其输出真实的类别。对于不负责任何类别的那些cell 的预测值，不必进行优化。</p>
<h4 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h4><p>SSD 也是一种one-stage的直接检测的模型。它相比起YOLO v1主要的改进点在于两个方面：</p>
<ol>
<li>利用了先验框（Prior Box）的方法，预先给定scale 和aspect ratio，实际上就是之前Faster R-CNN 中的anchor box的概念。</li>
<li>多尺度（multi-scale）预测，即对CNN输出的后面的多个不同尺度的feature map 都进行预测。</li>
</ol>
<h4 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h4><ol>
<li>对所有卷积层增加了BN层。</li>
<li>用高分辨率的图片fine-tune 网络10个epoch。</li>
<li>通过k-means进行聚类，得到 $k$ 个手工选择的先验框（Prior anchor box）。这里的聚类用到的距离函数为 $1 - IoU$ ，这个距离函数可以很直接地反映出IoU 的情况。</li>
<li>直接预测位置坐标。之前的坐标回归实际上回归的不是坐标点，而是需要对预测结果做一个变换才能得到坐标点，即 $x = tx \times wa − xa$ （纵坐标同理），其中 $tx$ 为预测的直接结果。从该变换的形式可以看出，对于坐标点的预测不仅和直接预测位置结果相关，还和预测的宽和高也相关。因此，这样的预测方式可以使得任何anchor box可以出现在图像中的任意位置，导致模型可能不稳定。在YOLO v2 中，中心点预测结果为相对于该cell的角点的坐标（0-1 之间）。</li>
<li>多尺度训练（随机选择一个缩放尺度）、跳连层（paththrough layer）将前面的fine-grained特征直接拼接到后面的feature map 中。</li>
</ol>
<h4 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h4><p>通过将所有scale 的feature map 进行打通和结合，兼顾了速度和准确率。</p>
<p>FPN的block 结构分为两个部分：一个自顶向下通路（top-down pathway），另一个是侧边通路（lateral pathway）。所谓自顶向下通路，具体指的是上一个小尺寸的feature map（语义更高层）做2倍上采样，并连接到下一层。而侧边通路则指的是下面的feature map（高分辨率低语义）先利用一个1x1 的卷积核进行通道压缩，然后和上面下来的采样后结果进行合并。合并方式为逐元素相加（element-wise addition）。合并之后的结果在通过一个3x3的卷积核进行处理，得到该scale下的feature map。</p>
<h4 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h4><p>RetinaNet 的最大的贡献不在于网络结构，而是在于提出了一个one-stage 检测的重要的问题，及其对应的解决方案。这个问题就是one-stage 为何比two-stage 的准确率低，两者的区别在哪里？解决方案就是平衡正负样本+平衡难易样本的focal loss。</p>
<h4 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h4><p>本模型将实例分割（instance segmentation）与目标检测（object detection）两个任务相结合，并在两个任务上都达到了SOTA。</p>
<p>整个过程的pipeline 如下：首先，输入图片，根据RoI进行RoIAlign操作，得到该区域内的特征，然后将该特征feature map 进行逐点sigmoid（pixel-wise sigmoid），用于产生mask。另外，还有两个支路用于分类和回归。</p>
<h4 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h4><p>YOLO v3 是针对YOLO模型的又一次改进版本，是一个incremental improvement，并无太大创新，基本都是一些调优和trick。主要包括以下几个方面。</p>
<ol>
<li>用单类别的binary logistic 作为分类方式，代替全类别的softmax（和mask R-CNN 的mask 生成方式类似）。这样的好处在于可以处理有些数据集中有目标重叠的情况。</li>
<li>YOLO v3采用了FPN网络做预测，并且沿用了k-means聚类选择先验框，v3中选择了9个prior box，并选择了三个尺度。</li>
<li>backbone做了改进，从darknet-19变成了darknet-53，darknet-53除了3x3和1x1的交替以外，还加入了residual方法，因此层数得到扩展。</li>
</ol>
<h3 id="参考自"><a href="#参考自" class="headerlink" title="参考自"></a>参考自</h3><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/Hh5EioN_pVnstfHcR777VQ" >从R-CNN到YOLO，2020 图像目标检测算法综述<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<hr>
<h2 id="15-过拟合的原因"><a href="#15-过拟合的原因" class="headerlink" title="15. 过拟合的原因"></a>15. 过拟合的原因</h2><ul>
<li>训练集的数量和模型的复杂度不匹配，比如训练集太小或者模型太复杂</li>
<li>训练集和测试集分布不一致</li>
<li>训练集的噪声样本太多，导致模型只学习到了噪声特征，反而忽略了真实的输入输出关系</li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>常见冷启动解决方法</title>
    <url>/2023/08/01/%E5%B8%B8%E8%A7%81%E5%86%B7%E5%90%AF%E5%8A%A8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p><a class="link"   href="https://www.zhihu.com/question/19843390/answer/343050630" >https://www.zhihu.com/question/19843390/answer/343050630<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>冷启动</tag>
      </tags>
  </entry>
  <entry>
    <title>常见的LLM推理加速解决方案</title>
    <url>/2024/01/26/%E5%B8%B8%E8%A7%81%E7%9A%84LLM%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
    <content><![CDATA[<ul>
<li>KV Cache：用空间换时间<ul>
<li>当decoder输入序列是 $t_1, t_2, \dots, t_n$ 时，预测$t_{n+1}$，只需利用到 $q^n$ 以及历史所有的 $k^i, v^i, i \in \{1,\dots,n \}$ ：<script type="math/tex; mode=display">
h_n = \sum_{i=1}^{n} softmax(q^n \cdot k^i) \cdot v^i \\
t_{n+1} = f(h_n)</script>无须冗余attention计算 $h_1, \dots, h_{n-1}$ 以及 qkv映射 $q_1=W_q(t_1), k_1=W_k(t_1), k_1=W_v(t_1), \dots, q_{n-1}=W_q(t_{n-1}), k_1=W_k(t_{n-1}), k_1=W_v(t_{n-1})$</li>
</ul>
</li>
</ul>
<span id="more"></span>
<ul>
<li>int量化</li>
<li>PagedAttention</li>
<li>GQA</li>
<li>Speculative Decoding<ul>
<li><a class="link"   href="https://github.com/pytorch-labs/gpt-fast/blob/main/generate.py#L76" >code<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://pytorch.org/blog/accelerating-generative-ai-2/?utm_content=273712248&amp;utm_medium=social&amp;utm_source=twitter&amp;hss_channel=tw-776585502606721024" >Accelerating Generative AI with PyTorch II: GPT, Fast<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://arxiv.org/pdf/2211.17192.pdf" >Fast Inference from Transformers via Speculative Decoding<i class="fas fa-external-link-alt"></i></a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://mp.weixin.qq.com/s/sQJK8hO5L_SNczUaUXucJQ" >PyTorch造大模型“加速包”，不到1000行代码提速10倍！英伟达科学家：minGPT以来最好的教程式repo之一<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/6q2LmwoFG2LcN0iHoZjjqw" >图解大模型推理优化之 KV Cache<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>LLM</tag>
        <tag>推理加速</tag>
      </tags>
  </entry>
  <entry>
    <title>常见的相似性度量方法</title>
    <url>/2023/07/29/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>有如下几种计算相似性方法：</p>
<span id="more"></span>
<h2 id="点积相似度"><a href="#点积相似度" class="headerlink" title="点积相似度"></a>点积相似度</h2><script type="math/tex; mode=display">
\begin{aligned}
    X \cdot Y &= |X||Y|cos\theta \\
    &= \sum_{i=1}^n x_i * y_i
\end{aligned}</script><p>向量内积的结果是没有界限的，解决办法就是先归一化再相乘，就是下面的余弦相似度了。</p>
<h2 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h2><script type="math/tex; mode=display">
X \cdot Y = \frac{\sum_{i=1}^n x_i * y_i}{\sqrt{\sum_{i=1}^n (x_i)^2} * \sqrt{\sum_{i=1}^n (y_i)^2}}</script><p>余弦相似度衡量两个向量在方向上的相似性，并不关注两个向量的实际长度，即对绝对数据不敏感。</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><blockquote>
<p>用户对内容评分，5分制。A和B两个用户对两个商品的评分分别为A：(1,2)和B：(4,5)。使用余弦相似度得出的结果是0.98，看起来两者极为相似，但从评分上看A不喜欢这两个东西，而B比较喜欢。造成这个现象的原因就在于，余弦相似度没法衡量每个维数值的差异，对数值的不敏感导致了结果的误差。<br>需要修正这种不合理性，就出现了调整余弦相似度，即所有维度上的数值都减去一个均值。<br>比如A和B对两部电影评分的均值分别是(1+4)/2=2.5,(2+5)/2=3.5。那么调整后为A和B的评分分别是：(-1.5,-1.5)和(1.5,2.5)，再用余弦相似度计算，得到-0.98，相似度为负值，显然更加符合现实。</p>
</blockquote>
<p>注：为什么是在所有用户对同一物品的打分上求均值，每个人打分标准不一，对所有用户求均值，等于是所有用户的打分映射到了同一空间内。上述是在计算两个用户的相似度，以此类推计算两个物品的相似度，就要计算所有物品的均值了。</p>
<p>修正的余弦相似度可以说就是对余弦相似度进行归一化处理的算法，公式如下：</p>
<script type="math/tex; mode=display">
s(A, B)=\frac{\sum_{i \in I}\left(R_{A, i}-\bar{R_i}\right)\left(R_{B, i}-\bar{R_i}\right)}{\sqrt{\sum_{i \in I}\left(R_{A, i}-\bar{R_i}\right)^2} \sqrt{\sum_{i \in I}\left(R_{B, i}-\bar{R_i}\right)^2}}</script><p>$R_{A,i}$ 表示用户A在商品i上的打分，$\bar{R_i}$表示商品i在所有用户上的打分均值。</p>
<h2 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h2><p>Pearson 相关系数是用来检测两个连续型变量之间线性相关的程度，它解决了余弦相似度会收到向量平移影响的问题。取值范围为 [−1,1]，正值表示正相关，负值表示负相关，绝对值越大表示线性相关程度越高：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \rho_{\boldsymbol{x}, \boldsymbol{y}} &= \frac{\operatorname{cov}(\boldsymbol{x}, \boldsymbol{y})}{\sigma_{\boldsymbol{x}} \sigma_{\boldsymbol{y}}} \\
    &= \frac{E\left[\left(\boldsymbol{x}-\mu_{\boldsymbol{x}}, \boldsymbol{y}-\mu_{\boldsymbol{y}}\right)\right]}{\sigma_{\boldsymbol{x}} \sigma_{\boldsymbol{y}}} \\
    &= \frac{\sum_i\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sqrt{\sum_i\left(x_i-\bar{x}\right)^2} \sqrt{\sum_i\left(y_i-\bar{y}\right)^2}}
\end{aligned}</script><p>如果把 $x’=x-\bar{x}, y’=y-\bar{y}$ ，那么皮尔逊系数计算的就是 $x’ 和 y’$ 的余弦相似度。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>皮尔逊相关系数评估的是两个连续型变量的相关性，是两列，比如<a class="link"   href="https://tongyi.aliyun.com/qianwen/share?shareId=3dfb3489-9502-49be-ac97-d925f1f65063" >睡眠与考试成绩的关系<i class="fas fa-external-link-alt"></i></a>。</li>
<li>余弦相似度评估的是两个向量的相关性，是两行。</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/159244903" >点积相似度、余弦相似度、欧几里得相似度<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://guyuecanhui.github.io/2019/07/20/feature-selection-pearson/" >常用的特征选择方法之 Pearson 相关系数<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://juejin.cn/post/6933963966829985799" >图片向量相似检索服务(2)——四种基本距离计算原理<i class="fas fa-external-link-alt"></i></a><ul>
<li>这篇博客倒是很简洁，适合速读</li>
</ul>
</li>
<li><a class="link"   href="https://blog.csdn.net/wind82465/article/details/118309225" >点积相似度、余弦相似度、欧几里得相似度<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://leovan.me/cn/2019/01/similarity-and-distance-measurement/#%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF-inner-product-of-vectors" >相似性和距离度量 (Similarity &amp; Distance Measurement)<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>相似性</tag>
      </tags>
  </entry>
  <entry>
    <title>常见的离散型分布律</title>
    <url>/2019/06/30/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%A6%BB%E6%95%A3%E5%9E%8B%E5%88%86%E5%B8%83%E5%BE%8B/</url>
    <content><![CDATA[<p>在学习一些算法的过程中，总是会遇到各种各样的离散型概率分布。对于它们的分布律不够熟悉，在此记录一下。</p>
<span id="more"></span>
<h2 id="退化分布"><a href="#退化分布" class="headerlink" title="退化分布"></a>退化分布</h2><blockquote>
<p>设 $X$ 是随机变量，$a$ 是常数，若：</p>
<script type="math/tex; mode=display">
P\lbrace X = a \rbrace = 1</script><p>则称 $X$ 服从退化分布。</p>
</blockquote>
<p>退化分布某种程度上已经丧失了随机性，就像随机事件里的不可能事件和必然事件。我们可以将退化分布理解为分布的某种极端。</p>
<h2 id="两点分布"><a href="#两点分布" class="headerlink" title="两点分布"></a>两点分布</h2><blockquote>
<p>设 $X$ 是随机变量，$0 &lt; p &lt; 1$ 是常数，$q = 1 - p$，若：</p>
<script type="math/tex; mode=display">
P \lbrace X = 1 \rbrace = p, \quad P \lbrace X = 0 \rbrace = 1 - p = q</script><p>则称 $X$ 服从参数为 $p$ 的两点分布($0-1$ 分布)</p>
</blockquote>
]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>分布律</tag>
        <tag>离散型变量</tag>
      </tags>
  </entry>
  <entry>
    <title>常见金融术语</title>
    <url>/2024/07/16/%E5%B8%B8%E8%A7%81%E9%87%91%E8%9E%8D%E6%9C%AF%E8%AF%AD/</url>
    <content><![CDATA[<p><a class="link"   href="https://m.cfa.cn/cfa/2413.html" >https://m.cfa.cn/cfa/2413.html<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>Finance</category>
      </categories>
      <tags>
        <tag>金融术语</tag>
      </tags>
  </entry>
  <entry>
    <title>广告算法业务知识入门</title>
    <url>/2022/10/09/%E5%B9%BF%E5%91%8A%E7%AE%97%E6%B3%95%E4%B8%9A%E5%8A%A1%E7%9F%A5%E8%AF%86%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<p><a class="link"   href="https://mp.weixin.qq.com/s/Q-AhkxVTO6uz5bYrw6IytQ" >20分钟吃掉广告算法业务知识<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>广告</tag>
      </tags>
  </entry>
  <entry>
    <title>广告转化延迟建模</title>
    <url>/2025/02/11/%E5%B9%BF%E5%91%8A%E8%BD%AC%E5%8C%96%E5%BB%B6%E8%BF%9F%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p><a class="link"   href="https://zhuanlan.zhihu.com/p/555950153" >https://zhuanlan.zhihu.com/p/555950153<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>精排</tag>
        <tag>Delayed Feedback Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title>延迟预估</title>
    <url>/2024/04/09/%E5%BB%B6%E8%BF%9F%E9%A2%84%E4%BC%B0/</url>
    <content><![CDATA[<p>转化数据延迟久，但实时反馈至关重要涉及到模型预估准度，进而影响客户成本。</p>
<span id="more"></span>
<p>现状：超过12h的转化被视为负例，12h以内的为正例，这样会导致模型低估。公示如下：</p>
<script type="math/tex; mode=display">
P(转化|点击) = \frac{延迟\leq 12h的转化}{未转化点击 + 延迟\leq 12h的转化 + 延迟 > 12h的转化}</script><h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>建模延迟率预估，类似于esmm：</p>
<script type="math/tex; mode=display">
P(转化|点击) = \frac{P(转化延迟\leq 12h | 点击)}{P(转化延迟 \leq 12h | 转化)}</script><p>模型如下：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.1setm3qqft.png"  alt="model"></p>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>延迟预估</tag>
      </tags>
  </entry>
  <entry>
    <title>扩散原理详解与实战</title>
    <url>/2023/05/15/%E6%89%A9%E6%95%A3%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/</url>
    <content><![CDATA[<p>学习一下扩散模型的数学原理。</p>
<span id="more"></span>
<h2 id="前向扩散"><a href="#前向扩散" class="headerlink" title="前向扩散"></a>前向扩散</h2><script type="math/tex; mode=display">
q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_t ; \sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)</script><p>其中，$\alpha_t = 1-\beta_t$</p>
<ul>
<li>前向扩散过程没有可训练参数，$\beta_t$ 是人工设置的超参</li>
<li>$x_0$ 可以直接推导得到 $x_T$，而无须一步步迭代</li>
</ul>
<h2 id="逆向扩散"><a href="#逆向扩散" class="headerlink" title="逆向扩散"></a>逆向扩散</h2><script type="math/tex; mode=display">
q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \tilde{\boldsymbol{\mu}}\left(\mathbf{x}_t, \mathbf{x}_0\right), \tilde{\beta}_t \mathbf{I}\right) \\

Using Bayes' rule, we have: \\

\begin{aligned}
q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right) & =q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0\right) \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_0\right)}{q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)} \\
& \propto \exp \left(-\frac{1}{2}\left(\frac{\left(\mathbf{x}_t-\sqrt{\alpha_t} \mathbf{x}_{t-1}\right)^2}{\beta_t}+\frac{\left(\mathbf{x}_{t-1}-\sqrt{\alpha_{t-1}} \mathbf{x}_0\right)^2}{1-\bar{\alpha}_{t-1}}-\frac{\left(\mathbf{x}_t-\sqrt{\bar{\alpha}_t} \mathbf{x}_0\right)^2}{1-\bar{\alpha}_t}\right)\right) \\
& =\exp \left(-\frac{1}{2}\left(\left(\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \mathbf{x}_{t-1}^2-\left(\frac{2 \sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t+\frac{2 \sqrt{\bar{\alpha}_t}}{1-\bar{\alpha}_t} \mathbf{x}_0\right) \mathbf{x}_{t-1}+C\left(\mathbf{x}_t, \mathbf{x}_0\right)\right)\right)
\end{aligned} \\

\begin{aligned}
\tilde{\beta}_t & =1 /\left(\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar{\alpha}_{t-1}}\right)=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \cdot \beta_t \\
\tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right) & =\left(\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t+\frac{\sqrt{\bar{\alpha}_t}}{1-\bar{\alpha}_t} \mathbf{x}_0\right) /\left(\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar{\alpha}_{t-1}}\right)=\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathbf{x}_t+\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_0
\end{aligned}</script><ul>
<li>逆向扩散过程一步步去噪，需要训练参数</li>
<li>$x_T$ 不能一步推导到 $x_0$，需要一步步迭代</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><script type="math/tex; mode=display">
\begin{aligned}
& -\log p_\theta\left(\mathbf{x}_0\right) \leq-\log p_\theta\left(\mathbf{x}_0\right)+D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right) \| p_\theta\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)\right) \\
& =-\log p_\theta\left(\mathbf{x}_0\right)+\mathbb{E}_{\mathbf{x}_{1: T \sim} q\left(\mathbf{x}_{\left.1: T \mid \mathbf{x}_0\right)}\right.}\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)}{p_\theta\left(\mathbf{x}_{0, T}\right) / p_\theta\left(\mathbf{x}_0\right)}\right] \\
& =-\log p_\theta\left(\mathbf{x}_0\right)+\mathbb{E}_q\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)}{p_\theta\left(\mathbf{x}_{0: T}\right)}+\log p_\theta\left(\mathbf{x}_0\right)\right] \\
& =\mathbb{E}_q\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)}{p_\theta\left(\mathbf{x}_{0: T}\right)}\right] \\
& \text { Let } L_{\mathrm{VLB}}=\mathbb{E}_{q\left(\mathbf{x}_{0: T)}\right.}\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)}{p_\theta\left(\mathbf{x}_{0: T}\right)}\right] \geq-\mathbb{E}_{q\left(\mathbf{x}_0\right)} \log p_\theta\left(\mathbf{x}_0\right) \\
&
\end{aligned}</script><p>最终化简得到的损失函数为：</p>
<script type="math/tex; mode=display">
L_{\text {simple }}(\theta):=\mathbb{E}_{t, \mathbf{x}_0, \epsilon}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}, t\right)\right\|^2\right]</script><p>确实非常简洁。。。</p>
<ul>
<li>具体的推导可见：<a class="link"   href="https://www.bilibili.com/video/BV1b541197HX?t=2902.9" >对数似然下界推导<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<h2 id="模型训练与推理"><a href="#模型训练与推理" class="headerlink" title="模型训练与推理"></a>模型训练与推理</h2><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.6wx11ayv6y40.webp"  alt="model"></p>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ol>
<li>为什么建模目标不是直接预测原始输入 $x_0$ ？</li>
</ol>
<p>这个得看看原论文了。其实逆向扩散过程中模型预测的是噪音残差，跟ResNet思想一致。</p>
<ol>
<li>前向可以 $x_0$ 可以直接推导得到 $x_T$，为什么后向不能$x_T$ 可以直接推导得到 $x_0$？</li>
</ol>
<p>其实是可以的，但一步到位直接预测 $x_0$，但生成的图片效果较差。还是需要马尔科夫过程一步步生成高质量的图片。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://jalammar.github.io/illustrated-stable-diffusion/" >The Illustrated Stable Diffusion<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://www.bilibili.com/video/BV1b541197HX/?spm_id_from=333.999.0.0&amp;vd_source=4bddf76b04f5705292d795a2246cdb65" >54、Probabilistic Diffusion Model概率扩散模型理论与完整PyTorch代码详细解读<i class="fas fa-external-link-alt"></i></a><ul>
<li><a class="link"   href="https://t.bilibili.com/700526762586538024?spm_id_from=333.999.0.0" >上述视频的置顶评论，是up主有关视频疑问的详细讲解<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/TransformersWsz/Diffusion-Models/blob/main/Diffusion%20Model.ipynb" >上述up主的代码实现<i class="fas fa-external-link-alt"></i></a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Diffusion</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>扔鸡蛋问题</title>
    <url>/2021/08/25/%E6%89%94%E9%B8%A1%E8%9B%8B%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>这是一道非常经典的google面试题，在此记录一下。</p>
<span id="more"></span>
<p>具体案例引导可见：<a class="link"   href="https://blog.csdn.net/qq249356520/article/details/89207891" >扔鸡蛋问题（四种解法）<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>这里介绍动态规划的解法：</p>
<p>我们假设 $F(K,N)$ 表示有 $K$ 个鸡蛋、$N$ 层楼，测出其摔碎临界点所需的最少次数，那么有如下状态转移公式：</p>
<script type="math/tex; mode=display">
F(K, N) = 1 + min_{1 \leq i \leq N} max(F(K, N-i), F(K-1, i-1))</script><ul>
<li>$F(K, N-i)$ : 如果第一个鸡蛋在第 $i$ 层没有摔碎，那么我们还有 $K$ 个鸡蛋以及剩余 $N-i$ 个楼层测试</li>
<li>$F(K-1, i-1)$ : 如果第一个鸡蛋在第 $i$ 层摔碎，那么我们还有 $K-1$ 个鸡蛋以及剩余 $i-1$ 个楼层测试</li>
<li>取两者最坏情况，再取所有情况中最小的值，表示最少测试次数。</li>
</ul>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>具体编程实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">superEggDrop</span>(<span class="params">self, K: <span class="built_in">int</span>, N: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [ [<span class="number">0</span>]*(N+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(K+<span class="number">1</span>) ]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">            dp[<span class="number">1</span>][i] = i</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, K+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">                min_drop = N</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N+<span class="number">1</span>):</span><br><span class="line">                    tmp_max = <span class="built_in">max</span>(dp[k-<span class="number">1</span>][i-<span class="number">1</span>], dp[k][n-i])</span><br><span class="line">                    min_drop = <span class="built_in">min</span>(min_drop, <span class="number">1</span>+tmp_max)</span><br><span class="line">                dp[k][n] = min_drop</span><br><span class="line">        <span class="keyword">return</span> dp[K][N]</span><br></pre></td></tr></table></figure>
<p>上述代码在<a class="link"   href="https://leetcode-cn.com/problems/super-egg-drop/" >leetcode<i class="fas fa-external-link-alt"></i></a>上超时了，复制粘贴了官方的代码ac的。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://leetcode-cn.com/problems/super-egg-drop/" >887. 鸡蛋掉落<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://baike.baidu.com/item/%E6%89%94%E9%B8%A1%E8%9B%8B%E9%97%AE%E9%A2%98/24626883?fr=aladdin" >扔鸡蛋问题<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/qq249356520/article/details/89207891" >扔鸡蛋问题（四种解法）<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>智力题</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>扭矩&amp;马力</title>
    <url>/2025/07/06/%E6%89%AD%E7%9F%A9-%E9%A9%AC%E5%8A%9B/</url>
    <content><![CDATA[<p>马力和扭矩是衡量发动机性能的两个核心指标，但它们的物理意义和对车辆性能的影响截然不同。</p>
<span id="more"></span>
<h3 id="1-基本定义"><a href="#1-基本定义" class="headerlink" title="1. 基本定义"></a><strong>1. 基本定义</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><strong>指标</strong></th>
<th><strong>物理意义</strong></th>
<th><strong>单位</strong></th>
<th><strong>类比解释</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>扭矩</strong></td>
<td>发动机<strong>瞬时扭转力</strong>的大小</td>
<td>牛·米（N·m）</td>
<td>相当于“肌肉爆发力”</td>
</tr>
<tr>
<td><strong>马力</strong></td>
<td>发动机<strong>持续做功的效率</strong></td>
<td>千瓦（kW）或马力（PS）</td>
<td>相当于“长跑耐力”</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-核心区别"><a href="#2-核心区别" class="headerlink" title="2. 核心区别"></a><strong>2. 核心区别</strong></h3><h4 id="扭矩（Torque）"><a href="#扭矩（Torque）" class="headerlink" title="扭矩（Torque）"></a><strong>扭矩（Torque）</strong></h4><ul>
<li><strong>作用</strong>：决定车辆的<strong>瞬时加速能力</strong>和<strong>脱困能力</strong>（如爬坡、拖拽）</li>
<li><strong>特点</strong>：  <ul>
<li>低转速时即可输出峰值（如柴油车）</li>
<li>直接影响起步、超车时的推背感</li>
</ul>
</li>
<li><strong>典型场景</strong>：  <ul>
<li>越野车需要<strong>高扭矩</strong>（如500N·m）在泥地中脱困</li>
<li>电动车扭矩瞬间爆发（如特斯拉Model S Plaid 1,420N·m）</li>
</ul>
</li>
</ul>
<h4 id="马力（Power）"><a href="#马力（Power）" class="headerlink" title="马力（Power）"></a><strong>马力（Power）</strong></h4><ul>
<li><strong>作用</strong>：决定车辆的<strong>最高速度</strong>和<strong>持续加速能力</strong></li>
<li><strong>公式</strong>：马力 = 扭矩 × 转速 ÷ 常数（如1PS=0.7355kW</li>
<li><strong>特点</strong>：  <ul>
<li>高转速时才能输出峰值（如汽油跑车）</li>
<li>影响极速和高速区间的再加速能力</li>
</ul>
</li>
<li><strong>典型场景</strong>：  <blockquote>
<p>F1赛车<strong>高马力</strong>（超1,000PS）维持极速<br>家用车马力适中（如150PS）兼顾油耗</p>
</blockquote>
</li>
</ul>
<h3 id="3-对驾驶体验的影响"><a href="#3-对驾驶体验的影响" class="headerlink" title="3. 对驾驶体验的影响"></a><strong>3. 对驾驶体验的影响</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th><strong>性能需求</strong></th>
<th><strong>依赖指标</strong></th>
<th><strong>具体表现</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>红绿灯快速起步</td>
<td><strong>扭矩</strong></td>
<td>扭矩越大，起步越迅猛（电动车优势）</td>
</tr>
<tr>
<td>高速超车</td>
<td><strong>马力</strong></td>
<td>高马力车在80km/h后加速更快</td>
</tr>
<tr>
<td>爬坡/载重</td>
<td><strong>扭矩</strong></td>
<td>卡车/SUV需要高扭矩应对重载</td>
</tr>
<tr>
<td>极速（如赛道）</td>
<td><strong>马力</strong></td>
<td>马力决定能突破的空气阻力上限</td>
</tr>
</tbody>
</table>
</div>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><strong>扭矩是“爆发力”</strong>，决定你<strong>能不能快速动起来</strong></li>
<li><strong>马力是“持久力”</strong>，决定你<strong>能跑多快、跑多久</strong></li>
</ul>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>扭矩</tag>
        <tag>马力</tag>
      </tags>
  </entry>
  <entry>
    <title>拉取测试别人的PR</title>
    <url>/2022/10/16/%E6%8B%89%E5%8F%96%E6%B5%8B%E8%AF%95%E5%88%AB%E4%BA%BA%E7%9A%84PR/</url>
    <content><![CDATA[<p>有的时候别人的pr很简单，那么可以直接合并。如果复杂的话，需要我们拉取到本地测试一下，再合并。下面是具体的流程：<br><a class="link"   href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/reviewing-changes-in-pull-requests/checking-out-pull-requests-locally" >Checking out pull requests locally<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>VCS</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Pull Request</tag>
      </tags>
  </entry>
  <entry>
    <title>描述秋天美景的诗句</title>
    <url>/2024/03/10/%E6%8F%8F%E8%BF%B0%E7%A7%8B%E5%A4%A9%E7%BE%8E%E6%99%AF%E7%9A%84%E8%AF%97%E5%8F%A5/</url>
    <content><![CDATA[<p><a class="link"   href="https://baijiahao.baidu.com/s?id=1776646546359667213&amp;wfr=spider&amp;for=pc" >https://baijiahao.baidu.com/s?id=1776646546359667213&amp;wfr=spider&amp;for=pc<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>文学</tag>
        <tag>秋天</tag>
      </tags>
  </entry>
  <entry>
    <title>斯诺克专业术语</title>
    <url>/2025/05/14/%E6%96%AF%E8%AF%BA%E5%85%8B%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD/</url>
    <content><![CDATA[<p>恭喜赵心童夺得世锦赛冠军！借此机会整理了完整的斯诺克术语表格，包含基本规则、杆法技巧、球桌区域、战术策略、装备术语等：  </p>
<span id="more"></span>
<div class="table-container">
<table>
<thead>
<tr>
<th>分类</th>
<th>术语</th>
<th>英文</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>基本规则</td>
<td>单杆</td>
<td>Break</td>
<td>一名球员连续击球得分的一系列击球</td>
</tr>
<tr>
<td></td>
<td>单杆过百</td>
<td>Century Break</td>
<td>一名球员连续得分达到或超过100分</td>
</tr>
<tr>
<td></td>
<td>满分杆</td>
<td>Maximum Break</td>
<td>一杆清台获得147分（最高分）</td>
</tr>
<tr>
<td></td>
<td>清台</td>
<td>Clearance</td>
<td>将台面上所有球按规则击入袋中</td>
</tr>
<tr>
<td></td>
<td>安全球</td>
<td>Safety</td>
<td>一种防守性击球，目的是让对手难以得分</td>
</tr>
<tr>
<td></td>
<td>斯诺克</td>
<td>Snooker</td>
<td>使对手无法直接击打到目标球的防守性击球</td>
</tr>
<tr>
<td></td>
<td>自由球</td>
<td>Free Ball</td>
<td>当一方犯规后，对手可以指定任何彩球作为红球击打</td>
</tr>
<tr>
<td></td>
<td>重置球</td>
<td>Re-spot</td>
<td>将某些球重新放回台面指定位置</td>
</tr>
<tr>
<td></td>
<td>犯规</td>
<td>Foul</td>
<td>违反比赛规则的行为，通常导致罚分</td>
</tr>
<tr>
<td></td>
<td>罚分</td>
<td>Penalty</td>
<td>犯规后对手获得的额外分数</td>
</tr>
<tr>
<td>球桌区域</td>
<td>底库</td>
<td>Bottom Cushion</td>
<td>球桌靠近开球区（D区）的短边库，通常指靠近黄球、绿球、棕球的一侧</td>
</tr>
<tr>
<td></td>
<td>顶库</td>
<td>Top Cushion</td>
<td>球桌远离开球区的短边库，通常指靠近黑球的一侧</td>
</tr>
<tr>
<td></td>
<td>长库</td>
<td>Long Cushion</td>
<td>球桌的两条长边库</td>
</tr>
<tr>
<td></td>
<td>开球区</td>
<td>Baulk Area</td>
<td>台面一端由半圆形线划定的区域</td>
</tr>
<tr>
<td></td>
<td>D区</td>
<td>D</td>
<td>开球区的半圆形区域</td>
</tr>
<tr>
<td></td>
<td>袋口</td>
<td>Pocket</td>
<td>台面四角和长边中部的六个球袋</td>
</tr>
<tr>
<td>杆法技巧</td>
<td>加塞</td>
<td>Side Spin / English</td>
<td>击打母球时施加侧向旋转，使母球走弧线或改变碰库后的反弹角度</td>
</tr>
<tr>
<td></td>
<td>高杆</td>
<td>Top Spin / Follow</td>
<td>击打母球上部，使其在击中目标球后继续向前滚动</td>
</tr>
<tr>
<td></td>
<td>低杆</td>
<td>Back Spin / Screw</td>
<td>击打母球下部，使其在击中目标球后向后滚动</td>
</tr>
<tr>
<td></td>
<td>左塞</td>
<td>Left Side / Left English</td>
<td>击打母球左侧，使其产生向左的旋转</td>
</tr>
<tr>
<td></td>
<td>右塞</td>
<td>Right Side / Right English</td>
<td>击打母球右侧，使其产生向右的旋转</td>
</tr>
<tr>
<td></td>
<td>顺塞</td>
<td>Running Side</td>
<td>母球碰库后，旋转方向与运动方向相同，会加速反弹</td>
</tr>
<tr>
<td></td>
<td>反塞</td>
<td>Check Side</td>
<td>母球碰库后，旋转方向与运动方向相反，会减速反弹</td>
</tr>
<tr>
<td></td>
<td>扎杆</td>
<td>Screw Shot</td>
<td>使母球在击中目标球后向后滚动的击球方式</td>
</tr>
<tr>
<td></td>
<td>推杆</td>
<td>Push Shot</td>
<td>球杆持续接触母球超过合理时间的犯规击球</td>
</tr>
<tr>
<td></td>
<td>跳球</td>
<td>Jump Shot</td>
<td>使母球跳起越过障碍球的击球方式</td>
</tr>
<tr>
<td></td>
<td>弧线球</td>
<td>Swerve</td>
<td>通过杆法使母球走弧线的击球方式</td>
</tr>
<tr>
<td></td>
<td>定杆</td>
<td>Stop Shot</td>
<td>使母球在击中目标球后停在原地的击球方式</td>
</tr>
<tr>
<td></td>
<td>跟杆</td>
<td>Follow Shot</td>
<td>使母球在击中目标球后继续向前运动的击球方式</td>
</tr>
<tr>
<td></td>
<td>缩杆</td>
<td>Draw Shot</td>
<td>使母球在击中目标球后向后滚动的击球方式</td>
</tr>
<tr>
<td></td>
<td>薄球</td>
<td>Thin Hit</td>
<td>母球只轻微擦到目标球的击球方式</td>
</tr>
<tr>
<td></td>
<td>厚球</td>
<td>Thick Hit</td>
<td>母球较多地击中目标球的击球方式</td>
</tr>
<tr>
<td>战术策略</td>
<td>贴库球</td>
<td>Frozen Ball</td>
<td>目标球紧贴库边，击打难度较大</td>
</tr>
<tr>
<td></td>
<td>K球</td>
<td>Kiss Shot</td>
<td>母球或目标球在运动中与其他球发生轻微碰撞​​，从而改变原有路径的击球方式</td>
</tr>
<tr>
<td></td>
<td>组合球</td>
<td>Plant / Combination</td>
<td>通过击打一颗球去撞击另一颗球进袋</td>
</tr>
<tr>
<td></td>
<td>翻袋</td>
<td>Double / Bank Shot</td>
<td>目标球先碰库边再反弹进袋</td>
</tr>
<tr>
<td></td>
<td>摔袋</td>
<td>Scratch</td>
<td>母球意外进袋，属于犯规，对手获得自由球</td>
</tr>
<tr>
<td></td>
<td>走位</td>
<td>Position Play</td>
<td>通过控制母球走位，为下一杆击球创造有利位置</td>
</tr>
<tr>
<td></td>
<td>力度控制</td>
<td>Weight / Power Control</td>
<td>调整击球力量，使母球停在理想位置</td>
</tr>
<tr>
<td>球类术语</td>
<td>母球</td>
<td>Cue Ball</td>
<td>击球者用球杆直接击打的白色球</td>
</tr>
<tr>
<td></td>
<td>目标球</td>
<td>Object Ball</td>
<td>击球者试图击打的球</td>
</tr>
<tr>
<td></td>
<td>红球</td>
<td>Red Ball</td>
<td>比赛开始时台面上的15颗红色球，每颗1分</td>
</tr>
<tr>
<td></td>
<td>彩球</td>
<td>Colour Ball</td>
<td>黄(2)、绿(3)、棕(4)、蓝(5)、粉(6)、黑(7)球</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Snooker</tag>
      </tags>
  </entry>
  <entry>
    <title>搜广推场景下的长序列建模问题</title>
    <url>/2023/03/08/%E6%90%9C%E5%B9%BF%E6%8E%A8%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E9%95%BF%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>用户序列长度跟模型AUC有直接正相关性，模型能处理的序列长度越长，AUC就越高。但长序列对于样本处理和模型训练非常不友好，耗时长且显存占用大。更重要的是线上预估服务的约束，必须要在规定时间内完成推算。针对长序列的建模，出现了如下代表性工作：</p>
<span id="more"></span>
<h2 id="DIN"><a href="#DIN" class="headerlink" title="DIN"></a>DIN</h2><p>具体的讲解见：<a class="link"   href="https://transformerswsz.github.io/2023/02/12/DIN%E8%A7%A3%E8%AF%BB/" >DIN解读<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="SIM"><a href="#SIM" class="headerlink" title="SIM"></a>SIM</h2><p>SIM利用两阶段来捕捉用户在广告上的精准兴趣表达：</p>
<ol>
<li>提出GSU(General Search Unit)结构：从原始的超长用户行为中根据广告信息搜索到和广告相关的用户行为。搜索后的用户行为数据能够由原来的上万长度降低到数百长度，与此同时大部分和当前广告无关的信息都会被过滤。搜索方法有如下两种：<ul>
<li>hard模式：将该用户行为序列中与target item具有相同类别的items聚合起来</li>
<li>soft模式：用聚类方法将用户行为序列中的items进行聚类</li>
</ul>
</li>
<li>提出ESU(Exact Search Unit，精准搜索单元)结构：利用第一阶段提取出和广告相关的用户行为和候选的广告信息来精准建模用户的多样兴趣。在ESU中，采用类似DIN这样复杂的模型来捕捉用户和广告相关的动态兴趣表达</li>
</ol>
<h4 id="SIM存在的问题"><a href="#SIM存在的问题" class="headerlink" title="SIM存在的问题"></a>SIM存在的问题</h4><ul>
<li>目标不一致：GSU建立索引使用的item embedding不是SIM模型生成的，可能是预训练的，也有可能是直接拿item的类别建立的索引，本质上是内容embdding上的索引。但是SIM模型本身的item embdding不一定是这样的内容embdding，要不然就不会有啤酒尿布这样经典的案例（毕竟啤酒和尿布分属不同类别）</li>
<li>更新频率不一致：SIM两阶段的更新频率不一致。线上模型都是以online learning的方式频繁更新的，以捕捉用户实时的兴趣变化，但是SIM中所使用的索引都是离线建立好的，成为性能的短板</li>
</ul>
<h2 id="ETA"><a href="#ETA" class="headerlink" title="ETA"></a>ETA</h2><p>模型结构如下：<br><img   src="/2023/03/08/%E6%90%9C%E5%B9%BF%E6%8E%A8%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E9%95%BF%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%E9%97%AE%E9%A2%98/eta_model.png"  class="model"></p>
<p>SimHash伪代码如下：<br><img   src="/2023/03/08/%E6%90%9C%E5%B9%BF%E6%8E%A8%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E9%95%BF%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%E9%97%AE%E9%A2%98/eta_algo.png"  class="algo"></p>
<p>在使用SimHash算法前，Multi-Head Target Attention的复杂度为 $O(B \times L \times d)$，$B$ 为候选集个数（即target item数量），$L$ 为用户长序列长度，$d$ 为item embedding维度。</p>
<p>使用SimHash算法后，候选集的所有item与历史行为序列计算汉明距离的复杂度为 $O(B \times L)$，接着取出TopK相似的历史item，并进行Attention操作，复杂度为 $O(B \times K \times d)$，总体复杂度 $O(B \times L + B \times K \times d) \approx O(B \times K \times d) \ll O(B \times L \times d)$</p>
<h2 id="SDIM"><a href="#SDIM" class="headerlink" title="SDIM"></a>SDIM</h2><p>模型结构如下：<br><img   src="/2023/03/08/%E6%90%9C%E5%B9%BF%E6%8E%A8%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E9%95%BF%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%E9%97%AE%E9%A2%98/sdim.png"  class="model"></p>
<p>通过对用户行为序列中与target item具有相同哈希次序列的item的embedding求和，再归一化后得到用户兴趣表达，将时间复杂度进一步降低到 $O(B \times m \times log(d))$，$m$ 为hash函数个数。</p>
<p>SDIM结构简单，去掉了ETA的TopK检索部分（只取部分，丢失了用户的部分序列信息），也去掉了Multi-Head Attention，直接在原始、完整的序列上建模，效果也更好。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对于长序列建模的思路，主流都是将Attention操作($O(d)$)优化成hash操作($O(1)$)。模型结构上倒不是很复杂，比拼的更多是工程能力、基建能力，比如：</p>
<ul>
<li>模型的超参设置多大合适？</li>
<li>千亿商品能建下多大的索引？</li>
<li>机器资源能容得下多大并发？</li>
<li>不同的场景是否sota模型真的有效？</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/540579632" >再见Attention：建模用户长期兴趣的新范式<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/547087040" >长序列建模（一）：阿里ETA（End-to-end Target Attention）模型<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/560657191" >长序列建模（二）：美团SDIM（Sampling-based Deep Interest Modeling）模型<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>Long Sequence</tag>
        <tag>SimHash</tag>
        <tag>Locality Sensitive Hashing</tag>
      </tags>
  </entry>
  <entry>
    <title>新一代粗排系统COLD</title>
    <url>/2024/02/21/%E6%96%B0%E4%B8%80%E4%BB%A3%E7%B2%97%E6%8E%92%E7%B3%BB%E7%BB%9FCOLD/</url>
    <content><![CDATA[<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master//image.2vvvk4167lw0.webp"  alt="cold"></p>
<p>为了让粗排支持交叉特征来提升模型性能，同时又为了降低引入交叉特征、复杂模型所带来的预估延迟和资源消耗，阿里团队提出了COLD，在模型效果和算力间取得了平衡。</p>
<span id="more"></span>
<h2 id="模型层面优化"><a href="#模型层面优化" class="headerlink" title="模型层面优化"></a>模型层面优化</h2><ol>
<li>引入SENet，筛选出重要特征</li>
</ol>
<h2 id="算力层面优化"><a href="#算力层面优化" class="headerlink" title="算力层面优化"></a>算力层面优化</h2><ol>
<li>并行拿特征</li>
<li>列式计算：对于不同广告，它们的某些特征可能是相同，对列计算进行优化</li>
<li>优化组合特征算子</li>
<li>fp16加速</li>
</ol>
<h2 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h2><p>更多的是一篇工程优化文章，为了提升业务指标，对系统性能进行了各方面极致优化，叠加了不少硬件资源上去。但却给人一种大杂烩的感觉，没有从模型层面来优雅地创新。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/186320100" >阿里定向广告最新突破：面向下一代的粗排排序系统COLD<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>粗排</tag>
        <tag>交叉特征</tag>
      </tags>
  </entry>
  <entry>
    <title>旋转位置编码</title>
    <url>/2023/09/04/%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</url>
    <content><![CDATA[<p>旋转位置编码具有良好的外推性，即模型在预测时可以处理比训练时更长的序列。</p>
<span id="more"></span>
<p>想要获得良好的外推性，必须使用相对位置编码。Transformer使用的是绝对位置编码，外推性不强。</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.9kfzfqsilq.webp"  alt="pos"></p>
<p>那么，如何使用绝对位置编码来实现相对位置编码呢？</p>
<h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><script type="math/tex; mode=display">
<f_q\left(x_m, m\right), f_k\left(x_n, n\right)>=g\left(x_m, x_n, m-n\right)</script><p>欧拉公式：$e^{i x}=\cos x+i \sin x$</p>
<script type="math/tex; mode=display">
\begin{gathered}
f_q\left(x_m, m\right)=\left(W_q x_m\right) e^{i m \theta}=q_m e^{i m \theta} \\
=\left[q_m^{(1)} \cos (m \theta)-q_m^{(2)} \sin (m \theta), q_m^{(2)} \cos (m \theta)+q_m^{(1)} \sin (m \theta)\right] \\
=\left(\begin{array}{cc}
\cos (m \theta) & -\sin (m \theta) \\
\sin (m \theta) & \cos (m \theta)
\end{array}\right)\binom{q_m^{(1)}}{q_m^{(2)}}
\end{gathered}</script><script type="math/tex; mode=display">
\begin{gathered}
<f_q\left(x_m, m\right), f_k\left(x_n, n\right)> \\
=\left(\left(\begin{array}{cc}
\cos (m \theta) & -\sin (m \theta) \\
\sin (m \theta) & \cos (m \theta)
\end{array}\right)\binom{q_m^{(1)}}{q_m^{(2)}}\right)^T\left(\left(\begin{array}{cc}
\cos (n \theta) & -\sin (n \theta) \\
\sin (n \theta) & \cos (n \theta)
\end{array}\right)\binom{k_n^{(1)}}{k_n^{(2)}}\right) \\
=\left(\begin{array}{cc}
q_m^{(1)} & q_m^{(2)}
\end{array}\right)\left(\begin{array}{cc}
\cos (m \theta) & \sin (m \theta) \\
-\sin (m \theta) & \cos (m \theta)
\end{array}\right)\left(\begin{array}{cc}
\cos (n \theta) & -\sin (n \theta) \\
\sin (n \theta) & \cos (n \theta)
\end{array}\right)\binom{k_n^{(1)}}{k_n^{(2)}} \\
= \left(\begin{array}{cc}
q_m^{(1)} & q_m^{(2)}
\end{array}\right)\left(\begin{array}{cc}
\cos (m \theta) \cos (n \theta)+\sin (m \theta) \sin (n \theta) & -\cos (m \theta) \sin (n \theta)+\sin (m \theta) \cos (n \theta) \\
-\sin (m \theta) \cos (n \theta)+\cos (m \theta) \sin (n \theta) & \sin (m \theta) \sin (n \theta)+\cos (m \theta) \cos (n \theta)
\end{array}\right)\left(\begin{array}{cc}
\cos (n \theta) & -\sin (n \theta) \\
\sin (n \theta) & \cos (n \theta)
\end{array}\right) \\
=\left(\begin{array}{ll}
q_m^{(1)} & q_m^{(2)}
\end{array}\right)\left(\begin{array}{cc}
\cos ((m-n) \theta) & -\sin ((m-n) \theta) \\
\sin ((m-n) \theta) & \cos ((m-n) \theta)
\end{array}\right)\binom{k_n^{(1)}}{k_n^{(2)}} \\
= g\left(x_m, x_n, m-n\right)
\end{gathered}</script><p>其中，$m$ 就是位置下标，$\theta_j=10000^{-2(j-1) / d}, j \in[1,2, \ldots, d / 2]$，跟transformer基本一致。</p>
<p><a class="link"   href="https://zhuanlan.zhihu.com/p/642884818" >https://zhuanlan.zhihu.com/p/642884818<i class="fas fa-external-link-alt"></i></a></p>
<p>下面这是极简的证明：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.3d5a6ljjj3k0.png"  alt="prove"></p>
<h2 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h2><p>LLaMA的官方代码并不是直接乘以一个旋转矩阵，而是利用复数乘法性质来实现RoPE。我们的目标是对 $x_m$ 添加位置编码，即：</p>
<script type="math/tex; mode=display">
f_q\left(x_m, m\right) = (W_q x_m)e^{im\theta} = (q_m^{(1)} + iq_m^{(2)}) * (cos(m\theta) + isin(m\theta))</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_freqs_cis</span>(<span class="params">dim: <span class="built_in">int</span>, seq_len: <span class="built_in">int</span>, theta: <span class="built_in">float</span> = <span class="number">10000.0</span></span>):</span><br><span class="line">    <span class="comment"># 计算词向量元素两两分组之后，每组元素对应的旋转角度</span></span><br><span class="line">    freqs = <span class="number">1.0</span> / (theta ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>)[: (dim // <span class="number">2</span>)].<span class="built_in">float</span>() / dim))</span><br><span class="line">    <span class="comment"># 生成 token 序列索引 t = [0, 1,..., seq_len-1]</span></span><br><span class="line">    t = torch.arange(seq_len, device=freqs.device)</span><br><span class="line">    <span class="comment"># freqs.shape = [seq_len, dim // 2] </span></span><br><span class="line">    freqs = torch.outer(t, freqs).<span class="built_in">float</span>()</span><br><span class="line">    <span class="comment"># torch.polar 的文档</span></span><br><span class="line">    <span class="comment"># https://pytorch.org/docs/stable/generated/torch.polar.html</span></span><br><span class="line">    <span class="comment"># 计算结果是个复数向量：e^&#123;im\theta&#125;</span></span><br><span class="line">    <span class="comment"># polar(abs, angle, *, out=None) -&gt; Tensor: abs是幅值，angle是相位角</span></span><br><span class="line">    <span class="comment"># 假设 freqs = [x, y]</span></span><br><span class="line">    <span class="comment"># 则 freqs_cis = [cos(x) + sin(x)i, cos(y) + sin(y)i]</span></span><br><span class="line">    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)</span><br><span class="line">    <span class="keyword">return</span> freqs_cis</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params"></span></span><br><span class="line"><span class="params">    xq: torch.Tensor,</span></span><br><span class="line"><span class="params">    xk: torch.Tensor,</span></span><br><span class="line"><span class="params">    freqs_cis: torch.Tensor,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]:</span><br><span class="line">    <span class="comment"># xq.shape = [batch_size, seq_len, dim]</span></span><br><span class="line">    <span class="comment"># xq_.shape = [batch_size, seq_len, dim // 2, 2]</span></span><br><span class="line">    xq_ = xq.<span class="built_in">float</span>().reshape(*xq.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    xk_ = xk.<span class="built_in">float</span>().reshape(*xk.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 转为复数域：q_m^&#123;(1)&#125; + iq_m^&#123;(2)&#125;</span></span><br><span class="line">    xq_ = torch.view_as_complex(xq_)</span><br><span class="line">    xk_ = torch.view_as_complex(xk_)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 应用旋转操作，然后将结果转回实数域</span></span><br><span class="line">    <span class="comment"># xq_out.shape = [batch_size, seq_len, dim]</span></span><br><span class="line">    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="number">2</span>)</span><br><span class="line">    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args: ModelArgs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.wq = Linear(...)</span><br><span class="line">        self.wk = Linear(...)</span><br><span class="line">        self.wv = Linear(...)</span><br><span class="line">        </span><br><span class="line">        self.freqs_cis = precompute_freqs_cis(dim, max_seq_len * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        bsz, seqlen, _ = x.shape</span><br><span class="line">        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)</span><br><span class="line"></span><br><span class="line">        xq = xq.view(batch_size, seq_len, dim)</span><br><span class="line">        xk = xk.view(batch_size, seq_len, dim)</span><br><span class="line">        xv = xv.view(batch_size, seq_len, dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention 操作之前，应用旋转位置编码</span></span><br><span class="line">        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># scores.shape = (bs, seqlen, seqlen)</span></span><br><span class="line">        scores = torch.matmul(xq, xk.transpose(<span class="number">1</span>, <span class="number">2</span>)) / math.sqrt(dim)</span><br><span class="line">        scores = F.softmax(scores.<span class="built_in">float</span>(), dim=-<span class="number">1</span>)</span><br><span class="line">        output = torch.matmul(scores, xv)  <span class="comment"># (batch_size, seq_len, dim)</span></span><br><span class="line">  <span class="comment"># ......</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/642884818" >一文看懂 LLaMA 中的旋转式位置编码（Rotary Position Embedding）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://mp.weixin.qq.com/s/SnPvTkeVUj2vxO8QP8s2xw" >十分钟读懂旋转编码（RoPE）<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zh.wikipedia.org/wiki/%E6%97%8B%E8%BD%AC%E7%9F%A9%E9%98%B5" >旋转矩阵<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>复数</tag>
        <tag>外推性</tag>
      </tags>
  </entry>
  <entry>
    <title>显存大小&amp;显存位宽&amp;显存频率</title>
    <url>/2024/04/09/%E6%98%BE%E5%AD%98%E5%A4%A7%E5%B0%8F&amp;%E6%98%BE%E5%AD%98%E4%BD%8D%E5%AE%BD&amp;%E6%98%BE%E5%AD%98%E9%A2%91%E7%8E%87/</url>
    <content><![CDATA[<p>简单说来，如果把显存比作一个加油站，那么:</p>
<ul>
<li>显存大小就是加油机</li>
<li>显存位宽就是进出加油站路的宽度，路越宽，能进出加油站的车辆就越多</li>
<li>显存频率相当于汽车进出加油站的速度，速度越快，汽车进出就越快</li>
</ul>
<span id="more"></span>
<ul>
<li>显存带宽就是汽车流量，也是位宽和频率的乘积，显存的频率和位宽同时决定了显存带宽，显存带宽和容量有同时决定了最终加油站员工的业绩，也就是这个显卡整体的性能那个情况。</li>
</ul>
<p>你购买显卡的时候，需要根据你的应用场景，综合考虑显存大小和显存带宽。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.zhihu.com/question/613145168/answer/3139767786" >显卡是显存重要还是位宽重要? - 居家研究院的回答 - 知乎<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>有限预算分配下的01背包问题</title>
    <url>/2025/03/29/%E6%9C%89%E9%99%90%E9%A2%84%E7%AE%97%E5%88%86%E9%85%8D%E4%B8%8B%E7%9A%8401%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>有限预算的权益分配本质上是个升级版的背包问题。假设总预算为$C$，用户$i$在券$j$下的核销率是$p_{ij}$，发券面额是$c_{ij}$，我们的求解目标是总预算约束下的订单最大化：</p>
<span id="more"></span>
<script type="math/tex; mode=display">
max \sum_{i,j} x_{ij} p_{ij} \\

\begin{aligned}
\text { s.t. } x_{ij} & \in \{0, 1\} \\
\sum_{j} x_{ij} &= 1 \\
\sum_{i,j} x_{ij} c_{ij} &\leq C \\
\end{aligned}</script><p>将上述业务问题抽象成01背包问题就是，在背包容量限制下的物品价值最大化。但传统的背包问题对应的是给同一个用户发多张券，而营销场景则是给多个用户分别只发一张券，相当于二维化传统背包问题了。</p>
<p>定义$dp[i][j][k]$为在预算$k$下给用户$i$发放券$j$后的累计最大订单量，那么动态转移方程如下：</p>
<script type="math/tex; mode=display">
dp[i][j][k] = max(dp[i-1][:J][:k])+p_{ij}</script><p>其中$J$表示券的总数，是个枚举值。</p>
<p>代码示例：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">max_value</span>(<span class="params">users_coupons, C</span>):</span><br><span class="line">    <span class="comment"># 用户数</span></span><br><span class="line">    user_num = <span class="built_in">len</span>(users_coupons)</span><br><span class="line">    <span class="comment"># 券数</span></span><br><span class="line">    coupon_num = <span class="built_in">len</span>(users_coupons[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    dp = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(user_num):</span><br><span class="line">        tmp = [[<span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)] * (C + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(coupon_num)]</span><br><span class="line">        dp.append(tmp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dp[i][j][k]：在预算k下，给用户i发放券j下的的累计最大订单</span></span><br><span class="line">    <span class="comment"># 第1个用户初始化</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(coupon_num):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, C + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> users_coupons[<span class="number">0</span>][j][<span class="number">0</span>] &lt;= k:</span><br><span class="line">                dp[<span class="number">0</span>][j][k] = users_coupons[<span class="number">0</span>][j][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, user_num):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(coupon_num):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, C + <span class="number">1</span>):</span><br><span class="line">                <span class="comment"># 发券成本，核销率</span></span><br><span class="line">                cost, cvr = users_coupons[i][j]</span><br><span class="line">                <span class="comment"># 说明此刻预算能给用户i发放券j</span></span><br><span class="line">                <span class="keyword">if</span> cost &lt;= k:</span><br><span class="line">                    <span class="comment"># 发放券j后，剩余的预算</span></span><br><span class="line">                    gap = k - cost</span><br><span class="line">                    <span class="comment"># 遍历上一个用户的所有可能发放券，获取剩余预算下的最大订单量</span></span><br><span class="line">                    <span class="keyword">for</span> prev_j <span class="keyword">in</span> <span class="built_in">range</span>(coupon_num):</span><br><span class="line">                        prev_max_value = <span class="built_in">max</span>(dp[i - <span class="number">1</span>][prev_j][:gap + <span class="number">1</span>])</span><br><span class="line">                        dp[i][j][k] = <span class="built_in">max</span>(prev_max_value + cvr, dp[i][j][k])</span><br><span class="line"></span><br><span class="line">    ret = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(coupon_num):</span><br><span class="line">        ret = <span class="built_in">max</span>(<span class="built_in">max</span>(dp[user_num - <span class="number">1</span>][j]), ret)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    users_coupons = [</span><br><span class="line">        [[<span class="number">2</span>, <span class="number">0.3</span>], [<span class="number">4</span>, <span class="number">0.6</span>], [<span class="number">1</span>, <span class="number">0.2</span>]],</span><br><span class="line">        [[<span class="number">5</span>, <span class="number">0.1</span>], [<span class="number">3</span>, <span class="number">0.8</span>], [<span class="number">2</span>, <span class="number">0.6</span>]],</span><br><span class="line">        [[<span class="number">7</span>, <span class="number">0.3</span>], [<span class="number">8</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">0.9</span>]]</span><br><span class="line">    ]</span><br><span class="line">    C = <span class="number">15</span></span><br><span class="line">    ret = max_value(users_coupons, C)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;max accumulate cvr: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(ret))</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>备注：上述代码实现的时空复杂度过高，一天的预算都有几个亿，不可能初始化这么大的数组，且寻找最优解耗时也长。业界对于此营销问题的解决方案都是走运筹，具体见：<a class="link"   href="https://transformerswsz.github.io/2025/01/15/%E7%BA%BF%E4%B8%8A%E8%BF%90%E7%AD%B9%E4%BC%98%E5%8C%96%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" >线上运筹优化公式推导<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>Marketing</category>
      </categories>
      <tags>
        <tag>Dynamic Programming</tag>
      </tags>
  </entry>
  <entry>
    <title>查看github仓库创建时间</title>
    <url>/2022/03/17/%E6%9F%A5%E7%9C%8Bgithub%E4%BB%93%E5%BA%93%E5%88%9B%E5%BB%BA%E6%97%B6%E9%97%B4/</url>
    <content><![CDATA[<p>github是没有直接的图形化界面来显示仓库的最早创建时间的，我们可以通过调用api的形式来查看，格式如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">https://api.github.com/repos/&#123;username&#125;/&#123;reponame&#125;</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><ul>
<li>确定 <code>username</code> 和 <code>reponame</code> ：<br><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/1.5wtegw7m1ow0.webp"  alt="1"></li>
<li>在终端中输入：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -k  https://api.github.com/repos/bojone/bert4keras | jq . | grep created_at</span><br></pre></td></tr></table></figure>
<ul>
<li>结果如下：<br><img   src="https://raw.githubusercontent.com/TransformersWsz/image_hosting/master/case.2krf83ywy1g0.webp"  alt="2"></li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/Jack_lzx/article/details/117480746" >一键查看GitHub仓库的创建日期<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/bojone/bert4keras" >bojone/bert4keras: keras implement of transformers for humans<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>模型训练的显存占用分布</title>
    <url>/2024/05/05/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%9A%84%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E5%88%86%E5%B8%83/</url>
    <content><![CDATA[<p>训练过程中，显存消耗主要有模型参数、梯度、optimizer状态值和中间激活值。</p>
<span id="more"></span>
<ol>
<li>模型参数：词表embedding部分占大头，与输入序列长度无关</li>
<li>梯度：每个参数对应有一个梯度</li>
<li>优化器状态值：每个参数有一个对应梯度，每个参数又对应优化器一个一阶动量和二阶动量</li>
<li>激活值：保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。与输入序列长度呈正相关</li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>显存</tag>
      </tags>
  </entry>
  <entry>
    <title>模型量化入门</title>
    <url>/2024/01/24/%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<p>量化已经是LLM部署和推理的必备环节了，在此了解一下：</p>
<span id="more"></span>
<p><a class="link"   href="https://bbs.huaweicloud.com/blogs/390419" >https://bbs.huaweicloud.com/blogs/390419<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Quantize</tag>
      </tags>
  </entry>
  <entry>
    <title>模型预估打分对运筹跟踪的影响</title>
    <url>/2025/08/03/%E6%A8%A1%E5%9E%8B%E9%A2%84%E4%BC%B0%E6%89%93%E5%88%86%E5%AF%B9%E8%BF%90%E7%AD%B9%E8%B7%9F%E8%B8%AA%E7%9A%84%E5%BD%B1%E5%93%8D/</url>
    <content><![CDATA[<p>在uplift建模中，模型离线指标(QINI、AUUC)提升并不意味着在线A/B实验的收益，因为在线运筹还需要$\lambda$约束。如果模型打分不满足单调增且roi边际递减，那么$\lambda$运筹求解会非常不稳定，导致线上发券偏高，毛利无法兜住。</p>
<span id="more"></span>
<p>下面用 <strong>两个数值化示例</strong> 直观对比：</p>
<h2 id="示例-1：-p-i-单调增但不满足边际递减-⇒-lambda-搜索不稳定"><a href="#示例-1：-p-i-单调增但不满足边际递减-⇒-lambda-搜索不稳定" class="headerlink" title="示例 1：$p_i$ 单调增但不满足边际递减 ⇒ $\lambda$ 搜索不稳定"></a>示例 1：$p_i$ 单调增但<strong>不满足</strong>边际递减 ⇒ $\lambda$ 搜索<strong>不稳定</strong></h2><ul>
<li><p><strong>样本数</strong>：5</p>
</li>
<li><p><strong>成本</strong>：全部 $c_i=1$</p>
</li>
<li><p><strong>预算</strong>：$B=3$</p>
</li>
<li><p><strong>打分</strong> $p_i$（严格单调增，但 $\Delta p_i$ = $p_i - p_{i-1}$ 不递减／有重复）：</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">i</th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
<th style="text-align:center">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$p_i$</td>
<td style="text-align:center">0.10</td>
<td style="text-align:center">0.20</td>
<td style="text-align:center">0.40</td>
<td style="text-align:center">0.40</td>
<td style="text-align:center">0.50</td>
</tr>
<tr>
<td style="text-align:center">$\Delta p_i$</td>
<td style="text-align:center">—</td>
<td style="text-align:center">0.10</td>
<td style="text-align:center">0.20</td>
<td style="text-align:center">0.00</td>
<td style="text-align:center">0.10</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>阈值集</strong> $\{p_i/c_i\}=\{0.10,0.20,0.40,0.40,0.50\}$。</li>
</ul>
<ul>
<li>当 $\lambda$ <strong>越过 0.40</strong> 时，会<strong>同时</strong>将样本 3、4 都剔除，令选中数 $C(\lambda)$ 从 3 直接跳到 1，形成大阶梯。</li>
</ul>
<script type="math/tex; mode=display">
C(\lambda)=\#\{i: p_i>\lambda\}
\quad=\begin{cases}
5,&\lambda<0.10;\\
3,&0.10\le\lambda<0.20;\\
3,&0.20\le\lambda<0.40;\\
1,&0.40\le\lambda<0.50;\\
0,&\lambda\ge0.50.
\end{cases}</script><p><strong>二分搜索行为</strong>：</p>
<ul>
<li>在 $[0.20,0.40)$ 内，任意 mid 都命中 $C=3$，算法只能不断逼近 0.40，永远无法见到$C&lt;3$的分支判定，也就卡在边界来回，无法稳定收敛到唯一解。</li>
</ul>
<h2 id="示例-2：-p-i-单调增且满足边际递减-⇒-lambda-搜索稳定"><a href="#示例-2：-p-i-单调增且满足边际递减-⇒-lambda-搜索稳定" class="headerlink" title="示例 2：$p_i$ 单调增且满足边际递减 ⇒ $\lambda$ 搜索稳定"></a>示例 2：$p_i$ 单调增且<strong>满足</strong>边际递减 ⇒ $\lambda$ 搜索<strong>稳定</strong></h2><ul>
<li><p><strong>样本数</strong>：5</p>
</li>
<li><p><strong>成本</strong>：全部 $c_i=1$</p>
</li>
<li><p><strong>预算</strong>：$B=3$</p>
</li>
<li><p><strong>打分</strong> $p_i$（严格单调增 且 $\Delta p_i$ 递减）：</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$i$</th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
<th style="text-align:center">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$p_i$</td>
<td style="text-align:center">0.10</td>
<td style="text-align:center">0.18</td>
<td style="text-align:center">0.24</td>
<td style="text-align:center">0.28</td>
<td style="text-align:center">0.30</td>
</tr>
<tr>
<td style="text-align:center">$\Delta p_i$</td>
<td style="text-align:center">—</td>
<td style="text-align:center">0.08</td>
<td style="text-align:center">0.06</td>
<td style="text-align:center">0.04</td>
<td style="text-align:center">0.02</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>阈值集</strong> $\{0.10,0.18,0.24,0.28,0.30\}$，且每次跨过一个阈值，只会剔除一个样本。</li>
</ul>
<script type="math/tex; mode=display">
C(\lambda)=\#\{i: p_i>\lambda\}
\quad=\begin{cases}
5,&\lambda<0.10;\\
4,&0.10\le\lambda<0.18;\\
3,&0.18\le\lambda<0.24;\\
2,&0.24\le\lambda<0.28;\\
1,&0.28\le\lambda<0.30;\\
0,&\lambda\ge0.30.
\end{cases}</script><p><strong>二分搜索行为</strong>：</p>
<ul>
<li><strong>目标</strong>：$C(\lambda)=3$。</li>
</ul>
<ul>
<li>初始区间 $[0.10,0.30]$，mid=0.20 → $C(0.20)=3$ → 收缩右端 → $[0.10,0.20]$。</li>
<li>mid=0.15 → $C=4&gt;3$ → 收缩右端 → $[0.10,0.15]$。</li>
<li>… 依次剔除第2号、第3号样本，每次跨过一个阈值，$C$ 变化为 4→3→2…，二分能稳定地一步步逼近恰好使 $C=3$ 的 $\lambda$。</li>
</ul>
<h2 id="核心对比"><a href="#核心对比" class="headerlink" title="核心对比"></a>核心对比</h2><div class="table-container">
<table>
<thead>
<tr>
<th>条件</th>
<th>阶梯跳变</th>
<th>二分稳定性</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>示例1</strong>：边际不递减或重复值</td>
<td>大阶梯（一次掉多个）</td>
<td>卡在大跳点来回</td>
</tr>
<tr>
<td><strong>示例2</strong>：边际严格递减</td>
<td>小阶梯（一次掉一个）</td>
<td>逐次逼近，稳定收敛</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>只有当每次 $\lambda$ 触碰一个阈值，就只影响一个样本时，累积成本 $C(\lambda)$ 曲线才近似“单调平滑”，二分才能一步步稳定逼近目标预算。</li>
<li>如果一次跨越多个阈值（示例1），或阈值间距极小/重复（前例），则会出现“跳变过大”或“可行区间过窄”，导致二分收敛失灵或来回摆动。</li>
</ul>
]]></content>
      <categories>
        <category>Marketing</category>
      </categories>
      <tags>
        <tag>Monotonical</tag>
        <tag>Convex Hull</tag>
      </tags>
  </entry>
  <entry>
    <title>正排索引&amp;倒排索引</title>
    <url>/2023/08/29/%E6%AD%A3%E6%8E%92%E7%B4%A2%E5%BC%95-%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/</url>
    <content><![CDATA[<p>常见的两种索引：</p>
<span id="more"></span>
<h2 id="正排索引"><a href="#正排索引" class="headerlink" title="正排索引"></a>正排索引</h2><p>根据文档找关键词：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.14aoq6bvzlr4.webp"  alt="1"></p>
<ul>
<li>优点：索引结构简单，维护容易</li>
<li>缺点：需要遍历所有文档，找到所需关键词，耗时长，检索效率低</li>
</ul>
<h2 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h2><p>根据关键词找文档：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.20tisy7awqm8.webp"  alt="2"></p>
<ul>
<li>优点：查询效率远高于正排索引</li>
<li>缺点：结构复杂，维护较为困难，建索引耗时</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.cnblogs.com/lotuslaw/p/16393064.html" >3-正排索引和倒排索引<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title>毫米波雷达&amp;激光雷达</title>
    <url>/2025/11/30/%E6%AF%AB%E7%B1%B3%E6%B3%A2%E9%9B%B7%E8%BE%BE-%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE/</url>
    <content><![CDATA[<p>在自动驾驶和智能感知领域，毫米波雷达和激光雷达都是不可或缺的组件。</p>
<span id="more"></span>
<h3 id="一句话核心比喻"><a href="#一句话核心比喻" class="headerlink" title="一句话核心比喻"></a>一句话核心比喻</h3><ul>
<li><strong>毫米波雷达</strong>：像一只<strong>蝙蝠</strong>。它通过发射和接收看不见的声波（这里是无线电波）来感知物体的距离和速度。它不知道物体的具体形状，但能知道“那里有个东西在动”。</li>
<li><strong>激光雷达</strong>：像一个<strong>快速扫描的尺子</strong>。它通过发射激光束并测量光返回的时间，来创建周围环境的<strong>高精度3D地图</strong>。它能清晰地“看到”物体的轮廓。</li>
</ul>
<h3 id="详细对比解析"><a href="#详细对比解析" class="headerlink" title="详细对比解析"></a>详细对比解析</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">特性</th>
<th style="text-align:left"><strong>毫米波雷达</strong></th>
<th style="text-align:left"><strong>激光雷达</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>工作原理</strong></td>
<td style="text-align:left">发射<strong>毫米波</strong>（无线电波），通过计算回波的时间差和频率变化来测距和测速。</td>
<td style="text-align:left">发射<strong>激光束</strong>，通过计算激光返回的时间来测距，并生成点云图。</td>
</tr>
<tr>
<td style="text-align:left"><strong>输出结果</strong></td>
<td style="text-align:left">物体的<strong>距离、相对速度、方位角</strong>。点数据稀疏。</td>
<td style="text-align:left">周围环境的<strong>高精度三维点云图</strong>，能清晰显示物体形状。</td>
</tr>
<tr>
<td style="text-align:left"><strong>优势</strong></td>
<td style="text-align:left">1. <strong>极佳的测速能力</strong>（多普勒效应）。<br>2. <strong>全天候工作</strong>：不受雨、雪、雾、尘等恶劣天气影响。<br>3. <strong>穿透性强</strong>：可探测被遮挡的物体。<br>4. <strong>成本较低</strong>。</td>
<td style="text-align:left">1. <strong>超高分辨率</strong>：精度可达厘米级。<br>2. <strong>3D建模能力强</strong>：能精确识别物体轮廓、类别（人、车、障碍物）。</td>
</tr>
<tr>
<td style="text-align:left"><strong>劣势</strong></td>
<td style="text-align:left">1. <strong>分辨率低</strong>：难以识别物体具体形状和类型。<br>2. <strong>无法识别细节</strong>：不能“看到”交通标志、文字等。</td>
<td style="text-align:left">1. <strong>受天气影响大</strong>：大雨、浓雾、烟尘会严重衰减激光信号。<br>2. <strong>成本高昂</strong>（虽在下降，但仍比雷达贵）。<br>3. 难以探测玻璃等透明物体。</td>
</tr>
<tr>
<td style="text-align:left"><strong>主要应用</strong></td>
<td style="text-align:left">1. <strong>汽车自适应巡航（ACC）、自动紧急刹车（AEB）</strong><br>2. <strong>盲点监测</strong><br>3. <strong>智能交通监控</strong></td>
<td style="text-align:left">1. <strong>高级别自动驾驶（L3+）</strong>：车辆的核心环境感知传感器。<br>2. <strong>高精度地图测绘</strong><br>3. <strong>机器人、无人机避障与导航</strong></td>
</tr>
</tbody>
</table>
</div>
<h3 id="深入原理说明"><a href="#深入原理说明" class="headerlink" title="深入原理说明"></a>深入原理说明</h3><h4 id="1-毫米波雷达"><a href="#1-毫米波雷达" class="headerlink" title="1. 毫米波雷达"></a>1. 毫米波雷达</h4><ul>
<li><strong>什么是毫米波？</strong><br>毫米波是指波长在1~10毫米之间的电磁波，其频率约为30~300 GHz。它介于微波和红外线之间。</li>
<li><strong>核心优势解析：</strong><ul>
<li><strong>全天候工作</strong>：毫米波的波长相对较长，绕过雨滴、尘埃等微小颗粒的能力更强，因此恶劣天气下性能衰减较小。</li>
<li><strong>直接测速</strong>：利用<strong>多普勒效应</strong>（和警用雷达测速仪原理一样），可以直接、精确地测量目标与雷达之间的相对速度。这是它无可替代的优势。</li>
<li><strong>成本低</strong>：技术相对成熟，特别是用于盲点监测等的普通雷达，已实现大规模量产。</li>
</ul>
</li>
</ul>
<h4 id="2-激光雷达"><a href="#2-激光雷达" class="headerlink" title="2. 激光雷达"></a>2. 激光雷达</h4><ul>
<li><strong>工作原理</strong>：<br>激光雷达通过快速旋转的发射器，向周围环境发射数百万个激光点。每个点碰到物体后会反射回来，被传感器接收。通过计算激光从发射到返回的时间（Time of Flight, ToF），就能精确计算出每个点与雷达的距离。所有这些点集合起来，就形成了所谓的“点云”，这是一幅极其精细的3D环境地图。</li>
<li><strong>核心优势解析：</strong><ul>
<li><strong>高精度3D建模</strong>：这是激光雷达最核心的价值。它能让自动驾驶汽车清晰地“看到”前方是一个行人、一辆自行车还是一个塑料袋，并能精确感知其轮廓和运动轨迹，这是实现安全决策的基础。</li>
</ul>
</li>
</ul>
<h3 id="在自动驾驶中的应用与融合"><a href="#在自动驾驶中的应用与融合" class="headerlink" title="在自动驾驶中的应用与融合"></a>在自动驾驶中的应用与融合</h3><p>在自动驾驶领域，<strong>没有一种传感器是完美的</strong>，因此需要“传感器融合”技术，让它们取长补短。</p>
<ul>
<li><strong>摄像头</strong>：就像人的“眼睛”，提供丰富的颜色、纹理信息（如识别红绿灯、路牌），但无法直接提供精确的距离信息，且受光线影响大。</li>
<li><strong>毫米波雷达</strong>：就像“顺风耳”，擅长测速和在大雾大雨天气下工作，告诉你“有物体在靠近，速度很快”。</li>
<li><strong>激光雷达</strong>：就像“尺子精”，提供精确的3D空间信息，告诉你“那个物体是个人，离你20.5米，正在横穿马路”。</li>
</ul>
<p><strong>典型的融合方案（以Waymo为例）：</strong><br>激光雷达生成高精度的3D地图 -&gt; 摄像头对地图中的物体进行识别和分类（这是一个人，那是一辆车）-&gt; 毫米波雷达提供物体的精确速度数据，并在激光雷达和摄像头性能下降时（如恶劣天气）提供冗余保障。</p>
<p><strong>特斯拉的独特路线</strong>：<br>特斯拉长期坚持<strong>纯视觉方案</strong>（主要依靠摄像头+AI算法），认为这更接近人类驾驶。但近年来，特斯拉也开始在其车型上引入<strong>毫米波雷达</strong>（后又移除，策略有变化），并探索高分辨率雷达，这也从侧面说明了多传感器融合的重要性。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><strong>毫米波雷达</strong>是“务实的功能性传感器”，性价比高，全天候可靠，尤其擅长测速。</li>
<li><strong>激光雷达</strong>是“精确的建模型传感器”，能构建高精地图，是高级别自动驾驶的“眼睛”，但成本高且受天气制约。</li>
</ul>
<p>它们是智能系统感知世界的两种互补技术，共同确保了车辆或机器人在复杂环境下的安全运行。</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>LIDAR</tag>
      </tags>
  </entry>
  <entry>
    <title>法拉第笼原理</title>
    <url>/2025/08/17/%E6%B3%95%E6%8B%89%E7%AC%AC%E7%AC%BC%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>比亚迪汽车遭遇三次雷击后，车身仍然保持完好无损，内部系统正常运转，乘客平安无事，这是由于法拉第笼(<code>Faraday Cage</code>)原理造成的。</p>
<span id="more"></span>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>法拉第笼</strong>是一种由良导体（通常是金属网或金属壳）制成的封闭或半封闭空间。它的主要作用是 <strong>屏蔽电场和电磁波</strong>，从而保护内部的电子设备或生物体不受外界电磁干扰。</p>
<h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p>它的核心原理来自 <strong>静电学</strong> 和 <strong>电磁学</strong>：</p>
<ol>
<li><p><strong>静电平衡</strong></p>
<ul>
<li>当一个导体整体处于静电平衡时，导体内部的电场强度为零。</li>
<li>外部电荷在导体表面重新分布，使得内部电荷产生的电场刚好抵消外部电场。</li>
</ul>
<p>因此，笼子内部不受外部静电场影响。数学上可用高斯定律解释：</p>
<script type="math/tex; mode=display">
\oint_S \mathbf{E}\cdot d\mathbf{A} = \frac{Q_{\text{in}}}{\varepsilon_0}</script><p>如果笼子是闭合导体，且内部无净电荷（$Q_{\text{in}}=0$），则内部电场 $\mathbf{E} = 0$。</p>
</li>
<li><p><strong>电磁波屏蔽</strong></p>
<ul>
<li>外部的电磁波作用到金属时，会在金属中激发感应电流。</li>
<li>这些电流产生的电磁场会抵消电磁波在金属内部的传播。</li>
<li><p>电磁波的衰减程度与金属的厚度和电导率有关，用“<strong>趋肤效应</strong>”来描述：</p>
<script type="math/tex; mode=display">
\delta = \sqrt{\frac{2}{\mu \sigma \omega}}</script><ul>
<li>$\delta$：趋肤深度</li>
<li>$\mu$：磁导率</li>
<li>$\sigma$：电导率</li>
<li>$\omega$：角频率</li>
</ul>
</li>
<li>高频电磁波只会在导体表面的一层很薄区域传播，进入深层会被快速衰减。</li>
</ul>
</li>
</ol>
<h2 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h2><ul>
<li><strong>实验室/数据中心</strong>：隔绝外部电磁干扰，保证精密仪器稳定运行。</li>
<li><strong>汽车/飞机机舱</strong>：车体/机舱就是一个天然的法拉第笼，可以在雷雨天气里保护乘客（雷击电流沿外壳走，不进到内部）。</li>
<li><strong>手机信号屏蔽袋</strong>：防止手机信号泄漏或被监测。</li>
<li><strong>MRI 机房</strong>：墙壁和门上有金属网，防止外界无线信号干扰成像。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>法拉第笼的核心原理就是<strong>导体会重新分布电荷，以抵消内部电场</strong>，再加上<strong>金属的趋肤效应削弱高频电磁波</strong>，因此能有效屏蔽电场和电磁波。</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Faraday Cage</tag>
      </tags>
  </entry>
  <entry>
    <title>测不准关系</title>
    <url>/2025/03/13/%E6%B5%8B%E4%B8%8D%E5%87%86%E5%85%B3%E7%B3%BB/</url>
    <content><![CDATA[<p>在物理学中，测不准关系（Uncertainty Principle），又称为海森堡不确定性原理（Heisenberg Uncertainty Principle），是由德国物理学家维尔纳·海森堡于1927年提出的量子力学的一个基本原理。这个原理表明，在量子尺度上，某些对易不为零的物理量（比如位置和动量）是不可能同时被精确测量的。</p>
<span id="more"></span>
<p>具体来说，测不准关系可以用以下数学形式表示：</p>
<script type="math/tex; mode=display">
 \Delta x \Delta p_x \geq \frac{\hbar}{2}</script><p>这里，$\Delta x$ 表示位置的不确定度，$\Delta p_x$ 表示动量的不确定度，而 $\hbar$ 是约化普朗克常数（$\hbar = \frac{h}{2 \pi}$）。<br>这个不等式说明，如果我们试图非常精确地测量一个粒子的位置（即$\Delta x$非常小），那么对该粒子的动量（即$\Delta p_x$）的不确定度就会变得非常大，反之亦然。这不是因为测量技术上的限制，而是量子现象固有的特性。<br>测不准关系有几个重要的含义：</p>
<ol>
<li>它揭示了在量子世界中，粒子的行为与宏观世界的物体大相径庭。在宏观尺度上，我们可以同时精确测量一个物体的位置和速度，但在量子尺度上这是不可能的。</li>
<li>测不准关系与波粒二象性有关。根据量子力学，粒子如电子既可以表现出波动性，也可以表现出粒子性。波函数的宽度（与位置的不确定度相关）与动量的分布（与动量的不确定度相关）之间存在反向关系。</li>
<li>这个原理并不是说我们不能测量粒子的位置和动量，而是说我们不能同时得到它们的确切值。在实际操作中，我们可以测量其中一个量并得到非常精确的结果，但那时我们对另一个量的知识就会变得模糊。</li>
</ol>
<p>测不准关系是量子力学的一个基本特征，它对我们理解微观世界的本质有着深远的影响。</p>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>Uncertainty Principle</tag>
      </tags>
  </entry>
  <entry>
    <title>海量数据处理面试题</title>
    <url>/2021/08/10/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<p>百度三面考到了海量数据处理题，真的是血泪教训，在此记录一下。</p>
<span id="more"></span>
<h2 id="1-海量日志数据，提取出某日访问百度次数最多的那个IP"><a href="#1-海量日志数据，提取出某日访问百度次数最多的那个IP" class="headerlink" title="1. 海量日志数据，提取出某日访问百度次数最多的那个IP"></a>1. 海量日志数据，提取出某日访问百度次数最多的那个IP</h2><p>可以采用映射的方法，比如模1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map进行频率统计，然后再找出频率最大的几个）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。</p>
<h2 id="2-有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。"><a href="#2-有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。" class="headerlink" title="2. 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。"></a>2. 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。</h2><ol>
<li>顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。 </li>
<li>找一台内存在2G左右的机器，依次用hash_map(query, query_count)来统计每个query出现的次数。</li>
<li>利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件。</li>
<li>对这10个文件进行归并排序（内排序与外排序相结合）。</li>
</ol>
<h2 id="3-给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？"><a href="#3-给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？" class="headerlink" title="3. 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？"></a>3. 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？</h2><ol>
<li>遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。</li>
<li>遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小文件（a0 vs b0,a1 vs b1,…,a999 vs b999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。</li>
<li>求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。</li>
</ol>
<h2 id="4-腾讯面试题：给40亿个不重复的unsigned-int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？"><a href="#4-腾讯面试题：给40亿个不重复的unsigned-int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？" class="headerlink" title="4. 腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？"></a>4. 腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？</h2><h4 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h4><p>申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。</p>
<h4 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h4><p>我们把40亿个数中的每一个用32位的二进制来表示假设这40亿个数开始放在一个文件中。</p>
<p>然后将这40亿个数分成两类: 1.最高位为0 2.最高位为1，并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=20亿，而另一个&gt;=20亿（相当于折半）；与要查找的数的最高位比较并接着进入相应的文件再查找。</p>
<p>再然后把这个文件为又分成两类: 1.次最高位为0 2.次最高位为1，并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=10亿，而另一个&gt;=10亿（相当于折半）；与要查找的数的次最高位比较并接着进入相应的文件再查找。以此类推，就可以找到了,而且时间复杂度为O(logn)。</p>
<hr>
<p>其它的场景及解决方案可继续参考：<a class="link"   href="https://zhuanlan.zhihu.com/p/341386422" >十道海量数据处理面试题与十个方法大总结<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>大数据</tag>
        <tag>topK</tag>
      </tags>
  </entry>
  <entry>
    <title>点沙成硅</title>
    <url>/2024/08/19/%E7%82%B9%E6%B2%99%E6%88%90%E7%A1%85/</url>
    <content><![CDATA[<p>将一粒沙子转化为芯片的过程是一个复杂而精密的制造流程。芯片制造始于原材料硅的提纯，然后经过多步骤的工艺，最终变成用于计算机、手机等设备的半导体芯片。以下是这个过程的主要步骤：</p>
<span id="more"></span>
<h2 id="1-提取和提纯硅"><a href="#1-提取和提纯硅" class="headerlink" title="1. 提取和提纯硅"></a>1. 提取和提纯硅</h2><ul>
<li>原材料： 沙子的主要成分是二氧化硅（SiO₂）。为了制造芯片，需要将沙子中的硅提纯。</li>
<li>提纯过程： 将沙子加热至高温，结合碳元素提取出硅，得到纯度较高的冶金级硅（约98%纯度）。</li>
<li>进一步提纯： 冶金级硅通过化学气相沉积（CVD）或西门子法进一步提纯，最终获得电子级硅，纯度达到99.9999999%（7N）。<h2 id="2-单晶硅的制造"><a href="#2-单晶硅的制造" class="headerlink" title="2. 单晶硅的制造"></a>2. 单晶硅的制造</h2></li>
<li>单晶硅棒： 通过区熔法或柴氏拉晶法将高纯度的硅制成单晶硅棒。这些棒材是后续制造芯片的基础材料。</li>
<li>切片： 将单晶硅棒切割成薄片，称为晶圆。晶圆的厚度通常为数百微米。<h2 id="3-晶圆的清洗和准备"><a href="#3-晶圆的清洗和准备" class="headerlink" title="3. 晶圆的清洗和准备"></a>3. 晶圆的清洗和准备</h2></li>
<li>清洗： 切割后的晶圆表面需要清洗以去除杂质。</li>
<li>氧化和光刻： 清洗后的晶圆表面会被氧化形成一层薄薄的氧化物，然后通过光刻技术在晶圆上绘制出电路图案。<h2 id="4-光刻与蚀刻"><a href="#4-光刻与蚀刻" class="headerlink" title="4. 光刻与蚀刻"></a>4. 光刻与蚀刻</h2></li>
<li>光刻： 在晶圆上涂上一层感光材料（光刻胶），然后通过掩膜版曝光，形成电路图案。</li>
<li>蚀刻： 通过化学或等离子体蚀刻技术，将暴露的部分材料去除，使得电路图案在晶圆上显现。<h2 id="5-离子注入与扩散"><a href="#5-离子注入与扩散" class="headerlink" title="5. 离子注入与扩散"></a>5. 离子注入与扩散</h2></li>
<li>离子注入： 在晶圆表面注入杂质元素（如磷、硼），控制硅的导电性。</li>
<li>扩散： 通过加热使得注入的杂质均匀分布，形成P型或N型半导体区。<h2 id="6-多层金属化与连接"><a href="#6-多层金属化与连接" class="headerlink" title="6. 多层金属化与连接"></a>6. 多层金属化与连接</h2></li>
<li>金属化： 在晶圆表面沉积金属层（如铝、铜）以形成电极和连接线。</li>
<li>多层互连： 复杂的芯片通常需要多层电路，这些电路层通过绝缘层和通孔相互连接。<h2 id="7-封装与测试"><a href="#7-封装与测试" class="headerlink" title="7. 封装与测试"></a>7. 封装与测试</h2></li>
<li>切割： 制造完成后，将整个晶圆切割成一个个独立的芯片。</li>
<li>封装： 将芯片装入保护壳中，并焊接引脚，方便与电路板连接。</li>
<li>测试： 封装后的芯片经过一系列电性能测试，确保其功能正常。<h2 id="8-最终产品"><a href="#8-最终产品" class="headerlink" title="8. 最终产品"></a>8. 最终产品</h2></li>
<li>装配： 经过测试的芯片会被集成到各类电子设备中，成为计算机、手机等现代电子产品的核心。</li>
</ul>
<p>从沙子到芯片的过程，每一步骤都要求高度的精度和纯度，以确保最终的芯片能够以极高的效率执行复杂的计算任务。</p>
]]></content>
      <categories>
        <category>Hardware</category>
      </categories>
      <tags>
        <tag>Silicon</tag>
        <tag>Semiconductor</tag>
      </tags>
  </entry>
  <entry>
    <title>现代GPU内存分级结构</title>
    <url>/2024/04/22/%E7%8E%B0%E4%BB%A3GPU%E5%86%85%E5%AD%98%E5%88%86%E7%BA%A7%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<p>要实现CUDA高性能编程，就必须对GPU内存结构有深刻的了解。</p>
<span id="more"></span>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.5tqtj239zp.png"  alt="GPU"></p>
<h4 id="全局内存-off-chip"><a href="#全局内存-off-chip" class="headerlink" title="全局内存(off-chip)"></a>全局内存(off-chip)</h4><p>就是我们常说的显存，其容量最大、带宽最小、延迟最高。</p>
<h4 id="常量内存-off-chip"><a href="#常量内存-off-chip" class="headerlink" title="常量内存(off-chip)"></a>常量内存(off-chip)</h4><p>存储在片下存储的设备内存上，但是通过特殊的常量内存缓存进行缓存读取，常量内存为只读内存，只有64KB。由于有缓存，常量内存的访问速度比全局内存高。</p>
<p>使用常量内存的方法是在核函数外面用 <code>__constant__</code> 定义变量，并用函数 <code>cudaMemcpyToSymbol</code> 将数据从主机端复制到设备的常量内存后供核函数使用。</p>
<h4 id="纹理内存和表面内存-off-chip"><a href="#纹理内存和表面内存-off-chip" class="headerlink" title="纹理内存和表面内存(off-chip)"></a>纹理内存和表面内存(off-chip)</h4><p>纹理内存和表面内存类似于常量内存，也是一种具有缓存的全局内存，有相同的可见范围和生命周期，而且一般仅可读(表面内存也可写)。不同的是，纹理内存和表面内存容量更大，而且使用方式和常量内存也不一样。</p>
<h4 id="共享内存-on-chip"><a href="#共享内存-on-chip" class="headerlink" title="共享内存(on-chip)"></a>共享内存(on-chip)</h4><p>共享内存存在于芯片上，具有仅次于寄存器的读写速度，数量也有限。一个使用共享内存的变量可以 <code>__shared__</code> 修饰符来定义。该变量对block内的所有线程可见。</p>
<h4 id="寄存器-on-chip"><a href="#寄存器-on-chip" class="headerlink" title="寄存器(on-chip)"></a>寄存器(on-chip)</h4><p>寄存器是一个线程能独立访问的资源，它所在的位置与局部内存不一样，是在片上（on chip）的存储，用来存储当前线程的一些暂存数据。寄存器的速度是访问中最快的，但是它的容量较小。</p>
<p>在核函数中定义的不加任何限定符的变量一般来说就存放于寄存器(register)中。各种内建变量，如 <code>gridDim、blockDim、blockIdx、 threadIdx 及 warpSize</code> 都保存在特殊的寄存器中，以便高效访问。举例如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> n = blockDim.x * blockIdx.x + threadIdx.x;</span><br><span class="line">c[n] = a[n] + b[n];</span><br></pre></td></tr></table></figure>
<p><code>n</code> 也是一个寄存器变量，当只能被当前线程访问。</p>
<h4 id="局部内存-off-chip"><a href="#局部内存-off-chip" class="headerlink" title="局部内存(off-chip)"></a>局部内存(off-chip)</h4><p>局部内存和寄存器几乎一样，核函数中定义的不加任何限定符的变量有可能在寄存器中，也有可能在局部内存中。寄存器中放不下的变量，以及索引值不能在编译时就确定的数组，都有可能放在局部内存中。</p>
<p>虽然局部内存在用法上类似于寄存器，但从硬件来看，局部内存只是全局内存的一部分。所以，局部内存的延迟也很高。每个线程最多能使用高达512KB的局部内存，但使用过多会降低程序的性能。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.b8p2wws76.webp"  alt="detail"></p>
<hr>
<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/654027980" >CUDA（二）：GPU的内存体系及其优化指南<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/651179378" >GPU 内存概念浅析<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>生成式召回-TIGER范式</title>
    <url>/2025/07/27/%E7%94%9F%E6%88%90%E5%BC%8F%E5%8F%AC%E5%9B%9E-TIGER%E8%8C%83%E5%BC%8F/</url>
    <content><![CDATA[<p>TIGER（Transformer Index for Generative Recommenders）是生成式召回的经典力作，其核心思想就是语义ID+Seq2Seq，这一范式启发了后续大量生成式推荐工作。</p>
<span id="more"></span>
<h2 id="📌-背景与痛点"><a href="#📌-背景与痛点" class="headerlink" title="📌 背景与痛点"></a>📌 背景与痛点</h2><p>item id是商品在候选库中的独特标识，其具有高度稀疏性，且没有任何物理含义，对于模型训练、新品冷启、可解释性都极不友好。假设一个item的side info足够多，可以完整刻画该item的属性，那么item id完全是可以舍弃的。</p>
<p>TIGER利用多个语义id来表征该item，极大地减少了id embedding词表空间，对工业界生产环境友好；通过模型结构共享相似item的语义信息，提升模型泛化性，利好新品冷启。</p>
<h2 id="✅-TIGER-解决的痛点和优势"><a href="#✅-TIGER-解决的痛点和优势" class="headerlink" title="✅ TIGER 解决的痛点和优势"></a>✅ TIGER 解决的痛点和优势</h2><div class="table-container">
<table>
<thead>
<tr>
<th>痛点</th>
<th>TIGER 的解决方法</th>
<th>优势</th>
</tr>
</thead>
<tbody>
<tr>
<td>embedding 太大 / 存储高</td>
<td>Semantic ID token 数量极小，token vocabulary 可控制</td>
<td>内存友好、减小表规模</td>
</tr>
<tr>
<td>冷启动 item embedding 缺失</td>
<td>Semantic ID 来源于 item 内容特征</td>
<td>可推广至新 item，无需训练 embedding</td>
</tr>
<tr>
<td>类似 item 无共享</td>
<td>相似内容生成相近的 Semantic ID</td>
<td>用户语义共享，加强泛化</td>
</tr>
<tr>
<td>模型检索复杂</td>
<td>Transformer decoder 直接生成</td>
<td>端到端简洁流程</td>
</tr>
</tbody>
</table>
</div>
<h2 id="🧠-核心创新点"><a href="#🧠-核心创新点" class="headerlink" title="🧠 核心创新点"></a>🧠 核心创新点</h2><h3 id="Semantic-ID表示"><a href="#Semantic-ID表示" class="headerlink" title="Semantic ID表示"></a>Semantic ID表示</h3><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.4g4sr2jrvg.webp"  alt="semantic id"></p>
<ul>
<li>使用内容编码（如 SentenceT5）生成 item embedding</li>
<li>将embedding经RQ-VAE量化为一系列codeword Tuple，即 Semantic ID</li>
<li>各token具有语义信息，编码符号总量远小于item总量</li>
</ul>
<h3 id="生成式检索（Generative-Retrieval）"><a href="#生成式检索（Generative-Retrieval）" class="headerlink" title="生成式检索（Generative Retrieval）"></a>生成式检索（Generative Retrieval）</h3><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.6m47cudpo8.webp"  alt="seq2seq"></p>
<p>通过自回归解码生成目标item id，而不是传统embedding + ANN。Transformer的decoder直接输出item的Semantic ID作为推荐结果。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.64e5o9fh1w.webp"  alt="result"></p>
<p>别看实验结果相对值提升很大，很唬人，其实绝对值提升很小。但TIGER范式建模确是一个极大的创新，为生成式推荐打开了思路。</p>
<h2 id="🧾-总结"><a href="#🧾-总结" class="headerlink" title="🧾 总结"></a>🧾 总结</h2><ul>
<li>TIGER是第一篇将 <strong>Generative Retrieval 自回归生成方式</strong> 应用于推荐系统的工作；</li>
<li>它通过<strong>Semantic ID 和 Seq2Seq Transformer</strong>，突破embedding + ANN的传统限制；</li>
<li>在<strong>冷启动、多样性、效率和泛化能力</strong>上展现强优势；</li>
<li>适用于大规模推荐场景，尤其是content-rich、item海量、频繁上线新品的平台。</li>
</ul>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://papers.neurips.cc/paper_files/paper/2023/file/20dcab0f14046a5c6b02b61da9f13229-Paper-Conference.pdf" >Recommender Systems with Generative Retrieval<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/1897030256965177585" >【谷歌2023】TIGER：基于生成式召回的推荐系统<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/716122180" >NIPS‘23「谷歌」语义ID｜TIGER：Recommender Systems with Generative Retrieval<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>LLM</tag>
        <tag>Retrieval</tag>
        <tag>RQ-VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>理财基础知识</title>
    <url>/2024/12/25/%E7%90%86%E8%B4%A2%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p>了解一下最基础的理财入门知识：</p>
<span id="more"></span>
<h2 id="基金"><a href="#基金" class="headerlink" title="基金"></a>基金</h2><h4 id="基金类型"><a href="#基金类型" class="headerlink" title="基金类型"></a>基金类型</h4><p>基金是一种集合投资方式，通过汇集众多投资者的资金，由专业的基金经理进行管理和投资。基金种类繁多，各自有不同的特点，以下是一些主要的基金种类及其特点：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>特点</th>
<th>分类</th>
</tr>
</thead>
<tbody>
<tr>
<td>股票型基金</td>
<td>主要投资于股票市场，风险较高，但预期收益也较高</td>
<td>按投资风格可分为价值型、成长型和平衡型基金</td>
</tr>
<tr>
<td>债券型基金</td>
<td>主要投资于债券市场，风险相对较低，收益稳定</td>
<td>按债券种类可分为国债基金、企业债基金等</td>
</tr>
<tr>
<td>货币市场基金</td>
<td>主要投资于短期货币市场工具，如国债、央行票据等，风险极低，流动性好，适合短期资金管理</td>
<td>收益相对较低，但通常高于银行活期存款</td>
</tr>
<tr>
<td>混合型基金</td>
<td>同时投资于股票、债券等不同资产类别，风险和收益介于股票型基金和债券型基金之间</td>
<td>按股票和债券的配置比例可分为偏股型、偏债型等</td>
</tr>
<tr>
<td>指数型基金</td>
<td>跟踪某个特定的指数，如沪深300指数、纳斯达克100指数等，风险和收益与指数走势密切相关</td>
<td>按复制方法可分为完全复制型、抽样复制型等</td>
</tr>
<tr>
<td>QDII基金</td>
<td>投资于海外市场，风险和收益受国际市场影响较大</td>
<td>适合希望分散投资、获取海外市场收益的投资者</td>
</tr>
<tr>
<td>FOF基金</td>
<td>投资于其他基金，如股票型基金、债券型基金等，风险和收益取决于所投资的基金</td>
<td>适合风险厌恶型投资者，通过分散投资降低风险</td>
</tr>
<tr>
<td>ETF基金</td>
<td>交易所交易基金，可以在证券交易所上市交易，交易便捷，费用低廉</td>
<td>通常跟踪某个特定的指数，如沪深300ETF、纳斯达克100ETF等</td>
</tr>
<tr>
<td>LOF基金</td>
<td>上市开放式基金，可以在证券交易所上市交易，也可以在基金公司申购赎回</td>
<td>适合希望投资于特定主题或行业的投资者</td>
</tr>
<tr>
<td>分级基金</td>
<td>通过分级设计，将基金份额分为优先级和劣后级，优先级享有固定收益，劣后级承担风险和收益</td>
<td>风险较高，适合风险承受能力较强的投资者</td>
</tr>
</tbody>
</table>
</div>
<p>以上只是基金种类中的一部分，投资者在选择基金时，应根据自身的风险承受能力、投资目标和资金使用期限等因素进行综合考虑。同时，建议在投资前咨询专业的金融顾问，以获取更详细和个性化的投资建议</p>
<h4 id="同种债券A-amp-C"><a href="#同种债券A-amp-C" class="headerlink" title="同种债券A&amp;C"></a>同种债券A&amp;C</h4><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/637453d82cfe01a6b8ebe8c68e26f89.77dmhjm8rn.webp"  alt="富荣中短债"></p>
<p>富荣中短债债券A和富荣中短债债券C是同一基金的不同份额类别，主要区别在于收费方式。以下是它们的具体区别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>富荣中短债债券A</th>
<th>富荣中短债债券C</th>
</tr>
</thead>
<tbody>
<tr>
<td>收费方式</td>
<td>认购、申购时收取费用</td>
<td>不收取认购、申购费用</td>
</tr>
<tr>
<td>销售服务费</td>
<td>不从基金资产中计提</td>
<td>从基金资产中计提（0.2%）</td>
</tr>
<tr>
<td>适合人群</td>
<td>长期持有投资者</td>
<td>短期持有投资者</td>
</tr>
<tr>
<td>费用结构</td>
<td>收取申购费，不收销售服务费</td>
<td>不收申购费，收销售服务费</td>
</tr>
<tr>
<td>适合期限</td>
<td>长期持有</td>
<td>短期持有</td>
</tr>
</tbody>
</table>
</div>
<h2 id="股票"><a href="#股票" class="headerlink" title="股票"></a>股票</h2><h3 id="道琼斯工业平均指数-amp-纳斯达克综合指数-amp-标普指数"><a href="#道琼斯工业平均指数-amp-纳斯达克综合指数-amp-标普指数" class="headerlink" title="道琼斯工业平均指数 &amp; 纳斯达克综合指数 &amp; 标普指数"></a>道琼斯工业平均指数 &amp; 纳斯达克综合指数 &amp; 标普指数</h3><p>道琼斯工业平均指数（DJIA）、标准普尔500指数（S&amp;P 500）和纳斯达克综合指数（NASDAQ Composite）是美国股市中三大最重要的股票市场指数，它们各自有不同的计算方式、成分股、行业集中度以及投资者的使用目的。下面是这三个指数的主要区别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特征</th>
<th>道琼斯工业平均指数（DJIA）</th>
<th>标准普尔500指数（S&amp;P 500）</th>
<th>纳斯达克综合指数（NASDAQ Composite）</th>
</tr>
</thead>
<tbody>
<tr>
<td>成分股数量</td>
<td>30只</td>
<td>500只</td>
<td>约3000只</td>
</tr>
<tr>
<td>选股标准</td>
<td>根据公司规模、行业代表性等选股</td>
<td>市值、流动性和财务状况等标准</td>
<td>在纳斯达克交易所上市的所有公司</td>
</tr>
<tr>
<td>计算方法</td>
<td>价格加权</td>
<td>市值加权</td>
<td>市值加权</td>
</tr>
<tr>
<td>行业分布</td>
<td>传统行业（如工业、金融、消费品）为主</td>
<td>广泛涵盖多个行业</td>
<td>主要以科技股为主，尤其是互联网和高科技公司</td>
</tr>
<tr>
<td>波动性</td>
<td>较低，稳定性较强</td>
<td>中等波动性</td>
<td>较高，科技股影响较大</td>
</tr>
<tr>
<td>代表性</td>
<td>美国经济骨干、传统行业的代表</td>
<td>代表美国股市整体表现</td>
<td>科技股和创新型企业的代表</td>
</tr>
<tr>
<td>重要公司</td>
<td>苹果、微软、可口可乐、迪士尼、摩根大通等</td>
<td>苹果、微软、谷歌母公司、亚马逊、特斯拉等</td>
<td>苹果、微软、谷歌、Facebook、亚马逊等</td>
</tr>
</tbody>
</table>
</div>
<p>总结来说，道琼斯是一个代表美国传统大型企业的指数，适合反映美国经济的稳定性；标准普尔500是一个全面反映美国股市表现的指数，适合广泛的投资者；而纳斯达克综合指数则更加关注科技股和创新企业的表现，适合关注科技和成长型企业的投资者。</p>
<h3 id="K线图"><a href="#K线图" class="headerlink" title="K线图"></a>K线图</h3><p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.969t9azi3v.webp"  alt="K"></p>
]]></content>
      <categories>
        <category>Finance</category>
      </categories>
      <tags>
        <tag>Fund</tag>
      </tags>
  </entry>
  <entry>
    <title>用Sql Server编写一个存储过程</title>
    <url>/2017/06/06/%E7%94%A8Sql%20Server%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>今天数据库上机要求编写一个存储过程来体会sql server的可编程性。</p>
<span id="more"></span>
<h2 id="题目如下："><a href="#题目如下：" class="headerlink" title="题目如下："></a>题目如下：</h2><blockquote>
<p>数据库中有一张表 student, 有两列分别是xh varchar(10), xm  varchar(50)，xh是主码。 现在要求编写一个存储过程，传入两个用分号分隔的字符串（如xhStr=’01;02;03;04’, xmStr=’张三;李斯;王五;赵六’, 其中字符串的长度不限，里面的分号数目也不限，由用户传入）, 存储过程完成如下功能：<br>把对应的两个字符串中的分号前面的字符提取，插入到student表对应的xh和xm列中。<br>注意：需要判断传入的字符串中分号数目是否一致，否则不让插入需要判断学号是否存在，如果存在，就不插入，而是更新姓名。</p>
</blockquote>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--下面是定义函数（计算某字符在字符串中出现的次数）</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">function</span> CalcCounts</span><br><span class="line">(	</span><br><span class="line">	<span class="variable">@searchstr</span> <span class="type">varchar</span>(max),</span><br><span class="line">	<span class="variable">@valuestr</span> <span class="type">varchar</span>(max)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">returns</span> <span class="type">int</span></span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@index</span> <span class="type">int</span></span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@count</span> <span class="type">int</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">set</span> <span class="variable">@index</span> <span class="operator">=</span> charindex(<span class="variable">@valuestr</span>,<span class="variable">@searchstr</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">set</span> <span class="variable">@count</span> <span class="operator">=</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">	while <span class="variable">@index</span> <span class="operator">&gt;</span> <span class="number">0</span></span><br><span class="line">	<span class="keyword">begin</span></span><br><span class="line">		<span class="keyword">set</span> <span class="variable">@count</span> <span class="operator">=</span> <span class="variable">@count</span><span class="operator">+</span><span class="number">1</span></span><br><span class="line">		<span class="keyword">set</span> <span class="variable">@searchstr</span> <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@searchstr</span>,<span class="variable">@index</span><span class="operator">+</span>len(<span class="variable">@valuestr</span>),len(<span class="variable">@searchstr</span>))</span><br><span class="line">		<span class="keyword">set</span> <span class="variable">@index</span> <span class="operator">=</span> charindex(<span class="variable">@valuestr</span>,<span class="variable">@searchstr</span>,<span class="number">0</span>)</span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> <span class="variable">@count</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--编写存储过程</span></span><br><span class="line"><span class="keyword">create</span> proc say_hello</span><br><span class="line">	<span class="variable">@xhstr</span> <span class="type">varchar</span>(max),</span><br><span class="line">	<span class="variable">@valuestr</span> <span class="type">varchar</span>(max),</span><br><span class="line">	<span class="variable">@xmstr</span> <span class="type">varchar</span>(max)</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@xhindex</span> <span class="type">int</span></span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@xmindex</span> <span class="type">int</span></span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@indexcount</span> <span class="type">int</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@xm</span>_toinsert <span class="type">varchar</span>(<span class="number">50</span>)</span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@subxh</span>_front <span class="type">varchar</span>(<span class="number">10</span>)</span><br><span class="line">	<span class="keyword">declare</span> <span class="variable">@subxm</span>_front <span class="type">varchar</span>(<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">	if(dbo.CalcCounts(<span class="variable">@xhstr</span>,<span class="variable">@valuestr</span>)<span class="operator">=</span> dbo.CalcCounts(<span class="variable">@xmstr</span>,<span class="variable">@valuestr</span>))</span><br><span class="line">	<span class="keyword">begin</span></span><br><span class="line">		print(<span class="string">&#x27;分号一致，可以插入&#x27;</span>)</span><br><span class="line">		<span class="keyword">set</span> <span class="variable">@indexcount</span> <span class="operator">=</span> dbo.CalcCounts(<span class="variable">@xhstr</span>,<span class="variable">@valuestr</span>)</span><br><span class="line"></span><br><span class="line">		while <span class="variable">@indexcount</span> <span class="operator">&gt;=</span> <span class="number">0</span></span><br><span class="line">		<span class="keyword">begin</span></span><br><span class="line"></span><br><span class="line">			if <span class="variable">@indexcount</span> <span class="operator">=</span> <span class="number">0</span></span><br><span class="line">			<span class="keyword">begin</span></span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@subxh</span>_front <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xhstr</span>,<span class="number">1</span>,len(<span class="variable">@xhstr</span>))</span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@subxm</span>_front <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xmstr</span>,<span class="number">1</span>,len(<span class="variable">@xmstr</span>))</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">			<span class="keyword">else</span></span><br><span class="line">			<span class="keyword">begin</span></span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@xhindex</span> <span class="operator">=</span> charindex(<span class="variable">@valuestr</span>,<span class="variable">@xhstr</span>,<span class="number">1</span>)</span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@xmindex</span> <span class="operator">=</span> charindex(<span class="variable">@valuestr</span>,<span class="variable">@xmstr</span>,<span class="number">1</span>)</span><br><span class="line">		</span><br><span class="line">				<span class="comment">--截取xh待插入部分</span></span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@subxh</span>_front <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xhstr</span>,<span class="number">1</span>,<span class="variable">@xhindex</span><span class="number">-1</span>)</span><br><span class="line">			</span><br><span class="line">				<span class="comment">--截取xm待插入部分</span></span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@subxm</span>_front <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xmstr</span>,<span class="number">1</span>,<span class="variable">@xmindex</span><span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">				<span class="comment">--截取字符串后面部分</span></span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@xhstr</span> <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xhstr</span>,<span class="variable">@xhindex</span><span class="operator">+</span><span class="number">1</span>,len(<span class="variable">@xhstr</span>))</span><br><span class="line">				<span class="keyword">set</span> <span class="variable">@xmstr</span> <span class="operator">=</span> <span class="built_in">substring</span>(<span class="variable">@xmstr</span>,<span class="variable">@xmindex</span><span class="operator">+</span><span class="number">1</span>,len(<span class="variable">@xmstr</span>))</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line">			</span><br><span class="line">			<span class="comment">--执行插入过程</span></span><br><span class="line">			<span class="keyword">select</span> <span class="variable">@xm</span>_toinsert <span class="operator">=</span> xm <span class="keyword">from</span> student <span class="keyword">where</span> xh <span class="operator">=</span> <span class="variable">@subxh</span>_front</span><br><span class="line">			if <span class="variable">@xm</span>_toinsert <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span></span><br><span class="line">			<span class="keyword">begin</span></span><br><span class="line">				<span class="keyword">update</span> student <span class="keyword">set</span> xm <span class="operator">=</span> <span class="variable">@subxm</span>_front <span class="keyword">where</span> xh <span class="operator">=</span> <span class="variable">@subxh</span>_front</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">			<span class="keyword">else</span></span><br><span class="line">			<span class="keyword">begin</span></span><br><span class="line">				<span class="keyword">insert</span> <span class="keyword">into</span> student <span class="keyword">values</span>(<span class="variable">@subxh</span>_front,<span class="variable">@subxm</span>_front)</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">			<span class="keyword">set</span> <span class="variable">@indexcount</span> <span class="operator">=</span> <span class="variable">@indexcount</span><span class="number">-1</span></span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	<span class="keyword">begin</span></span><br><span class="line">		print(<span class="string">&#x27;分号不一致，无法插入&#x27;</span>)</span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>本道编程题较为基础，算是练一下手了！</p>
]]></content>
      <categories>
        <category>DataBase</category>
      </categories>
      <tags>
        <tag>Sql Server</tag>
        <tag>存储过程</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵的秩</title>
    <url>/2025/04/21/%E7%9F%A9%E9%98%B5%E7%9A%84%E7%A7%A9/</url>
    <content><![CDATA[<p>矩阵的秩（Rank）是线性代数中的一个重要概念，表示矩阵中线性无关的行（或列）的最大数量。它反映了矩阵所包含的“有效信息”的维度，矩阵秩越大，代表其有效信息越多，自由度越高。</p>
<span id="more"></span>
<hr>
<h2 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h2><ol>
<li>行秩与列秩：<ul>
<li>行秩：矩阵中线性无关的行向量的最大个数。</li>
<li>列秩：矩阵中线性无关的列向量的最大个数。</li>
<li>关键性质：对任意矩阵，行秩 = 列秩，因此统称为“秩”。</li>
</ul>
</li>
<li>几何意义：<ul>
<li>秩描述了矩阵对应的线性变换后空间的维度。例如：一个3×3矩阵的秩为2，表示它将三维空间压缩到一个二维平面。</li>
</ul>
</li>
</ol>
<h2 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h2><ol>
<li>初等变换法：<ul>
<li>通过初等行变换将矩阵化为行阶梯形（REF），非零行的数量即为秩。</li>
<li>示例：非零行有2行，故秩为2。<a class="link"   href="https://yuanbao.tencent.com/bot/app/share/chat/aWX82V8QoKZB" >求解矩阵秩demo<i class="fas fa-external-link-alt"></i></a><script type="math/tex; mode=display">
\left[\begin{array}{lll}
   1 & 2 & 3 \\
   0 & 1 & 4 \\
   0 & 0 & 0
\end{array}\right]</script></li>
</ul>
</li>
<li>行列式法（仅适用于方阵）：<ul>
<li>矩阵的秩是其最高阶非零子式的阶数。例如，若存在一个2阶子式不为零，但所有3阶子式为零，则秩为2。</li>
</ul>
</li>
</ol>
<h2 id="重要性质"><a href="#重要性质" class="headerlink" title="重要性质"></a>重要性质</h2><ol>
<li><p>秩的范围：</p>
<ul>
<li>对于 $m \times n$ 矩阵，$0 \leq rank(A) \leq min(m, n)$</li>
<li>若秩达到最大值$min(m, n)$，称矩阵为满秩矩阵。</li>
</ul>
</li>
<li><p>与线性方程组的关系：</p>
<ul>
<li>有解条件：方程组 $Ax = b$ 有解当且仅当 $\text{rank}(A) = \text{rank}([A|b])$。</li>
<li>解的个数：<ul>
<li>若 $\text{rank}(A) = n$（未知数个数），则唯一解。</li>
<li>若 $\text{rank}(A) &lt; n$，则有无穷多解（自由变量存在）。</li>
</ul>
</li>
</ul>
</li>
<li><p>矩阵运算的影响：</p>
<ul>
<li>$\text{rank}(A+B) \leq \text{rank}(A) + \text{rank}(B)$</li>
<li>$\text{rank}(AB) \leq \min(\text{rank}(A), \text{rank}(B))$</li>
</ul>
</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>矩阵的秩本质上是其行或列向量的独立信息量的度量，决定了矩阵在变换中的“自由度”。理解秩有助于分析方程组、空间变换以及矩阵的稳定性等问题。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Matrix</tag>
        <tag>Rank</tag>
      </tags>
  </entry>
  <entry>
    <title>用tensorboard支持pytorch训练可视化</title>
    <url>/2025/03/22/%E7%94%A8tensorboard%E6%94%AF%E6%8C%81pytorch%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<p>在工作用了tensorboard来可视化模型训练过程后，发现还挺香的。另外pytorch也正式支持tensorboard了，这里记录一下。</p>
<span id="more"></span>
<h2 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h2><p>安装tensorboard：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install tensorboard</span><br></pre></td></tr></table></figure>
<h2 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h2><ol>
<li>指定tensorboard输出日志：<code>writer = SummaryWriter(log_dir=LOG_DIR)</code></li>
<li>将模型和数据集添加到writer中：<code>writer.add_graph(model, images.to(device))</code></li>
<li>记录过程数据指标：<code>writer.add_scalar(&#39;Test Loss&#39;, avg_loss, epoch)</code></li>
<li>当模型开始训练后，启动tensorboard：<code>tensorboard --logdir=runs</code>。打开链接就能看到模型过程指标了：<a class="link"   href="http://localhost:6006/" >http://localhost:6006/<i class="fas fa-external-link-alt"></i></a></li>
</ol>
<h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 设置参数</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">EPOCHS = <span class="number">100</span></span><br><span class="line">LEARNING_RATE = <span class="number">0.001</span></span><br><span class="line">NUM_CLASSES = <span class="number">10</span></span><br><span class="line">LOG_DIR = <span class="string">&quot;runs/fashion_mnist_experiment_&quot;</span> + datetime.now().strftime(<span class="string">&quot;%Y%m%d_%H%M%S&quot;</span>)</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 准备数据集</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=transform</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=transform</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    train_set,</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    shuffle=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    test_set,</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    shuffle=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FashionMNISTModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, num_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = FashionMNISTModel(NUM_CLASSES).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 初始化TensorBoard Writer</span></span><br><span class="line">writer = SummaryWriter(log_dir=LOG_DIR)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 添加模型结构和数据集到TensorBoard</span></span><br><span class="line">images, _ = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"><span class="comment"># note: 模型和数据集要么都在cpu，要么都在gpu；不然报错</span></span><br><span class="line">writer.add_graph(model, images.to(device))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 训练循环</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment"># 用累加loss，不然单个batch loss下降不明显</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="comment"># 每100个batch记录一次</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;Training Loss&#x27;</span>,</span><br><span class="line">                              loss.item(),</span><br><span class="line">                              epoch * <span class="built_in">len</span>(train_loader) + batch_idx)</span><br><span class="line">            running_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 测试函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    test_loss = <span class="number">0.0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">            images = images.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(images)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            test_loss += loss.item()</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    accuracy = <span class="number">100</span> * correct / total</span><br><span class="line">    avg_loss = test_loss / <span class="built_in">len</span>(test_loader)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 记录测试结果</span></span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Test Loss&#x27;</span>, avg_loss, epoch)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Test Accuracy&#x27;</span>, accuracy, epoch)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;EPOCHS&#125;</span>], &quot;</span></span><br><span class="line">          <span class="string">f&quot;Test Loss: <span class="subst">&#123;avg_loss:<span class="number">.4</span>f&#125;</span>, &quot;</span></span><br><span class="line">          <span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9. 主训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    train()</span><br><span class="line">    test()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 10. 关闭Writer</span></span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练完成！&quot;</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>TensorBoard</tag>
        <tag>Visualization</tag>
      </tags>
  </entry>
  <entry>
    <title>离线运筹求解流程</title>
    <url>/2025/07/09/%E7%A6%BB%E7%BA%BF%E8%BF%90%E7%AD%B9%E6%B1%82%E8%A7%A3%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p>基于<a class="link"   href="https://transformerswsz.github.io/2025/01/15/%E7%BA%BF%E4%B8%8A%E8%BF%90%E7%AD%B9%E4%BC%98%E5%8C%96%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" >线上运筹优化公式推导<i class="fas fa-external-link-alt"></i></a>，概述一下如何用二分搜索来运筹求解 $\lambda$ 。</p>
<span id="more"></span>
<p>原问题 $\sum_{i} max_{j} (p_{ij} - \lambda c_{ij})$ 是一个求解 $\lambda$ 最优值的线性规划问题，其目标是找到使得目标函数最大化的 $\lambda$。我们可以使用二分搜索来求解。</p>
<h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><h3 id="输入参数"><a href="#输入参数" class="headerlink" title="输入参数"></a>输入参数</h3><ul>
<li>$p_{ij}$：核销率</li>
<li>$c_{ij}$：发券成本</li>
</ul>
<h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><ul>
<li>求解值 $\lambda$</li>
</ul>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li><strong>初始化搜索区间</strong>：由于 $\lambda \geq 0$，设置左右初始区间 $[L, R]$ ，其中 $L=0$，$R$ 可以是任意足够大的数</li>
<li><strong>二分循环</strong>（直到  $|R-L| \leq \epsilon$：<ol>
<li>$m = \frac{L+R}{2}$</li>
<li>遍历每个用户 $i$，找出券 $j$，使得 $max_{j} (p_{ij} - m c_{ij})$ 最大化</li>
<li>如果$\sum_{i} max_{j} (p_{ij} - m c_{ij}) \gt C$，表明 $m$ 过小，则更新搜索区间$L = m$，否则 $R = m$</li>
</ol>
</li>
<li><strong>返回结果</strong>：$\lambda = m$</li>
</ol>
]]></content>
      <categories>
        <category>Marketing</category>
      </categories>
      <tags>
        <tag>Lagrangian Dual</tag>
      </tags>
  </entry>
  <entry>
    <title>程序后台运行并实时输出日志</title>
    <url>/2022/08/06/%E7%A8%8B%E5%BA%8F%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E5%B9%B6%E5%AE%9E%E6%97%B6%E8%BE%93%E5%87%BA%E6%97%A5%E5%BF%97/</url>
    <content><![CDATA[<p>将程序非挂断放在后台执行，命令如下：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">nohup</span> python -u main.py &gt; run.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<p>这里是将程序的标准输出和标准错误都重定向到了 <code>run.log</code> 文件中。</p>
<p>需要注意的是，python程序的输出有缓冲，不会立刻写入到日志文件中，使用 <code>-u</code> 参数来解决此问题：</p>
<blockquote>
<p>Force the binary I/O layers of stdout and stderr to be unbuffered.  stdin is always buffered.  The text I/O layer will still be line-buffered.</p>
</blockquote>
<p>程序放到后台执行了，每次手动查看日志很麻烦，使用 <code>tail</code> 命令来实时查看日志：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">tail</span> -f run.log</span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/qq_43159578/article/details/123249606?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-123249606-blog-79607961.pc_relevant_multi_platform_featuressortv2dupreplace&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-123249606-blog-79607961.pc_relevant_multi_platform_featuressortv2dupreplace&amp;utm_relevant_index=2" >Linux nohup 实现命令后台运行并输出或记录到指定日志文件<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/Lison_Zhu/article/details/111501410" >nohup后台运行不能及时打印print<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Python</tag>
        <tag>nohup</tag>
      </tags>
  </entry>
  <entry>
    <title>策略梯度</title>
    <url>/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/</url>
    <content><![CDATA[<p>在看师兄的论文时，里面涉及到强化学习的 <strong>Policy Gradient</strong> 。看了网上好多博客，觉得公式推导太复杂了，断断续续地持续了三周。今天静下心来看了一遍，发现没有那么难，果然做学术还是不能浮躁啊！</p>
<span id="more"></span>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p> 强化学习是机器学习的一个分支，但是它与我们常见监督式学习不太一样。从学习方式上讲强化学习更加接近人类的学习，例如当你接触一款新的电子游戏的时候，虽然看不懂屏幕的提示，但是经过自己的摸索也能掌握游戏方法，这个摸索的过程其实就是通过试错逐渐了解游戏规则的学习过程。同样，强化学习也是通过一系列的尝试并根据得到的反馈不断调整自己的行为来学习陌生的对象。 </p>
<p>强化学习主要包括如下几个部分：</p>
<img   src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/composition.png"  class="">
<ul>
<li><strong>主体（Agent）</strong>： 指能够通过动作与环境交互的对象，强化学习中主体通常是运行中的算法，比如在游戏中的主体是用于控制本方球拍的算法。</li>
<li><strong>环境（Environment）</strong>： 指主体动作作用的对象， 比如游戏本身。 </li>
<li><strong>动作（Action）</strong>: 指所有可能作用于环境上的操作，比如游戏中算法控制球拍上下移动。 </li>
<li><strong>状态（State）</strong>: 指可被主体感知的关于环境的信息，比如游戏中屏幕显示的球和球拍的位置以及移动方向和速度信息。 </li>
<li><strong>奖励（Reward）</strong>: 指由环境回馈给主体的描述上一个动作效果的信息，比如游戏中球拍动作导致双方的得分变化。 </li>
</ul>
<p>强化学习的过程是一个通过和环境交互获得反馈，再根据反馈调整动作以期使总奖励最大化的过程，这个是一个多步 (multi timestep) 的交互的过程，每一步交互都会影响其后的所有步骤。强化学习中的一次交互是指主体对环境施加一个动作，环境的状态发生改变并且回馈给主体一个奖励（奖励既可以是正向的，如本方得分增加；也可以是负向的，如对方得分增加）。强化学习的目标就是寻找一个最优的策略使得整个学习过程（从开始状态到终结状态）获得的奖励最大化。</p>
<p>在实现上，强化学习是一个通过多个轮次逐渐优化算法的参数从而增强学习效果的过程，每个轮次包含两部分：前向反馈（feed forward）和反向传播（back propagation）。处于初始状态的主体根据算法的当前参数生成动作作用于环境，环境返回给主体新的状态和对动作的奖励，在轮次结束后算法通过汇总所有在本轮收集到的反馈调整算法的参数开始下一轮的学习，直到学习的效果不再增强。</p>
<p>强化学习包括了一系列不同的算法（如下图），其中比较常见的是基于值（Value-based）的方法和基于策略（Policy-based）的方法。这两类方法各有特点，适用于解决不同的问题。一般来说，基于值的方法适用于比较简单（状态空间比较小）的问题，它有较高的数据利用率并且能稳定收敛；而基于策略的方法适用于复杂问题，但是高方差是这类方法会存在的一个比较明显的问题。</p>
<img   src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/category.png"  class="">
<h2 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p> 基于值的方法一般是确定性的，给定一个状态就能计算出每种可能动作的奖励（确定值），但这种确定性的方法恰恰无法处理一些现实的问题，比如玩 100 把石头剪刀布的游戏，最好的解法是随机的使用石头、剪刀和布并尽量保证这三种手势出现的概率一样，因为任何一种手势的概率高于其他手势都会被对手注意到并使用相应的手势赢得游戏。 </p>
<p>策略梯度正是为了解决上面的问题产生的，而它的秘密武器就是随机（Stochastic）。首先随机能提供非确定的结果，但这种非确定的结果并不是完全的随意而是服从某种概率分布的随机，策略梯度不计算奖励（reward）而是使用概率选择动作，这样就避免了因为计算奖励而维护状态表。策略梯度的基本原理是通过反馈调整策略，具体来说就是在得到正向奖励时，增加相应的动作的概率；得到负向的奖励时，降低相应动作的概率。下面左图中的绿点表示获得正向奖励的动作，右图表示更新后的策略，可以发现产生正向奖励的区域的概率都增加了（离圆心的距离更近）。</p>
<img   src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/justify.png"  class="">
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li><p><strong>对象系统</strong>：策略梯度的学习对象，这个对象即可以是一个系统，比如汽车或一个游戏，也可以是一个对手，比如势头剪刀布的游戏对手或者一个职业的围棋手。</p>
</li>
<li><p><strong>Policy（策略）</strong>：$\pi_\theta(a|s)$ 表示在状态 $s$ 和参数 $\theta$ 条件下发生 $a$ 动作的概率。</p>
</li>
<li><p><strong>Episode（轮次）</strong>：表示从起始状态开始使用某种策略产生动作与对象系统交互，直到某个终结状态结束。比如在围棋游戏中的一个轮次就是从棋盘中的第一个落子开始直到对弈分出胜负，或者自动驾驶的轮次指从汽车启动一直到顺利抵达指定的目的地，当然撞车或者开进水塘也是种不理想的终结状态。</p>
</li>
<li><p><strong>Trajectory（轨迹 $\tau$ ）</strong>：表示在 PG 一个轮次的学习中状态 $s$ ，动作 $a$ 和奖励 $r$ 的顺序排列。由于策略产生的是非确定的动作，同一个策略在多个轮次可以产生多个不同的轨迹。$\tau=(s_1, a_1, \dots, s_t, a_t)$</p>
</li>
<li><p><strong>轮次奖励 $\sum r(\tau)$ </strong>：表示在一个轮次中依次动作产生的奖励的总和。 因此在实现中对每个策略会求多个轮次的平均值。</p>
</li>
</ul>
<p>策略梯度的学习是一个策略的优化过程，最开始随机的生成一个策略，当然这个策略对对象系统一无所知，所以用这个策略产生的动作会从对象系统那里很可能会得到一个负面奖励，这个过程就好像在PONG游戏中我们对飞来的乒乒球无动于衷而导致对方的得分增加。为了击败对手我们需要逐渐的改变策略，使得本方的比分增加。策略梯度在一轮的学习中使用同一个策略直到该轮结束，通过梯度上升改变策略并开始下一轮学习，如此往复直到轮次累计奖励不再增长停止。 </p>
<h3 id="理论推导"><a href="#理论推导" class="headerlink" title="理论推导"></a>理论推导</h3><p>首先需要将策略参数话 $\pi(a|s,\theta)=\pi_\theta(a|s)$ ，从轨迹 $\tau$ 中直接找到策略上升的方向，定义这条轨迹在策略 $\pi_\theta$ 下出现的概率为： </p>
<script type="math/tex; mode=display">
p_\theta(\tau) = p_\theta(s_1, a_1, \dots, s_T, a_T) = p(s_1) \prod_{t=1}^{T} \pi_\theta(a_t|s_t) p(s_{t+1} | s_t, a_t)</script><img   src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/trajectory.jpg"  class="">
<p>我们需要定义长期汇报 $J(\theta)$ ，目标最大化它。过程如下：</p>
<img   src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/derive.png"  class="">
<p>由于 $\nabla_{\theta} J(\theta)$ 无法直接求出，因此采用蒙特卡洛采样法来近似求解。然后根据梯度上升公式更新参数 $\theta$ 直至收敛，流程如下：</p>
<img   src="/2019/11/06/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/procedure.png"  class="">
<p>从机器学习的原理的角度来看，策略梯度和传统的监督式学习的学习过程还是比较相似的，每轮次都由前向反馈和反向传播构成，前向反馈负责计算目标函数，反向传播负责更新算法的参数，依此进行多轮次的学习指导学习效果稳定收敛。唯一不同的是，监督式学习的目标函数相对直接，即目标值和真实值的差，这个差值通过一次前向反馈就能得到；而策略梯度的目标函数源自轮次内所有得到的奖励，并且需要进行一定的数学转换才能计算，另外由于用抽样模拟期望，也需要对同一套参数进行多次抽样来增加模拟的准确性。 </p>
<h3 id="缺陷及改进"><a href="#缺陷及改进" class="headerlink" title="缺陷及改进"></a>缺陷及改进</h3><p>我们把 $R(\tau^n)$ 看作是 $\sum_{t=1}^T \nabla_\theta log \pi_\theta (a_t|s_t)$ 的权重，这样会存在两个问题：</p>
<ul>
<li>$R(\tau^n)$  能始终为正，也就是会导致所有策略都会增强，而我们的初衷是降低表现差的行动的概率，提升表现好的行动的概率。</li>
<li>$R(\tau^n) = \sum_{t=1}^T r_t$ ，对于序列中的每一时间段的元组 $(s_t, a_t)$ 只能影响 $t$ 时刻之后的回报，不能影响之前的回报。</li>
</ul>
<p>针对上述两个问题，解决方案如下：</p>
<ul>
<li>引入基线：权重项变为 $R(\tau^n)-b$ ，通常 $b=E[R(\tau^n)]$ ，表示对所有轨迹的累计回报求平均。引入 $b$ 不会对 $\nabla_\theta J(\theta)$ 产生影响，证明如下：</li>
</ul>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\nabla_\theta J(\theta) &= \sum_\tau [R(\tau)-b] \nabla_\theta p_\theta(\tau) \\
                        &= \sum_\tau R(\tau) \nabla_\theta p_\theta(\tau) - b\sum_\tau \nabla_\theta p_\theta(\tau) \\
                        &= \sum_\tau R(\tau) \nabla_\theta p_\theta(\tau) - b\sum_\tau \nabla_\theta 1 \\
                        &= \sum_\tau R(\tau) \nabla_\theta p_\theta(\tau) \\
\end{aligned}
\end{equation}</script><ul>
<li>减少无效元素：权重项变为 $\sum_{t’=t}^T \gamma^{t’-t}r_t$ ，$\gamma$ 表示衰减系数，该式表示只计算t时刻之后的回报，即未来不影响过去。 </li>
</ul>
<p>综上改进后的式子为 $\nabla_\theta J(\theta) \approx \frac {1} {N} \sum_{n=1}^N [(\sum_{t=1}^T \nabla_\theta log \pi_\theta (a_t^n|s_t^n))(\sum_{t’=t}^T \gamma^{t’-t}r_t^n - b)] $</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p> 策略梯度基本靠“猜”。这里的猜不是瞎猜，而是用随机（Stochastic）的方式控制动作的产生进而影响策略的变化，随机既保证了非确定性又能通过控制概率避免完全盲目，是策略梯度解决复杂问题的核心和基础。然而双刃剑的另一面是，”猜“造成了策略梯度方差大、收敛慢的缺点，这是源于策略梯度为了避免遍历所有状态而不得不付出的代价，无法完全避免。 但是瑕不掩瑜，策略梯度除了理论上的处理复杂问题的优势，在实践应用中也有明显的优势，那就是它可以仅靠与目标系统交互进行学习，而不需要标签数据，可以节省了大量的人力。 目前层出不穷的 variance reduction 的方法也证明了人们不仅没有因为策略梯度的缺点放弃它，反而正在通过不断的改进使其扬长避短，发扬光大。 </p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://www.ibm.com/developerworks/cn/analytics/library/ba-lo-deep-introduce-policy-gradient/index.html" >Machine learning and gaming<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/55298602" >策略梯度理解及代码实现<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://karpathy.github.io/2016/05/31/rl/" >Deep Reinforcement Learning: Pong from Pixels<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Policy Gradient</tag>
      </tags>
  </entry>
  <entry>
    <title>策略梯度与Q-Learning的区别</title>
    <url>/2025/04/27/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E4%B8%8EQ-Learning%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>PG和Q-Learning都是RL的两大主流算法，记录下两者差异。</p>
<span id="more"></span>
<h2 id="策略梯度（Policy-Gradient）简介"><a href="#策略梯度（Policy-Gradient）简介" class="headerlink" title="策略梯度（Policy Gradient）简介"></a>策略梯度（Policy Gradient）简介</h2><p>策略梯度（Policy Gradient, PG）是强化学习中的一类直接优化策略的方法，通过梯度上升（Gradient Ascent）更新策略参数，以最大化期望回报。与Q-Learning等基于值函数的方法不同，PG直接对策略 $\pi_\theta(a|s)$（参数为$\theta$）进行优化，适用于连续动作空间或随机策略的场景。</p>
<hr>
<h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><ol>
<li>策略参数化：用神经网络或其他函数近似策略 $\pi_\theta(a|s)$，输入状态s，输出动作a的概率分布（或连续动作的均值/方差）</li>
<li>目标函数：最大化期望回报 $J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]$，其中$\tau$是轨迹（状态-动作序列），$R(\tau)$是轨迹的总奖励</li>
<li>梯度上升：计算目标函数对策略参数$\theta$的梯度 $\nabla_\theta J(\theta)$，并沿梯度方向更新参数：<script type="math/tex; mode=display">
\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)</script></li>
</ol>
<h3 id="策略梯度定理"><a href="#策略梯度定理" class="headerlink" title="策略梯度定理"></a>策略梯度定理</h3><p>梯度 $\nabla_\theta J(\theta)$ 的数学形式为：</p>
<script type="math/tex; mode=display">
\nabla_\theta J(\theta)=\mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) \cdot Q^{\pi_\theta}\left(s_t, a_t\right)\right]</script><ul>
<li>$\log \pi_\theta(a_t|s_t)$：策略的对数概率  </li>
<li>$Q^{\pi_\theta}(s_t, a_t)$：状态-动作值函数（即从$s_t$执行$a_t$后的累计奖励期望）</li>
</ul>
<h3 id="经典算法：REINFORCE"><a href="#经典算法：REINFORCE" class="headerlink" title="经典算法：REINFORCE"></a>经典算法：REINFORCE</h3><p>REINFORCE 是最简单的策略梯度算法，通过蒙特卡洛采样估计梯度：</p>
<ol>
<li>用当前策略 $\pi_\theta$ 生成一条轨迹 $\tau = (s_0, a_0, r_0, \dots, s_T)$</li>
<li>计算轨迹的累计奖励 $R(\tau) = \sum_{t=0}^T \gamma^t r_t$（$\gamma$为折扣因子）</li>
<li>更新策略参数：<script type="math/tex; mode=display">
\theta \leftarrow \theta+\alpha \gamma^t R(\tau) \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)</script></li>
</ol>
<h3 id="举例1（离散型动作）"><a href="#举例1（离散型动作）" class="headerlink" title="举例1（离散型动作）"></a>举例1（离散型动作）</h3><p><strong>CartPole（平衡杆问题）</strong></p>
<ul>
<li>目标：控制小车左右移动，使杆子保持直立不倒</li>
<li>状态$s$：小车位置、速度、杆子角度、角速度</li>
<li>动作$a$：离散（左移/右移）或连续（施加的力）</li>
<li>奖励$r$：每步杆子未倒下时+1，倒下后终止</li>
</ul>
<p><strong>PG实现步骤（以REINFORCE为例）</strong></p>
<ol>
<li>初始化策略网络：输入状态s，输出动作概率（如左移概率0.7，右移0.3）</li>
<li>采样轨迹：根据当前策略运行游戏，得到轨迹 $\tau = (s_0, a_0, r_0, \dots, s_T)$</li>
<li>计算梯度：对每一步t，计算 $\nabla_\theta \log \pi_\theta(a_t|s_t)$，乘以累计奖励 $R(\tau)$</li>
<li>更新策略：沿梯度方向调整 $\theta$，使高奖励动作的概率增加</li>
</ol>
<p>经过多次迭代，策略会学会在杆子右倾时选择左移动作（反之亦然），最终保持平衡</p>
<h3 id="举例2（连续型动作）"><a href="#举例2（连续型动作）" class="headerlink" title="举例2（连续型动作）"></a>举例2（连续型动作）</h3><p>当动作空间是连续的（例如机器人控制、自动驾驶中的转向角度等），策略梯度方法可以通过以下方式输出连续动作：</p>
<ol>
<li>参数学习：策略网络 $\pi_\theta(a|s)$ 不再输出离散动作的概率分布，而是输出连续动作的概率分布参数，通常选择高斯分布（正态分布），其参数为：<ul>
<li>均值 $\mu$：表示动作的中心值（如转向角度为0.5弧度）</li>
<li>标准差 $\sigma$（或对数标准差 $\log \sigma$）：表示动作的探索范围（$\sigma=0.1$表示小幅随机扰动）</li>
</ul>
</li>
</ol>
<p>代码示例：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PolicyNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, state_dim, action_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.fc1 = nn.Linear(state_dim, <span class="number">64</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.mu_head = nn.Linear(<span class="number">64</span>, action_dim)    <span class="comment"># 输出均值</span></span><br><span class="line">        self.log_std_head = nn.Linear(<span class="number">64</span>, action_dim)  <span class="comment"># 输出对数标准差</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, state</span>):</span><br><span class="line">        x = torch.relu(self.fc1(state))</span><br><span class="line">        x = torch.relu(self.fc2(x))</span><br><span class="line">        mu = torch.tanh(self.mu_head(x))  <span class="comment"># 均值限制在[-1,1]（假设动作范围）</span></span><br><span class="line">        log_std = self.log_std_head(x)    <span class="comment"># 对数标准差</span></span><br><span class="line">        <span class="keyword">return</span> mu, log_std</span><br></pre></td></tr></table></figure></p>
<ol>
<li>动作采样：在状态$s$下，策略网络输出均值和标准差后，通过重参数化技巧采样动作：<script type="math/tex; mode=display">
a \sim \mathcal{N}(\mu_\theta(s), \sigma_\theta(s))</script>代码示例：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">select_action</span>(<span class="params">state</span>):</span><br><span class="line">    mu, log_std = policy_network(state)</span><br><span class="line">    std = torch.exp(log_std)  <span class="comment"># 保证标准差为正</span></span><br><span class="line">    normal_dist = torch.distributions.Normal(mu, std)</span><br><span class="line">    action = normal_dist.rsample()  <span class="comment"># 可导的采样</span></span><br><span class="line">    <span class="keyword">return</span> action.clamp(-<span class="number">1.0</span>, <span class="number">1.0</span>)  <span class="comment"># 限制动作范围</span></span><br></pre></td></tr></table></figure></li>
<li>策略梯度的计算：连续动作空间的策略梯度公式与离散情况类似，但需使用连续概率密度函数（PDF）的对数：<script type="math/tex; mode=display">
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot Q^{\pi_\theta}(s_t, a_t) \right]</script>其中 $\log \pi_\theta(a_t|s_t)$ 是高斯分布的对数概率密度：<script type="math/tex; mode=display">
\log \pi_\theta(a_t|s_t) = -\frac{(a_t - \mu_\theta(s_t))^2}{2 \sigma_\theta(s_t)^2} - \log \sigma_\theta(s_t) - \text{常数}</script>代码示例：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">states, actions, rewards</span>):</span><br><span class="line">    mu, log_std = policy_network(states)</span><br><span class="line">    std = torch.exp(log_std)</span><br><span class="line">    normal_dist = torch.distributions.Normal(mu, std)</span><br><span class="line">    log_probs = normal_dist.log_prob(actions)  <span class="comment"># 计算对数概率</span></span><br><span class="line">    loss = -log_probs * rewards  <span class="comment"># 梯度上升转化为梯度下降</span></span><br><span class="line">    <span class="keyword">return</span> loss.mean()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>具体例子：连续动作的平衡杆问题</strong></p>
<p>假设我们需要控制小车的连续推力（范围$[-1, 1]$）：</p>
<ol>
<li>状态$s$：杆子角度、角速度、小车位置、速度</li>
<li>动作$a$：推力值（如0.3表示向右的力，-0.8表示向左的力）</li>
<li>策略网络：输出均值$\mu \in [-1,1]$和标准差$\sigma$（探索噪声）</li>
<li>训练过程：<ul>
<li>采样动作 $a \sim \mathcal{N}(\mu, \sigma)$</li>
<li>执行动作后获得奖励（如杆子保持直立的时间）</li>
<li>用策略梯度更新网络，使高奖励动作的$\mu$向$a$靠近，同时自适应调整$\sigma$</li>
</ul>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>离散动作：策略网络输出离散动作的概率分布（如Softmax）</li>
<li>连续动作：策略网络输出高斯分布的参数（$\mu, \sigma$），通过采样得到连续值  </li>
<li>核心公式：$\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) \cdot Q(s,a)]$，其中$\log \pi_\theta(a|s)$需按连续分布计算</li>
</ul>
<hr>
<h2 id="PG与Q-Learning差异"><a href="#PG与Q-Learning差异" class="headerlink" title="PG与Q-Learning差异"></a>PG与Q-Learning差异</h2><p>以下是两者的核心对比：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.64e2128tb1.webp"  alt="diff"></p>
<h3 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h3><p><strong>场景：CartPole（平衡杆问题）</strong></p>
<ul>
<li>状态$s$：杆子角度、角速度、小车位置、速度</li>
<li>动作$a$：离散（左/右移动）或连续（施加的力）</li>
<li>奖励$r$：每步杆子未倒下时+1，倒下后终止</li>
</ul>
<h4 id="Q-Learning-的实现（离散动作）"><a href="#Q-Learning-的实现（离散动作）" class="headerlink" title="Q-Learning 的实现（离散动作）"></a>Q-Learning 的实现（离散动作）</h4><ol>
<li>Q表或Q网络：维护 $Q(s,a)$，输入状态$s$，输出每个动作的Q值（如左/右）  </li>
<li><p>动作选择：</p>
<ul>
<li>训练时：ε-greedy（以概率ε随机探索，否则选$\arg\max_a Q(s,a)$）  </li>
<li>测试时：纯贪婪策略 $\arg\max_a Q(s,a)$  </li>
</ul>
</li>
<li><p>更新规则（TD学习）：  </p>
<script type="math/tex; mode=display">
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]</script><p>其中 $s’$ 是下一状态，$\gamma$ 是折扣因子</p>
</li>
</ol>
<p>代码示例（DQN）：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Q网络预测Q值</span></span><br><span class="line">q_values = q_network(state)</span><br><span class="line"><span class="comment"># ε-greedy选择动作</span></span><br><span class="line"><span class="keyword">if</span> np.random.rand() &lt; epsilon:</span><br><span class="line">    action = env.action_space.sample()  <span class="comment"># 随机探索</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    action = np.argmax(q_values)  <span class="comment"># 贪婪动作</span></span><br><span class="line"><span class="comment"># 更新Q网络（最小化TD误差）</span></span><br><span class="line">loss = (target_q - q_values[action]) ** <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>特点：  </p>
<ul>
<li>只能处理离散动作（如左/右）  </li>
<li>策略隐含在Q值中（$\arg\max Q$ 是确定性策略）  </li>
</ul>
<h4 id="策略梯度的实现（连续动作）"><a href="#策略梯度的实现（连续动作）" class="headerlink" title="策略梯度的实现（连续动作）"></a>策略梯度的实现（连续动作）</h4><ol>
<li>策略网络：输出动作分布参数（如高斯分布的均值$\mu$和标准差$\sigma$）  </li>
<li>动作采样：<ul>
<li>从分布中采样连续动作 $a \sim \mathcal{N}(\mu, \sigma)$  </li>
<li>例如：输出推力值 $a \in [-1,1]$  </li>
</ul>
</li>
<li>更新规则（REINFORCE）：  <script type="math/tex; mode=display">
\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) \cdot R(\tau)</script>其中 $R(\tau)$ 是轨迹的总奖励</li>
</ol>
<p>代码示例（PG）：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 策略网络输出均值和标准差</span></span><br><span class="line">mu, log_std = policy_network(state)</span><br><span class="line">std = torch.exp(log_std)</span><br><span class="line">normal_dist = torch.distributions.Normal(mu, std)</span><br><span class="line">action = normal_dist.rsample()  <span class="comment"># 可导采样</span></span><br><span class="line"><span class="comment"># 计算对数概率</span></span><br><span class="line">log_prob = normal_dist.log_prob(action)</span><br><span class="line"><span class="comment"># 更新策略网络（最大化期望奖励）</span></span><br><span class="line">loss = -log_prob * discounted_reward</span><br></pre></td></tr></table></figure></p>
<p>特点：  </p>
<ul>
<li>直接输出连续动作（如推力值0.73）  </li>
<li>策略本身是随机的（通过$\sigma$控制探索）  </li>
</ul>
<h3 id="关键区别总结"><a href="#关键区别总结" class="headerlink" title="关键区别总结"></a>关键区别总结</h3><ol>
<li>策略 vs Q值：  <ul>
<li>PG显式学习策略 $\pi_\theta(a|s)$，适合复杂动作空间（如机器人控制）  </li>
<li>Q-Learning隐式策略依赖Q值最大化，难以处理连续动作（需DDPG等扩展）  </li>
</ul>
</li>
<li>探索方式：  <ul>
<li>PG通过策略的随机性自然探索（如高斯噪声）  </li>
<li>Q-Learning需人工设计探索（如ε-greedy）  </li>
</ul>
</li>
<li>适用场景：  <ul>
<li>PG：连续控制（如机械臂抓取）、需要随机策略的任务（如博弈）  </li>
<li>Q-Learning：离散动作空间（如游戏AI、广告推荐）  </li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Q-Learning</tag>
        <tag>Policy Gradient</tag>
      </tags>
  </entry>
  <entry>
    <title>线上运筹优化公式推导</title>
    <url>/2025/01/15/%E7%BA%BF%E4%B8%8A%E8%BF%90%E7%AD%B9%E4%BC%98%E5%8C%96%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/</url>
    <content><![CDATA[<p>营销本质是个预算分配问题，即如何在有限资源约束下实现收益最大化。当用户进入营销场景时，我们需要确定<strong>是否给该用户发放红包</strong>以及<strong>发放红包的面额</strong>。</p>
<span id="more"></span>
<p>选择发放用户和发放面额是一个典型的分组背包问题(MCKP, Multiple Choice Knapsack Problem)，在成本约束下的的券核销率最大化。</p>
<h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><script type="math/tex; mode=display">
min -\sum_{i,j} x_{ij}p_{ij} \\

\begin{aligned}
\text { s.t. } x_{ij} & \in \{0, 1\} \\
\sum_{j} x_{ij} &= 1 \\
\sum_{i,j} x_{ij} c_{ij} &\leq C \\
\end{aligned}</script><ul>
<li>$x_{ij}$ 表示是否给用户$i$发放红包$j$</li>
<li>$p_{ij}$ 表示用户$i$在红包$j$下的核销率，由量价模型预估产生</li>
<li>$c_{ij}$ 表示给用户$i$发放红包$j$的成本，即红包$j$的面额</li>
<li>$C$ 表示总成本</li>
</ul>
<h2 id="构造拉格朗日对偶函数"><a href="#构造拉格朗日对偶函数" class="headerlink" title="构造拉格朗日对偶函数"></a>构造拉格朗日对偶函数</h2><script type="math/tex; mode=display">
\begin{aligned}
L(x, \lambda, u) & =-\sum_{i, j} x_{ij}p_{ij}+ \lambda\left(\sum_{i,j}  x_{ij}c_{ij}-C\right) \\
& =\sum_{i,j} x_{ij}\left(-p_{ij}+ \lambda c_{ij}\right) - \lambda C \\
\text { s.t. } &\lambda \geq 0, \ u_i \geq 0, \ \sum_{j} x_{ij} = 1
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
D(\lambda, u) & =\max _{\lambda, u} \min _x L(x, \lambda, u) \\
& =\max _{\lambda, u} \min _x\left(\sum_{i,j} x_{ij}\left(-p_{ij}+ \lambda c_{ij}\right) - \lambda C \right) \\
\text { s.t. } &\lambda \geq 0, \ u_i \geq 0, \ \sum_{j} x_{ij} = 1
\end{aligned}</script><p>如果给每个用户都发券，即$x_{ij}=1$，则$D(\lambda, u) = \max _{\lambda, u} \min _x\left(\sum_{i,j} -p_{ij}+ \lambda (c_{ij}-avg_{ij}) \right)$，$avg_{ij}$表示单均约束，$\sum_{ij} avg_{ij} = C$。</p>
<h2 id="求解最优解"><a href="#求解最优解" class="headerlink" title="求解最优解"></a>求解最优解</h2><p>针对上述经典优化问题，每条样本的最优解满足如下条件：</p>
<script type="math/tex; mode=display">
\forall i,j, \quad x_{ij} = 1 \Longleftrightarrow j = argmin_j -p_{ij^{\prime}}+ \lambda c_{ij^{\prime}}</script><p>假设给定用户$i$，以及确定发放红包$j^{\prime}$，为了使$-p_{ij^{\prime}}+ \lambda c_{ij^{\prime}}$最小，则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
j^{\prime} &= argmin_{j} -p_{ij}+ \lambda c_{ij} \\
&= argmax_{j} p_{ij} - \lambda c_{ij} \\
\end{aligned}</script><p>线上运筹的时候，选择发放$j^{\prime}$红包。</p>
]]></content>
      <categories>
        <category>Marketing</category>
      </categories>
      <tags>
        <tag>Lagrangian Dual</tag>
      </tags>
  </entry>
  <entry>
    <title>股市指数点位计算逻辑</title>
    <url>/2025/08/24/%E8%82%A1%E5%B8%82%E6%8C%87%E6%95%B0%E7%82%B9%E4%BD%8D%E8%AE%A1%E7%AE%97%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>以沪深300指数点位的计算方式为例：</p>
<span id="more"></span>
<h2 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h2><p>沪深300指数采用 <strong>加权平均法</strong>（自由流通市值加权），核心公式是：</p>
<script type="math/tex; mode=display">
指数点位 = \frac{现时样本股自由流通市值}{基期样本股自由流通市值} \times 基点</script><p>其中：</p>
<ul>
<li>自由流通市值 = 股价 × 自由流通股本</li>
<li>基期样本股自由流通市值：2004年12月31日的数据（基点设为1000点）</li>
<li>基点：1000</li>
</ul>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>假设沪深300只有3支股票（真实情况是300，只是为了演示）。</p>
<h3 id="（1）基期（2004年12月31日）"><a href="#（1）基期（2004年12月31日）" class="headerlink" title="（1）基期（2004年12月31日）"></a>（1）基期（2004年12月31日）</h3><div class="table-container">
<table>
<thead>
<tr>
<th>股票</th>
<th>自由流通股本(股)</th>
<th>股价(元)</th>
<th>自由流通市值(元)</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1亿</td>
<td>10</td>
<td>10亿</td>
</tr>
<tr>
<td>B</td>
<td>2亿</td>
<td>5</td>
<td>10亿</td>
</tr>
<tr>
<td>C</td>
<td>5千万</td>
<td>20</td>
<td>10亿</td>
</tr>
<tr>
<td><strong>合计</strong></td>
<td>—</td>
<td>—</td>
<td><strong>30亿</strong></td>
</tr>
</tbody>
</table>
</div>
<p>基期总市值 = 30亿，对应指数点位 = <strong>1000点</strong>。</p>
<h3 id="（2）现在的情况"><a href="#（2）现在的情况" class="headerlink" title="（2）现在的情况"></a>（2）现在的情况</h3><div class="table-container">
<table>
<thead>
<tr>
<th>股票</th>
<th>自由流通股本(股)</th>
<th>股价(元)</th>
<th>自由流通市值(元)</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1亿</td>
<td>15</td>
<td>15亿</td>
</tr>
<tr>
<td>B</td>
<td>2亿</td>
<td>8</td>
<td>16亿</td>
</tr>
<tr>
<td>C</td>
<td>5千万</td>
<td>25</td>
<td>12.5亿</td>
</tr>
<tr>
<td><strong>合计</strong></td>
<td>—</td>
<td>—</td>
<td><strong>43.5亿</strong></td>
</tr>
</tbody>
</table>
</div>
<p>现时总市值 = 43.5亿。</p>
<h3 id="（3）指数点位计算"><a href="#（3）指数点位计算" class="headerlink" title="（3）指数点位计算"></a>（3）指数点位计算</h3><script type="math/tex; mode=display">
指数点位 = \frac{43.5亿}{30亿} \times 1000 = 1450点</script><p>也就是说，这3支股票（代表整个指数）的市值相比基期上涨了 <strong>45%</strong>，所以指数点位从1000点涨到1450点。</p>
]]></content>
      <categories>
        <category>personal</category>
      </categories>
      <tags>
        <tag>Stock Market</tag>
      </tags>
  </entry>
  <entry>
    <title>自定义CUDA算子融合实现模型推理加速</title>
    <url>/2025/03/28/%E8%87%AA%E5%AE%9A%E4%B9%89CUDA%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/</url>
    <content><![CDATA[<p>对模型进行推理加速的最常用方法就是算子融合，这里用个简单demo记录下：</p>
<span id="more"></span>
<p>总共有如下三个步骤：</p>
<h2 id="导出模型权重"><a href="#导出模型权重" class="headerlink" title="导出模型权重"></a>导出模型权重</h2><p>用pytorch定义一个多层DNN模型，然后导出其各层的网络参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># export_model.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义PyTorch模型结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DNNModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer1 = nn.Linear(<span class="number">100</span>, <span class="number">128</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.layer2 = nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line">        self.layer3 = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.relu1(self.layer1(x))</span><br><span class="line">        x = self.relu2(self.layer2(x))</span><br><span class="line">        <span class="keyword">return</span> self.layer3(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型并随机初始化</span></span><br><span class="line">model = DNNModel()</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出权重为二进制文件</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    param.detach().cpu().numpy().tofile(<span class="string">f&quot;<span class="subst">&#123;name&#125;</span>.bin&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;权重导出完成！&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行 <code>python export_model.py</code> 。</p>
<h2 id="编写CUDA融合算子"><a href="#编写CUDA融合算子" class="headerlink" title="编写CUDA融合算子"></a>编写CUDA融合算子</h2><p>神经网络的每一层前向传播，都先从全局内存中读取tensor到寄存器内存，完成计算后再写回到全局内存，IO次数较多。利用算子融合，将多次计算融合成一次计算，减少IO读写，从而实现模型推理加速。</p>
<p>具体的代码包括如下三个步骤：</p>
<ol>
<li>加载pytorch导出的模型参数</li>
<li>将多次前向传播融合到一个函数中</li>
<li>将优化后的函数绑定到python模块中</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// fused_op.cu</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">fused_forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">float</span>* input,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span>* output,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">float</span>* W1, <span class="type">const</span> <span class="type">float</span>* b1,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">float</span>* W2, <span class="type">const</span> <span class="type">float</span>* b2,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">float</span>* W3, <span class="type">const</span> <span class="type">float</span>* b3,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">int</span> batch_size, <span class="type">int</span> in_dim, <span class="type">int</span> hid1, <span class="type">int</span> hid2, <span class="type">int</span> out_dim</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 每个线程处理一个样本的完整计算</span></span><br><span class="line">    <span class="type">int</span> sample_idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (sample_idx &gt;= batch_size) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指向当前样本的输入和输出</span></span><br><span class="line">    <span class="type">const</span> <span class="type">float</span>* x = input + sample_idx * in_dim;</span><br><span class="line">    <span class="type">float</span>* out = output + sample_idx * out_dim;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 第一层：Linear + ReLU</span></span><br><span class="line">    <span class="type">float</span> hidden1[<span class="number">128</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; hid1; ++i) &#123;</span><br><span class="line">        <span class="type">float</span> sum = b1[i];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; in_dim; ++j) &#123;</span><br><span class="line">            sum += x[j] * W1[j * hid1 + i]; <span class="comment">// 转置访问权重</span></span><br><span class="line">        &#125;</span><br><span class="line">        hidden1[i] = <span class="built_in">fmaxf</span>(sum, <span class="number">0.0f</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 第二层：Linear + ReLU</span></span><br><span class="line">    <span class="type">float</span> hidden2[<span class="number">64</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; hid2; ++i) &#123;</span><br><span class="line">        <span class="type">float</span> sum = b2[i];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; hid1; ++j) &#123;</span><br><span class="line">            sum += hidden1[j] * W2[j * hid2 + i]; <span class="comment">// 转置访问权重</span></span><br><span class="line">        &#125;</span><br><span class="line">        hidden2[i] = <span class="built_in">fmaxf</span>(sum, <span class="number">0.0f</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 第三层：Linear</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; out_dim; ++i) &#123;</span><br><span class="line">        <span class="type">float</span> sum = b3[i];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; hid2; ++j) &#123;</span><br><span class="line">            sum += hidden2[j] * W3[j * out_dim + i]; <span class="comment">// 转置访问权重</span></span><br><span class="line">        &#125;</span><br><span class="line">        out[i] = sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">fused_forward_cuda</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor W1, torch::Tensor b1,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor W2, torch::Tensor b2,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Tensor W3, torch::Tensor b3</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> batch_size = input.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> in_dim = W1.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="type">int</span> hid1 = W1.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> hid2 = W2.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> out_dim = W3.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    torch::Tensor output = torch::<span class="built_in">zeros</span>(&#123;batch_size, out_dim&#125;, input.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每个block处理多个样本，根据GPU配置调整</span></span><br><span class="line">    <span class="type">int</span> threads = <span class="number">256</span>;</span><br><span class="line">    <span class="type">int</span> blocks = (batch_size + threads - <span class="number">1</span>) / threads;</span><br><span class="line"></span><br><span class="line">    fused_forward&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(</span><br><span class="line">        input.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;(),</span><br><span class="line">        output.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;(),</span><br><span class="line">        W1.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;(),</span><br><span class="line">        b1.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;(),</span><br><span class="line">        W2.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;(),</span><br><span class="line">        b2.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;(),</span><br><span class="line">        W3.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;(),</span><br><span class="line">        b3.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;(),</span><br><span class="line">        batch_size, in_dim, hid1, hid2, out_dim</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;fused_forward&quot;</span>, &amp;fused_forward_cuda, <span class="string">&quot;Fused forward pass (CUDA)&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码用到了<code>libtorch</code>库，不过我们不需要手动安装，只要本地有pytorch库就可以。在绑定python模块的时候，pytorch会自动将其转换。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># setup.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> CUDAExtension, BuildExtension</span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&#x27;fused_op&#x27;</span>,</span><br><span class="line">    ext_modules=[</span><br><span class="line">        CUDAExtension(</span><br><span class="line">            name=<span class="string">&#x27;fused_op&#x27;</span>,</span><br><span class="line">            sources=[<span class="string">&#x27;fused_op.cu&#x27;</span>]  <span class="comment"># 根据GPU架构调整</span></span><br><span class="line">        )</span><br><span class="line">    ],</span><br><span class="line">    cmdclass=&#123;</span><br><span class="line">        <span class="string">&#x27;build_ext&#x27;</span>: BuildExtension</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>运行 <code>python setup.py install</code></p>
<h2 id="测试加速效果"><a href="#测试加速效果" class="headerlink" title="测试加速效果"></a>测试加速效果</h2><p>主要为了验证模型推理耗时和结果一致性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> fused_op  <span class="comment"># 导入CUDA模块</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载原始PyTorch模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DNNModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer1 = torch.nn.Linear(<span class="number">100</span>, <span class="number">128</span>)</span><br><span class="line">        self.relu1 = torch.nn.ReLU()</span><br><span class="line">        self.layer2 = torch.nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.relu2 = torch.nn.ReLU()</span><br><span class="line">        self.layer3 = torch.nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.relu1(self.layer1(x))</span><br><span class="line">        x = self.relu2(self.layer2(x))</span><br><span class="line">        <span class="keyword">return</span> self.layer3(x)</span><br><span class="line"></span><br><span class="line">origin_model = DNNModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载原始模型的网络参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_transposed_weights</span>(<span class="params">filename, original_shape</span>):</span><br><span class="line">    <span class="comment"># 从文件加载并重塑为转置后的形状</span></span><br><span class="line">    transposed_weights = np.fromfile(filename, dtype=np.float32)</span><br><span class="line">    transposed_shape = (original_shape[<span class="number">1</span>], original_shape[<span class="number">0</span>])  <span class="comment"># 交换维度</span></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(</span><br><span class="line">        transposed_weights.reshape(transposed_shape).T  <span class="comment"># 转置回原始形状</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">origin_model.layer1.weight.data = load_transposed_weights(<span class="string">&quot;layer1.weight.bin&quot;</span>, (<span class="number">128</span>, <span class="number">100</span>))</span><br><span class="line">origin_model.layer2.weight.data = load_transposed_weights(<span class="string">&quot;layer2.weight.bin&quot;</span>, (<span class="number">64</span>, <span class="number">128</span>))</span><br><span class="line">origin_model.layer3.weight.data = load_transposed_weights(<span class="string">&quot;layer3.weight.bin&quot;</span>, (<span class="number">10</span>, <span class="number">64</span>))</span><br><span class="line">origin_model.layer1.bias.data = torch.from_numpy(np.fromfile(<span class="string">&quot;layer1.bias.bin&quot;</span>, dtype=np.float32))</span><br><span class="line">origin_model.layer2.bias.data = torch.from_numpy(np.fromfile(<span class="string">&quot;layer2.bias.bin&quot;</span>, dtype=np.float32))</span><br><span class="line">origin_model.layer3.bias.data = torch.from_numpy(np.fromfile(<span class="string">&quot;layer3.bias.bin&quot;</span>, dtype=np.float32))</span><br><span class="line"></span><br><span class="line">origin_model = origin_model.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试函数（返回时间和结果）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">measure_time</span>(<span class="params">func, input_data, repeats=<span class="number">30</span></span>):</span><br><span class="line">    timings = []</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(repeats):</span><br><span class="line">            start = time.time()</span><br><span class="line">            output = func(input_data)</span><br><span class="line">            end = time.time()</span><br><span class="line">            timings.append(end - start)</span><br><span class="line">            outputs.append(output)</span><br><span class="line">    <span class="keyword">return</span> np.mean(timings), outputs[<span class="number">0</span>]  <span class="comment"># 返回平均时间和单次结果</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入数据并固定随机种子</span></span><br><span class="line"><span class="comment"># np.random.seed(42)</span></span><br><span class="line"><span class="comment"># torch.manual_seed(42)</span></span><br><span class="line">input_data = torch.randn(<span class="number">32</span>, <span class="number">100</span>, dtype=torch.float32).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载优化后的模型权重，并将其作为参数传入</span></span><br><span class="line">layer1_weight = origin_model.layer1.weight</span><br><span class="line">layer2_weight = origin_model.layer2.weight</span><br><span class="line">layer3_weight = origin_model.layer3.weight</span><br><span class="line">layer1_bias = origin_model.layer1.bias</span><br><span class="line">layer2_bias = origin_model.layer2.bias</span><br><span class="line">layer3_bias = origin_model.layer3.bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试原始模型</span></span><br><span class="line">origin_time, origin_output = measure_time(<span class="keyword">lambda</span> x: origin_model(x), input_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;原始PyTorch推理时间: <span class="subst">&#123;origin_time * <span class="number">1000</span>:<span class="number">.2</span>f&#125;</span> ms&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试CUDA融合模块</span></span><br><span class="line">optimised_time, optimised_output = measure_time(</span><br><span class="line">    <span class="keyword">lambda</span> x: fused_op.fused_forward(x, layer1_weight, layer1_bias, layer2_weight, layer2_bias, layer3_weight, layer3_bias), input_data</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;CUDA融合推理时间: <span class="subst">&#123;optimised_time * <span class="number">1000</span>:<span class="number">.2</span>f&#125;</span> ms&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果对比</span></span><br><span class="line">native_result = origin_output.cpu().numpy()</span><br><span class="line">cuda_result = optimised_output.cpu().numpy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">abs_diff = np.<span class="built_in">abs</span>(native_result - cuda_result)</span><br><span class="line">max_abs_diff = np.<span class="built_in">max</span>(abs_diff)</span><br><span class="line">rel_diff = np.mean(abs_diff / (np.<span class="built_in">abs</span>(native_result) + <span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最大绝对误差: <span class="subst">&#123;max_abs_diff:<span class="number">.6</span>e&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;平均相对误差: <span class="subst">&#123;rel_diff:<span class="number">.6</span>e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> max_abs_diff &lt; <span class="number">1e-5</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;✅ 结果一致，优化成功！&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;❌ 结果不一致，可能存在错误！&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行 <code>python test.py</code>：</p>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.3goknytxty.webp"  alt="result"></p>
<hr>
<ul>
<li><a class="link"   href="https://github.com/TransformersWsz/cuda_examples/blob/main/op_fuse/README.md" >op_fuse<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>莫比乌斯召回系统介绍</title>
    <url>/2023/09/11/%E8%8E%AB%E6%AF%94%E4%B9%8C%E6%96%AF%E5%8F%AC%E5%9B%9E%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p>当前召回系统只能召回相关性高的广告，但不能保证该广告变现能力强。莫比乌斯做了如下两点创新：</p>
<span id="more"></span>
<ul>
<li>在召回阶段，引入CPM等业务指标作为召回依据</li>
<li>在召回阶段，引入CTR模型，从而召回更多相关性高且变现能力强的广告</li>
</ul>
<p><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.4dgl7vlu3je0.png"  alt="recall &amp; ctr"><br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.2lo7mmos2js0.webp"  alt="model"></p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://zhuanlan.zhihu.com/p/146210155" >百度凤巢新一代广告召回系统——“莫比乌斯”<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="http://research.baidu.com/Public/uploads/5d12eca098d40.pdf" >MOBIUS<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>CTR</tag>
        <tag>召回</tag>
      </tags>
  </entry>
  <entry>
    <title>装饰器模式</title>
    <url>/2017/10/24/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<p>学习一下python装饰器模式的概念与基本使用。</p>
<span id="more"></span>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>由于函数也是对象，而且函数对象可以被赋值给变量。所以，通过变量也能调用该函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">now</span>():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Time is 2017-10-23&quot;</span></span><br><span class="line"></span><br><span class="line">f = now</span><br><span class="line">f()</span><br></pre></td></tr></table></figure>
<p>运行结果输出为: <font color="red">“Time is 2017-10-23”</font></p>
<p>现在，假设我们要增强 <code>now()</code> 函数的功能。比如，在函数调用前后自动打印日志，但又不希望修改 <code>now()</code> 函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”(Decorator)。</p>
<p>本质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator，可以定义如下:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args,**kw</span>):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;call %s():&#x27;</span> % func.__name__</span><br><span class="line">        <span class="keyword">return</span> func(*args,**kw)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@log</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">now</span>():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Time is 2017-10-23&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    now()</span><br></pre></td></tr></table></figure><br>运行结果如下:</p>
<img   src="/2017/10/24/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/1.png"  class="">
<p>观察上面的log，因为它是一个decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助Python的@语法，把decorator置于函数的定义处:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@log</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">now</span>():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Time is 2017-10-23&quot;</span></span><br></pre></td></tr></table></figure><br>调用 <code>now()</code> 函数，不仅会运行 <code>now()</code> 函数本身，还会在 <code>now()</code> 函数前打印一行日志。</p>
<p>把 <code>@log</code> 放到 <code>now()</code> 函数的定义处，相当于执行了语句:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">now = log(now)</span><br></pre></td></tr></table></figure><br>由于 <code>log()</code> 是一个decorator，返回一个函数，所以原来的 <code>now()</code> 函数依然存在，只是现在同名的now变量指向了新的函数，于是调用 <code>now()</code> 将执行新函数，即在 <code>log()</code> 函数中返回的 <code>wrapper()</code> 函数。</p>
<p><code>wrapper()</code> 函数的参数 <code>(*args,**kw)</code>，因此， <code>wrapper()</code> 函数可以接受任意参数的调用。在 <code>wrapper()</code> 函数内，首先打印日志，再紧接着调用原始函数。</p>
<p>如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写起来会更复杂。比如，要自定义log的文本:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kw</span>):</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&#x27;%s %s():&#x27;</span> % (text, func.__name__)</span><br><span class="line">            <span class="keyword">return</span> func(*args, **kw)</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br></pre></td></tr></table></figure><br>这个3层嵌套的decorator用法如下:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@log(<span class="params"><span class="string">&#x27;execute&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">now</span>():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Time is 2017-10-23&quot;</span></span><br></pre></td></tr></table></figure><br>执行结果如下:</p>
<img   src="/2017/10/24/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/2.png"  class="">
<p>和两层嵌套的decorator相比，3层嵌套的效果是这样的:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">now = log(<span class="string">&#x27;execute&#x27;</span>)(now)</span><br></pre></td></tr></table></figure><br>我们来剖析上面的语句，首先执行 <code>log(&#39;execute&#39;)</code> ，返回的是 <code>decorator</code> 函数，再调用返回的函数，参数是 <code>now</code> 函数，返回值最终是 <code>wrapper</code> 函数。</p>
<p>以上两种decorator的定义都没有问题，但还差最后一步。因为我们讲了函数也是对象，它有 <code>__name__</code> 等属性，但你去看经过decorator装饰之后的函数，它们的 <code>__name__</code> 已经从原来的 <code>now</code> 变成了 <code>wrapper</code></p>
<hr>
<h2 id="装饰器的那些坑"><a href="#装饰器的那些坑" class="headerlink" title="装饰器的那些坑"></a>装饰器的那些坑</h2><h3 id="位置错误的代码"><a href="#位置错误的代码" class="headerlink" title="位置错误的代码"></a>位置错误的代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">html_tags</span>(<span class="params">tag_name</span>):</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;begin outer function.&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper_</span>(<span class="params">func</span>):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;begin of inner wrapper function.&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            content = func(*args, **kwargs)</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;&lt;&#123;tag&#125;&gt;&#123;content&#125;&lt;/&#123;tag&#125;&gt;&quot;</span>.<span class="built_in">format</span>(tag=tag_name, content=content)</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&#x27;end of inner wrapper function.&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&#x27;end of outer function&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> wrapper_</span><br><span class="line"></span><br><span class="line"><span class="meta">@html_tags(<span class="params"><span class="string">&#x27;b&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hello</span>(<span class="params">name=<span class="string">&#x27;Toby&#x27;</span></span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello &#123;&#125;!&#x27;</span>.<span class="built_in">format</span>(name)</span><br><span class="line"></span><br><span class="line">hello()</span><br><span class="line">hello()</span><br></pre></td></tr></table></figure>
<p>在装饰器中我在各个可能的位置都加上了print语句，用于记录被调用的情况。你知道他们最后打印出来的顺序吗？如果你心里没底，那么最好不要在装饰器函数之外添加逻辑功能，否则这个装饰器就不受你控制了。以下是输出结果：</p>
<img   src="/2017/10/24/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/3.png"  class="">
<h3 id="错误的函数签名和文档"><a href="#错误的函数签名和文档" class="headerlink" title="错误的函数签名和文档"></a>错误的函数签名和文档</h3><p>装饰器装饰过的函数看上去名字没变，其实已经变了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">logging</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;print log before a function.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;[DEBUG] &#123;&#125;: enter &#123;&#125;()&quot;</span>.<span class="built_in">format</span>(datetime.now(), func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@logging</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">say</span>(<span class="params">something</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;say something&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;say &#123;&#125;!&quot;</span>.<span class="built_in">format</span>(something)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> say.__name__  <span class="comment"># wrapper</span></span><br></pre></td></tr></table></figure>
<p>为什么会这样呢？只要你想想装饰器的语法糖@代替的东西就明白了。@等同于这样的写法。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">say = logging(say)</span><br></pre></td></tr></table></figure><br><code>logging</code> 其实返回的函数名字刚好是 <code>wrapper</code> ，那么上面的这个语句刚好就是把这个结果赋值给 <code>say</code>， <code>say</code> 的 <code>__name__</code> 自然也就是 <code>wrapper</code> 了。不仅仅是 <code>name</code>，其他属性也都是来自 <code>wrapper</code> ，比如 <code>doc</code> ，<code>source</code> 等等。</p>
<p>使用标准库的 <code>functools.wraps</code>，可以基本解决这个问题。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logging</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;print log before a function.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;[DEBUG] &#123;&#125;: enter &#123;&#125;()&quot;</span>.<span class="built_in">format</span>(datetime.now(), func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@logging</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">say</span>(<span class="params">something</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;say something&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;say &#123;&#125;!&quot;</span>.<span class="built_in">format</span>(something)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> say.__name__  <span class="comment"># say</span></span><br><span class="line"><span class="built_in">print</span> say.__doc__ <span class="comment"># say something</span></span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a class="link"   href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017451662295584" >装饰器<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://segmentfault.com/a/1190000007321935" >详解Python的装饰器<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>高阶函数</tag>
      </tags>
  </entry>
  <entry>
    <title>解决wsl2参考的对象类型不支持尝试的操作</title>
    <url>/2022/09/09/%E8%A7%A3%E5%86%B3wsl2%E5%8F%82%E8%80%83%E7%9A%84%E5%AF%B9%E8%B1%A1%E7%B1%BB%E5%9E%8B%E4%B8%8D%E6%94%AF%E6%8C%81%E5%B0%9D%E8%AF%95%E7%9A%84%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p>最近windows的代理软件出现了问题，导致winsock出现问题，连锁反应就是wsl也用不了了。</p>
<span id="more"></span>
<p>解决方法就是从winsock中排除wsl：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">Windows Registry Editor Version <span class="number">5.00</span></span><br><span class="line"> </span><br><span class="line">[<span class="type">HKEY_LOCAL_MACHINE</span>\<span class="type">SYSTEM</span>\<span class="type">CurrentControlSet</span>\<span class="type">Services</span>\<span class="type">WinSock2</span>\<span class="type">Parameters</span>\<span class="type">AppId_Catalog</span>\<span class="number">0408</span><span class="type">F7A3</span>]</span><br><span class="line"><span class="string">&quot;AppFullPath&quot;</span>=<span class="string">&quot;C:\\Windows\\System32\\wsl.exe&quot;</span></span><br><span class="line"><span class="string">&quot;PermittedLspCategories&quot;</span>=dword:<span class="number">80000000</span></span><br></pre></td></tr></table></figure>
<p>新建文本文档，复制上述代码，后缀修改为reg并双击运行，问题解决。</p>
<hr>
<h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><ul>
<li><a class="link"   href="https://blog.csdn.net/marin1993/article/details/119841299" >一劳永逸，wsl2出现“参考的对象类型不支持尝试的操作”的解决办法<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>WSL</tag>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title>记一次JavaWeb的开发经历</title>
    <url>/2019/12/17/%E8%AE%B0%E4%B8%80%E6%AC%A1JavaWeb%E7%9A%84%E5%BC%80%E5%8F%91%E7%BB%8F%E5%8E%86/</url>
    <content><![CDATA[<p>这两周断断续续地帮老东家做了一个很low的网站，总体来说感触还挺多的。</p>
<span id="more"></span>
<h2 id="回忆"><a href="#回忆" class="headerlink" title="回忆"></a>回忆</h2><p>还记得大三的时候跟着<a class="link"   href="https://blog.trickotreat.cn/" >学长<i class="fas fa-external-link-alt"></i></a>后面学SSM（Spring + SpringMVC + MyBatis），光环境搭建就花了一周的时间。<code>web.xml</code> 的网站配置，Mybatis的集成配置，Bean对象的生成配置，SQLMapper的CURD编写，基本上全在死磕xml，头都搞炸了。网上介绍Spring时说是JavaWeb的轻量级框架，这还轻量？好吧，我可能没接触过Struts2\JSF等重量级框架吧。在学了一段时间后，没学出头绪就放弃了。</p>
<p>到了大三下，经典boy推荐了springboot，当时尝了下鲜，真的惊呆了，0 xml配置。直接使用IDEA提供的脚手架就能运行了，再稍微配置下数据库的连接信息就能开发web了，开发体验好到爆炸。但后来因为没有实际需求，也没有用它来搞事情。</p>
<p>到了大四，去了一家外企实习（上述所说的老东家）。10月份入职，正赶上一个创新项目结点，要赶着做一款蓝牙通信的iOS APP。谈到做这个app真的是一把辛酸泪（此处省去1万字）。老外要查看月活量，因此需要一个website来进行可视化展示。正好就用上了springboot，前端用的echarts库，花了一下午做好了，成就感满满。看到特效这么炫酷的图表，老外连说cool😎。</p>
<p>后来部门就顺着这个思路给员工做一个工时可视化的网站。前端仍然是传统的那一套Bootstrap+Jquery+ECharts，说到这三个（尤其前两个），真的是太羞愧，互联网行业基本上没人用这些了。后端采用了流行的基于js的express框架。总体上来说，上手快，爽是爽，但基于事件驱动的编程仍给人一种无规范、无管理的感觉。</p>
<h2 id="现在"><a href="#现在" class="headerlink" title="现在"></a>现在</h2><p>如今读研了，学习重点偏研究性质，工程开发搁在了一边。由于有之前的样板代码，这两周做的这个网站就照搬了，但在细节地方提升了不少。主要有如下几点：</p>
<ul>
<li>网络异常判断：老东家的wifi访问外部网站特别慢，以至于发送一次请求会导致圈圈转半天。这里设置了时间限制，若超时则取消请求。</li>
<li>限流：为了防止用户手速过快，操作按钮连点多次以至发送多次网络请求，这里采用的方法就是按钮点击后将其设置为不可点击状态，直至后端返回数据。</li>
<li>404和500处理：两个页面就直接显示<code>404 Not Found</code> 和 <code>500 Internal Error</code> 。为了将这两端文字在网页正中央水平垂直居中，花了不少时间，请教了其他人，真是羞耻。</li>
<li>在线状态的判定：如果用户没有登录，则重定向至登录界面。</li>
<li>Docker打包：win10上开发使用的是jdk13，目前docker仓库里还没有openjdk13，因此就不放到docker里运行了。至于mysql数据库，给大家一个忠告，千万不要在宿主服务器上安装mysql！！！不管是yum源安装，还是手动安装。一旦mysql服务宕掉，那么你很有可能无法重启mysql服务，你会遇到各种各样的报错。虽说网上的解决方案有很多，但很少能帮到你的，反正我是不知道那些博主是怎么解决掉的。因此将mysql放到docker里运行是最安全，服务的关闭和启动都非常地容易。</li>
<li>数据库的定时备份：由于本人只有一台服务器，搞不了网上那么高大上的mysql集群。对于数据的可用性，只能定时备份了。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>不管做的什么项目，成就感都不是很高，甚至挫败感很强。主要有如下几点原因：</p>
<h4 id="前端"><a href="#前端" class="headerlink" title="前端"></a>前端</h4><p>我前端采用的是low得不能再low的技术栈。jquery写起来虽然很爽，但是业务逻辑一多，一个页面洋洋洒洒写下来起码几千行代码，又臭又长。第一感觉就是可读性和可维护性极差。</p>
<p>暑假用过<code>vue</code>来做前后端分离，当时的感觉就是前端不仅仅只是单纯的页面了，而是一个系统的工程。一个小小的按钮都是封装好的组件。视图也是直接与数据绑定的，当后端数据来了，我们可以直接修改数据就能更新视图了，再也不需要用jquery去手动操作dom了，确实很爽。另外，<code>vue</code>脚手架生成的目录结构就很工程化，便于维护。但工程化了，要学的东西就变多了，比如<code>vuex</code>，<code>webpack</code>，<code>babel</code>，<code>ssr</code>，<code>sass</code>等一系列工具链，怪不得说学前端太累了。</p>
<h4 id="后端"><a href="#后端" class="headerlink" title="后端"></a>后端</h4><p>后端的业务逻辑无非就是增删查改，没什么太难的。而且像mybatis这样的orm框架，我没用到它多少特性，全部是手写sql，这样简单直观，但维护性上不强。</p>
<hr>
<font color="red">总的来说，我所用到的东西全是用的别人的，全是套的框架。我的水平仅局限于能用就行，完全没有深入理解过这些框架，更别提造轮子了。</font>

<font color="red">我是一个活在框架下的码农，想提升自身竞争力，给自己增值，还有很长的路要走。。。</font>

<h4 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h4><p>到现在我还会时不时地想起老东家，虽说前一个月很苦逼，但剩下的时间真的很舒服，你有大量的时间去学想学的东西，同事之间很和谐，公司人文关怀很到位，年轻漂亮妹子很多。突然想起了我那一段“伤心”的往事。不说了，这公司就是 <font color="green"><strong>BOSCH</strong></font> 。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>程序人生</tag>
      </tags>
  </entry>
  <entry>
    <title>负采样后的CTR预估矫正</title>
    <url>/2025/01/06/%E8%B4%9F%E9%87%87%E6%A0%B7%E5%90%8E%E7%9A%84CTR%E9%A2%84%E4%BC%B0%E7%9F%AB%E6%AD%A3/</url>
    <content><![CDATA[<p>在搜广推场景中，正负样本不平衡是个普遍现象。通常做法是对负样本进行降采样，但采样后训练的模型预估概率会比实际概率高估。</p>
<span id="more"></span>
<p>举例来说，线上真实样本的CTR是0.001，即正负样本比为1:1000。现对负样本降采样$w=0.01$，即采样后正负样本比为1:10，那么训练后的模型预估CTR为0.1，出现高估的情况。</p>
<p>预估率矫正推导如下：</p>
<ul>
<li>$N_{pos}$ 为线上正样本个数</li>
<li>$N_{neg}$ 为线上负样本个数</li>
<li>$N_{neg_down}$ 为降采样后的负样本个数</li>
<li>$w$ 为降采样概率</li>
<li>$q$ 为线上CTR</li>
<li>$p$ 为模型预估CTR</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
    N_{neg\_down} &= N_{neg} * w \\
    p &= \frac{N_{pos}}{N_{neg\_down} + N_{pos}} \\
    q &= \frac{N_{pos}}{N_{neg}+N_{pos}} \\
    \frac{\frac{1}{p}-1}{w} &= \frac{1}{q} - 1 \\
    q &= \frac{p}{p + \frac{1-p}{w}}
\end{aligned}</script><hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://quinonero.net/Publications/predicting-clicks-facebook.pdf" >Practical Lessons from Predicting Clicks on Ads at Facebook<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://blog.csdn.net/zc02051126/article/details/54379244?spm=1001.2014.3001.5506" >CTR模型中的频率矫正过程<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>Calibration</tag>
      </tags>
  </entry>
  <entry>
    <title>赛睿鼠标设置宏</title>
    <url>/2025/04/21/%E8%B5%9B%E7%9D%BF%E9%BC%A0%E6%A0%87%E8%AE%BE%E7%BD%AE%E5%AE%8F/</url>
    <content><![CDATA[<p>鼠标宏是一种​​自动化脚本​​，用于记录并重复执行一系列鼠标操作（如点击、移动、滚轮滚动等），以提高操作效率或实现复杂操作。它通常用于​​游戏、办公自动化、设计​​等领域。记录一下赛睿鼠标的宏设置。</p>
<span id="more"></span>
<ol>
<li>点击设备进入设置页<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.1lc0pf6q22.webp"  alt="image.1lc0pf6q22.webp"><br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.92qbvwlid3.webp"  alt="image.92qbvwlid3.webp"></li>
<li>选择要绑定宏命令的按键，这里以<code>B3</code>为例<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.6m43gzhh1r.webp"  alt="image.6m43gzhh1r.webp"></li>
<li>点击打开编辑器，然后点击启动，就开始录制按键<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.4xuqjswt0a.webp"  alt="image.4xuqjswt0a.webp"><br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.5xatwz1o8v.webp"  alt="image.5xatwz1o8v.webp"></li>
<li>选择按键之间无延迟，即可快速输入。<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.7i0kwfzq78.webp"  alt="image.7i0kwfzq78.webp"></li>
<li>保存并启动该宏命令。<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.2yyjtgvmd7.webp"  alt="image.2yyjtgvmd7.webp"><br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.5c16aoe8fu.webp"  alt="image.5c16aoe8fu.webp"></li>
<li>点击<code>B3</code>键可以看到自动化输入<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.3d4zkcf8e6.webp"  alt="image.3d4zkcf8e6.webp"></li>
<li>如果想关闭宏命令，则重新选择默认即可<br><img   src="https://github.com/TransformersWsz/picx-images-hosting/raw/master/image.13lz0uq2b7.webp"  alt="image.13lz0uq2b7.webp"></li>
</ol>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>SteelSeries</tag>
        <tag>Macro</tag>
      </tags>
  </entry>
  <entry>
    <title>运筹求解的最优点选择</title>
    <url>/2025/02/20/%E8%BF%90%E7%AD%B9%E6%B1%82%E8%A7%A3%E7%9A%84%E6%9C%80%E4%BC%98%E7%82%B9%E9%80%89%E6%8B%A9/</url>
    <content><![CDATA[<p><a class="link"   href="https://transformerswsz.github.io/2025/01/15/%E7%BA%BF%E4%B8%8A%E8%BF%90%E7%AD%B9%E4%BC%98%E5%8C%96%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" >上一节<i class="fas fa-external-link-alt"></i></a>讲述了线上运筹发放红包的流程，但在实践中发现发券分布异常极端，倾向于发在两端，即要么最小面额（占绝大多数），要么最大面额。如果长期按照这种分布发放，将会极大影响用户核销体验以及平台订单的持久增长。</p>
<p>上述问题迫使我们思考，除了满足预算约束外，我们的量价模型还应该具备哪些能力？</p>
<span id="more"></span>
<p><a class="link"   href="https://dl.acm.org/doi/pdf/10.1145/3580305.3599764" >美团的论文<i class="fas fa-external-link-alt"></i></a>给出了答案：量价模型预估的核销率需满足单调递增和边际递减。</p>
<p>以下图为例进行证明：</p>
<p><img   src="https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.9gwp90k3ci.webp"  alt="example"></p>
<h2 id="单调递增性"><a href="#单调递增性" class="headerlink" title="单调递增性"></a>单调递增性</h2><p>利用反证法，假设最优发放点$(C_d, P_d)$是单调减的。</p>
<p>根据单调减可得：</p>
<script type="math/tex; mode=display">
\frac{P_d - P_c}{C_d - C_c} < 0</script><p>根据最优发放可得：</p>
<script type="math/tex; mode=display">
P_d - \lambda C_d > P_c - \lambda C_c \\
\frac{P_d - P_c}{C_d - C_c} > \lambda > 0</script><p>两者矛盾，所以最优发放点在单调增的曲线上。</p>
<h2 id="边际递减"><a href="#边际递减" class="headerlink" title="边际递减"></a>边际递减</h2><p>利用反证法，假设最优发放点$(C_d, P_d)$是边际递增的。</p>
<p>根据边际增可得：</p>
<script type="math/tex; mode=display">
\frac{P_b - P_a}{C_b - C_a} < \frac{P_c - P_b}{C_c - C_b}</script><p>根据最优发放可得：</p>
<script type="math/tex; mode=display">
P_b - \lambda C_b > P_a - \lambda C_a \\
P_b - \lambda C_b > P_c - \lambda C_c \\
\frac{P_b - P_a}{C_b - C_a} > \lambda \\
\frac{P_c - P_b}{C_c - C_b} < \lambda \\
\frac{P_b - P_a}{C_b - C_a} > \frac{P_c - P_b}{C_c - C_b}</script><p>两者矛盾，所以最优发放点在边际递减的曲线上。</p>
<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a class="link"   href="https://dl.acm.org/doi/pdf/10.1145/3580305.3599764" >A Multi-stage Framework for Online Bonus Allocation Based on Constrained User Intent Detection<i class="fas fa-external-link-alt"></i></a></li>
</ul>
]]></content>
      <categories>
        <category>Marketing</category>
      </categories>
      <tags>
        <tag>Monotonical</tag>
        <tag>Convex Hull</tag>
      </tags>
  </entry>
  <entry>
    <title>馈电油耗</title>
    <url>/2025/08/31/%E9%A6%88%E7%94%B5%E6%B2%B9%E8%80%97/</url>
    <content><![CDATA[<p><strong>馈电油耗</strong> 一般出现在 <strong>插电混合动力汽车（PHEV）</strong> 或 <strong>混合动力汽车（HEV）</strong> 的指标描述里。</p>
<span id="more"></span>
<h3 id="📌-定义"><a href="#📌-定义" class="headerlink" title="📌 定义"></a>📌 定义</h3><p><strong>馈电油耗</strong>：指在车辆 <strong>电池电量消耗殆尽（进入电量维持模式）</strong> 后，汽车仅依靠 <strong>发动机+能量回收（刹车发电）</strong> 工作时的百公里油耗。</p>
<ul>
<li>插电混动在电量充足时，主要靠电驱动，油耗很低甚至为零。</li>
<li>但当电池电量降到设定的 <strong>最低SOC（荷电状态）</strong> 后，车辆必须依赖发动机驱动，同时少量用电机辅助。</li>
<li>这时的油耗，就是 <strong>馈电油耗</strong>。</li>
</ul>
<h3 id="📌-为什么要有这个指标？"><a href="#📌-为什么要有这个指标？" class="headerlink" title="📌 为什么要有这个指标？"></a>📌 为什么要有这个指标？</h3><ol>
<li><strong>真实使用场景</strong>：很多车主不一定天天充电，如果经常“油养车”，实际体验就接近馈电状态。</li>
<li><strong>能效评估</strong>：国家对插混车的节能效果考核里，馈电油耗是重要指标。</li>
<li><strong>与传统燃油车对比</strong>：馈电油耗越低，意味着在无充电条件下也更省油。</li>
</ol>
<h3 id="📌-举个例子"><a href="#📌-举个例子" class="headerlink" title="📌 举个例子"></a>📌 举个例子</h3><p>假设某款插混车：</p>
<ul>
<li><strong>纯电续航</strong>：80 km</li>
<li><strong>馈电油耗</strong>：5.0 L/100km</li>
</ul>
<p>那它的表现就是：</p>
<ul>
<li>如果每天通勤 &lt;80 km 且能充电，几乎不用油。</li>
<li>如果跑长途，电池用完进入馈电模式，油耗大约就是 5L/100km，跟一辆省油的燃油车差不多。</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>馈电油耗 = 插混车 <strong>没电之后</strong> 当作燃油车开时的百公里油耗。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>馈电油耗</tag>
      </tags>
  </entry>
</search>
